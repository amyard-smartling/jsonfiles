{
    "url": "http://mongodb.com/docs/datalake",
    "includeInGlobalSearch": true,
    "documents": [
        {
            "slug": "limitations",
            "title": "Data Lake Limitations",
            "headings": [],
            "paragraphs": "Atlas Data Lake doesn't support  M0 ,  M2 , or  M5  clusters. It\nsupports  M10  or higher clusters only. Atlas Data Lake doesn't support sharded clusters. Atlas Data Lake provides optimized storage in the following  AWS (Amazon Web Services)  regions\nonly: Data Lake  Regions AWS (Amazon Web Services)  Regions Virginia, USA us-east-1 Oregon, USA us-west-2 Sao Paulo, Brazil sa-east-1 Ireland eu-west-1 London, England eu-west-2 Frankfurt, Germany eu-central-1 Mumbai, India ap-south-1 Singapore ap-southeast-1 Sydney, Australia ap-southeast-2 You can create up to 25 Atlas Data Lake pipelines per  Atlas  project.",
            "code": [],
            "preview": null,
            "tags": "Data Lake",
            "facets": null
        },
        {
            "slug": "get-started",
            "title": "Get Started",
            "headings": [
                "Overview"
            ],
            "paragraphs": "Atlas Data Lake is MongoDB's solution for storing and querying historical data\nfrom your  Atlas  cluster in a performant analytic storage service. The following pages guide you through the process of creating and\nconnecting to a sample Data Lake in  Atlas  and running MongoDB Query\nLanguage (MQL) operations against the data in your Data Lake. Step 1:   Create an Atlas Data Lake Pipeline . Step 2:   Set Up a Federated Database Instance for Your Dataset . Step 3:   Connect to Your Federated Database Instance . Step 4:   Run Queries Against Your Data Lake Dataset .",
            "code": [],
            "preview": "Atlas Data Lake is MongoDB's solution for storing and querying historical data\nfrom your Atlas cluster in a performant analytic storage service.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "manage-adl-dataset-pipeline",
            "title": "Manage Atlas Data Lake Pipeline",
            "headings": [],
            "paragraphs": "You can perform the following actions on your Data Lake pipeline: View and edit your Data Lake pipelines. Manually trigger ingestion of data from your snapshot. Pause and resume ingestion of snapshot data from the  Atlas \ncluster to Atlas Data Lake datasets. To learn more, see\n Pause Data Ingestion for Your Data Lake Pipeline . Delete your Data Lake pipeline at any time. To learn more, see\n Delete Atlas Data Lake Pipeline .",
            "code": [],
            "preview": "You can perform the following actions on your Data Lake pipeline:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "",
            "title": "Atlas Data Lake",
            "headings": [
                "About Atlas Data Lake",
                "Prerequisites",
                "Supported Types of Data Source",
                "Data Storage Format and Query Support",
                "Sample Uses",
                "Atlas Data Lake Regions",
                "Billing",
                "Extraction Costs",
                "Storage Costs"
            ],
            "paragraphs": "MongoDB Atlas Data Lake is now an analytic-optimized object storage service for\nextracted data. Atlas Data Lake provides an analytic storage service optimized\nfor flat or nested data with low latency query performance. Atlas Data Lake requires an  M10  or higher backup-enabled  Atlas  cluster\nwith cloud backup jobs running on a specified cadence. To learn more\nabout cloud backups, see  Back Up Your Database Deployment . Atlas Data Lake supports collection snapshots from  Atlas  clusters as a data\nsource for extracted data. Atlas Data Lake automatically ingests data from the\nsnapshots, and partitions and stores data in an analytics-optimized\nformat. It doesn't support creating pipelines for  Views . Atlas Data Lake stores data in an analytic oriented format that is based on open\nsource standards with support for polymorphic data. Data is fully\nmanaged, partition level indexed, and balanced as data grows. Atlas Data Lake\noptimizes data extraction for analytic type queries. When Atlas Data Lake\nextracts new data, it re-balances existing files to ensure consistent\nperformance and minimize data scan. Atlas Data Lake stores data in a format that best fits its structure to allow\nfor fast point-queries and aggregate queries. For point-queries,\nAtlas Data Lake's storage format improves performance by finding partitions\nfaster. Aggregate type queries only scan the column required to provide\nresults. Additionally, Atlas Data Lake partition indexes improve performance for\naggregate queries by returning results directly from the partition\nindex without needing to scan underlying files. You can use Atlas Data Lake to: Isolate analytical workloads from your operational cluster. Provide a consistent view of cluster data from a snapshot for long\nrunning aggregations using  $out . Query and compare across versions of your cluster data at different\npoints in time. Atlas Data Lake provides optimized storage in the following  AWS (Amazon Web Services)  regions: Atlas Data Lake automatically selects the region closest to your  Atlas \ncluster for storing ingested data. Data Lake  Regions AWS (Amazon Web Services)  Regions Virginia, USA us-east-1 Oregon, USA us-west-2 Sao Paulo, Brazil sa-east-1 Ireland eu-west-1 London, England eu-west-2 Frankfurt, Germany eu-central-1 Mumbai, India ap-south-1 Singapore ap-southeast-1 Sydney, Australia ap-southeast-2 You incur Atlas Data Lake charges per GB per month based on the  AWS (Amazon Web Services)  region\nwhere the ingested data is stored. You incur Atlas Data Lake costs for the\nfollowing items: Ingestion of data from your data source Storage on the cloud object storage Atlas Data Lake charges you for the resources utilized to extract, upload, and\ntransfer data. Atlas Data Lake charges for the  snapshot export  operations is based on the following: Cost per GB for snapshot extraction Cost per hour on the  AWS (Amazon Web Services)  server for snapshot export download Cost per GB per hour for snapshot export restore storage Cost per  IOPS (Input/Output Operations per Second)  per hour for snapshot export storage  IOPS (Input/Output Operations per Second) Atlas Data Lake charges for storing and accessing stored data is based on the\nfollowing: To learn more, see the  Atlas pricing page . Cost per GB per day Cost for every one thousand storage access requests when querying\nData Lake datasets using Atlas Data Federation. Each access request corresponds to a\npartition of data from a Data Lake dataset that Atlas Data Federation fetches to\nprocess for a query. You can now set limits on the amount of data that Atlas Data Federation processes\nfor your queries to control costs. To learn more, see\n Manage Atlas Data Federation Query Limits .",
            "code": [],
            "preview": "MongoDB Atlas Data Lake is now an analytic-optimized object storage service for\nextracted data. Atlas Data Lake provides an analytic storage service optimized\nfor flat or nested data with low latency query performance.",
            "tags": "|data-lake|",
            "facets": null
        },
        {
            "slug": "administration/config-dataset-retention",
            "title": "Configure Dataset Retention Policy",
            "headings": [
                "Procedure",
                "Log in to MongoDB Atlas.",
                "Select Data Lake from the Deployment section in the left-hand navigation.",
                "Click the Edit  icon in the Actions column for the pipeline that you want to modify.",
                "Configure a Dataset Retention Policy.",
                "Apply the changes."
            ],
            "paragraphs": "A dataset retention policy can help manage your Data Lake storage by\nautomatically deleting old datasets after a duration you specify. You\ncan configure a retention polity for your Data Lake dataset through the\n Atlas   UI (User Interface) . To configure a dataset retention policy for a pipeline, you must do the\nfollowing: Toggle  Dataset Retention Policy  to  ON . Enter a time value, in days, weeks, or months. Click  Continue . Click  Review Changes  to review the changes. Click  Apply Changes  for the changes to take effect. Changes to your pipeline's dataset retention policy take effect\nimmediately. If you lower the retention duration for a pipeline with\nexisting datasets, then datasets that are beyond the new duration\nexpire immediately.",
            "code": [],
            "preview": "A dataset retention policy can help manage your Data Lake storage by\nautomatically deleting old datasets after a duration you specify. You\ncan configure a retention polity for your Data Lake dataset through the\nAtlas UI (User Interface).",
            "tags": null,
            "facets": null
        },
        {
            "slug": "administration/ingest-data-on-demand",
            "title": "Trigger Data Ingestion On Demand",
            "headings": [
                "Procedure",
                "Watch for a Pipeline Run to Complete",
                "Log in to MongoDB Atlas.",
                "Select Data Lake under Deployment on the left-hand navigation.",
                "Click the vertical ellipsis () for the Data Lake for which you configured On Demand ingestion and select Trigger an On Demand Pipeline Run.",
                "Select the snapshot, from which to ingest data, from the dropdown.",
                "Click Confirm."
            ],
            "paragraphs": "You can manually trigger an ingestion of snapshot data from the\n Atlas  cluster to Atlas Data Lake datasets if you configured  On\nDemand  extraction in your Data Lake pipeline. You can trigger data\ningestion from the  Atlas   UI (User Interface) , the Data Lake Pipelines  API (Application Programming Interface) , or the\nAtlas CLI. To trigger the specified data lake pipeline for your project using the\nAtlas CLI, run the following command: To learn more about the command syntax and parameters, see the\nAtlas CLI documentation for  atlas dataLakePipelines trigger . Install the Atlas CLI Connect to the Atlas CLI To watch for the specified data lake pipeline run to complete using the\nAtlas CLI, run the following command: To learn more about the command syntax and parameters, see the\nAtlas CLI documentation for  atlas dataLakePipelines runs watch . Install the Atlas CLI Connect to the Atlas CLI To trigger data ingestion through the  API (Application Programming Interface) , send a  POST  request\nto the  Data Lake   trigger \nendpoint with the name of the pipeline for which you want to trigger\ndata ingestion. To learn more about the  API (Application Programming Interface)  syntax and parameters for\nthe  trigger  endpoint, see  Trigger On-Demand Snapshot\nIngestion . You can send a  GET  request to the  Data Lake   availableSnapshots  endpoint to retrieve\nthe list of backup snapshots that you can use to trigger an\non-demand pipeline run. To learn more about the  API (Application Programming Interface)  syntax and\noptions for the  availableSnapshots  endpoint, see\n Return Available Backup Snapshots for One Data Lake\nPipeline . The dropdown shows a list of all the snapshots on your  Atlas \ncluster. However, you can select only the snapshots from which\nData Lake hasn't yet ingested data; the grayed-out snapshots are\nsnapshots from which your Data Lake has already ingested data. You can also send a  GET  request to the  Data Lake   availableSnapshots  endpoint to\nretrieve the list of backup snapshots that you can use to trigger\nan on-demand pipeline run. To learn more about the  API (Application Programming Interface)  syntax\nand options for the  availableSnapshots  endpoint, see\n Return Available Backup Snapshots for One Data\nLake Pipeline . Atlas  displays a blue banner at the top of the page that\nshows the data ingestion status.",
            "code": [
                {
                    "lang": "sh",
                    "value": "\natlas dataLakePipelines trigger <pipelineName> [options]\n"
                },
                {
                    "lang": "sh",
                    "value": "\natlas dataLakePipelines runs watch <pipelineName> [options]\n"
                }
            ],
            "preview": "You can manually trigger an ingestion of snapshot data from the\nAtlas cluster to Atlas Data Lake datasets if you configured On\nDemand extraction in your Data Lake pipeline. You can trigger data\ningestion from the Atlas UI (User Interface), the Data Lake Pipelines API (Application Programming Interface), or the\nAtlas CLI.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "administration/pause-resume-data-extraction",
            "title": "Pause Data Ingestion for Your Data Lake Pipeline",
            "headings": [
                "Pause Data Ingestion for Your Data Lake Pipeline",
                "Navigate to Atlas Data Lake in the Atlas UI.",
                "Click the pause button for the pipeline that you wish to pause.",
                "Click Confirm in the Pause Ingestion confirmation window.",
                "Resume Data Ingestion for Your Data Lake Pipeline",
                "Navigate to Atlas Data Lake in the Atlas UI.",
                "Click  in the Actions column for the pipeline that you wish to resume and select Resume Ingestion.",
                "Click Confirm in the Resume Ingestion confirmation window."
            ],
            "paragraphs": "You can pause and resume ingestion of snapshot data from the  Atlas \ncluster to Atlas Data Lake datasets. You can't pause on-demand ingestion of\nsnapshot data. When you pause your Data Lake pipeline,  Atlas  doesn't ingest new\ndatasets. You can continue to query previous snapshots from which data\nhas been ingested. To pause the specified data lake pipeline for your project using the\nAtlas CLI, run the following command: To learn more about the command syntax and parameters, see the\nAtlas CLI documentation for  atlas accessLists create . Install the Atlas CLI Connect to the Atlas CLI To pause a pipeline through the  API (Application Programming Interface) , send a  POST  request to the\n Data Lake   pause  endpoint\nwith the name of the pipeline for which you want to pause the ingestion\nschedule. To learn more about the  API (Application Programming Interface)  syntax and parameters for the\n pause  endpoint, see  Pause One Data Lake Pipeline . To pause a pipeline from the  Atlas   UI (User Interface) : To navigate to the Atlas Data Lake page: Log in to  MongoDB Atlas . Select  Data Lake  under  Deployment \non the left-hand navigation. When you pause your Data Lake pipeline, the  Last Updated \ncolumn for the pipeline in the  Atlas  UI shows the status\nfor the pipeline as  Paused . When you resume data ingestion for a paused Atlas Data Lake pipeline,\n Atlas  begins to take snapshots, which are then ingested in to your\nData Lake datasets. To start the specified data lake pipeline for your project using the\nAtlas CLI, run the following command: To learn more about the command syntax and parameters, see the\nAtlas CLI documentation for  atlas dataLakePipelines start . Install the Atlas CLI Connect to the Atlas CLI To resume data ingestion for a pipeline from the  API (Application Programming Interface) , send a  POST \nrequest to the   Data Lake \n resume  endpoint with the name of the pipeline for which you want to\nresume the data ingestion. To learn more about the  API (Application Programming Interface)  syntax and\nparameters for the  resume  endpoint, see  Resume One\nData Lake Pipeline . To resume data ingestion from the  Atlas   UI (User Interface) : To navigate to the Atlas Data Lake page: Log in to  MongoDB Atlas . Select  Data Lake  under  Deployment \non the left-hand navigation. When you resume data ingestion for a paused Atlas Data Lake pipeline,\nthe  Last Run Time  column for the pipeline in the\n Atlas  UI shows the date and time when data ingestion for\nthe pipeline resumed.",
            "code": [
                {
                    "lang": "sh",
                    "value": "\natlas accessLists create [entry] [options]\n"
                },
                {
                    "lang": "sh",
                    "value": "\natlas dataLakePipelines start <pipelineName> [options]\n"
                }
            ],
            "preview": "You can pause and resume ingestion of snapshot data from the Atlas\ncluster to Atlas Data Lake datasets. You can't pause on-demand ingestion of\nsnapshot data.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "administration/edit-data-pipeline",
            "title": "Edit an Atlas Data Lake Pipeline",
            "headings": [
                "Procedure",
                "Log in to MongoDB Atlas.",
                "Select Data Lake under Deployment on the left-hand navigation.",
                "Click  in the Actions column for the pipeline that you wish to modify.",
                "(Optional) Configure a Dataset Retention Policy.",
                "(Optional) Make changes to your data extraction schedule.",
                "(Optional) Make changes to your data storage region.",
                "Click Continue.",
                "(Optional) Make changes to the fields excluded from your Data Lake datasets.",
                "Click Review Changes to review the changes to your pipeline.",
                "Click Apply Changes for the changes to take effect."
            ],
            "paragraphs": "You can make changes to your Data Lake pipelines through the  Atlas \n UI (User Interface) , Data Lake Pipelines  API (Application Programming Interface) , and the Atlas CLI, including: Edit the data extraction schedule Edit the dataset retention policy Edit the data storage region Change the fields to exclude from your Data Lake datasets To modify the details of the specified data lake pipeline for your project using the\nAtlas CLI, run the following command: To learn more about the command syntax and parameters, see the\nAtlas CLI documentation for  atlas dataLakePipelines update . Install the Atlas CLI Connect to the Atlas CLI To edit a pipeline through the  API (Application Programming Interface) , send a  PATCH  request to the\n Data Lake   pipelines  endpoint\nwith the name of the pipeline that you want to edit. To learn more\nabout the  pipelines  endpoint syntax and parameters for updating a\npipeline, see  Update One Data Lake Pipeline . You can send a  GET  request to the\n availableSchedules  endpoint to retrieve the\nlist of backup schedule policy items that you can use to change the\nschedule of your Data Lake pipelines of type  PERIODIC_DPS . A dataset retention policy can help manage your Data Lake\nstorage by automatically deleting old datasets after a\nduration you specify. To configure a dataset retention policy for a pipeline,\ntoggle  Dataset Retention Policy  to  ON  and\nenter a time value, in days, weeks, or months. Changes to your pipeline's dataset retention policy take effect\nimmediately. If you lower the retention duration for a pipeline with\nexisting datasets, then datasets that are beyond the new duration\nexpire immediately. Before making changes to your  Basic Schedule , ensure\nthat your desired data extraction frequency is similar to your\ncurrent backup schedule. For example, if you wish to switch to\n Daily , you must have a  Daily  backup schedule configured\nin your policy. Or, if you want to switch to a schedule of once a\nweek, you must have a  Weekly  backup schedule configured in\nyour policy. To learn more, see  Backup Scheduling .\nYou can also send a  GET  request to the Data Lake\n availableSchedules  endpoint to retrieve the\nlist of backup schedule policy items that you can use to change\nthe schedule of your Data Lake pipeline. Atlas Data Lake provides optimized storage in the following  AWS (Amazon Web Services)  regions: Data Lake  Regions AWS (Amazon Web Services)  Regions Virginia, USA us-east-1 Oregon, USA us-west-2 Sao Paulo, Brazil sa-east-1 Ireland eu-west-1 London, England eu-west-2 Frankfurt, Germany eu-central-1 Mumbai, India ap-south-1 Singapore ap-southeast-1 Sydney, Australia ap-southeast-2 Click  Add Field  and specify  Field Name \nto add fields to the excluded fields list. Click  Delete All  to remove all the fields from the\nexcluded fields list. Click   next to a field to remove that\nfield from the excluded fields list.",
            "code": [
                {
                    "lang": "sh",
                    "value": "\natlas dataLakePipelines update <pipelineName> [options]\n"
                }
            ],
            "preview": "You can make changes to your Data Lake pipelines through the Atlas\nUI (User Interface), Data Lake Pipelines API (Application Programming Interface), and the Atlas CLI, including:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "administration/view-datalake-pipelines",
            "title": "View Atlas Data Lake Pipelines",
            "headings": [
                "Procedure",
                "View Details of an Atlas Data Lake Pipeline",
                "View All Available Schedules for an Atlas Data Lake Pipeline",
                "View All Available Backup Snapshots for an Atlas Data Lake Pipeline",
                "View Atlas Data Lake Pipeline Runs",
                "View Details of an Atlas Data Lake Pipeline Run",
                "View Atlas Data Lake Pipeline Runs",
                "Log in to MongoDB Atlas.",
                "Select Data Lake under Deployment on the left-hand navigation."
            ],
            "paragraphs": "You can view all of your Data Lake pipelines and view the details of a specified\nData Lake Pipeline in your project through the  Atlas   UI (User Interface) , Data Lake Pipelines  API (Application Programming Interface) ,\nand the Atlas CLI. You can also retrieve all of your completed Data Lake pipeline\ndata ingestion jobs from the  API (Application Programming Interface)  and the Atlas CLI. To return all data lake pipelines for your project using the\nAtlas CLI, run the following command: To learn more about the command syntax and parameters, see the\nAtlas CLI documentation for  atlas dataLakePipelines list . Install the Atlas CLI Connect to the Atlas CLI To return the details for the specified data lake pipeline for your project using the\nAtlas CLI, run the following command: To learn more about the command syntax and parameters, see the\nAtlas CLI documentation for  atlas dataLakePipelines describe . Install the Atlas CLI Connect to the Atlas CLI To return all available schedules for the specified data lake pipeline using the\nAtlas CLI, run the following command: To learn more about the command syntax and parameters, see the\nAtlas CLI documentation for  atlas dataLakePipelines availableSchedules list . Install the Atlas CLI Connect to the Atlas CLI To return all available backup snapshots for the specified data lake pipeline using the\nAtlas CLI, run the following command: To learn more about the command syntax and parameters, see the\nAtlas CLI documentation for  atlas dataLakePipelines availableSnapshots list . Install the Atlas CLI Connect to the Atlas CLI To returns all data lake pipeline runs for your project using the\nAtlas CLI, run the following command: To learn more about the command syntax and parameters, see the\nAtlas CLI documentation for  atlas dataLakePipelines runs list . Install the Atlas CLI Connect to the Atlas CLI To return the details for the specified data lake pipeline run for your project using the\nAtlas CLI, run the following command: To learn more about the command syntax and parameters, see the\nAtlas CLI documentation for  atlas dataLakePipelines runs describe . Install the Atlas CLI Connect to the Atlas CLI To retrieve all your Data Lake pipelines for a project through the  API (Application Programming Interface) ,\nsend a  GET  request to the  Data Lake   pipelines  endpoint. To learn more about\nthe  pipelines  endpoint syntax and parameters for retrieving all of\nyour Data Lake pipelines, see  Return All Data Lake\nPipelines from One Project . To retrieve one of your Data Lake pipelines through the  API (Application Programming Interface) , send a\n GET  request to the  Data Lake \n pipelines  endpoint with the name of the Data Lake pipeline that you\nwant to retrieve. To learn more about the  pipelines  endpoint syntax\nand parameters for retrieving one of your Data Lake pipelines, see\n Return One Data Lake Pipeline . To retrieve all the completed Data Lake pipeline data ingestion jobs for a\nproject through the  API (Application Programming Interface) , send a  GET  request to the\n Data Lake   runs  endpoint. To\nlearn more about the  API (Application Programming Interface)  syntax and options for the  runs \nendpoint, see  Return All Data Lake Pipeline Runs from\nOne Project . To retrieve the details of one of your completed Data Lake pipeline data\ningestion jobs through the  API (Application Programming Interface) , send a  GET  request to the\n Data Lake   runs  endpoint with\nthe unique identifier of the completed Data Lake pipeline data ingestion\njob that you want to retrieve. To learn more about the  API (Application Programming Interface)  syntax and\noptions for the  runs  endpoint, see  Return One Data\nLake Pipeline Run . The page displays all the Data Lake pipelines in the project. For\neach Data Lake pipeline, the service also displays the following\ninformation: Column Name Description Pipeline Name Name of your Data Lake pipeline. Each pipeline can produce\nmultiple datasets. You can expand the name to view the\ndatasets in the pipeline. Data Source Source for the data in the pipeline datasets. For data\nfrom a collection on the  Atlas  cluster, this column\nshows the cluster name, the database name, and the\ncollection name separated by  | . Data Size Size of data for each dataset. Last Run Time Date and time when the pipeline ran to ingest data for\neach dataset. Status Status of the pipeline. Value can be one of the following\nfor a pipeline: Active  - indicates that the pipeline is active Paused  - indicates that data ingestion for the\npipeline is paused Frequency Frequency at which cluster data is ingested and stored\nfor querying. Actions Actions you can take for each pipeline. You can click\none of the following: ||  to pause data ingestion and\n  to resume data ingestion. You\ncan't pause on-demand ingestion of data.  to edit the data ingestion schedule for\nthe pipeline.  to do the following: Delete a pipeline. You can't undo this action. If you\ndelete a pipeline, Atlas Data Lake deletes the datasets,\nincluding the data, and removes the datasets from the\nFederated Database Instances where they are referenced. If you delete a\ndataset inside a pipeline, Atlas Data Lake removes the dataset\nfrom the Federated Database Instance storage configuration where the\ndataset is referenced. Trigger an on-demand pipeline run.",
            "code": [
                {
                    "lang": "sh",
                    "value": "\natlas dataLakePipelines list [options]\n"
                },
                {
                    "lang": "sh",
                    "value": "\natlas dataLakePipelines describe <pipelineName> [options]\n"
                },
                {
                    "lang": "sh",
                    "value": "\natlas dataLakePipelines availableSchedules list [options]\n"
                },
                {
                    "lang": "sh",
                    "value": "\natlas dataLakePipelines availableSnapshots list [options]\n"
                },
                {
                    "lang": "sh",
                    "value": "\natlas dataLakePipelines runs list [options]\n"
                },
                {
                    "lang": "sh",
                    "value": "\natlas dataLakePipelines runs describe <pipelineRunId> [options]\n"
                }
            ],
            "preview": "You can view all of your Data Lake pipelines and view the details of a specified\nData Lake Pipeline in your project through the Atlas UI (User Interface), Data Lake Pipelines API (Application Programming Interface),\nand the Atlas CLI. You can also retrieve all of your completed Data Lake pipeline\ndata ingestion jobs from the API (Application Programming Interface) and the Atlas CLI.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "administration/delete-datalake-pipeline",
            "title": "Delete Atlas Data Lake Pipeline",
            "headings": [
                "Procedure",
                "Delete a Pipeline Dataset",
                "Navigate to Atlas Data Lake in the Atlas UI (User Interface).",
                "Click  in the Actions column for the pipeline that you wish to delete.",
                "Click Delete in the confirmation window."
            ],
            "paragraphs": "You can delete your Data Lake pipeline at any time from\nthe  Atlas   UI (User Interface) , Data Lake Pipelines  API (Application Programming Interface) , and the Atlas CLI.\nWhen you delete your Data Lake pipeline,  Atlas  deletes all underlying\ndatasets, including the data, and removes the Data Lake datasets from your\nfederated database instances where it's referenced.\nYou can't undo this operation. You can also delete a dataset from your Data Lake pipeline at any time from\nthe Atlas CLI. When you delete a dataset from your Data Lake pipeline,\n Atlas  removes the Data Lake dataset from your federated database instances\nwhere it's referenced. You can't undo this operation. To remove the specified data lake pipeline from your project using the\nAtlas CLI, run the following command: To learn more about the command syntax and parameters, see the\nAtlas CLI documentation for  atlas dataLakePipelines delete . Install the Atlas CLI Connect to the Atlas CLI To remove the specified data lake pipeline dataset from your project using the\nAtlas CLI, run the following command: To learn more about the command syntax and parameters, see the\nAtlas CLI documentation for  atlas dataLakePipelines datasets delete . Install the Atlas CLI Connect to the Atlas CLI To delete a pipeline through the  API (Application Programming Interface) , send a  DELETE  request to\nthe  Data Lake   pipelines \nendpoint with the name of the pipeline that you want to delete. To\nlearn more about the  pipelines  endpoint syntax and parameters for\ndeleting a pipeline, see  Remove One Data Lake Pipeline . To delete a Data Lake pipeline: To navigate to the Atlas Data Lake page: Log in to  MongoDB Atlas . Select  Data Lake  under  Deployment \non the left-hand navigation.",
            "code": [
                {
                    "lang": "sh",
                    "value": "\natlas dataLakePipelines delete <pipelineName> [options]\n"
                },
                {
                    "lang": "sh",
                    "value": "\natlas dataLakePipelines datasets delete <pipelineRunId> [options]\n"
                }
            ],
            "preview": "You can delete your Data Lake pipeline at any time from\nthe Atlas UI (User Interface), Data Lake Pipelines API (Application Programming Interface), and the Atlas CLI.\nWhen you delete your Data Lake pipeline, Atlas deletes all underlying\ndatasets, including the data, and removes the Data Lake datasets from your\nfederated database instances where it's referenced.\nYou can't undo this operation.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/adl-run-sample-queries",
            "title": "Run Queries Against Your Data Lake Dataset",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Switch to Database0 database:",
                "Copy, paste, and run the following queries in your terminal."
            ],
            "paragraphs": "This page guides you through the steps for running queries in\n mongosh  against Data Lake dataset for the  sample_mflix.movies \ncollection. Before you run the queries, you must complete the following using the\nexamples shown in the procedures: Create an Atlas Data Lake Pipeline  for the  sample_mflix.movies \ncollection (For  On Demand  schedule only) Manually trigger\n Ingestion of data  from your snapshot Set Up a Federated Database Instance for Your Dataset  for the Data Lake dataset that is a\nsnapshot of data in the  sample_mflix.movies  collection Connect to Your Federated Database Instance  to run the queries The following queries use the  sample_mflix.movies  collection\nfor which you  created  the pipeline. Find movie with the title  The Frozen Ground  released between\n2010 and 2015: Find all movies whose title includes the word  Ground  and\nlimit the number of documents returned to  10 .",
            "code": [
                {
                    "lang": "sh",
                    "value": "use Database0"
                },
                {
                    "lang": "sh",
                    "value": "db.Collection0.find({ \"year\": {$gt: 2010, $lt: 2015}, \"title\": \"The Frozen Ground\" }, {\"title\": 1, \"year\": 1, \"genres\": 1 })"
                },
                {
                    "lang": "sh",
                    "value": "db.Collection0.find({ \"year\": {$gt: 2010, $lt: 2015}, title: /Ground/ }, {\"title\": 1, \"year\": 1, \"genres\": 1 }).limit(10)"
                }
            ],
            "preview": "This page guides you through the steps for running queries in\nmongosh against Data Lake dataset for the sample_mflix.movies\ncollection.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "release-notes/data-lake",
            "title": "Release Notes",
            "headings": [
                "2023 Releases",
                "05 December 2023 Release",
                "2022 Releases",
                "19 December 2022 Release",
                "13 September 2022 Release",
                "23 August 2022 Release",
                "21 June 2022 Release",
                "07 June 2022 Release"
            ],
            "paragraphs": "MongoDB releases Atlas Data Lake every two weeks, continuously improving\nAtlas Data Lake performance and stability. These release notes capture only\nthose releases that contain feature changes. If a particular Atlas Data Lake\nrelease contains only performance and stability improvements, it is not\nincluded in these release notes. To identify which release version you\nare using, check the release version string for the release date. Supports retention policies for Atlas Data Lake datasets that automatically delete datasets after the duration you specify when configuring the pipelines. To learn more, see  Edit an Atlas Data Lake Pipeline . Adds  AWS region :  ap-southeast-1  (Singapore). Adds support for the Public API for configuring and utilizing Data\nLake Pipelines. Improves performance and stability. Supports On-Demand Pipelines that can\nbe triggered to ingest any existing backup snapshot. Improves performance and stability. Launches new Atlas Data Lake. To learn more, see  About Atlas Data Lake . The federated query engine service previously called Atlas Data Lake is\nnow called Atlas Data Federation. To learn more about Atlas Data Federation, see\n Set Up and Query Data Federation .",
            "code": [],
            "preview": null,
            "tags": "|data-lake| release notes",
            "facets": null
        },
        {
            "slug": "tutorial/adl-connect-federated-db-instance",
            "title": "Connect to Your Federated Database Instance",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Navigate to Atlas Data Federation in the Atlas UI.",
                "Click Connect for the Federated Database Instance that you wish to connect to.",
                "Get the connection string for your preferred method of connecting.",
                "Connect your client to your Federated Database Instance using the connection string.",
                "Next Steps"
            ],
            "paragraphs": "This page guides you through the steps for connecting to the federated\ndatabase instance to query your Data Lake dataset. Before you begin, you must have the following: An IP address added to the access list of the project that contains\nthe cluster. To learn more, see  Add Your Connection IP\nAddress to Your IP Access List . A database user for your cluster in the project that contains the\nFederated Database Instance that you want to connect to. To learn\nmore, see  Create a Database User for Your Cluster . A Federated Database Instance on the  Atlas  cluster mapped to your\nAtlas Data Lake dataset. To learn more, see  Set Up a Federated Database Instance for Your Dataset . To navigate to the Atlas Data Federation page: Log in to  MongoDB Atlas . Select  Data Federation  under  Data\nServices  on the left-hand navigation. If you are connecting to the Federated Database Instance for\nquerying the Atlas Data Lake dataset that you created using the\nexamples, click  Connect  for  DataLake0 . You can connect using  mongosh , a  MongoDB Driver ,\nand  MongoDB Compass . To learn how to get the\nconnection string for your client, click the tab below for your\nconnection method. If you are following the examples in the steps for deploying an\nAtlas Data Lake for the  sample_mflix.movies  collection and setting up\na Federated Database Instance for this Data Lake dataset, click the\nMongoDB Shell tab below to connect using  mongosh . Click  Connect with the MongoDB Shell . Based on whether or not you have  mongosh  installed,\ndo the following: If you already have  mongosh , select your installed\n mongosh  version and copy the provided connection\nstring to your clipboard. If you don't have  mongosh , install  mongosh  and\ncopy the provided connection string to your clipboard. Click  Close . Click  Connect your application . Select your driver and the driver version. Copy the provided connection string to your clipboard. Click  Close . Click  Connect using MongoDB Compass . Based on whether you have  MongoDB Compass  installed, do the\nfollowing: If you already have  MongoDB Compass , select your  MongoDB Compass \nversion and copy the provided connection string to\nyour clipboard. If you don't have  MongoDB Compass ,  install   MongoDB Compass  and follow the steps above to\nconnect with  MongoDB Compass . Click  Close . To learn more, see  Connect with MongoDB Shell . Paste and run the connection string in a terminal. When prompted, enter your database user's password. To learn more, see  Driver Examples  in the  Atlas \n Connect via Your Application \npage. To learn more, see  Connect with MongoDB Compass . Open  MongoDB Compass  and paste the connection string into the\n URI  field. Click  Connect . Now that you are connected to your federated database instance, proceed\nto  Run Queries Against Your Data Lake Dataset .",
            "code": [],
            "preview": "This page guides you through the steps for connecting to the federated\ndatabase instance to query your Data Lake dataset.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/set-up-federated-database",
            "title": "Set Up a Federated Database Instance for Your Dataset",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Navigate to Atlas Data Federation in the Atlas UI.",
                "Click Create Federated Database.",
                "Select the configuration method.",
                "Create virtual databases, collections, and views and map them to your Data Lake dataset.",
                "Click Save to create the Federated Database Instance with virtual databases, collections, and views mapped to your Data Lake dataset.",
                "Next Steps"
            ],
            "paragraphs": "This page guides you through the steps for creating a federated\ndatabase instance for you Data Lake dataset. Before you begin, you must have the following: An Atlas Data Lake dataset in the same project where you intend to create the\nfederated database instance. Project Owner  role for the project where you want to\ncreate the federated database instance. To navigate to the Atlas Data Federation page: Log in to  MongoDB Atlas . Select  Data Federation  under  Data\nServices  on the left-hand navigation. For a guided experience, enable  Visual Editor . (Default) To edit the raw  JSON (Javascript Object Notation) , disable  Visual Editor . Follow the steps in the tab below for your preferred Editor view\nin the UI. (Optional) Click the   for the: You can click: Federated Database Instance  to specify a\nname for your federated database instance. Defaults to\n FederatedDatabaseInstance[n] . Database to edit the database name. Defaults to\n Database[n] . Corresponds to  databases.[n].name   JSON (Javascript Object Notation) \nconfiguration setting. Collection to edit the collection name. Defaults to\n Collection[n] . Corresponds to  databases.[n].collections.name \n JSON (Javascript Object Notation)  configuration setting. View to edit the view name. Add Database  to add databases and\ncollections.  associated with the database to\nadd collections to the database.  associated with the collection to\nadd  views  on the\ncollection. To create a view, you must specify: The name of the view. The  pipeline \nto apply to the view. To learn more about views, see: The view definition pipeline can't include the\n $out  or the  $merge  stage. If the view\ndefinition includes nested pipeline stages such\nas  $lookup  or  $facet , this restriction\napplies to those nested pipelines as well. Views db.createView  associated with the database,\ncollection, or view to remove it. The  sample queries  that you\ncan run later in this tutorial use the names\n Database0  for the virtual database name and\n Collection0  for the virtual collection name. If\nyou modify the names here, make sure to modify the\nnames in the sample queries also before you run them. Drag and drop the  Data Lake Dataset  to map\nwith the collection. Corresponds to\n databases.[n].collections.[n].dataSources   JSON (Javascript Object Notation) \nconfiguration setting. If you are creating a Federated Database Instance for\nthe Atlas Data Lake dataset that you created for the sample\ndata using the examples in  Create an Atlas Data Lake Pipeline : Under  Datasets , select\n Ingestion Pipeline  from the dropdown\nif it isn't already selected. Under  Data Lake Dataset  section, drag\nthe dataset named  sample_mflix.movies  and drop\nit under the collection. Define your dataset as a data store in your Federated Database Instance\nstorage configuration. Edit the  JSON (Javascript Object Notation)  configuration settings shown in the UI\nfor  stores . Your  stores  cofiguration setting\nshould resemble the following: To learn more about these settings, see  Storage\nConfiguration For Atlas Data Lake Datasets . If you are creating a Federated Database Instance for\nthe Atlas Data Lake pipeline that you created for the sample\ndata using the examples in  Create an Atlas Data Lake Pipeline ,\nreplace the  stores  in the  JSON (Javascript Object Notation)  configuration\nsettings shown in the UI with the following: Define virtual databases, collections, and views for\nyour dataset in the Atlas Data Federation storage configuration. To learn more about these settings, see  Storage\nConfiguration For Atlas Data Lake Datasets . If you are creating a Federated Database Instance for\nthe Atlas Data Lake dataset that you created for the sample\ndata using the examples in  Create an Atlas Data Lake Pipeline ,\nreplace the  databases  in the  JSON (Javascript Object Notation)  configuration\nsettings shown in the UI with the following: Now that you've created a Federated Database Instance for your Data Lake\ndataset, proceed to  Connect to Your Federated Database Instance .",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"stores\": [\n    {\n      \"name\": \"<store-name>\",\n      \"provider\": \"<cloud-storage-provider-name>\",\n      \"region\": \"<cloud-storage-provider-region>\"\n    }\n  ]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"stores\": [\n    {\n      \"name\": \"dls-store-us-east-1\",\n      \"provider\": \"dls:aws\",\n      \"region\": \"US_EAST_1\"\n    }\n  ]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"databases\": [\n    {\n      \"name\": \"<database-name>\",\n      \"collections\": [\n        {\n          \"name\": \"<collection-name>\",\n          \"dataSources\": [\n            {\n              \"storeName\": \"<store-name>\",\n              \"datasetName\": \"<snapshot-name>\"\n            }\n          ]\n        }\n      ],\n      \"views\": []\n    }\n  ]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"databases\": [\n    {\n      \"name\": \"Database0\",\n      \"collections\": [\n        {\n          \"name\": \"Collection0\",\n          \"dataSources\": [\n            {\n              \"storeName\": \"dls-store-us-east-1\",\n              \"datasetName\": \"v1$atlas$snapshot$dlsTest$sample_mflix$movies$$.<snapshot-id>\"\n            }\n          ]\n        }\n      ],\n      \"views\": []\n    }\n  ]\n}"
                }
            ],
            "preview": "This page guides you through the steps for creating a federated\ndatabase instance for you Data Lake dataset.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/add-dataset-pipeline",
            "title": "Create an Atlas Data Lake Pipeline",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Watch for a Pipeline to Complete",
                "Navigate to Atlas Data Lake in the Atlas UI (User Interface).",
                "Click Add Data Lake Pipeline.",
                "Define the data source for the pipeline.",
                "Specify an ingestion schedule for your cluster data.",
                "Select the AWS (Amazon Web Services) region for storing your extracted data.",
                "Specify fields in your collection to create partitions.",
                "(Optional) Specify fields inside your documents to exclude.",
                "Click Finish to create the Data Lake.",
                "Next steps"
            ],
            "paragraphs": "You can create Atlas Data Lake pipelines using the  Atlas   UI (User Interface) , Data Lake\nPipelines  API (Application Programming Interface) , and the Atlas CLI. This page guides you through\nthe steps for creating an Atlas Data Lake pipeline. Before you begin, you must have the following: Backup-enabled   M10  or higher\n Atlas  cluster. Project Owner  role for the project for which you want to\ndeploy a Data Lake. Sample data  loaded on your cluster (if you wish\nto try the example in the following\n Procedure ). To create a new Data Lake pipeline using the\nAtlas CLI, run the following command: To learn more about the command syntax and parameters, see the\nAtlas CLI documentation for  atlas dataLakePipelines create . Install the Atlas CLI Connect to the Atlas CLI To watch for the specified data lake pipeline to complete using the\nAtlas CLI, run the following command: To learn more about the command syntax and parameters, see the\nAtlas CLI documentation for  atlas dataLakePipelines watch . Install the Atlas CLI Connect to the Atlas CLI To create an Atlas Data Lake pipeline through the  API (Application Programming Interface) , send a  POST \nrequest to the  Data Lake \n pipelines  endpoint. To learn more about the  pipelines  endpoint\nsyntax and parameters for creating a pipeline, see\n Create One Data Lake Pipeline . You can send a  GET  request to the  Data Lake   availableSchedules  endpoint to retrieve the\nlist of backup schedule policy items that you can use to create your\nData Lake pipeline of type  PERIODIC_DPS . To navigate to the Atlas Data Lake page: Log in to  MongoDB Atlas . Select  Data Lake  under  Deployment  on the\nleft-hand navigation panel. You can create a copy of data on your  Atlas  cluster in\nMongoDB-managed cloud object storage optimized for analytic\nqueries with workload isolation. To set up a pipeline, specify the following in the\n Setup Pipeline  page: Select the  Atlas  cluster from the dropdown. If you loaded the sample data on your cluster, select the\n Atlas  cluster where you loaded the sample data. Select the database on the specified cluster from the\ndropdown, or type the database name in the field if the\ndatabase isn't listed in the dropdown. Atlas Data Lake won't display the database if it's unable to\nfetch the names of the databases for the specified cluster. If you selected the cluster where the sample data is\nloaded, select  sample_mflix . Select the collection in the specified database from the\ndropdown, or type the collection name in the field if the\ncollection isn't available. Atlas Data Lake won't display the collection if it's unable to\nfetch the collection namespace for the specified cluster. Atlas Data Lake doesn't support  Views  as a\ndata source for pipelines. You must select a collection from\nyour cluster. If you selected the  sample_mflix  database, select the\n movies  collection in the  sample_mflix  database. Enter a name for the pipeline. Atlas Data Lake pipeline names can't exceed 64 characters and can't contain: Forward slashes ( / ), Backward slashes ( \\ ) Empty spaces Dollar signs ( $ ) If you are following the examples in this tutorial, enter\n sample_mflix.movies  in the  Pipeline Name \nfield. Click  Continue . You can specify how frequently your cluster data is extracted\nfrom your  Atlas  Backup Snapshots and ingested into Data Lake\nDatasets. Each snapshot represents your data at that point in\ntime, which is stored in a workload isolated, analytic storage.\nYou can query any snapshot data in the Data Lake datasets. You can choose  Basic Schedule  or  On\nDemand . Basic Schedule  lets you define the frequency\nfor automatically ingesting data from available snapshots.\nYou must choose from the following schedules. Choose the\n Snapshot Schedule  that is similar to your\nbackup schedule: For example, if you select  Every day , you must have a\n Daily  backup schedule configured in your policy. Or, if\nyou want to select a schedule of once a week, you must have\na  Weekly  backup schedule configured in your policy. To\nlearn more, see  Backup Scheduling .\nYou can send a  GET  request to the\n Data Lake \n availableSchedules  endpoint to\nretrieve the list of backup schedule policy items that you\ncan use in your Data Lake pipeline. Every day Every Saturday Last day of the month For this tutorial, select  Daily  from the\n Snapshot Schedule  dropdown if you don't have\na backup schedule yet. If you have a backup schedule,\nthe available options are based on the schedule you have\nset for your backup schedule. On Demand  lets you manually trigger ingestion\nof data from available snapshots whenever you want. For this tutorial, if you select  On Demand ,\nyou must manually trigger the ingestion of data from\nthe snapshot after creating the pipeline. To learn more,\nsee  Trigger Data Ingestion On Demand . Atlas Data Lake provides optimized storage in the following  AWS (Amazon Web Services)  regions: By default, Atlas Data Lake automatically selects the region closest to\nyour  Atlas  cluster for storing extracted data. If\nAtlas Data Lake is unable to determine the region, it defaults to\n us-east-1 . Data Lake  Regions AWS (Amazon Web Services)  Regions Virginia, USA us-east-1 Oregon, USA us-west-2 Sao Paulo, Brazil sa-east-1 Ireland eu-west-1 London, England eu-west-2 Frankfurt, Germany eu-central-1 Mumbai, India ap-south-1 Singapore ap-southeast-1 Sydney, Australia ap-southeast-2 Enter the most commonly queried fields from the collection in the\n Partition Attributes  section. To specify nested\nfields, use the  dot notation . Do not include quotes ( \"\" )\naround nested fields that you specify using  dot notation . You can't specify fields inside\nan array. The specified fields are used to partition your data. The most frequently queried fields should be listed towards the\ntop because they will have a larger impact on performance and\ncost than fields listed lower down the list. The order of fields\nis important in the same way as it is for  Compound\nIndexes . Data is optimized for queries\nby the first field, followed by the second field, and so on. You can't specify field names that contain periods ( . ) for\npartitioning. Enter  year  in the  Most commonly queried field \nfield and  title  in the  Second most commonly\nqueried field  field. Atlas Data Lake optimizes performance for the  year  field, followed\nby the  title  field. If you configure a Federated Database Instance for your\nData Lake dataset, Atlas Data Federation optimizes performance for queries on\nthe following fields: Atlas Data Federation can also support a query on the  title  field only.\nHowever, in this case, Atlas Data Federation wouldn't be as efficient in\nsupporting the query as it would be if the query were on the\n title  field only. Performance is optimized in order; if a\nquery omits a particular partition, Atlas Data Federation is less efficient\nin making use of any partitions that follow that. You can run Atlas Data Federation queries on fields not specified here, but\nAtlas Data Lake is less efficient in processing such queries. the  year  field, and the  year  field and the  title  field. By default, Atlas Data Lake extracts and stores all fields inside the\ndocuments in your collection. To specify fields to exclude: Click  Add Field . Enter field name in the  Add Transformation Field\nName  window. (Optional) Enter  fullplot  to exclude the field named\n fullplot  in the  movies  collection. Click  Done . Repeat steps for each field you wish to exclude. To remove a\nfield from this list, click  . Now that you've created your Data Lake pipeline, proceed to\n Set Up a Federated Database Instance for Your Dataset .",
            "code": [
                {
                    "lang": "sh",
                    "value": "\natlas dataLakePipelines create <pipelineName> [options]\n"
                },
                {
                    "lang": "sh",
                    "value": "\natlas dataLakePipelines watch <pipelineName> [options]\n"
                }
            ],
            "preview": "You can create Atlas Data Lake pipelines using the Atlas UI (User Interface), Data Lake\nPipelines API (Application Programming Interface), and the Atlas CLI. This page guides you through\nthe steps for creating an Atlas Data Lake pipeline.",
            "tags": null,
            "facets": null
        }
    ]
}