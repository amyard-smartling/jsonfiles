{
    "url": "http://mongodb.com/docs/kubernetes-operator/v1.17",
    "includeInGlobalSearch": false,
    "documents": [
        {
            "slug": "openshift-quick-start",
            "title": "OpenShift Quick Start",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Clone the MongoDB Enterprise Kubernetes Operator repository.",
                "Create a  for your  deployment.",
                "Configure kubectl to default to your namespace.",
                "Create a  that contains credentials authorized to pull images from the registry.connect.redhat.com repository.",
                "Install the ",
                "Create credentials and store them as a secret.",
                "Invoke the following command to create a ConfigMap.",
                "Find the organization's ID.",
                "Copy and paste the orgId into the URL.",
                "Find the organization's ID.",
                "Copy and paste the orgId into the URL.",
                "Deploy the replica set resource.",
                "Create a secret with your database user password",
                "Create a database user.",
                "Optional: View the newly created user in .",
                "Connect to the replica set."
            ],
            "paragraphs": " uses the   API and tools to manage MongoDB\nclusters.   works together with MongoDB  . This\ntutorial demonstrates how to deploy and connect to your first replica\nset in   from OpenShift with  . This tutorial requires: A running   cluster. By default, The   uses the  mongodb  namespace. To\nsimplify your installation, consider creating a namespace labeled\n mongodb  using the following   command: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you created: If you use the   to deploy MongoDB\nresources to  multiple namespaces  or with\na  cluster-wide scope , create the secret\nonly in the namespace where you intend to deploy the  . The\n  synchronizes the secret across all watched namespaces. If you have not already, obtain a Red Hat subscription. Create a  Registry Service Account . Click on your Registry Service Account, then click the\n Docker Configuration  tab. Download the  <account-name>-auth.json  file and open it in a\ntext editor. Copy the  registry.redhat.io  object, and paste another instance\nof this object into the file. Remember to add a comma after the\nfirst object. Rename the second object\n registry.connect.redhat.com , then save the file: Create an  openshift-pull-secret.yaml  file and add the contents\nof the modified  <account-name>-auth.json  file as\n stringData  named  .dockerconfigjson  to the\n openshift-pull-secret.yaml  secret file. The value you provide in the  metadata.name  field contains\nthe secret name. Provide this value when asked for the\n <openshift-pull-secret> . Create a   from the  openshift-pull-secret.yaml \nfile in the same namespace in which you will deploy the  . Invoke the following   command to install the   for\nMongoDB deployments: Add your  <openshift-pull-secret>  to the  ServiceAccount \ndefinitions in the     file. Invoke the following   command to install  : Run the following command: Provide your Public and Private Key values for the following\nparameters. To learn more, see  Create Credentials for the  . Key Type Description Example metadata.name string Name of the    . Resource names must be 44 characters or less.  documentation on  names .\nThis name must follow  RFC1123  naming\nconventions, using only lowercase alphanumeric\ncharacters,  -  or  . , and must start and end with an\nalphanumeric character. my-project metadata.namespace string    where the   creates this\n  and other  . mongodb data.projectName string Label for your  \n Project . The   creates the   project if it does\nnot exist. If you omit the  projectName , the  \ncreates a project with the same name as your\n  resource. To use an existing project in a  \norganization, locate\nthe  projectName  by clicking the  All Clusters \nlink at the top left of the   page, and\nsearching by name in the  Search \nbox, or scrolling to find the name in the list.\nEach card in this list represents the\ncombination of one    Organization  and  Project . myProjectName data.orgId string 24 character hex string that uniquely\nidentifies your\n   Organization . Depending on your    credentials , this field is: You can use the   to deploy MongoDB resources with\n  and with   version 5.0.x or later. You can use the  Atlas Operator \nto deploy MongoDB resources to  . Required  for  Global Programmatic API Keys . Optional  for  Organization Programmatic API Keys . You must specify an  existing Organization . Click  Settings  in the left navigation bar. Select your organization, view the current  \nin your browser and copy the value displayed in\nthe  <orgId>  placeholder as follows: https://ops.example.com:8443/ \n v2#/org/<orgId>/projects If specified, the   links to the organization. To find the  orgID  of your organization: If omitted,   creates an organization called\n projectName  that contains a project also called\n projectName . You must have the  Organization Project Creator \nrole to create a new project within an existing\n  organization. Click  Settings  in the left navigation bar. Select your organization, view the current  \nin your browser and copy the value displayed in\nthe  <orgId>  placeholder as follows: https://ops.example.com:8443/ \n v2#/org/<orgId>/projects 5b890e0feacf0b76ff3e7183 data.baseUrl string  to your   including the   and port\nnumber. If you deploy   with the   and   will\nmanage MongoDB database resources deployed  outside  of the  \ncluster it's deployed to, you must set  data.baseUrl  to the same\nvalue of the\n spec.configuration.mms.centralUrl \nsetting in the   resource specification. If you're using  , set the  data.baseUrl  value\nto  https://cloud.mongodb.com . https://ops.example.com:8443 Copy and save the following   file: Run the following command: You can choose to use a cleartext password or a Base64-encoded\npassword. Plaintext passwords use  stringData.password  and\nBase64-encoded passwords use  data.password . For a cleartext password, create and save the following   file: For a Base64-encoded password, create and save the following YAML\nfile: Provide your values for the following parameters. To learn more,\nsee the  parameter descriptions . Copy and save the following   file: Run the following command: You can view the newly-created user in  : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. Perform the following steps in the   application: Click  Deployment  in the left navigation. Click   for the deployment to which you want\nto connect. Click  Connect to this instance . Run the connection command in a terminal to connect to the\ndeployment.",
            "code": [
                {
                    "lang": "sh",
                    "value": "git clone https://github.com/mongodb/mongodb-enterprise-kubernetes.git"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=mongodb"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"auths\": {\n    \"registry.redhat.io\": {\n      \"auth\": \"<encoded-string>\"\n    },\n    \"registry.connect.redhat.com\": {\n      \"auth\": \"<encoded-string>\"\n    }\n  }\n}"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: openshift-pull-secret\nstringData:\n  .dockerconfigjson: |\n      {\n        \"auths\": {\n          \"registry.redhat.io\": {\n            \"auth\": \"<encoded-string>\"\n          },\n          \"registry.connect.redhat.com\": {\n            \"auth\": \"<encoded-string>\"\n          }\n        }\n      }\ntype: kubernetes.io/dockerconfigjson"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f openshift-pull-secret.yaml -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-appdb\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-database-pods\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f mongodb-enterprise-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb \\\n  create secret generic ops-manager-admin-key \\\n  --from-literal=\"publicKey=<publicKey>\" \\\n  --from-literal=\"privateKey=<privateKey>\""
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-project\n  namespace: mongodb\ndata:\n  projectName: myProjectName # this is an optional parameter\n  orgId: 5b890e0feacf0b76ff3e7183 # this is an optional parameter\n  baseUrl: https://ops.example.com:8443\n\nEOF"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: demo-mongodb-cluster-1\n  namespace: mongodb\nspec:\n  members: 3\n  version: 4.4.5-ent\n  type: ReplicaSet\n  authentication:\n    enabled: true\n    modes: [\"SHA\"]\n  cloudManager:\n    configMapRef:\n      name: my-project\n  credentials: organization-secret\n  persistent: true\n  podSpec:\n    podTemplate:\n      spec:\n       containers:\n        - name: mongodb-enterprise-database\n          resources:\n            limits:\n              cpu: 2\n              memory: 1.5G\n            requests:\n              cpu: 1\n              memory: 1G\n            persistence:\n              single:\n                storage: 10Gi"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mms-user-1-password\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\nstringData:\n  password: <my-plain-text-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mms-user-1-password\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\ndata:\n  password: <base-64-encoded-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: mms-scram-user-1\nspec:\n  passwordSecretKeyRef:\n    name: mms-user-1-password\n    # Match to metadata.name of the User Secret\n    key: password\n  username: \"mms-scram-user-1\"\n  db: \"admin\" #\n  mongodbResourceRef:\n    name: \"demo-mongodb-cluster-1\"\n    # Match to MongoDB resource using authenticaiton\n  roles:\n  - db: \"admin\"\n    name: \"clusterAdmin\"\n  - db: \"admin\"\n    name: \"userAdminAnyDatabase\"\n  - db: \"admin\"\n    name: \"readWrite\"\n  - db: \"admin\"\n    name: \"userAdminAnyDatabase\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                }
            ],
            "preview": " uses the  API and tools to manage MongoDB\nclusters.  works together with MongoDB . This\ntutorial demonstrates how to deploy and connect to your first replica\nset in  from OpenShift with .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "manage-users",
            "title": "Manage Database Users",
            "headings": [],
            "paragraphs": "Manage database users using   authentication on MongoDB\ndeployments. Manage database users using SCRAM authentication on MongoDB\ndeployments. Manage database users for deployments running with TLS and X.509\ninternal cluster authentication enabled.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "multi-cluster-troubleshooting",
            "title": "Troubleshoot Multi-Cluster Deployments",
            "headings": [
                "Recovering from Cluster Failure"
            ],
            "paragraphs": "To troubleshoot your  , use the procedures in\nthis section. This procedure uses the same cluster names as in the  Prerequisites .\nIf the cluster  MDB_CLUSTER_1  that holds MongoDB nodes goes down, and\nif you provision a new cluster named  MDB_CLUSTER_4  instead of\n MDB_CLUSTER_1  to hold the new MongoDB nodes, run the\n multi-cluster kubeconfig creator \ntool with the updated list of member clusters, and then edit the  MongoDBMulti \nCustomResource spec on the central cluster. To reconfigure the multi-cluster deployment after a cluster failure,\nreplace the failed cluster with the newly provisioned cluster as follows: Run the  multi-cluster kubeconfig creator \ntool with the new cluster  MDB_CLUSTER_4  specified in the\n -member-clusters  flag. This enables the   to\ncommunicate with the new cluster to schedule MongoDB nodes on it. In\nthe following example,  -member-clusters  contains  ${MDB_CLUSTER_4_FULL_NAME} . On the central cluster, locate and edit the  MongoDBMulti \nCustomResource spec to add the new cluster name to the\n clusterSpecList  and remove the failed cluster from this list.\nThe resulting list of cluster names should be similar to the following: Restart the   Pod. After the restart, the  \nshould reconcile the MongoDB deployment on the newly created\n MDB_CLUSTER_4  cluster that has been created as a replacement for\nthe  MDB_CLUSTER_1  failure. To learn more about resource\nreconciliation, see  Multi-Cluster Deployment Architecture .",
            "code": [
                {
                    "lang": "sh",
                    "value": "go run tools/multicluster/main.go \\\n  -central-cluster=\"${MDB_CENTRAL_CLUSTER_FULL_NAME}\" \\\n  -member-clusters=\"${MDB_CLUSTER_4_FULL_NAME},${MDB_CLUSTER_2_FULL_NAME},${MDB_CLUSTER_3_FULL_NAME}\" \\\n  -member-cluster-namespace=\"mongodb\" \\\n  -central-cluster-namespace=\"mongodb\""
                },
                {
                    "lang": "sh",
                    "value": "clusterSpecList:\n   clusterSpecs:\n    - clusterName: ${MDB_CLUSTER_4_FULL_NAME}\n      members: 3\n    - clusterName: ${MDB_CLUSTER_2_FULL_NAME}\n      members: 2\n    - clusterName: ${MDB_CLUSTER_3_FULL_NAME}\n      members: 3"
                }
            ],
            "preview": "To troubleshoot your , use the procedures in\nthis section.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "third-party-integrations",
            "title": "Third-Party Integrations",
            "headings": [],
            "paragraphs": " offers the following third-party integrations. Third-Party Service Configuration Details Grafana  offers a  sample Grafana dashboard  that you can  import into Grafana . Prometheus You can use   to  deploy  a\n MongoDB resource  or\n application database  to use with\nPrometheus.",
            "code": [],
            "preview": " offers the following third-party integrations.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "deploy",
            "title": "Deploy a MongoDB Database Resource",
            "headings": [],
            "paragraphs": "Use   to deploy a new standalone MongoDB instance. Use   to deploy a replica set. Use   to deploy a sharded cluster. Use   to deploy a MongoDB resource to use with\nPrometheus monitoring enabled.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "kind-quick-start",
            "title": "Quick Start",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Add the  repository to Helm.",
                "Install the ",
                "Configure kubectl to default to your namespace.",
                "Configure the Kubernetes Operator",
                "Copy and save the ConfigMap.",
                "Copy and save the Secret.",
                "Apply the ConfigMap and Secret.",
                "Deploy the replica set resource.",
                "Create a secret with your database user password",
                "Create a database user.",
                "Optional: View the newly created user in .",
                "Connect to the replica set."
            ],
            "paragraphs": " uses the   API and tools to manage MongoDB\nclusters.   works together with MongoDB  . This\ntutorial demonstrates how to deploy and connect to your first replica\nset in   with  . You can use   to quickly set\nup a cluster. To learn more, see  Kind . This tutorial requires: A running   cluster. A running   cluster. To  install the Kubernetes Operator with the Helm Chart , see the\ninstructions in the repository. Example The following command installs the   in the  mongodb \nnamespace with the optional  --create-namespace  option. By\ndefault,   uses the  default  namespace. If you haven't already, run the following command to execute all\n kubectl  commands in the namespace you created: Go to the Kubernetes Setup Page in the Cloud Manager UI . Click  Create New API Keys  or\n Use Existing API Keys . Complete the form. To learn more, see\n Programmatic Access to Cloud Manager . Click  Generate Key and YAML . Copy and save the generated  config-map.yaml  file. Example: To learn more, see the  parameter descriptions . Copy and save the generated  secret.yaml  file. Example: For security purposes,   displays this file only once. Run the following command: Copy and save the following   file: Run the following command: You can choose to use a cleartext password or a Base64-encoded\npassword. Plaintext passwords use  stringData.password  and\nBase64-encoded passwords use  data.password . For a cleartext password, create and save the following   file: For a Base64-encoded password, create and save the following YAML\nfile: Provide your values for the following parameters. To learn more,\nsee the  parameter descriptions . Copy and save the following   file: Run the following command: You can view the newly-created user in  : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. Perform the following steps in the   application: Click  Deployment  in the left navigation. Click   for the deployment to which you want\nto connect. Click  Connect to this instance . Run the connection command in a terminal to connect to the\ndeployment.",
            "code": [
                {
                    "lang": "sh",
                    "value": "helm repo add mongodb https://mongodb.github.io/helm-charts"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator --namespace mongodb --create-namespace"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-project\n  namespace: mongodb\ndata:\n  baseUrl: https://cloud.mongodb.com\n\n  # Optional Parameters\n  # projectName: My Ops/Cloud Manager Project\n\n  orgId: 5ecd252f8c1a75033c74106c"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: organization-secret\n  namespace: mongodb\nstringData:\n  user: <public_key>\n  publicAPIKey: <private_key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f secret.yaml -f config-map.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: demo-mongodb-cluster-1\n  namespace: mongodb\nspec:\n  members: 3\n  version: 4.4.5-ent\n  type: ReplicaSet\n  authentication:\n    enabled: true\n    modes: [\"SHA\"]\n  cloudManager:\n    configMapRef:\n      name: my-project\n  credentials: organization-secret\n  persistent: true\n  podSpec:\n    podTemplate:\n      spec:\n       containers:\n        - name: mongodb-enterprise-database\n          resources:\n            limits:\n              cpu: 2\n              memory: 1.5G\n            requests:\n              cpu: 1\n              memory: 1G\n            persistence:\n              single:\n                storage: 10Gi"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mms-user-1-password\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\nstringData:\n  password: <my-plain-text-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mms-user-1-password\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\ndata:\n  password: <base-64-encoded-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: mms-scram-user-1\nspec:\n  passwordSecretKeyRef:\n    name: mms-user-1-password\n    # Match to metadata.name of the User Secret\n    key: password\n  username: \"mms-scram-user-1\"\n  db: \"admin\" #\n  mongodbResourceRef:\n    name: \"demo-mongodb-cluster-1\"\n    # Match to MongoDB resource using authenticaiton\n  roles:\n  - db: \"admin\"\n    name: \"clusterAdmin\"\n  - db: \"admin\"\n    name: \"userAdminAnyDatabase\"\n  - db: \"admin\"\n    name: \"readWrite\"\n  - db: \"admin\"\n    name: \"userAdminAnyDatabase\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                }
            ],
            "preview": " uses the  API and tools to manage MongoDB\nclusters.  works together with MongoDB . This\ntutorial demonstrates how to deploy and connect to your first replica\nset in  with . You can use  to quickly set\nup a cluster. To learn more, see Kind.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference",
            "title": "Reference",
            "headings": [],
            "paragraphs": "Review the     object specification. Review the MongoDB   object specifications. Review the   installation settings. Review settings that only the   can set. Review the EOL dates for   versions. Review the available third-party integrations. Open Source licenses that the   uses.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "mdb-resources",
            "title": "Deploy and Configure MongoDB Database Resources",
            "headings": [],
            "paragraphs": "You can use the   to deploy and manage MongoDB clusters\nfrom the    , without having to configure them in\n  or  . Review the MongoDB database custom resources architecture. Configure the   to deploy MongoDB database resources. Deploy a standalone, replica set, or sharded cluster resource with\nor without   encryption. Modify the configuration of a MongoDB database resource. Configure authentication for client applications. Configure authentication for MongoDB database users. Access database resources from inside or outside  . Delete a MongoDB database resource.",
            "code": [],
            "preview": "You can use the  to deploy and manage MongoDB clusters\nfrom the  , without having to configure them in\n or .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "multi-cluster-quick-start",
            "title": "Multi-Cluster Quick Start",
            "headings": [
                "Services and Tools"
            ],
            "paragraphs": "Use the quick start to deploy a MongoDB replica set across three  \nmember clusters, using   and   service mesh. The quick start relies on the following\nservices, tools, and their documentation:  clusters. The quick start uses   to provision multiple\n  clusters. Each   member cluster hosts a MongoDB replica set\ndeployment and represents a data center that serves your application. MongoDB Enterprise Kubernetes Operator repository  with configuration\nfiles that the   needs to deploy a   cluster.  with\ncharts for  .  service mesh. The quick start uses   to facilitate\n DNS resolution \nfor MongoDB replica sets deployed in different   clusters. You\ncan use another service mesh solution as long as you ensure that\ncross-cluster service FQDNs are resolvable. Documentation from   to  Install Multicluster . install_istio_separate_network script \nthat is based on Istio documentation and provides an example\ninstallation that uses the  multi-primary mode on different networks .\nIf you use another service mesh solution, create your own script for\nconfiguring separate networks to facilitate DNS resolution. multi-cluster kubeconfig creator \ntool that performs the following actions: Creates a single  mongodb  namespace in the central cluster and\neach member cluster. Creates Service Accounts, Roles, and RoleBindings in the central\ncluster and each member cluster. Puts Service Account token secrets from each member cluster into a\nsingle  kubeconfig  file and saves the file in the central cluster.\nThis enables authorized access from the   installed in\nthe central cluster to the member clusters. Set up   clusters, install tools, set the deployment's scope, and\ncheck connectivity across member clusters. Deploy  .",
            "code": [],
            "preview": "Use the quick start to deploy a MongoDB replica set across three \nmember clusters, using  and  service mesh.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "multi-cluster-quick-start-procedure",
            "title": "Quick Start Procedure",
            "headings": [
                "Considerations"
            ],
            "paragraphs": "Before you begin, learn about   implementation,\n multi-cluster services and tools ,\nand complete the prerequisite steps: Overview Prerequisites The following procedure scopes your   to a single\n  named  mongodb . You can  set scope for your\ndeployment  and use another  ,\nmultiple, or all namespaces. When you create a   with the\n , you must choose whether to encrypt connections using\n  certificates. The following procedure for  TLS-Encrypted  connections: The following procedure for  Non-Encrypted Connections : Select the appropriate tab based on whether you want to encrypt your\nreplica set connections with  . Establishes  -encrypted connections between MongoDB hosts in a\nreplica set. Establishes  -encrypted connections between client applications\nand MongoDB deployments. Requires valid certificates for   encryption. Doesn't encrypt connections between MongoDB hosts in a\nreplica set. Doesn't encrypt connections between client applications\nand MongoDB deployments. Has fewer setup requirements than a deployment with  -encrypted\nconnections.",
            "code": [],
            "preview": "Before you begin, learn about  implementation,\nmulti-cluster services and tools,\nand complete the prerequisite steps:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "",
            "title": "MongoDB Enterprise Kubernetes Operator",
            "headings": [],
            "paragraphs": "The   translates the human knowledge of creating a MongoDB\ninstance into a scalable, repeatable, and standardized method.\nKubernetes needs help creating and managing stateful applications like\ndatabases. It needs to configure the network, persist storage, and\ndedicate computing capacity without additional human effort on each\ncontainer. The   manages the typical lifecycle events for a MongoDB\ncluster: provisioning storage and computing power, configuring network\nconnections, setting up users, and changing these settings as needed.\nIt accomplishes this using the Kubernetes API and tools. You provide the Operator with the specifications for your MongoDB\ncluster. The Operator uses this information to tell Kubernetes how to\nconfigure that cluster including provisioning storage, setting up the\nnetwork connections, and configuring other resources. The   works together with MongoDB  , which further\nconfigures to MongoDB clusters. When MongoDB is deployed and running in\nKubernetes, you can manage MongoDB tasks using  . You can then deploy MongoDB databases as you deploy them now after the\ncluster is created. You can use the   console to run MongoDB at\noptimal performance.",
            "code": [],
            "preview": "The  translates the human knowledge of creating a MongoDB\ninstance into a scalable, repeatable, and standardized method.\nKubernetes needs help creating and managing stateful applications like\ndatabases. It needs to configure the network, persist storage, and\ndedicate computing capacity without additional human effort on each\ncontainer.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "upgrade",
            "title": "Upgrade the  from Prior Versions",
            "headings": [],
            "paragraphs": "Upgrade the   to its latest version. Migrate your   deployments from Ubuntu-based container\nimages to UBI-based container images. To upgrade the versions of your   instance and  backing databases \nthat the   uses to manage your deployment, see  Upgrade Ops Manager and Backing Database Versions .\nTo upgrade the versions of your MongoDB resource, see  Upgrade MongoDB Version and FCV .",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "connect",
            "title": "Access Database Resources",
            "headings": [],
            "paragraphs": "The following pages describe how to connect to a MongoDB\ndatabase resource that is deployed to  : Connect to a MongoDB database resource from inside\nof the   cluster. Connect to a MongoDB database resource from outside\nof the   cluster.",
            "code": [],
            "preview": "The following pages describe how to connect to a MongoDB\ndatabase resource that is deployed to :",
            "tags": null,
            "facets": null
        },
        {
            "slug": "installation",
            "title": "Install and Configure the ",
            "headings": [],
            "paragraphs": "Review   deployment architecture, scopes, considerations, and\nprerequisites. Install the  . Upgrade from earlier versions of  .",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "multi-cluster-quick-start-prerequisites",
            "title": "Multi-Cluster Prerequisites",
            "headings": [
                "Set Environment Variables and GKE zones",
                "Set up GKE clusters",
                "Obtain User Authentication Credentials for Central and Member clusters",
                "Install Tools",
                "Set the Deployment's Scope",
                "Watch Resources in Multiple Namespaces",
                "Watch Resources in All Namespaces",
                "Check Connectivity Across Clusters",
                "Prepare for TLS-Encrypted Connections"
            ],
            "paragraphs": "This Quick Start tutorial requires that you complete these tasks: Set the environment variables with cluster names and the\n available GKE zones \nwhere you deploy the clusters, as in this example: Set up   clusters: Set up your Google Cloud account and the  gcloud  tool, using the\n Google Kubernetes Engine Quickstart . Create  one central cluster and one or more member clusters , specifying the GKE zones, the number\nof nodes, and the instance types, as in these examples: Obtain user authentication credentials for the central and member  \nclusters and save the credentials. You will later use these credentials\nfor running  kubectl  commands on these clusters. Run the following commands: Install the following tools: Install   in a  multi-primary mode on different networks ,\nusing the  install_istio_separate_network script . To learn more, see the  Install Multicluster  Istio\ndocumentation. Install Go  v1.17 or later. Install Helm . By default, the multi-cluster   is scoped to the  \nin which it is installed. The   reconciles the\n MongoDBMulti  custom resource deployed in the same namespace as the\n . When you run the  multi-cluster kubeconfig creator \ntool as part of the  multi-cluster Quick Start procedure , and don't modify the tool's\nsettings, the tool: Once the multi-cluster is deployed, the   starts watching\n  in the  mongodb   . To configure the   with the correct permissions to deploy\nin multiple or all namespaces, run the following command and specify the\nnamespaces that you would like the   to watch. When you install the   to multiple or all  , you\ncan configure the   to: Creates a single  mongodb  namespace in the central cluster and\neach member cluster. Creates Service Accounts, Roles, and RoleBindings in the central\ncluster and each member cluster. Applies the correct permissions for service accounts. Uses these settings to create your  . Watch Resources in Multiple Namespaces Watch Resources in All Namespaces If you set the scope for the   to many  , you can\nconfigure the   to watch   in these namespaces\nin the  . Use the  mongodb-enterprise.yaml \nsample   file from the MongoDB Enterprise Kubernetes Operator GitHub\nrepository. Set the  spec.template.spec.containers.name.env.name:WATCH_NAMESPACE  in\n mongodb-enterprise.yaml \nto the comma-separated list of namespaces that you would like\nthe   to watch: Run the following command and replace the values in the last line\nwith the namespaces that you would like the   to\nwatch. If you set the scope for the   to all   instead\nof the default  mongodb  namespace, you can configure the  \nto watch   in all namespaces in the  . Use the  mongodb-enterprise.yaml \nsample   file from the MongoDB Enterprise Kubernetes\nOperator GitHub repository. Set the  spec.template.spec.containers.name.env.name:WATCH_NAMESPACE \nin  mongodb-enterprise.yaml \nto  \"*\" . You must include the double quotation marks ( \" )\naround the asterisk ( * ) in the YAML file. Run the following command: Follow the steps in this procedure to verify that service FQDNs are\nreachable across   clusters. In this example, you deploy a sample application defined in\n sample-service.yaml \nacross two   clusters. Create a namespace in each of the   clusters to deploy the  sample-service.yaml . In certain service mesh solutions, you might need to annotate\nor label the namespace. Deploy the  sample-service.yaml  in both   clusters. Deploy the sample application on  CLUSTER_1 . Check that the  CLUSTER_1  hosting Pod is in the  Running  state. Deploy the sample application on  CLUSTER_2 . Check that the  CLUSTER_2  hosting Pod is in the  Running  state. Deploy the Pod in  CLUSTER_1  and check that you can reach the sample application in  CLUSTER_2 . You should see output similar to this example: Deploy the Pod in  CLUSTER_2  and check that you can reach the sample application in  CLUSTER_1 . You should see output similar to this example: If you plan to secure your multi-cluster MongoDB deployment using  \nencryption, complete the following tasks: To enable internal cluster authentication, create certificates for\nmember clusters in the  . Generate one   certificate covering the  s of all the member\nclusters in the  MongoDBMulti  resource. For each   service that the   generates corresponding\nto each Pod in each member cluster, add  s to the certificate.\nIn your   certificate, the   for each   service must\nuse the following format: where  n  ranges from  0  to  clusterSpecList[member_cluster_index].members - 1 . Generate one TLS certificate for your project's MongoDB Agents. For the MongoDB Agent   certificate: The Common Name in the   certificate must not be empty. The combined Organization and Organizational Unit in each  \ncertificate must differ from the Organization and Organizational\nUnit in the   certificate for your replica set members. You must possess the   certificate and the key that you used to\nsign your   certificates. The   uses  kubernetes.io/tls  secrets\nto store   certificates and private keys for   and MongoDB\nresources. Starting in   version 1.17.0, the\n  doesn't support concatenated   files stored as\n Opaque secrets . If you have a broken Application Database after upgrading to\n  version 1.14.0 or 1.15.0, see\n  in Failed State .",
            "code": [
                {
                    "lang": "sh",
                    "value": "export MDB_GKE_PROJECT={GKE project name}\n\nexport MDB_CENTRAL_CLUSTER=\"mdb-central\"\nexport MDB_CENTRAL_CLUSTER_ZONE=\"us-west1-a\"\n\nexport MDB_CLUSTER_1=\"mdb-1\"\nexport MDB_CLUSTER_1_ZONE=\"us-west1-b\"\n\nexport MDB_CLUSTER_2=\"mdb-2\"\nexport MDB_CLUSTER_2_ZONE=\"us-east1-b\"\n\nexport MDB_CLUSTER_3=\"mdb-3\"\nexport MDB_CLUSTER_3_ZONE=\"us-central1-a\"\n\nexport MDB_CENTRAL_CLUSTER_FULL_NAME=\"gke_${MDB_GKE_PROJECT}_${MDB_CENTRAL_CLUSTER_ZONE}_${MDB_CENTRAL_CLUSTER}\"\n\nexport MDB_CLUSTER_1_FULL_NAME=\"gke_${MDB_GKE_PROJECT}_${MDB_CLUSTER_1_ZONE}_${MDB_CLUSTER_1}\"\nexport MDB_CLUSTER_2_FULL_NAME=\"gke_${MDB_GKE_PROJECT}_${MDB_CLUSTER_2_ZONE}_${MDB_CLUSTER_2}\"\nexport MDB_CLUSTER_3_FULL_NAME=\"gke_${MDB_GKE_PROJECT}_${MDB_CLUSTER_3_ZONE}_${MDB_CLUSTER_3}\""
                },
                {
                    "lang": "sh",
                    "value": "gcloud container clusters create $MDB_CENTRAL_CLUSTER \\\n  --zone=$MDB_CENTRAL_CLUSTER_ZONE \\\n  --num-nodes=5 \\\n  --machine-type \"e2-standard-2\""
                },
                {
                    "lang": "sh",
                    "value": "gcloud container clusters create $MDB_CLUSTER_1 \\\n  --zone=$MDB_CLUSTER_1_ZONE \\\n  --num-nodes=5 \\\n  --machine-type \"e2-standard-2\""
                },
                {
                    "lang": "sh",
                    "value": "gcloud container clusters create $MDB_CLUSTER_2 \\\n  --zone=$MDB_CLUSTER_2_ZONE \\\n  --num-nodes=5 \\\n  --machine-type \"e2-standard-2\""
                },
                {
                    "lang": "sh",
                    "value": "gcloud container clusters create $MDB_CLUSTER_3 \\\n  --zone=$MDB_CLUSTER_3_ZONE \\\n  --num-nodes=5 \\\n  --machine-type \"e2-standard-2\""
                },
                {
                    "lang": "sh",
                    "value": "gcloud container clusters get-credentials $MDB_CENTRAL_CLUSTER \\\n  --zone=$MDB_CENTRAL_CLUSTER_ZONE\n\ngcloud container clusters get-credentials $MDB_CLUSTER_1 \\\n  --zone=$MDB_CLUSTER_1_ZONE\n\ngcloud container clusters get-credentials $MDB_CLUSTER_2 \\\n  --zone=$MDB_CLUSTER_2_ZONE\n\ngcloud container clusters get-credentials $MDB_CLUSTER_3 \\\n  --zone=$MDB_CLUSTER_3_ZONE"
                },
                {
                    "lang": "sh",
                    "value": "cd tools/multicluster\ngo run main.go \\\n -central-cluster=\"e2e.operator.mongokubernetes.com\" \\\n -member-clusters=\"e2e.cluster1.mongokubernetes.com,e2e.cluster2.mongokubernetes.com,e2e.cluster3.mongokubernetes.com\" \\\n -member-cluster-namespace=\"mongodb2\" \\\n -central-cluster-namespace=\"mongodb2\" \\\n -cluster-scoped=\"true\""
                },
                {
                    "lang": "sh",
                    "value": "WATCH_NAMESPACE: \"$namespace1,$namespace2,$namespace3\""
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade \\\n  --install \\\n  mongodb-enterprise-operator-multi-cluster \\\n  mongodb/enterprise-operator \\\n  --namespace mongodb \\\n  --set namespace=mongodb \\\n  --version <mongodb-kubernetes-operator-version>\\\n  --set operator.name=mongodb-enterprise-operator-multi-cluster \\\n  --set operator.createOperatorServiceAccount=false \\\n  --set \"multiCluster.clusters=$MDB_CLUSTER_1_FULL_NAME,$MDB_CLUSTER_2_FULL_NAME,$MDB_CLUSTER_3_FULL_NAME\"\n  --set operator.watchNamespace=\"$namespace1,$namespace2,$namespace3\""
                },
                {
                    "lang": "sh",
                    "value": "WATCH_NAMESPACE: \"*\""
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade \\\n  --install \\\n  mongodb-enterprise-operator-multi-cluster \\\n  mongodb/enterprise-operator \\\n  --namespace mongodb \\\n  --set namespace=mongodb \\\n  --version <mongodb-kubernetes-operator-version>\\\n  --set operator.name=mongodb-enterprise-operator-multi-cluster \\\n  --set operator.createOperatorServiceAccount=false \\\n  --set \"multiCluster.clusters=$MDB_CLUSTER_1_FULL_NAME,$MDB_CLUSTER_2_FULL_NAME,$MDB_CLUSTER_3_FULL_NAME\"\n  --set operator.watchNamespace=\"*\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create --context=\"${CTX_CLUSTER_1}\" namespace sample\nkubectl create --context=\"${CTX_CLUSTER_2}\" namespace sample"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply --context=\"${CTX_CLUSTER_1}\" \\\n   -f sample-service.yaml \\\n   -l service=helloworld1 \\\n   -n sample\n\nkubectl apply --context=\"${CTX_CLUSTER_2}\" \\\n   -f sample-service.yaml \\\n   -l service=helloworld2 \\\n   -n sample"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply --context=\"${CTX_CLUSTER_1}\" \\\n  -f sample-service.yaml \\\n  -l version=v1 \\\n  -n sample"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pod --context=\"${CTX_CLUSTER_1}\" \\\n  -n sample \\\n  -l app=helloworld"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply --context=\"${CTX_CLUSTER_2}\" \\\n  -f sample-service.yaml \\\n  -l version=v2 \\\n  -n sample"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pod --context=\"${CTX_CLUSTER_2}\" \\\n  -n sample \\\n  -l app=helloworld"
                },
                {
                    "lang": "sh",
                    "value": "kubectl run  --context=\"${CTX_CLUSTER_1}\" \\\n  -n sample \\\n  curl --image=radial/busyboxplus:curl \\\n  -i --tty \\\n  curl -sS helloworld2.sample:5000/hello"
                },
                {
                    "lang": "sh",
                    "value": "Hello version: v2, instance: helloworld-v2-758dd55874-6x4t8"
                },
                {
                    "lang": "sh",
                    "value": "kubectl run --context=\"${CTX_CLUSTER_2}\" \\\n  -n sample \\\n  curl --image=radial/busyboxplus:curl \\\n  -i --tty \\\n  curl -sS helloworld1.sample:5000/hello"
                },
                {
                    "lang": "sh",
                    "value": "Hello version: v1, instance: helloworld-v1-758dd55874-6x4t8"
                },
                {
                    "lang": "none",
                    "value": "<metadata.name>-<member_cluster_index>-<n>-svc.<namespace>.svc.cluster.local"
                }
            ],
            "preview": "This Quick Start tutorial requires that you complete these tasks:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "configure-k8s-operator-for-mdb-resources",
            "title": "Configure the  for MongoDB Database Resources",
            "headings": [],
            "paragraphs": "Create a   so the   can create and update\n  in your   Project. Create a   to link the   to your  \nProject. Create an X.509 certificate to connect to an X.509-enabled\nMongoDB deployment.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "multi-cluster-arch",
            "title": "Multi-Cluster Architecture",
            "headings": [
                "Features Not Available in the Beta Release",
                "Multi-Cluster Capabilities",
                "Multi-Cluster Deployment Architecture"
            ],
            "paragraphs": "The following features of the   and the underlying  \nclusters are not available in the beta release of the  : This feature is a beta release. Use  \ndeployments only in development environments. Sharded cluster deployments  version earlier than 5.0.7 This section describes the multi-cluster capabilities that you can\nconfigure using the same procedures as for single clusters deployed with\nthe  . Other multi-cluster capabilities have their own\ndocumentation in this guide. Capability Description Use   records for MongoDB access in  To connect to the   database as a user, you can use\nthe  connectionString.standardSrv :  DNS seed list connection string .\nThis string is included in the secret that the   creates\nfor your  . Use the same procedure for connecting to\nthe   as for single clusters deployed with  .\nSee  Connect to a MongoDB Database Resource from Inside Kubernetes ,\nand select the tab  Using the Kubernetes Secret . Secure database users in  Manage database users using: These procedures are the same as for single clusters\ndeployed with  , with the following exceptions: LDAP authentication SCRAM authentication TLS and X.509 for internal cluster authentication The procedures apply to replica sets only. Multi-cluster\ndeployments  do not support creating sharded clusters . In the  mongodbResourceRef , specify the name of the multi-cluster\nreplica set:  name: \"<my-multi-cluster-replica-set>\" . Configure queryable backups for   resources If you deploy   with the  , the central\ncluster may also host  . In this case, you can\n configure queryable backups \nfor   resources. The   runs in a central   cluster. The central cluster holds the  MongoDBMulti  CustomResource spec for\nthe MongoDB replica set.\nThe member   clusters host the MongoDB replica sets. If you deploy   with the  , the central cluster may also host  .\n  manages the discovery of MongoDB nodes deployed in different\n  member clusters. You can host your application on any of the member clusters inside the\nIstio service mesh, either on   clusters outside of the ones that you\ndeploy with the  , or on the member clusters that you deploy\nas part of this tutorial. The   performs these actions: The following diagram shows the high-level architecture of a  \nacross regions and availability zones: Identifies the cluster on which to deploy the MongoDB replica set\nusing the corresponding  MongoDBMulti  CustomResource spec, and\ndeploys the MongoDB replica sets. Watches for the  MongoDBMulti  CustomResource spec creation in the\ncentral cluster. Uses the mounted  kubeconfig  file to communicate with member clusters. Watches for the  CentralCluster  and  MemberCluster  events to\nconfirm that the   is in the desired state. Reconciles resources. Creates the necessary resources, such as\nConfigmaps, Secrets, Service objects, and Statefulset objects in\neach member cluster corresponding to the number of replica set members\nin the MongoDB cluster. To avoid storing secrets in  , you can migrate all  \nto a  secret storage tool .",
            "code": [],
            "preview": "The following features of the  and the underlying \nclusters are not available in the beta release of the :",
            "tags": null,
            "facets": null
        },
        {
            "slug": "multi-cluster",
            "title": "Deploy MongoDB Resources across Multiple Kubernetes Clusters (Beta)",
            "headings": [
                "Overview",
                "Central Cluster and Member Clusters"
            ],
            "paragraphs": "This feature is a beta release. Use  \ndeployments only in development environments. Using  , you can deploy   to manage\nMongoDB deployments that span two or more   clusters. During the\nbeta, the   supports deploying only replica sets across two or\nmore   clusters. Deploying sharded clusters across two or\nmore   clusters is not supported. The beta release of the   enables different levels of resilience, depending on the needs of your enterprise application:  deployments allow you to add MongoDB instances\nin global clusters that span multiple geographic regions for increased\navailability and global distribution of data. A service mesh is required to enable inter-cluster communication between\nthe replica set members deployed in different   clusters. MongoDB\ndevelopment has tested this feature using  , but any service mesh\nthat provides FQDN resolution between Pods across clusters should work. Single Region, Multi AZ . One or more   clusters where each\ncluster has nodes deployed in different zones in the same region.\nSuch deployments protect MongoDB instances backing your enterprise\napplications against zone and   cluster failures and offer increased\navailability, disaster recovery, and data distribution within one\ncloud region. Multi Region . One or more   clusters where you deploy each\ncluster in a different region, and within each region, deploy cluster\nnodes in different availability zones. This gives your database\nresilience against the loss of a   cluster, a zone, or an entire\ncloud region. MongoDB recommends that you identify one cluster to act as\na  central cluster . The central cluster hosts the   and\nacts as the control plane for the multi-cluster deployment. This central\ncluster can also host replica set members. This documentation refers to\nother   clusters that host replica set members as  member clusters . Communication between replica\nset members occurs over a service mesh, which means that your database\ndoesn't rely on the central cluster to function. Note that if the\ncentral cluster fails, you can't use the   to change your\ndeployment until access to this cluster is restored or until you\nredeploy the   to an available   cluster. You can host your application on any   cluster in the\nservice mesh. Your application can be co-located on a\nmember cluster with one of the replica set nodes that you deployed using\nthe  , or you can host your application on a cluster that\ndoesn't host replica set nodes or the  . To learn more, see the  Multi-Cluster Deployment Architecture .",
            "code": [],
            "preview": "Using , you can deploy  to manage\nMongoDB deployments that span two or more  clusters. During the\nbeta, the  supports deploying only replica sets across two or\nmore  clusters. Deploying sharded clusters across two or\nmore  clusters is not supported.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "om-resources",
            "title": "Deploy and Configure Ops Manager Resources",
            "headings": [],
            "paragraphs": "Review the   resource architecture. Review the   resource considerations and prerequisites. Use the   to deploy an   instance and encrypt the\nconnection between the application database's replica set members. Use the   to configure   to operate in  Remote  mode.\nIn Remote mode, the Backup Daemons and managed MongoDB resources download\ninstallation archives from HTTP endpoints on a web server or S3-compatible\nfile store deployed to your   cluster instead of from the Internet. Use the   to configure   to operate in  Local  mode.\nIn Local mode, the Backup Daemons and managed MongoDB resources download\ninstallation archives from a   that you create for the  \nStatefulSet instead of from the Internet. Upgrade the versions of your   instance and  backing databases \nthat the   uses to manage your deployment. Configure queryable backups for   deployments created with the  . Configure backup snapshot storage for   resources created with the  .",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "multi-cluster-connect",
            "title": "Access Resources in Multi-Cluster Deployments",
            "headings": [],
            "paragraphs": "The following section describes how to connect to a\n MongoDBMulti  resource that is deployed to  : Connect to Multi-Cluster Resource from Outside Kubernetes \nConnect to a  MongoDBMulti  resource from outside of the   clusters. To connect to a  MongoDBMulti  resource from within the   clusters,\nsee  Connect to a MongoDB Database Resource from Inside Kubernetes ,\nand select the tab  Using the Kubernetes Secret . This procedure\nis the same as for single clusters deployed with the  .",
            "code": [],
            "preview": "The following section describes how to connect to a\nMongoDBMulti resource that is deployed to :",
            "tags": null,
            "facets": null
        },
        {
            "slug": "third-party-licenses",
            "title": "Third-Party Licenses",
            "headings": [
                "Apache License 2.0",
                "BSD (Berkeley Software Distribution) 2-Clause",
                "BSD (Berkeley Software Distribution) 3-Clause",
                "ISC (Internet Systems Consortium) License",
                "MIT (Massachusetts Institute of Technology) License",
                "MPL (Mozilla Public License) 2.0"
            ],
            "paragraphs": "MongoDB   uses third-party libraries or other resources\nthat may be distributed under licenses different than the MongoDB\nsoftware.  depends upon the following third-party packages. These\npackages are licensed as shown in the following list. Should MongoDB\nhave accidentally failed to list a required license, please\n contact the MongoDB Legal Department . License:  TL;DR  |  Full Text Package Version github.com/go-logr/logr v1.2.3 github.com/golang/groupcache/lru v0.0.0-20210331224755-41bb18bfe9da github.com/google/gofuzz v1.1.0 github.com/googleapis/gnostic v0.5.7-v3refs github.com/go-openapi/jsonpointer v0.19.5 github.com/go-openapi/jsonreference v0.19.5 github.com/go-openapi/swag v0.19.14 github.com/matttproud/golang_protobuf_extensions/pbutil v1.0.2-0.20181231171920-c182affec369 github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd github.com/modern-go/reflect2 v1.0.2 github.com/oklog/run v1.0.0 github.com/prometheus/client_golang/prometheus v1.13.0 github.com/prometheus/client_model/go v0.2.0 github.com/prometheus/common v0.37.0 github.com/prometheus/procfs v0.8.0 github.com/xdg/stringprep v1.0.3 gomodules.xyz/jsonpatch/v2 v2.2.0 google.golang.org/genproto/googleapis/rpc/status v0.0.0-20220628213854-d9e0b6570c03 google.golang.org/grpc v1.47.0 gopkg.in/square/go-jose.v2 v2.5.1 gopkg.in/yaml.v2 v2.4.0 k8s.io/api v0.24.3 k8s.io/apiextensions-apiserver/pkg/apis/apiextensions v0.24.2 k8s.io/apimachinery/pkg v0.24.3 k8s.io/client-go v0.24.3 k8s.io/component-base/config v0.24.2 k8s.io/klog/v2 v2.60.1 k8s.io/kube-openapi/pkg v0.0.0-20220328201542-3ee0da9b0b42 k8s.io/kube-openapi/pkg/validation/spec v0.0.0-20220328201542-3ee0da9b0b42 k8s.io/utils v0.0.0-20220210201930-3a6ce19ff2f9 sigs.k8s.io/controller-runtime v0.12.3 sigs.k8s.io/json v0.0.0-20211208200746-9f7c6b3444d2 sigs.k8s.io/structured-merge-diff/v4 v4.2.1 License:  TL;DR  |  Full Text Package Version github.com/pkg/errors v0.9.1 License:  TL;DR  |  Full Text Package Version github.com/evanphx/json-patch v5.6.0 github.com/fsnotify/fsnotify v1.5.1 github.com/gogo/protobuf v1.3.2 github.com/golang/protobuf v1.5.2 github.com/golang/snappy v0.0.4 github.com/google/go-cmp/cmp v0.5.8 github.com/google/uuid v1.3.0 github.com/imdario/mergo v0.3.13 github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 github.com/pierrec/lz4 v2.5.2 github.com/pmezard/go-difflib/difflib v1.0.0 github.com/prometheus/common/internal/bitbucket.org/ww/goautoneg v0.37.0 github.com/PuerkitoBio/purell v1.1.1 github.com/PuerkitoBio/urlesc v0.0.0-20170810143723-de5bf2ad4578 github.com/spf13/pflag v1.0.5 golang.org/x/crypto golang.org/x/net golang.org/x/oauth2 golang.org/x/sys golang.org/x/term golang.org/x/text golang.org/x/time/rate google.golang.org/protobuf v1.28.1 gopkg.in/inf.v0 v0.9.1 gopkg.in/square/go-jose.v2 v2.5.1 k8s.io/apimachinery/third_party/forked/golang v0.24.3 k8s.io/utils/internal/third_party/forked/golang/net v0.0.0-20220210201930-3a6ce19ff2f9 License:  TL;DR  |  Full Text Package Version github.com/davecgh/go-spew/spew v1.1.1 License:  TL;DR  |  Full Text Package Version github.com/armon/go-metrics v0.3.9 github.com/armon/go-radix v1.0.0 github.com/beorn7/perks/quantile v1.0.1 github.com/blang/semver v3.5.1 github.com/cenkalti/backoff/v3 v3.0.0 github.com/cespare/xxhash/v2 v2.1.2 github.com/emicklei/go-restful v2.9.6 github.com/fatih/color v1.7.0 github.com/ghodss/yaml v1.0.0 github.com/hashicorp/go-hclog v0.16.2 github.com/josharian/intern v1.0.0 github.com/json-iterator/go v1.1.12 github.com/mailru/easyjson v0.7.6 github.com/mattn/go-colorable v0.1.6 github.com/mattn/go-isatty v0.0.12 github.com/mitchellh/copystructure v1.0.0 github.com/mitchellh/go-homedir v1.1.0 github.com/mitchellh/go-testing-interface v1.0.0 github.com/mitchellh/mapstructure v1.5.0 github.com/mitchellh/reflectwalk v1.0.0 github.com/ryanuber/go-glob v1.0.0 github.com/spf13/cast v1.5.0 github.com/stretchr/objx v0.4.0 github.com/stretchr/testify/assert v1.8.0 go.uber.org/atomic v1.9.0 go.uber.org/multierr v1.6.0 go.uber.org/zap v1.22.0 gopkg.in/yaml.v3 v3.0.1 sigs.k8s.io/yaml v1.3.0 License:  TL;DR  |  Full Text Package Version github.com/hashicorp/errwrap v1.1.0 github.com/hashicorp/go-cleanhttp v0.5.2 github.com/hashicorp/go-immutable-radix v1.3.1 github.com/hashicorp/go-multierror v1.1.1 github.com/hashicorp/go-plugin v1.4.3 github.com/hashicorp/go-retryablehttp v0.7.1 github.com/hashicorp/go-rootcerts v1.0.2 github.com/hashicorp/go-secure-stdlib/mlock v0.1.1 github.com/hashicorp/go-secure-stdlib/parseutil v0.1.6 github.com/hashicorp/go-secure-stdlib/strutil v0.1.2 github.com/hashicorp/go-sockaddr v1.0.2 github.com/hashicorp/go-uuid v1.0.2 github.com/hashicorp/go-version v1.2.0 github.com/hashicorp/golang-lru v0.5.4 github.com/hashicorp/hcl v1.0.0 github.com/hashicorp/vault/api v1.7.2 github.com/hashicorp/vault/sdk v0.5.1 github.com/hashicorp/yamux v0.0.0-20180604194846-3520598351bb",
            "code": [],
            "preview": "MongoDB  uses third-party libraries or other resources\nthat may be distributed under licenses different than the MongoDB\nsoftware.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/k8s-op-exclusive-settings",
            "title": "MongoDB  Exclusive Settings",
            "headings": [
                "Kubernetes Operator Overrides Some Ops Manager Settings"
            ],
            "paragraphs": "At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . Some settings that you configure using   cannot be set or\noverridden in the  . Settings that the   does\nnot manage are accepted. The following list of settings are exclusive to  . This list may\nchange at a later date. These settings can be found on the\n Automation Configuration \npage: In addition to the list of Automation settings, the   uses attributes\noutside of the deployment from the Monitoring and Backup Agent configurations. auth.autoAuthMechanisms auth.authoritativeSet auth.autoPwd auth.autoUser auth.deploymentAuthMechanisms auth.disabled auth.key auth.keyfile auth.keyfileWindows auth.usersWanted auth.usersWanted[n].mechanisms auth.usersWanted[n].roles auth.usersWanted[n].roles[m].role auth.usersWanted[n].roles[m].db auth.usersWanted[n].user auth.usersWanted[n].authenticationRestrictions processes.args2_6.net.port processes.args2_6.net.tls.certificateKeyFile processes.args2_6.net.tls.clusterFile processes.args2_6.net.tls.PEMKeyFile processes.args2_6.replication.replSetName processes.args2_6.sharding.clusterRole processes.args2_6.security.clusterAuthMode processes.args2_6.storage.dbPath processes.args2_6.systemLog.destination processes.args2_6.systemLog.path processes.authSchemaVersion processes.cluster  (mongos processes) processes.featureCompatibilityVersion processes.hostname processes.name processes.version replicaSets._id replicaSets.members._id replicaSets.members.host replicaSets.members replicaSets.version sharding.clusterRole  (config server) sharding.configServerReplica sharding.name sharding.shards._id sharding.shards.rs ssl.CAFilePath ssl.autoPEMKeyFilePath ssl.clientCertificateMode backupAgentTemplate.username backupAgentTemplate.sslPEMKeyFile monitoringAgentTemplate.username monitoringAgentTemplate.sslPEMKeyFile  creates a replica set of 3 members. You changed  storage.wiredTiger.engineConfig.cacheSizeGB \nto  40 . This setting is not in the   exclusive settings\nlist. You then use the   to scale the replica set to\n5 members. The  storage.wiredTiger.engineConfig.cacheSizeGB  on the\nnew members should still be  40 .",
            "code": [],
            "preview": "Some settings that you configure using  cannot be set or\noverridden in the . Settings that the  does\nnot manage are accepted.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/helm-operator-settings",
            "title": " Helm Installation Settings",
            "headings": [
                "appDb.name",
                "appDb.version",
                "database.name",
                "database.version",
                "initAppDb.name",
                "initAppDb.version",
                "initDatabase.name",
                "initDatabase.version",
                "initOpsManager.name",
                "initOpsManager.version",
                "managedSecurityContext",
                "namespace",
                "needsCAInfrastructure",
                "operator.deployment_name",
                "operator.env",
                "operator.name",
                "operator.version",
                "operator.watchNamespace",
                "operator.watchedResources",
                "opsManager.name",
                "registry.appDb",
                "registry.imagePullSecrets",
                "registry.initAppDb",
                "registry.initOpsManager",
                "registry.operator",
                "registry.opsManager",
                "subresourceEnabled"
            ],
            "paragraphs": "To provide optional settings, pass them to Helm using the  --set  argument.\nUse the following files that list value settings for your deployment type: To learn about optional   installation settings,\nsee  Operator Helm Installation Settings . Run the command as in the following example and the options that you\nspecified will be passed to your configuration: Vanilla  :  OpenShift:  Name of the Application Database image. The default value is  mongodb-enterprise-appdb . Version of the image that contains the MongoDB Agent that the Application\nDatabase uses. The default value is 10.2.15.5958-1_4.2.11-ent. Name of the MongoDB Enterprise Database image. The default value is  mongodb-enterprise-database . Version of the MongoDB Enterprise Database image that the  \ndeploys. Name of the  initContainer  image that contains the Application\nDatabase start-up scripts and the readiness probe. The default value is  mongodb-enterprise-init-appdb . Version of the  initContainer  image that contains the Application\nDatabase start-up scripts and the readiness probe. The default value is  1.0.6 . Name of the  initContainer  image that contains the MongoDB Agent\nstart-up scripts and the readiness probe. The default value is  mongodb-enterprise-init-database . Version of the  initContainer  image that contains the MongoDB Agent\nstart-up scripts and the readiness probe. The default value is  1.0.2 . Version of the  initContainer  image that contains the  \nstart-up scripts and the readiness probe. The default value is  mongodb-enterprise-init-ops-manager . Version of the  initContainer  image that contains the  \nstart-up scripts and the readiness probe. The default value is  1.0.3 . Flag that determines whether or not the   inherits the\n securityContext  settings that your   cluster manages. This value must be  true  if you want to run the  \nin OpenShift or in a restrictive environment. The default value is  false . The default value is  true .  in which you want to deploy the  . To use a namespace other than the default, specify the namespace in\nwhich you want to deploy the  . The default value is  mongodb . Flag that determines whether   creates a   that allows the\n  to sign   certificates using the\n certificates.k8s.io \nAPI. The default value is  true . Name of the   container. The default value is  mongodb-enterprise-operator . Label for the  s deployment environment. This value\naffects the default timeouts and the logging level and format: The default value is  prod . If the value is Log Level is set to Log Format is set to dev debug text prod info json Name that   assigns to   objects, such as Deployments,\nServiceAccounts, Roles, and Pods. This value also corresponds to the name of the container registry where\nthe   is located. The default value is  mongodb-enterprise-operator . Version of the   that you want to deploy. The default value is  \u200b . Namespaces that the   watches for  \nchanges. If this   differs from the default, ensure that\nthe   ServiceAccount  can access \nthis namespace. The default value is  <metadata.namespace> . To watch  all namespaces , specify  *  and assign the   to the\n mongodb-enterprise-operator  ServiceAccount that you use to run the\n . To watch a  subset of all namespaces , specify them in a\ncomma-separated list, escape each comma with a backslash,\nand surround the list in quotes, such as\n \"operator.watchNamespace=ns1\\,ns2\" . Watching a subset of namespaces is useful in deployments with\nmultiple   instances, where each   instance\nwatches a different subset of namespaces in your cluster. To deploy   and   to one or more   other\nthan the one where you deploy the  ,\nsee  Set Scope for   Deployment  for values you must use and\nadditional steps you might have to perform. Custom resources that the   watches. The   installs the   for and watches only the resources you specify. The   accepts the following values: Value Description mongodb Install the CustomResourceDefinitions for database resources\nand watch those resources. mongodbusers Install the CustomResourceDefinitions for MongoDB user resources\nand watch those resources. opsmanagers Install the CustomResourceDefinitions for   resources\nand watch those resources. Name of the   image. The default value is  mongodb-enterprise-ops-manager .  of the repository from which the   downloads\nthe Application Database image. The default value is  quay.io/mongodb . The default value is  registry.connect.redhat.com/mongodb .  that contains the credentials required to pull\nimages from the repository. OpenShift requires this setting. Define it in the\n imagePullSecrets  setting in this file or pass it when you install\nthe   using Helm.\nIf you use the   to deploy MongoDB resources to\n multiple namespaces  or with a\n cluster-wide scope , create the secret\nonly in the namespace where you installed the  .\nThe   synchronizes the secret across all watched\nnamespaces.  of the repository from which the  initContainer  image that\ncontains the Application Database start-up scripts and the readiness\nprobe is downloaded. The default value is  quay.io/mongodb . The default value is  registry.connect.redhat.com/mongodb .  of the repository from which the  initContainer  image that\ncontains the   start-up scripts and the readiness probe is\ndownloaded. The default value is  quay.io/mongodb . The default value is  registry.connect.redhat.com/mongodb . Repository from which the   image is pulled.\nSpecify this value if you want to pull the   image\nfrom a private repository. The default value is  quay.io/mongodb . The default value is  registry.connect.redhat.com/mongodb .  of the repository from which the image for an  Ops\nManager resource  is downloaded. The default value is  quay.io/mongodb . The default value is  registry.connect.redhat.com/mongodb . Flag that indicates whether subresources can be defined in the\n   . The default value is  true .",
            "code": [
                {
                    "lang": "sh",
                    "value": "helm upgrade enterprise-operator mongodb/enterprise-operator \\\n  --set registry.pullPolicy='IfNotPresent'"
                },
                {
                    "lang": "yaml",
                    "value": "appDb:\n  name: mongodb-enterprise-appdb\n  version: 10.2.15.5958-1_4.2.11-ent"
                },
                {
                    "lang": "yaml",
                    "value": "appDb:\n  name: mongodb-enterprise-appdb\n  version: 10.2.15.5958-1_4.2.11-ent"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-database\n  version: 2.0.0"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-database\n  version: 2.0.0"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-appdb\n  version: 1.0.6"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-appdb\n  version: 1.0.6"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-database\n  version: 1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-database\n  version: 1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-ops-manager\n  version: 1.0.3"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-ops-manager\n  version: 1.0.3"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: true"
                },
                {
                    "lang": "yaml",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "needsCAInfrastructure: true"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  deployment_name: mongodb-enterprise-operator"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  # Execution environment for the operator, dev or prod.\n  # Use dev for more verbose logging\n  env: prod"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  name: mongodb-enterprise-operator"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  version: \u200b"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchedResources:\n    - mongodbusers\n    - mongodb\n    - opsmanagers"
                },
                {
                    "lang": "yaml",
                    "value": "opsManager:\n  name: mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n# Specify the secret in the ``imagePullSecrets`` setting. If you\n# use the MongoDB Kubernetes Operator to deploy MongoDB resources\n# into multiple namespaces, create the secret only in the namespace\n# where you installed the Operator. The Operator synchronizes\n# the secret across all watched namespaces.\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "subresourceEnabled: true"
                }
            ],
            "preview": "To provide optional settings, pass them to Helm using the --set argument.\nUse the following files that list value settings for your deployment type:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/troubleshooting",
            "title": "Troubleshoot the ",
            "headings": [
                "Get Status of a Deployed Resource",
                "Review the Logs",
                "Review Logs from the ",
                "Find a Specific Pod",
                "Review Logs from Specific Pod",
                "View All  Specifications",
                "Restore StatefulSet that Failed to Deploy",
                "Replace a ConfigMap to Reflect Changes",
                "Remove  Components",
                "Remove a ",
                "Remove the ",
                "Remove the ",
                "Remove the ",
                "Create a New  after Deleting a Pod",
                "Disable  Feature Controls",
                "Debug a Failing Container",
                "Verify Corrrectness of Domain Names in TLS Certificates",
                "Verify the MongoDB Version when Running in Local Mode",
                "Upgrade Fails Using kubectl or oc",
                "Upgrade Fails Using Helm Charts",
                "Two Operator Instances After an Upgrade",
                " in Failed State"
            ],
            "paragraphs": "To find the status of a resource deployed with the  ,\ninvoke one of the following commands: The following key-value pairs describe the resource deployment statuses: For   resource deployments: The  status.applicationDatabase.phase  field displays the\nApplication Database resource deployment status. The  status.backup.phase  displays the backup daemon resource\ndeployment status. The  status.opsManager.phase  field displays the   resource\ndeployment status. The   controller watches the database resources\ndefined in the following settings: spec.backup.opLogStores spec.backup.s3Stores spec.backup.blockStores For MongoDB resource deployments: The  status.phase  field displays the MongoDB resource deployment\nstatus. Key Value message Message explaining why the resource is in a  Pending  or\n Failed  state. phase Status Meaning Pending The   is unable to reconcile the resource\ndeployment state. This happens when a reconciliation\ntimes out or if the   requires you to take\naction for the resource to enter a running state. If a resource is pending because a reconciliation timed\nout, the   attempts to reconcile the\nresource state in 10 seconds. Reconciling The   is reconciling the resource state. Resources enter this state after you create or update\nthem or if the   is attempting to reconcile\na resource previously in a  Pending  or  Failed \nstate. The   attempts to reconcile the resource\nstate in 10 seconds. Running The resource is running properly. Failed The resource is not running properly. The  message \nfield provides additional details. The   attempts to reconcile the resource\nstate in 10 seconds. lastTransition  when the last reconciliation happened. link Deployment   in  . backup.statusName If you enabled continuous backups with  spec.backup.mode \nin   for your MongoDB resource, this field indicates\nthe status of the backup, such as  backup.statusName:\"STARTED\" .\nPossible values are  STARTED ,  STOPPED , and  TERMINATED . Resource specific fields For descriptions of these fields, see\n MongoDB Database Resource Specification . To see the status of a replica set named  my-replica-set  in\nthe  developer  namespace, run: If  my-replica-set  is running, you should see: If  my-replica-set  is not running, you should see: To review the   logs, invoke this command: You could check the  Ops Manager Logs  as\nwell to see if any issues were reported to  . To find which pods are available, invoke this command first: If you want to narrow your review to a specific  , you can\ninvoke this command: If your  replica set  is labeled  myrs , run: This returns the  Automation Agent Log  for this\nreplica set. To view all   specifications in the provided\n : To read details about the  dublin  standalone resource, run\nthis command: This returns the following response: A StatefulSet   may hang with a status of  Pending  if it\nencounters an error during deployment. Pending    do not automatically terminate, even if you\nmake  and apply  configuration changes to resolve the error. To return the StatefulSet to a healthy state, apply the configuration\nchanges to the MongoDB resource in the  Pending  state, then delete\nthose pods. A host system has a number of running  : my-replica-set-2  is stuck in the  Pending  stage. To gather\nmore data on the error, run: The output indicates an error in memory allocation. Updating the memory allocations in the MongoDB resource is\ninsufficient, as the pod does not terminate automatically after\napplying configuration updates. To remedy this issue, update the configuration, apply the\nconfiguration, then delete the hung pod: Once this hung pod is deleted, the other pods restart with your new\nconfiguration as part of rolling upgrade of the Statefulset. To learn more about this issue, see\n Kubernetes Issue 67250 . If you cannot modify or redeploy an already-deployed resource\n ConfigMap  file using the  kubectl apply  command, run: This deletes and re-creates the  ConfigMap  resource file. This command is useful in cases where you want to make an immediate\nrecursive change, or you need to update resource files that cannot\nbe updated once initialized. To remove any component, you need the following permissions: Cluster Roles mongodb-enterprise-operator-mongodb-webhook mongodb-enterprise-operator-mongodb-certs Cluster Role Bindings mongodb-enterprise-operator-mongodb-webhook-binding mongodb-enterprise-operator-mongodb-certs To remove any instance that   deployed, you must use  . You can use only the   to remove  -deployed\ninstances. If you use   to remove the instance,   throws an\nerror. Deleting a MongoDB resource doesn't remove it from the   UI.\nYou must remove the resource from   manually. To learn more, see\n Remove a Process from Monitoring . Deleting a MongoDB resource for which you enabled backup doesn't\ndelete the resource's snapshots. You must  delete snapshots\nin Ops Manager . To remove a single MongoDB instance you created using  : To remove all MongoDB instances you created using  : To remove the  : Remove all Kubernetes resources : Remove the  : To remove the  : Remove all Kubernetes resources : Remove the  : To remove the  : Remove all Kubernetes resources : Remove the  : If you accidentally delete the MongoDB replica set Pod and its  ,\nthe   fails to reschedule the MongoDB Pod and issues\nthe following error message: To recover from this error, you must  manually create a new PVC \nwith the PVC object's name that corresponds to this replica set Pod,\nsuch as  data-<replicaset-pod-name> . When you manage an   project through the  , the\n  places the  EXTERNALLY_MANAGED_LOCK \n feature control policy \non the project. This policy disables certain features in the  \napplication that might compromise your   configuration. If\nyou need to use these blocked features, you can remove the policy\nthrough the  feature controls API ,\nmake changes in the   application, and then restore the original\npolicy through the  API . The following procedure enables you to use features in the  \napplication that are otherwise blocked by the  . Retrieve the feature control policies \nfor your   project. Save the response that the API returns. After you make changes in\nthe   application, you must add these policies back to\nthe project. Your response should be similar to: Note the highlighted fields and values in the following sample\nresponse. You must send these same fields and values in later\nsteps when you remove and add feature control policies. The  externalManagementSystem.version  field corresponds to the\n  version. You must send the exact same field value\nin your requests later in this task. Update \nthe  policies  array with an empty list: The previously blocked features are now available in the\n  application. The values you provide for the  externalManagementSystem \nobject, like the  externalManagementSystem.version  field, must\nmatch values that you received in the response in Step 1. Make your changes in the   application. Update \nthe  policies  array with the original feature control policies: The features are now blocked again, preventing you from making\nfurther changes through the   application. However, the\n  retains any changes you made in the  \napplication while features were available. The values you provide for the  externalManagementSystem \nobject, like the  externalManagementSystem.version  field, must\nmatch values that you received in the response in Step 1. A container might fail with an error that results in   restarting\nthat container in a loop. You may need to interact with that container to inspect files or run\ncommands. This requires you to prevent the container from restarting. In your preferred text editor, open the MongoDB resource you need to\nrepair. To this resource, add a  podSpec  collection that resembles the\nfollowing. The  sleep  command in the\n spec.podSpec.podTemplate.spec  instructs the container to\nwait for the number of seconds you specify. In this example, the\ncontainer will wait for 1 hour. Apply this change to the resource. Invoke the shell inside the container. A MongoDB replica set or sharded cluster may fail to reach\nthe  READY  state if the   certificate is invalid. When you  configure TLS  for MongoDB replica sets or sharded clusters, verify\nthat you specify a valid certificate. If you don't specify the correct Domain Name for each   certificate,\nthe    logs  may contain an error\nmessage similar to the following, where  foo.svc.local  is the\nincorrectly-specified Domain Name for the cluster member's Pod: To check whether you have correctly configured   certificates: To learn more about   certificate requirements, see the\nprerequisites on the  TLS-Encrypted Connections  tab in\n Deploy a Replica Set  or in  Deploy a Sharded Cluster . Each certificate should include a valid Domain Name. For each replica set or sharded cluster member, the Common Name, also\nknown as the Domain Name, for that member's certificate must match\nthe   of the pod this cluster member is deployed on. The   name in each certificate has the following syntax:\n pod-name.service-name.namespace.svc.cluster.local . This name is\ndifferent for each Pod hosting a member of the replica set or a\nsharded cluster. For example, for a member of a replica set deployed on a Pod with\nthe name  rs-mongos-0-0 , in the   service\nnamed  mongo-0  that is created in the default  mongodb \nnamespace, the   is: Run: Check for  -related messages in the   log files. MongoDB  CustomResource  may fail to reach a  Running  state\nif   is running in  Local Mode  and you specify either a version of MongoDB\nthat doesn't exist, or a valid version of MongoDB for which\n  deployed in local mode did not download a corresponding MongoDB archive. If you specify a MongoDB version that doesn't exist, or a valid MongoDB\nversion for which   could not download a MongoDB archive, then\neven though the Pods can reach the  READY  state,\nthe    logs  contain an\nerror message similar to the following: This may mean that the MongoDB Agent could not successfully download a\ncorresponding MongoDB binary to the  /var/lib/mongodb-mms-automation \ndirectory. In cases when the MongoDB Agent can download the MongoDB\nbinary for the specified MongoDB version successfully, this directory\ncontains a MongoDB binary folder, such as  mongodb-linux-x86_64-4.4.0 . To check whether a MongoDB binary folder is present: Specify the Pod's name to this command: Check whether a MongoDB binary folder is present in the\n /var/lib/mongodb-mms-automation  directory. If you cannot locate a MongoDB binary folder,\n copy the MongoDB archive \ninto the   Persistent Volume for each deployed   replica set. To resolve this error: You might receive the following error when you upgrade the\n : Remove the old   deployment. Removing the   deployment doesn\u2019t affect the lifecycle\nof your MongoDB resources. Repeat the  kubectl apply  command to upgrade to the new\nversion of the  . To resolve this error: You might receive the following error when you upgrade the\n : Remove the old   deployment. Removing the   deployment doesn\u2019t affect the lifecycle\nof your MongoDB resources. Repeat the  helm  command to upgrade to the new version of the\n . After you upgrade from   version 1.10 or earlier to a\nversion 1.11 or later, your   cluster might have two instances of\nthe   deployed. Use the  get pods  command to view your   pods: If the response contains both an  enterprise-operator  and a\n mongodb-enterprise-operator  pod, your cluster has two  \ninstances: You can safely remove the  enterprise-operator  deployment. Run the\nfollowing command to remove it: If you deployed the   to OpenShift, replace the\n kubectl  commands in this section with  oc  commands. However, if you have a broken Application Database after upgrading to\n  version 1.14.0 or 1.15.0, do the following steps:  version  1.15.1  fixes an issue\nthat prevented the   upgrade when managing a\nTLS-enabled Application Database whose TLS certificate is stored in an\n Opaque \nsecret. We recommend that you upgrade to   version 1.15.1\nor later. We strongly advise against upgrading to   version 1.14.0\nor 1.15.0. Run the following command to scale your   deployment to\nzero replicas: Run the following command to delete the AppDB StatefulSet: Run the following command to remove the Automation Config secret: Upgrade the   to version 1.15.1 or later.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl get <resource-name> -n <metadata.namespace> -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <metadata.namespace> -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb my-replica-set -n developer -o yaml"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n    lastTransition: \"2019-01-30T10:51:40Z\"\n    link: http://ec2-3-84-128-187.compute-1.amazonaws.com:9080/v2/5c503a8a1b90141cbdc60a77\n    members: 1\n    phase: Running\n    version: 4.2.2-ent"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  lastTransition: 2019-02-01T13:00:24Z\n  link: http://ec2-34-204-36-217.compute-1.amazonaws.com:9080/v2/5c51c040d6853d1f50a51678\n  members: 1\n  message: 'Failed to create/update replica set in Ops Manager: Status: 400 (Bad Request),\n    Detail: Something went wrong validating your Automation Config. Sorry!'\n  phase: Failed\n  version: 4.2.2-ent"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs -f deployment/mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs <podName> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs myrs-0 -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl get mdb -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl get mdb dublin -n <metadata.namespace> -o yaml"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"mongodb.com/v1\",\"kind\":\"MongoDB\",\"metadata\":{\"annotations\":{},\"name\":\"dublin\",\"namespace\":\"mongodb\"},\"spec\":{\"credentials\":\"credentials\",\"persistent\":false,\"podSpec\":{\"memory\":\"1Gi\"},\"project\":\"my-om-config\",\"type\":\"Standalone\",\"version\":\"4.0.0-ent\"}}\n  clusterDomain: \"\"\n  creationTimestamp: 2018-09-12T17:15:32Z\n  generation: 1\n  name: dublin\n  namespace: mongodb\n  resourceVersion: \"337269\"\n  selfLink: /apis/mongodb.com/v1/namespaces/mongodb/mongodbstandalones/dublin\n  uid: 7442095b-b6af-11e8-87df-0800271b001d\nspec:\n  credentials: my-credentials\n  type: Standalone\n  persistent: false\n  project: my-om-config\n  version: 4.2.2-ent"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods\n\nmy-replica-set-0     1/1 Running 2 2h\nmy-replica-set-1     1/1 Running 2 2h\nmy-replica-set-2     0/1 Pending 0 2h"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe pod my-replica-set-2\n\n<describe output omitted>\n\nWarning FailedScheduling 15s (x3691 over 3h) default-scheduler\n0/3 nodes are available: 1 node(s) had taints that the pod\ndidn't tolerate, 2 Insufficient memory."
                },
                {
                    "lang": "sh",
                    "value": "vi <my-replica-set>.yaml\n\nkubectl apply -f <my-replica-set>.yaml\n\nkubectl delete pod my-replica-set-2"
                },
                {
                    "lang": "shell",
                    "value": "kubectl replace -f <my-config-map>.yaml"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb <name> -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete deployment mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete crd mongodb.mongodb.com\nkubectl delete crd mongodbusers.mongodb.com\nkubectl delete crd opsmanagers.mongodb.com"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete namespace <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "scheduler error: pvc not found to schedule the pod"
                },
                {
                    "lang": "sh",
                    "value": "curl --user \"{USERNAME}:{APIKEY}\" --digest \\\n     --header \"Accept: application/json\" \\\n     --header \"Content-Type: application/json\" \\\n     --include \\\n     --request GET \"https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true\""
                },
                {
                    "lang": "json",
                    "value": "{\n \"created\": \"2020-02-25T04:09:42Z\",\n \"externalManagementSystem\": {\n   \"name\": \"mongodb-enterprise-operator\",\n   \"systemId\": null,\n   \"version\": \"1.4.2\"\n },\n \"policies\": [\n   {\n     \"disabledParams\": [],\n     \"policy\": \"EXTERNALLY_MANAGED_LOCK\"\n   },\n   {\n     \"disabledParams\": [],\n     \"policy\": \"DISABLE_AUTHENTICATION_MECHANISMS\"\n   }\n ],\n \"updated\": \"2020-02-25T04:10:12Z\"\n}"
                },
                {
                    "lang": "sh",
                    "value": "curl --user \"{USERNAME}:{APIKEY}\" --digest \\\n     --header \"Accept: application/json\" \\\n     --header \"Content-Type: application/json\" \\\n     --include \\\n     --request PUT \"https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true\" \\\n     --data\n       '{\n         \"externalManagementSystem\": {\n           \"name\": \"mongodb-enterprise-operator\",\n           \"systemId\": null,\n           \"version\": \"1.4.2\"\n         },\n         \"policies\": []\n       }'"
                },
                {
                    "lang": "sh",
                    "value": "curl --user \"{USERNAME}:{APIKEY}\" --digest \\\n     --header \"Accept: application/json\" \\\n     --header \"Content-Type: application/json\" \\\n     --include \\\n     --request PUT \"https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true\" \\\n     --data\n       '{\n         \"externalManagementSystem\": {\n           \"name\": \"mongodb-enterprise-operator\",\n           \"systemId\": null,\n           \"version\": \"1.4.2\"\n         },\n         \"policies\": [\n           {\n             \"disabledParams\": [],\n             \"policy\": \"EXTERNALLY_MANAGED_LOCK\"\n           },\n           {\n             \"disabledParams\": [],\n             \"policy\": \"DISABLE_AUTHENTICATION_MECHANISMS\"\n           }\n         ]\n       }'"
                },
                {
                    "lang": "yaml",
                    "value": "podSpec:\n  podTemplate:\n    spec:\n      containers:\n      - name: mongodb-enterprise-database\n        command: ['sh', '-c', 'echo \"Hello!\" && sleep 3600' ]"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl exec -it <pod-name> bash"
                },
                {
                    "lang": "sh",
                    "value": "TLS attempt failed : x509: certificate is valid for foo.svc.local,\nnot mongo-0-0.mongo-0.mongodb.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "rs-mongos-0-0.mongo-0.mongodb.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs -f <pod_name>"
                },
                {
                    "lang": "sh",
                    "value": "Failed to create/update (Ops Manager reconciliation phase):\nStatus: 400 (Bad Request), Detail:\nInvalid config: MongoDB <version> is not available."
                },
                {
                    "lang": "sh",
                    "value": "kubectl exec --stdin --tty $<pod_name> /bin/sh"
                },
                {
                    "lang": "sh",
                    "value": "Forbidden: updates to statefulset spec for fields other than\n'replicas', 'template', and 'updateStrategy' are forbidden"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete deployment/mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Error: UPGRADE FAILED: cannot patch \"mongodb-enterprise-operator\"\nwith kind Deployment: Deployment.apps \"mongodb-enterprise-operator\"\nis invalid: ... field is immutable"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete deployment/mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                           READY   STATUS    RESTARTS   AGE\nenterprise-operator-767884c9b4-ltkln           1/1     Running   0          122m\nmongodb-enterprise-operator-6d69686679-9fzs7   1/1     Running   0          68m"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete deployment/enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "kubectl scale deployment/mongodb-enterprise-operator --replicas 0"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete sts/<mongodb-resource-name>-db"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete secret/<mongodb-resource-name>-db-config"
                }
            ],
            "preview": "To find the status of a resource deployed with the ,\ninvoke one of the following commands:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/operator-settings",
            "title": " Installation Settings",
            "headings": [],
            "paragraphs": "When you install the  , you can provide optional settings\nthat affect your deployment.  How you provide these settings depends on\nthe environment to which you deploy the  . Review the settings that you can use when you install the\n  using   or  . Review the settings that you can use when you install the\n  using Helm.",
            "code": [],
            "preview": "When you install the , you can provide optional settings\nthat affect your deployment.  How you provide these settings depends on\nthe environment to which you deploy the .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "release-notes",
            "title": "Release Notes for ",
            "headings": [
                " 1.17 Series",
                " 1.17.2",
                " 1.17.1",
                "Breaking Changes",
                "Improvements",
                " 1.17.0",
                "Improvements",
                "Breaking Changes and Deprecations",
                " 1.16 Series",
                " 1.16.4",
                "MongoDB Resource",
                " 1.16.3",
                "MongoDB Resource",
                " 1.16.2",
                "MongoDB Resource",
                "MongoDBOpsManager Resource",
                "MongoDB Multi-Cluster Resource",
                " 1.16.1",
                "MongoDB Resource",
                " 1.16.0",
                "MongoDB Resource",
                "MongoDBOpsManager Resource",
                "MongoDBUser Resource",
                " 1.15 Series",
                " 1.15.2",
                "MongoDBOpsManager Resource",
                "Bug Fixes",
                " 1.15.1",
                "MongoDB Resource",
                "Changes",
                "MongoDBOpsManager Resource",
                "Bug Fixes",
                " 1.15.0",
                "MongoDB Resource",
                "Changes",
                "MongoDBOpsManager Resource",
                "Changes",
                "New Images",
                " 1.14 Series",
                " 1.14.0",
                "",
                "Changes",
                "MongoDB Resource",
                "Changes",
                "Bug Fixes",
                "MongoDBOpsManager Resource",
                "Changes",
                "Bug Fixes",
                " 1.13 Series",
                " 1.13.0",
                "",
                "Changes",
                "MongoDB Resource",
                "Changes",
                "Bug Fixes",
                "MongoDBOpsManager Resource",
                "MongoDBUser Resource",
                "Miscellaneous",
                " 1.12 Series",
                " 1.12.0",
                "MongoDB Resource",
                "Bug Fixes",
                "MongoDBOpsManager Resource",
                "Changes to Images and Supported Versions",
                " 1.11 Series",
                " 1.11.0",
                "",
                "Bug Fixes",
                "New Images",
                "MongoDBOpsManager Resource",
                " 1.10 Series",
                " 1.10.0",
                "",
                "Changes",
                "Bug Fixes",
                "MongoDBOpsManager Resource",
                "Known Issues",
                " 1.9 Series",
                " 1.9.2",
                "",
                "Bug Fixes",
                "MongoDBOpsManager Resource",
                "Known Issues",
                "New Images",
                " 1.9.1",
                "",
                "Bug Fixes",
                "MongoDB Resource",
                "Bug Fixes",
                "MongoDBOpsManager Resource",
                "Known Issues",
                "Changes",
                "New Images",
                " 1.9.0",
                "",
                "Bug Fixes",
                "MongoDB Resource",
                "Changes",
                "MongoDBOpsManager Resource",
                "Known Issues",
                "Changes",
                "New Images",
                " 1.8 Series",
                " 1.8.2",
                "Known Issues",
                "Bug Fix",
                " 1.8.1",
                "Known Issues",
                "Bug Fixes",
                "Improvements",
                "New Images",
                "New  Images",
                " 1.8.0",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes",
                "Bug Fixes",
                "Known Issues",
                " 1.7 Series",
                " 1.7.1",
                "MongoDB Resource Changes",
                "Bug Fixes",
                "Known Issues",
                " 1.7.0",
                "Docker Image Changes",
                "MongoDB Resource Changes",
                "Bug Fixes",
                "Known Issues",
                " 1.6 Series",
                " 1.6.1",
                "Ops Manager Resource Changes",
                "Docker Image Changes",
                "Bug Fixes",
                " 1.6.0",
                "MongoDB Resource Changes",
                "Kubernetes Operator Changes",
                "Bug Fixes",
                " 1.5 Series",
                " 1.5.5",
                "MongoDB Resource Changes",
                "Bug Fixes",
                " 1.5.4",
                "MongoDB Resource Changes",
                "Kubernetes Operator Changes",
                "Bug Fixes",
                " 1.5.3",
                "Kubernetes Operator Changes",
                "Bug Fixes",
                " 1.5.2",
                "Ops Manager Resource Changes",
                "Kubernetes Operator Changes",
                "Bug Fixes",
                " 1.5.1",
                "Bug Fixes",
                "Known Issues",
                " 1.5.0",
                " Changes",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes",
                " 1.4 Series",
                " 1.4.5",
                "CVE Description",
                "Common Weakness Enumeration",
                "Affected Versions",
                "Fixed Versions",
                " 1.4.4",
                "MongoDB Resource Changes",
                " Resource Changes (Beta Release)",
                "Bug Fixes",
                " 1.4.3",
                " Changes",
                "MongoDB Resource Changes",
                " Resource Changes (Beta Release)",
                "Bug Fixes",
                " 1.4.2",
                "MongoDB Resource Changes",
                " Resource Changes (Beta Release)",
                "Bug Fixes",
                " 1.4.1",
                " 1.4.0",
                "MongoDB Resource Changes",
                " Resource Changes (Beta Release)",
                "Bug Fixes",
                " 1.3 Series",
                " 1.3.1",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes (Alpha Release)",
                " 1.3.0",
                "Specification Schema Changes",
                "Ops Manager Resource Changes (Alpha Release)",
                "Bug Fixes",
                " 1.2 Series",
                " 1.2.5",
                "CVE Description",
                "Common Weakness Enumeration",
                "Affected Versions",
                "Fixed Versions",
                " 1.2.4",
                " 1.2.3",
                " 1.2.2",
                " 1.2.1",
                " 1.2.0",
                "GA Release",
                "Alpha Release",
                " 1.1 Series",
                " 1.1",
                " 1.0 Series",
                " 1.0",
                " Beta Series",
                " 0.12",
                " 0.11",
                " 0.10",
                " 0.9",
                " 0.8",
                " 0.7",
                " 0.6",
                " 0.5",
                " 0.4",
                " 0.3",
                " 0.2",
                " 0.1"
            ],
            "paragraphs": "Released 2022-10-18 Fixes the OpenShift installation issue mentioned in the\n v1.17.1 release notes . The  \nLifecycle Manager upgrade graph automatically skips the v1.17.1\nrelease and performs an upgrade from v1.17.0 directly to this release. Improves the reliability of upgrades by adding startup probes for\nMongoDB and OpsManager custom resources with some defaults. Use\n spec.podSpec.podTemplate  to override probe configurations. Released 2022-10-10 As a result of this issue, installing this release could result in\n ImagePullBackOff  errors in Pods hosting AppDB, the  database for  .\nErrors will look similar to the following: To continue using the   v1.17.1, use the following workaround\nand update the   Subscription with the following  spec.config.env : Remove this workaround as soon as you install the new   v1.17.2. This release has the following additional breaking change: This release has invalid  quay.io \ndigests referenced in the certified bundle's CSV. This affects only\nOpenShift deployments when you install or upgrade   from\nthe certified bundle (OperatorHub) in  quay.io . If you use   with OpenShift, we recommend that you do NOT\nupgrade to this release (v1.17.1), and instead upgrade to the\n  v1.17.2, which is due the week commencing 17th October 2022. Removes the  operator.deployment_name  parameter from   Helm charts.\nIn previous releases, you might have used this parameter to customize the name of the\n  container. Starting with this release, the value of the  operator.name  Helm chart parameter\ndetermines the name of the   container. This is a breaking change only if you set  operator.deployment_name  to a different\nvalue than  operator.name  and if you configured tooling to rely on the value of\n operator.deployment_name . Uses  Quay  as an image registry for\n  on OpenShift. When you upgrade your  \ndeployment, it automatically pulls new images from Quay. You don't need to\ntake any action. Released 2022-09-19 Introduces support for   6.0. Introduces the  spec.backup.s3OpLogStores.s3RegionOverride \nand  spec.backup.s3Stores.s3RegionOverride  parameters\nfor specifying the regions where the custom  -compatible buckets\nthat you use for the  oplog store  or a\n snapshot store  should reside. Improves security by introducing: The  readOnlyRootFilesystem  setting for all deployed containers.\nThis change also introduces additional\n volumes and volume mounts . The  allowPrivilegeEscalation  setting. This setting is by default\nset to  false  for all deployed containers. This release: Removes support for   4.4 due to its\n End of Life .\nIf you're using   4.4,  upgrade \nto a newer   version before you upgrade to   1.17. Deprecates Ubuntu base images. Starting with   1.19.0,\nUbuntu base images will no longer be made available. All existing\nUbuntu base images will continue to be supported until their version's\n End of Life (EOL) dates . We strongly\nrecommend that you  Migrate   from Ubuntu-based Images to UBI-based Images  as soon as possible. Removes support for   certificates in concatenated   format.\nThese certificates were deprecated in   1.13.0.\nIf you want to use these certificates, the last version to which you\ncan upgrade is   1.16.4. Starting with the   1.17.0 release, you must manually\nmigrate old-style   secrets from opaque to\n kubernetes.io/tls \ntype secrets by creating new secrets that contain the relevant\ncertificates and signing keys. To learn how to create these secrets,\nsee the following resources: Deploy a Replica Set Secure Internal Authentication with X.509 Released 2022-08-03 Init-Ops-Manager and Operator binaries now use Go 1.18.4, which\naddresses security issues. Released 2022-07-15 Fixed a bug where  securityContext  defined at the Pod level is not\nrespected as the   overrides it with a\n securityContext  at the container level. To learn more, see the\ndescription of the  spec.persistent  setting. Adds  timeoutMS , and  userCacheInvalidationInterval  fields to the\n spec.security.authentication.ldap  object. Fixes behavior where the  additionalMongodConfig.net.tls.mode  setting\nwas ignored for  mongos ,  configSrv , and  shard  objects when\nconfiguring  ShardedCluster  resources. Released 2022-06-28 This release removes WiredTiger cache computation, which was required\nfor MongoDB versions earlier than 4.0.9. Before you upgrade\nto this release, you must upgrade your database deployment to use\nMongoDB version 4.0.9 or later. To learn how to upgrade your deployment, see\n Upgrade MongoDB Version and FCV . Removes the  spec.podSpec.podAntiAffinityTopologyKey ,\n spec.podSpec.podAffinity , and  spec.podSpec.nodeAffinity \nsettings. Instead, use  spec.podSpec.podTemplate  to configure these\nparameters. Removes the  spec.applicationDatabase.podSpec.podAntiAffinityTopologyKey ,\n spec.applicationDatabase.podSpec.podAffinity , and\n spec.applicationDatabase.podSpec.nodeAffinity  settings. Instead, use  spec.applicationDatabase.podSpec.podTemplate  to configure these\nparameters. Added support for  LDAP client authentication  and for\n managing database users with LDAP \nto  . This feature is a beta release. Use  \ndeployments only in development environments. Released 2022-05-24 Deprecates the  spec.service  parameter. Use\n spec.statefulSet.spec.serviceName  to provide a custom service name. Released 2022-04-29 Removes the  spec.security.tls.secretRef.name  parameter.  version  v1.10.0  deprecated this parameter. To specify the secret name containing the certificate for the\ndatabase, use  spec.security.certsSecretPrefix . Create the secret containing the certificates accordingly. Removes the  spec.podSpec.cpu  and\n spec.podSpec.memory  parameters. To override the CPU/Memory resources for the database pod, set the\n statefulset  parameter under\n spec.podSpec.podTemplate.spec.containers . Propagates custom labels specified under  metadata.labels \nto the database   and the   objects. Allows adding Prometheus scraping endpoints to the MongoDB resources\nusing the  spec.prometheus  configuration attribute. Find a sample Prometheus configuration in the\n GitHub \nrepository. Removes the\n spec.applicationDatabase.security.tls.secretRef.name \nparameter.  version  v1.10.0  deprecated this parameter. To specify the secret name containing the certificate for AppDB,\nuse the\n spec.applicationDatabase.security.certsSecretPrefix \nparameter. Create the secret containing the certificates accordingly. Removes  spec.applicationDatabase.podSpec.cpu  and\n spec.applicationDatabase.podSpec.memory . To override the CPU/Memory resources for the appDB pod, use the\n statefulset  parameter under\n spec.applicationDatabase.podSpec.podTemplate.spec.containers . Propagates custom labels specified under\n metadata.labels  to the  , AppDB and BackupDaemon\n  and the   objects. Allows adding Prometheus scraping endpoints to the\n ApplicationDatabase  resources using the\n spec.applicationDatabase.prometheus  configuration\nattribute. Adds the optional parameter  spec.connectionStringSecretName . This\nparameter provides a deterministic secret name for the user-specific\nconnection string secret that   generates. Released 2022-03-24 To enable custom   certificates for   oplog stores,\nyou must configure the following settings: Specify  spec.security.tls.ca . Specify  spec.security.certsSecretPrefix . Set  spec.backup.s3OpLogStores.customCertificate \nto  true . To enable custom   certificates for   snapshot stores,\nyou must configure the following settings: Specify  spec.security.tls.ca . Specify  spec.security.certsSecretPrefix . Set  spec.backup.s3Stores.customCertificate \nto  true . Fixes an issue where the   mounted the incorrect   to the Application Database Pod. Released 2022-03-04 Init-database, Init-Ops-Manager, and Operator binaries now use\nGo 1.17.7 to prevent  CVE-2022-23773 . Fixes an issue that prevented the Operator upgrade when managing a\nTLS-enabled ApplicationDB whose TLS certificate is stored in a\n Secret  of type Opaque. Released 2022-02-11  version  1.15.1  fixes an issue\nthat prevented the   upgrade when managing a\nTLS-enabled Application Database whose TLS certificate is stored in an\n Opaque \nsecret. We recommend that you upgrade to   version 1.15.1\nor later. We strongly advise against upgrading to   version 1.14.0\nor 1.15.0. If you have a broken Application Database after upgrading to\n  version 1.14.0 or 1.15.0, see\n  in Failed State . The  spec.security.tls.enabled  and\n spec.security.tls.secretRef.prefix  fields are now\n deprecated  and will be removed in a future release. To enable   for MongoDB database resources, provide a value for\nthe  spec.security.certsSecretPrefix  field. Adds the  spec.backup.queryableBackupSecretRef  field.\nThis field's value references a   that stores\ncertificates for  Queryable Backups . Adds two fields to enable support for configuring custom  \ncertificates for the   Oplog and Snapshot Stores for backup:\n spec.security.tls.ca  and\n spec.security.tls.secretRef . Adds the ability to back up Application Databases. To back up an\napplication database, you must first disable its processes using the\n spec.applicationDatabase.automationConfig.processes[n].disabled \nfield. The  spec.security.tls.enabled ,\n spec.security.tls.secretRef.prefix ,\n spec.applicationDatabase.security.tls.enabled  and\n spec.applicationDatabase.security.tls.prefix  fields are\nnow  deprecated  and will be removed in a future release. To enable   for   resources, provide a value for\nthe  spec.security.certsSecretPrefix  field. To enable   for Application Database resources, provide a value for\nthe  spec.applicationDatabase.security.certsSecretPrefix \nfield. Find all new images at: https://quay.io/repository/mongodb  (ubuntu-based) https://connect.redhat.com/  (rhel-based) Released 2021-12-16  version  1.15.1  fixes an issue\nthat prevented the   upgrade when managing a\nTLS-enabled Application Database whose TLS certificate is stored in an\n Opaque \nsecret. We recommend that you upgrade to   version 1.15.1\nor later. We strongly advise against upgrading to   version 1.14.0\nor 1.15.0. If you have a broken Application Database after upgrading to\n  version 1.14.0 or 1.15.0, see\n  in Failed State . The   now supports   as a  . To\nstore secrets in   instead of    , see\n Configure Secret Storage . This release adds the  spec.backup.autoTerminateOnDeletion  setting,\nwhich indicates if the  \nshould stop and terminate the backup when you delete the  MongoDB  resource. Fixes an issue that caused a  ShardedCluster  resource to fail when disabling authentication. This release adds the ability to configure   oplog stores using the\n spec.backup.s3OpLogStores.name \nsetting and other related settings. Fixes an issue that prevented the   from triggering a\nresource reconciliation when rotating the Application Database   certificate. Fixes an issue where the   didn't mount the custom   specified in the  MongoDBOpsManager  resource\ninto the Backup Daemon Pod. This issue prevented backups from working\nwhen you configured   to run in  hybrid mode  and used a custom  . Released 2021-10-21 The   no longer generates   certificates for\n MongoDB  and  MongoDBOpsManager  resources. The   now integrates with the Gatekeeper Open Policy Agent (OPA).\nThis allows you to\n control your deployments with policies set in the OPA Gatekeeper . The   can now watch a list of namespaces. To learn more,\nsee  Operator Uses a Subset of Namespaces . When deploying resources to  more than one namespace , create  imagePullSecrets  only in the\nnamespace where you installed the  . The  \nsynchronizes this secret across all watched namespaces. The  spec.credentials  secret  now accepts  fields named\n publicKey  and  privateKey . Use these fields instead of the\n user  and  publicApiKey  fields supported in previous releases. This release deprecates  generic type secrets  for\n  certificates. The   now supports   secrets of the\n kubernetes.io/tls  type. The   reads these secrets and automatically generates\nnew  .pem  files that contain the concatenated  tls.crt  and\n tls.key  fields when you update these secrets. This removes the need to manually concatenate these vales to create\n .pem  files and enables you to natively reference secrets\nthat  -native tools, such as  cert-manager , generate. For  -enabled resources, the operator now watches the ConfigMap\nthat contains the   and the secrets that contain  \ncertificates. Changes to these ConfigMaps and secrets now trigger a\nreconciliation of the related resource. This release removes the  spec.project  setting from the\n MongoDB Database Resource Specification . If your  MongoDB  resource specifications use the\n spec.project  setting, update your specifications to instead\nuse  spec.opsManager.configMapRef.name  or\n spec.cloudManager.configMapRef.name  before you upgrade the\n  to 1.13.0 or later. This release adds several new fields that determine the names that you\nmust give the secrets that contain your   and X.509 certificates\nfor MongoDB resources. To learn more, see  spec.security.certsSecretPrefix  and the\n Secure Client Connections  tutorials. Fixes an issue where Sharded Cluster backups could not be correctly\nconfigured using  MongoDB  resource specifications. Fixes an issue where Backup Daemon fails to start after you update an\n  deployment by updating  spec.version . The   now reports the status of file system snapshot\nstores that you configure in the   spec.backup.fileSystemStores \nsetting in the  MongoDBOpsManager  resource specification. You must manually configure the file system snapshot stores. This release adds a new field,  spec.backup.externalServiceEnabled ,\nto the  MongoDBOpsManager  resource specification. By default, the   creates a  LoadBalancer  service when\nyou  enable queryable backups . Set  spec.backup.externalServiceEnabled  to  false  before you\nenable queryable backups to prevent the   from creating a\nLoadBalancer service. The   now automatically upgrades personal API keys to\nprogrammatic API keys when you upgrade an   deployment to\nversion 5.0.0 or later. You no longer must change the keys manually to\nupgrade your deployment. This release adds the  spec.security.certSecretPrefix  field to\ndetermine the name that you must give the secret that contains your\n  certificate for  MongoDBOpsManager  resources. To learn more, see  spec.security.certsSecretPrefix  and\nthe  HTTPS  tab in the  Deploy an   Resource  tutorial. This release removes the spec.project setting from the  MongoDBUser \nCustomResourceDefinition. If your  MongoDBUser  resource specifications use the\nspec.project setting, update your specifications to instead\nuse  spec.MongoDBResourceRef.name  before you upgrade the\n  to 1.13.0 or later.  4.4.7, 4.4.9, 4.4.10, 4.4.11, 4.4.12 and 4.4.13 base images\nhave been updated to Ubuntu 20.04.  versions 4.4.16 and 5.0.1 are now supported. Released 2021-07-15 If you set  spec.externalConnectivity  to  false  after it\nwas set to  true , the   deletes the corresponding service. Fixes a bug where you could specify  net.ssl.mode  and not  net.tls.mode \nin  spec.additionalMongodConfig . If you set  spec.externalConnectivity  to  false  after it\nwas set to  true , the   deletes the corresponding service. You can specify the number of backup daemon Pods with  spec.backup.members .\nIf not set, the value defaults to  1 . The   now supports the following   versions: 4.4.13, 4.4.14, 4.4.15, 4.2.25 and 5.0.0. Before upgrading   to version 5.0.0, check that the\n  uses a  programmatic API key . Ubuntu based   images are now based on Ubuntu 20.04 instead\nof Ubuntu 16.04. Ubuntu based MongoDB images starting from 2.0.1 are based on Ubuntu 18.04\ninstead of Ubuntu 16.04. MongoDB 4.0. does not support Ubuntu 18.04. If you want to use\nMongoDB 4.0. with the  , use previously released\nimages. Ubuntu based   images after 4.4.13 are based on Ubuntu 20.04\ninstead of Ubuntu 16.04. Newly released UBI images for the  ,   and\nMongoDB are based on  ubi-minimal  instead of  ubi . Released 2021-06-03 Removes the topic \"Migrate to One Resource per Project (Required for\nVersion 1.3.0)\" from the current documentation because  v.1.3.0 is EOL . This topic has been\n archived . Fixes an issue with the  Liveness Probe \nthat could cause the database Pods to be restarted in the middle of\na restore operation from Backup. mongodb-agent 10.29.0.6830-1  located in the following registries: UBI images:  quay.io/mongodb/mongodb-agent-ubi:10.29.0.6830-1 Ubuntu images:  quay.io/mongodb/mongodb-agent:10.29.0.6830-1 mongodb-enterprise-appdb-database  located in the following registries: UBI images:  quay.io/mongodb/mongodb-enterprise-appdb-database-ubi Ubuntu images:  quay.io/mongodb/mongodb-enterprise-appdb-database mongodb-enterprise-init-appdb 1.0.7  located in the following registries: UBI images:  quay.io/mongodb/mongodb-enterprise-init-appdb-ubi:1.0.7 Ubuntu images:  quay.io/mongodb/mongodb-enterprise-init-appdb:1.0.7 mongodb-enterprise-init-database 1.0.3  located in the following registries: UBI images:  quay.io/mongodb/mongodb-enterprise-init-database-ubi:1.0.3 Ubuntu images:  quay.io/mongodb/mongodb-enterprise-init-database:1.0.3 Beginning with this release, you can use any version of MongoDB\nfor the Application Database. You must specify this version\nexplicitly when you deploy the  MongoDBOpsManager  resource. To upgrade the  , you must specify the Application\nDatabase's version. Check that the  spec.applicationDatabase.version \nhas a value in your configuration files for the  MongoDBOpsManager \ncustom resource deployment. Each Application Database Pod consists of the following containers\n(instead of one container with a bundled MongoDB version, as in previous\nreleases): mongodb mongodb-agent mongodb-agent-monitoring The  spec.applicationDatabase.persistent  setting is removed. The\n  always uses persistent volumes for the Application\nDatabase deployed by your  MongoDBOpsManager  custom resources. Released 2020-03-25 Updates the   from the  v1beta1  version to the  v1 \nversion. Clusters on   1.16 and higher should remain unimpacted. The   cannot install in clusters on   versions lower than 1.16. Fixes an issue that prevented multiple   resources from\nhaving the same name in different namespaces. Fixes an issue that caused new MongoDB resources created with\n spec.backup.mode=disabled  to fail. Fixes an issue with saving changes on the  S3 Store  page. Fixes an issue that changed the replica set status to  Fail ,\nincreased the replica set members, and disabled  . When you use remote or hybrid mode, and set\n automation.versions.download.baseUrl , you must set the\n automation.versions.download.baseUrl.allowOnlyAvailableBuilds \nproperty to  false .   4.4.11 fixes this issue. Released 2020-02-05 Fixes errors in the CSV (This only effects the Red Hat market) You can't use MongoDB 4.4 as an application database for an  \nresource. You can find all images in the following registries: mongodb-enterprise-operator:1.9.2 Ubuntu-based images :  https://quay.io/repository/mongodb RHEL-based images :  /mongodb-enterprise-operator Released 2020-01-15 Fixes an issue where you could not specify the\n service-account-name  in the    podSpec \noverride. Removes the unnecessary  delete service  permission from Operator\nrole. Fixes an issue where removing the\n spec.security.roles.privileges  array in\n spec.security.roles  caused the resource to\nenter a bad state. This release introduces: A new Application Database image,\n mongodb-enterprise-appdb:10.2.15.5958-1_4.2.11-ent . The image\nincludes MongoDB  4.2.11-ent  instead of\n 4.2.2-ent . You must push the new image to any private\nrepositories that your   installation uses, otherwise\nthe  MongoDBOpsManager  resource won't start. A new required environment variable,\n APPDB_AGENT_VERSION . If you don't set  APPDB_AGENT_VERSION ,\nthe  MongoDBOpsManager  resource can't fetch the MongoDB Agent\nversion for the Application Database. You can't use MongoDB 4.4 as an application database for an  \nresource. The   user now has  backup ,  restore  and  hostManager  roles, allowing for backups\nand restores on the Application Database. If you omit  spec.applicationDatabase.version , the\n  uses  4.2.11-ent  as the default MongoDB version. You can find all images in the following registries: mongodb-enterprise-operator:1.9.1 mongodb-enterprise-appdb:10.2.15.5958-1_4.2.11-ent mongodb-enterprise-init-appdb:1.0.2 mongodb-enterprise-init-database:1.0.6 Ubuntu-based images :  https://quay.io/repository/mongodb RHEL-based images :  https://catalog.redhat.com/software/containers/mongodb/enterprise-operator/5b8052d069aea356ff258479 Released 2020-12-08 Fixes an issue where the   didn't close connections to\n , causing too many open file descriptors. You can now configure continuous backup for a MongoDB database\nresource in its  . To enable continuous backup in the MongoDB  , you must\n enable backup  in an  \ninstance that you deployed using the  . You can't use MongoDB 4.4 as an application database for an  \nresource. When you upgrade the   to this version, the\n  deletes and re-creates the Backup Daemon statefulset. This is a safe operation. The new   service that enables Queryable Backups requires a change\nto the  matchLabels  Backup Daemon   attribute. The   changes the way it collects the status of\nMongoDB Agents in Application Database  . You can find all images in the following registries: mongodb-enterprise-operator:1.9.0 Ubuntu-based images :  https://quay.io/repository/mongodb RHEL-based images :  https://catalog.redhat.com/software/containers/mongodb/enterprise-operator/5b8052d069aea356ff258479 Released 2020-11-16 You can't use MongoDB 4.4 as an application database for an  \nresource. Fixes an issue where the   resource would reach a  Failing \nstate when both  spec.externalConnectivity  and\n spec.backup.enabled  were enabled. Released 2020-11-13 You can't use MongoDB 4.4 as an application database for an  \nresource. When both  spec.externalConnectivity  and\n spec.backup.enabled  are enabled in   at the same\ntime, the   resource fails to reconcile. Fixes a bug where\n spec.security.authentication.ignoreUnknownUsers  could not\nbe modified after creating a MongoDB resource. Fixes failed queryable backups. The   now creates a\n  Service that   uses to access backups. Fixes an issue that made it impossible to move from non-  to a\n -enabled Application Database. Init containers do not run as root.  Backup daemon runs in unprivileged mode. To manage Database Pod resources, use the\n spec.podSpec.podTemplate  MongoDB Custom Resource attribute.\nFor an example resource definition of each supported type, see the\n samples/mongodb/podspec \ndirectory. The following attributes are deprecated: spec.podSpec.cpu spec.podSpec.cpuRequests spec.podSpec.memory spec.podSpec.memoryRequests Init-database 1.0.1 Ubi Ubuntu Init-ops-manager 1.0.3 Ubi Ubuntu Init-appdb 1.0.5 Ubi Ubuntu For a list of the packages installed and any security vulnerabilities\ndetected in the build process, see the Quay repository for the\n MongoDB Enterprise Operator \nand the  MongoDB Enterprise Database . Version 4.4.5 Ubi Ubuntu Version 4.2.21 Ubi Ubuntu Version 4.2.20 Ubi Ubuntu Released 2020-09-30 The MongoDB Enterprise Database image now requires an init container.\nIf you are using a private repository, you must set the  INIT_DATABASE_IMAGE_REPOSITORY \nenvironment variable in the Operator deployment, and the new\ninit container must exist inside this repository. Introduces new configuration fields: spec.security.authentication.requireClientTLSAuthentication \nfor using the MongoDB Agent client certificate authentication in\nconjunction with any other authentication mechanism. spec.security.authentication.agents.clientCertificateSecretRef \nfor configuring the client TLS certificate used by the MongoDB\nAgent when enabling ClientTLSAuthentication. Changes the default permissions of volumes created from secrets from  0644 \nto  0640 . Allows the Application Database to be configured with SCRAM-SHA-256\nauthentication when using   4.4 or newer version. Changes the validation of the    spec.version  field\nto allow for tags that do not match the  semver \nrequirements. The  spec.version  field must start with the\n Major.Minor.Patch  string that represents the   version. To\nlearn more about this field, see  Ops Manager Resource Specification . Fixes an issue that caused the Operator to choose an incorrect project\nname when creating MongoDB users. Fixes an issue that caused the MongoDB   CRD to have the CA\npath in the incorrect location. Fixes a bug where the MongoDB Agent could not correctly recognize the\nparameters that passed through  spec.agent.startupOptions . Fixes an issue that could cause potential deadlock when certain\nconfiguration options are modified in parallel. You can't use MongoDB 4.4 as an application database for an  \nresource. When you enable queryable backup, you must manually create two\nadditional services for: Exposing the queryable backup port (default: 25999) for the  \npod. The Backup Daemon pod, to ensure that it is resolvable from the   pod. If you deploy   in\n local mode  and upgrade from\nv4.4.1, you must upgrade the MongoDB tools located in the\n automation.versions.directory , which defaults to\n /mongodb-ops-manager/mongodb-releases/ . Released 2020-09-02 Supports setting the Distinguished Name (DN) of the LDAP group to\nwhich the MongoDB Agent user belongs with the\n spec.security.authentication.ldap.automationLdapGroupDN \nsetting. Requires you to provide\n spec.security.authentication.agents.mode  if you specify\nmore than one mode in  spec.security.authentication.modes . Supports setting MongoDB Agent startup parameters for MongoDB Database\nresources with the following settings: spec.applicationDatabase.agent.startupOptions spec.agent.startupOptions spec.configSrv.agent.startupOptions spec.mongos.agent.startupOptions spec.shard.agent.startupOptions  resources: Fixes a bug where you could not enable  SCRAM-SHA \nauthentication for application database resources using certain\nMongoDB versions with   4.4. Fixes a bug where application database monitoring was not correctly\nconfigured in   when you enabled   for the application\ndatabase. Fixes a bug to move the     configuration from\n spec.applicationDatabase.security.tls.ca  to\n spec.security.tls.ca . MongoDB resources: Fixes a bug that prevented you from increasing or decreasing the number\nof members in a replica set or a sharded cluster by more than one\nmember at a time for MongoDB 4.4 deployments. Fixes an issue where the   could not enable agent\nauthentication if you enabled  LDAP  authentication for a MongoDB\nresource. Fixes an issue where you could not create  SCRAM  users and enable\n SCRAM  authentication in any order for a MongoDB resource. Fixes an issue where the   did not remove the backup\nautomation configuration before starting the agent on a MongoDB\nresource  . If you enable   on the application database, you must not provide the\n spec.applicationDatabase.version  field in an  \nresource definition. You can't use MongoDB 4.4 as an application database for an  \nresource. When you upgrade to the   1.7.1, you might have to delete\nthe  mongodb-enterprise-operator  deployment due to deployment\nconfiguration changes. This is a safe operation. Deleting the\n mongodb-enterprise-operator  pod does not affect the MongoDB\n . If you use   certificates signed by a custom  , you must: Omit the  spec.version.applicationDatabase  setting from\nyour   resource definition, and Deploy   in  local mode . You must manually copy\ninstallation archives for all MongoDB versions you want to use to\na   for the   StatefulSet. Released 2020-08-14  1.7.x is the final minor version release series that\nsupports OpenShift 3.11. Do not upgrade to any future major or minor\nversion releases if you want to continue to deploy the  \nusing OpenShift 3.11. The planned end of life for the   1.7.x release series\nis July 2021. All   Red Hat Docker images are now based on UBI 8. In\nthe previous release,   Red Hat Docker images were based\non UBI 7. Supports LDAP as an authorization mechanism for MongoDB database\nresources you deploy with the  . For more information,\nsee the sample LDAP configurations on  GitHub Fixes a bug that prevented scaling down a replica set from three\nmembers to one member.  cannot monitor Application Databases secured using  . For MongoDB 4.4 deployments, you can increase or decrease the number\nof members in a replica set or a sharded cluster by only one member\nat a time. Released 2020-07-30  image for version 4.4.0 is available. The Red Hat  database  and  operator  Docker images are now based\non the latest UBI 7 release. Two high criticality issues have been\nresolved. The following Docker images have been released: Image Type Ubuntu 16.04 Red Hat UBI 7 quay.io/mongodb/mongodb-enterprise-operator:1.6.1 quay.io/mongodb/mongodb-enterprise-operator-ubi:1.6.1 MongoDB Database quay.io/mongodb/mongodb-enterprise-database:1.6.1 quay.io/mongodb/mongodb-enterprise-database-ubi:1.6.1 quay.io/mongodb/mongodb-enterprise-ops-manager:4.4.0 quay.io/mongodb/mongodb-enterprise-ops-manager-ubi:4.4.0 Fixes a bug where the   did not store a configuration of\nyour deployed resources in a  . Fixes a bug where the   did not allow passwords of any\nlength or complexity for Application Database, oplog store, and\nblockstore database resources defined in   resources. Fixes a bug where the authentication configuration was not removed\nfrom   or   projects when you remove a MongoDB\ndatabase resource. Released 2020-07-16 Supports LDAP as an authentication mechanism for MongoDB database\nresources you deploy with the  . For more information,\nsee the sample LDAP configurations on  GitHub . LDAP authorization is not yet supported. Preserves backup history by retaining   cluster records when\nyou enable backup. Fixes a bug that prevented the   from raising errors when\na  projectName  contained spaces. Fixes a bug that prevented   to monitor for all MongoDB\ndatabase resources that you deploy with the  . Released 2020-07-02 Provides additional options for more granular configuration of\n  /   processes. You can find an example of how to apply\nthese options in the  /samples/mongodb/mongodb-options  file of the\n MongoDB Enterprise Kubernetes Operator repository . Fixes a bug introduced in 1.5.4 where   would not tag\nprojects correctly when working on   versions older than 4.2.2.\nIn this version,   tags the projects correctly. Released 2020-06-22 Allows modification of authentication settings using the   UI if\nthe  spec.security.authentication  setting is not provided\nin the MongoDB resource object definition. Supports Helm  installation  with\n helm install  in addition to  helm template | kubectl apply .\n helm install  is now the recommended way to install with Helm. Supports configuring the MongoDB Agent authentication mechanism\nindependently from the cluster authentication mechanism. Supports configuring monitoring for the Application Database to send\nmetrics to  . To learn more about the monitoring function of\nthe MongoDB Agent, see\n MongoDB Agent . Fixes a bug that affected transitioning authentication mechanisms\nfrom X.509 to SCRAM. Fixes a bug that prevented the MongoDB Agent from reaching a goal\nstate if SCRAM configuration was changed in the   UI. Released 2020-05-29 Passes   and MongoDB deployment configuration properties as\n Secret environment variables . Correctly configures shutdown timeouts for   and the Backup\nDaemon. Fixes an issue where  -watched Secrets and ConfigMaps\ntriggered unnecessary reconciliations. Fixes an issue where the status of custom resources failed to update\nin OpenShift 3.11. Released 2020-05-08 Runs   and Backup Daemon pods under a dedicated service\naccount. Can configure the   to watch a subset of provided\n . You can find more information in the documentation. Can generate   without using subresources. Some versions of\nOpenshift 3.11 require this capability. To avoid using subresources,\nuse  --set subresourceEnabled=false  when installing the\n  with helm. Fixes setting the  spec.statefulSet  and\n spec.backup.statefulSet  fields on the\n MongoDBOpsManager  Resource. Fixes an issue that requires a restart of the   during\nsetup of webhook. Fixes an issue that could make an   resource to reach an\nunrecoverable state if the provided admin password has insufficient\nstrength. Released 2020-04-30 Deprecates the generation of   certificates by the  .\nIf you use  -generated certificates, warning messages now\nappear in the   logs. To configure secure deployments, see\n Secure Client Connections . Fixes an issue where, when no authentication is configured by the\n , the   disables authentication in  .\nThe   no longer disables authentication unless you\nexplicitly set  spec.security.authentication.enabled  to\n false . When you configure the\n spec.statefulSet.spec  and\n spec.backup.statefulSet.spec  settings of the\n MongoDBOpsManager  resource, you can only\nconfigure the  spec.statefulSet.spec.template  and\n spec.backup.statefulSet.spec.template  fields. Any other\n spec.statefulSet.spec  or\n spec.backup.statefulSet.spec  field has no effect. Released 2020-04-24 Adds the ability to start the   with some but not all\nMongoDB   installed. Administrators can specify the container\nargument  watch-resource  to limit the   to deploy either\nMonogDB instances or  , or both. Adds the following new   configuration properties: When using a private docker registry, these properties must point\nto the relevant registries after you copy the images from the MongoDB distribution channels. INIT_OPS_MANAGER_IMAGE_REPOSITORY INIT_APPDB_IMAGE_REPOSITORY APPDB_IMAGE_REPOSITORY Increases support for custom   certificates with the\n spec.security.tls.secretRef  and  spec.security.tls.ca \nconfiguration settings. Deprecates   certificate generation by the  .\nMigrating to custom   certificates is recommended. See the  sample YAML files \nfor new feature usage examples. Releases the  MongoDBOpsManager  resource as\nGenerally Available (GA). MongoDB now supports using the  \nto deploy   resources to   in production environments. Supports Backup Blockstore Snapshot Stores. Defaults to the Application Database as a metadata database for Backup\n  Snapshot Stores. Supports  spec.jvmParameters  and  spec.backup.jvmParameters  to add or\noverride JVM parameters in   and Backup Daemon processes. Automatically configures   and Backup Daemon JVM memory\nparameters based on pod memory availability. Supports   for   and the Application Database. Adds more detailed information to the  status  field. Supports   Local Mode for  MongoDBOpsManager  resources with\nmultiple replicas by enabling users to specify\n PersistentVolumeClaimTemplates  in  spec.statefulSet.spec . Implements a new image versioning scheme. Removes the  spec.podSpec  configuration setting. Use\n spec.statefulSet.spec  instead. Removes the  spec.backup.podSpec  configuration setting. Use\n spec.backup.statefulSet.spec  instead. Fixes CVE-2020-7922:   Operator generates potentially insecure certificates. X.509 certificates generated by the   may allow an attacker with\naccess to the   cluster improper access to MongoDB instances.\nCustomers who do not use X.509 authentication, and those who do not use\nthe   to generate their X.509 certificates are unaffected. CWE-295: Improper Certificate Validation\nCVSS score: 6.4\nCVSS:3.1/AV:A/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:N 1.0, 1.1 1.2.0 - 1.2.4 1.3.0 - 1.3.1 1.4.0 - 1.4.4 1.2.5 1.4.5 and above Supports changes in the  Cloud Manager API . Properly terminates resources with a termination hook. Implements stricter validations. MongoDB resources: Fixes an issue when working with   with custom  \ncertificates. Released 2020-02-24 Adds a  webhook  to validate\na   configuration. Adds support for sidecars for   pods using the\n spec.podSpec.podTemplate  setting. Allows users to change the  PodSecurityContext  to allow privileged\nsidecar containers. Adds the  spec.podSpec  configuration settings for\n , the Backup Daemon, and the Application Database. See\n Ops Manager Resource Specification .  image for version 4.2.8 is available. See the  sample YAML files  for new\nfeature usage examples. MongoDB resources: Fixes potential race conditions when deleting  .  resources: Supports the  spec.clusterDomain  setting for  \nand Application Database resources. No longer starts monitoring and backup processes for the Application\nDatabase. Released 2020-01-24 Runs MongoDB database   pods under a dedicated   service\naccount:  mongodb-enterprise-database-pods . Adds the  spec.podSpec.podTemplate  setting, which allows\nyou to apply templates to   pods that the  \ngenerates for each database  . Renames the  spec.clusterName  setting to\n spec.clusterDomain . Adds  offline mode support  for the Application\nDatabase. Bundles MongoDB Enterprise version 4.2.2 with the\nApplication Database image. Internet access is not required to\ninstall the application database if\n spec.applicationDatabase.version  is set to\n \"4.2.2-ent\"  or omitted. Renames the  spec.clusterName  setting to\n spec.clusterDomain .  images for versions 4.2.6 and 4.2.7 are available. See the  sample YAML files  for new\nfeature usage examples. MongoDB resources: Fixes the order of sharded cluster component creation. Allows   to be enabled on Amazon EKS.  resources: Enables the   to use the  spec.clusterDomain  setting. Released 2019-12-13 Includes  CVE fixes  and\n RHSA security fixes . Fixes an issue that prevented backup from starting on MongoDB 4.0. Released 2019-12-09 Adds split horizon DNS support for MongoDB replica sets, which allows\nclients to connect to a replica set from outside of the  \ncluster. Supports requests for  -generated certificates for\nadditional certificate domains, which makes them valid for the\nspecified subdomains. For more information on how to enable new features, see the sample YAML\nfiles in the  samples directory . Promotes the  MongoDBOpsManager   resource  to Beta.   version\n4.2.4 is available. Supports Backup and restore in  -deployed  \ninstances. This is a semi-automated process that deploys everything\nyou need to enable backups in  . You can enable Backup by\nsetting the  spec.backup.enabled  setting in the  \ncustom resource. You can configure the Head Database, Oplog Store, and\nS3 Snapshot Store by using the  MongoDBOpsManager   resource\nspecification . Supports access to   from outside the  \ncluster through the  spec.externalConnectivity  setting. Enables SCRAM-SHA-1 authentication  on  's\nApplication Database by default. Adds support for OpenShift (Red Hat UBI Images). Improves overall stability of X.509 user management. Released 2019-11-08 This release introduces significant changes that may not be\ncompatible with previous deployments or resource configurations.\n Migrate to one resource per project  before\nupgrading the  . Requires one MongoDB resource per   project. If you\nhave more than one MongoDB resource in a project, all resources will\nchange to a  Pending  status and the   won\u2019t perform\nany changes on them. The existing MongoDB databases will still be\naccessible. You must  migrate to one resource per project . Supports  SCRAM-SHA  authentication mode. See  the MongoDB\nEnterprise Kubernetes Operator GitHub repository \nfor examples. Requires that the project ( ConfigMap ) and\ncredentials ( secret )\nreferenced from a MongoDB resource be in the same namespace. Adds OpenShift installation files (  file and Helm chart\nconfiguration). Supports highly available  Ops Manager resources  by introducing the  spec.replicas \nsetting. Runs   as a non-root user. Released 2019-10-25 This release introduces significant changes that may not be\ncompatible with previous deployments or resource configurations.\n Migrate to one resource per project \nbefore installing or upgrading the  . Moves to a  one resource per project configuration .\nThis follows the warnings introduced in a  previous version of the operator .\nThe operator now requires each cluster to be contained within a new project. Authentication settings are now contained within the\n security section  of the MongoDB resource\nspecification rather than the project ConfigMap. Replaces the  project  field with the\n spec.opsManager.configMapRef.name  or\n spec.cloudManager.configMapRef.name  fields. User resources  now refer to MongoDB\nresources rather than project ConfigMaps. No longer requires  data.projectName  in the project ConfigMap. The\nname of the project defaults to the name of the MongoDB resource in\n . This release introduces significant changes to the   resource's\narchitecture. The   application database is now managed by\nthe  , not by  . Stops unnecessary recreation of NodePorts. Fixes logging so it's always in JSON format. Sets  USER  in the   Docker image. Fixes CVE-2020-7922:   Operator generates potentially insecure\ncertificates. X.509 certificates generated by the   may allow an attacker with\naccess to the   cluster improper access to MongoDB instances.\nCustomers who do not use X.509 authentication, and those who do not use\nthe   to generate their X.509 certificates are unaffected. CWE-295: Improper Certificate Validation\nCVSS score: 6.4\nCVSS:3.1/AV:A/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:N 1.0, 1.1 1.2.0 - 1.2.4 1.3.0 - 1.3.1 1.4.0 - 1.4.4 1.2.5 1.4.5 and above Released 2019-10-02 Increases stability of Sharded Cluster deployments. Improves internal testing infrastructure. Released 2019-09-13 Update:  The   will remove support for multiple\nclusters per project in a future release. If a project contains more\nthan one cluster, a warning will be added to the status of the\nMongoDB Resources. Additionally, any new cluster being added to a\nnon-empty project will result in a  Failed  state, and won't\nbe processed. Fix:  The overall stability of the operator has been improved. The\noperator is now more conservative in resource updates both on\n  and  . Released 2019-08-30 Security Fix:  Clusters configured by   versions\n1.0 through 1.2.1 used an insufficiently strong keyfile for internal\ncluster authentication between  mongod  processes. This only affects\nclusters which are using X.509 for user authentication, but are not\nusing X.509 for internal cluster authentication. Users are advised to\nupgrade to version 1.2.2, which will replace all managed keyfiles. Security Fix:  Clusters configured by   versions 1.0\nthrough 1.2.1 used an insufficiently strong password to authenticate\nthe MongoDB Agent. This only affects clusters which have been manually\nconfigured to enable  SCRAM-SHA-1 , which is not a supported\nconfiguration. Users are advised to upgrade to version 1.2.2, which\nwill reset these passwords. Released 2019-08-23 Fix:  The   no longer recreates   when X.509\nauthentication is enabled and the approved   have been deleted. Fix:  If the  OPERATOR_ENV  environment variable is set to\nsomething unrecognized by the  , it will no longer result\nin a  CrashLoopBackOff  of the pod. A default value of  prod  is\nused. The   now supports more than 100 agents in a given\nproject. Released 2019-08-13 Adds a\n readinessprobe \nto the MongoDB Pods to improve the reliability of rolling upgrades. This feature is an alpha release. It is not ready for production use. Can use the   to manage   4.2. To\n deploy an |onprem| instance ,\nyou use a new  resource :  MongoDBOpsManager . Released 2019-07-19 Fix:  Adds sample yaml files, in particular, the attribute related\nto\n featureCompatibilityVersion . Fix:    can be disabled in a deployment. Improvement:  Adds\n script \nin the\n support  directory that can gather\ninformation of your MongoDB resources in Kubernetes. Improvement:  In a   environment, the   can use a\ncustom  . All the certificates must be passed as  \nobjects. Released 2019-06-18 Supports Kubernetes v1.11 or later. Provisions any kind of MongoDB deployment in the Kubernetes Cluster\nof your Organization: Standalone Replica Set Sharded Cluster Configures   on the MongoDB deployments and encrypt all traffic.\nHosts and clients can verify each other\u2019s identities. Manages MongoDB users. Supports X.509 authentication to your MongoDB databases. If you have any questions regarding this release, use the\n #enterprise-kubernetes \nSlack channel. Released 2019-06-07 Rolling upgrades of MongoDB resources ensure that\n rs.stepDown()  is called for the primary\nmember. Requires MongoDB patch version 4.0.8 and later or MongoDB\npatch version 4.1.10 and later. During a MongoDB major version upgrade, the\n featureCompatibilityVersion  field can be set. Fixed a bug where replica sets with more than seven members could\nnot be created. X.509 Authentication can be enabled at the\n Project level . Requires  ,\n  patch version 4.0.11 and later, or   patch version\n4.1.7 and later. Internal cluster authentication based on X.509 can be enabled at the\n deployment  level. MongoDB users with X.509 authentication can be created, using the\nnew  MongoDBUser  custom resource. Released 2019-04-29 NodePort  service creation can be disabled.  can be enabled for internal authentication between MongoDB in\nreplica sets and sharded clusters. The   certificates are created\nautomatically by the  . Refer to the sample\n .yaml  files in the\n GitHub repository \nfor examples. Wide or asterisk roles have been replaced with strict listing of\nverbs in  roles.yaml . Printing  mdb  objects with  kubectl  will provide more\ninformation about the MongoDB object: type, state, and MongoDB server\nversion. Released 2019-04-02 The   and database images are now based on ubuntu:16.04. The   now uses a single   named  MongoDB \ninstead of the  MongoDbReplicaSet ,  MongoDbShardedCluster , and\n MongoDbStandalone  CRDs. Follow the  upgrade procedure  to\ntransfer existing  MongoDbReplicaSet ,  MongoDbShardedCluster ,\nand  MongoDbStandalone  resources to the new format. For a list of the packages installed and any security vulnerabilities\ndetected in our build process, see: MongoDB Enterprise Operator MongoDB Enterprise Database Released 2019-03-19 The Operator and Database images are now based on\n debian:stretch-slim  which is the latest and up-to-date Docker\nimage for Debian 9. Released 2019-02-26 Perform   clean-up on deletion of MongoDB resource without the\nuse of finalisers. Bug fix:  Race conditions when communicating with  . Bug fix:   ImagePullSecrets  being incorrectly initialized in\nOpenShift. Bug fix:  Unintended fetching of closed projects. Bug fix:  Creation of duplicate organizations. Bug fix:  Reconciliation could fail for the MongoDB resource if\nsome other resources in   were in error state. Released 2019-02-01 Improved detailed status field for MongoDB resources. The   watches changes to configuration parameters in a\nproject configMap and the credentials secret then performs a rolling\nupgrade for relevant Kubernetes resources. Added   structured logging for Automation Agent pods. Support   records for MongoDB access. Bug fix: Avoiding unnecessary reconciliation. Bug fix: Improved Ops Manager/Cloud Manager state management for\ndeleted resources. Released 2018-12-17 Refactored code to use the  controller-runtime  library to fix issues\nwhere Operator could leave resources in inconsistent state. This also\nintroduced a proper reconciliation process. Added new  status  field for all MongoDB Kubernetes resources. Can configure Operator to watch any single namespace or all\nnamespaces in a cluster (requires cluster role). Improved database logging by adding a new configuration property\n logLevel . This property is set to  INFO  by default.\nAutomation Agent and MongoDB logs are merged in to a single log\nstream. Added new configuration Operator timeout. It defines waiting time\nfor database pods start while updating  . Fix:  Fixed failure detection for  mongos . Released 2018-11-14 Image for database no longer includes the binary for the Automation\nAgent. The container downloads the Automation Agent binary from\n  when it starts. Fix:  Communication with   failed if the project with the same\nname existed in different organization. Released 2018-10-04 If a backup was enabled in   for a Replica Set or Sharded\nCluster that the   created, then the  \ndisables the backup before removing a resource. Improved persistence support: The data, journal and log directories are mounted to three\nmountpoints in one or three volumes depending upon the\n podSpec.persistence  setting. Prior to this release, only the data directory was mounted to\npersistent storage. Setting Mount Directories to podSpec.persistence.single One volume podSpec.persistence.multiple Three volumes A new parameter,  labelSelector , allows you to specify the\nselector for volumes that   should consider mounting. If   is not specified in the  persistence \nconfiguration, then the default  StorageClass  for the cluster is\nused. In most of public cloud providers, this results in dynamic\nvolume provisioning. Released 2018-08-07 The Operator no longer creates the CustomResourceDefinition objects.\nThe user needs to create them manually. Download and apply\n this new yaml file \n( crd.yaml ) to create/configure these objects. ClusterRoles are no longer required. How the Operator watches\nresources has changed. Until the last release, the Operator would\nwatch for any resource on any  . With 0.3, the Operator\nwatches for resources in the same namespace in which it was created.\nTo support multiple namespaces, multiple Operators can be installed.\nThis allows isolation of MongoDB deployments. Permissions changes were made to how PersistentVolumes are mounted. Added configuration to Operator to not create\n SecurityContexts \nfor  . This solves an issue with OpenShift which does not\nallow this setting when  SecurityContextContraints  are used. If you are using Helm, set  managedSecurityContext  to  true .\nThis tells the Operator to not create  SecurityContext  for\n , satisfying the OpenShift requirement. The combination of  projectName  and  orgId  replaces\n projectId  alone to configure the connection to  .\nThe project is created if it doesn't exist. Released 2018-08-03 Calculates WiredTiger memory cache. Released 2018-06-27 Initial Release Can deploy standalone instances, replica sets, sharded clusters\nusing   configuration files.",
            "code": [
                {
                    "lang": "sh",
                    "value": "Failed to pull image \"quay.io/mongodb/mongodb-agent-ubi@sha256:a4cadf209ab87eb7d121ccd8b1503fa5d88be8866b5c3cb7897d14c36869abf6\": rpc error: code = Unknown desc = reading manifest sha256:a4cadf209ab87eb7d121ccd8b1503fa5d88be8866b5c3cb7897d14c36869abf6 in quay.io/mongodb/mongodb-agent-ubi: manifest unknown: manifest unknown"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n config:\n   env:\n     - name: AGENT_IMAGE\n       value: >-\n       quay.io/mongodb/mongodb-agent-ubi@sha256:ffa842168cc0865bba022b414d49e66ae314bf2fd87288814903d5a430162620\n     - name: RELATED_IMAGE_AGENT_IMAGE_11_0_5_6963_1\n       value: >-\n       quay.io/mongodb/mongodb-agent-ubi@sha256:e7176c627ef5669be56e007a57a81ef5673e9161033a6966c6e13022d241ec9e\n     - name: RELATED_IMAGE_AGENT_IMAGE_11_12_0_7388_1\n       value: >-\n       quay.io/mongodb/mongodb-agent-ubi@sha256:ffa842168cc0865bba022b414d49e66ae314bf2fd87288814903d5a430162620\n     - name: RELATED_IMAGE_AGENT_IMAGE_12_0_4_7554_1\n       value: >-\n       quay.io/mongodb/mongodb-agent-ubi@sha256:3e07e8164421a6736b86619d9d72f721d4212acb5f178ec20ffec045a7a8f855"
                }
            ],
            "preview": "Released 2022-10-18",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/support-lifecycle",
            "title": "Support Lifecycle",
            "headings": [],
            "paragraphs": "The support lifecycle of the   is governed by the  MongoDB\nSupport Policy , where the\n  is considered an \"extension to  \". The following\ndates are derived from this policy and are published here for guidance\nonly. These dates might be subject to change.  Version End of Life Date 1.17.x TBD 1.16.x 2023-02-02 1.15.x 2022-11-16 1.14.x 2022-09-21 1.13.x 2022-07-26 1.12.x 2022-04-20 1.11.x 2022-03-09 1.10.x 2021-12-30 1.9.x 2021-09-13 1.8.x 2021-07-06 1.7.x 2021-07-01 1.6.x 2021-04-13 1.5.x 2021-01-28 1.4.x 2020-09-13 1.3.x 2020-07-30 1.2.x 2020-05-21 1.1 2020-04-23 1.0 2020-03-15",
            "code": [],
            "preview": "The support lifecycle of the  is governed by the MongoDB\nSupport Policy, where the\n is considered an \"extension to \". The following\ndates are derived from this policy and are published here for guidance\nonly. These dates might be subject to change.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/kubectl-operator-settings",
            "title": "  and  Installation Settings",
            "headings": [
                "APPDB_AGENT_VERSION",
                "APPDB_IMAGE_REPOSITORY",
                "DATABASE_VERSION",
                "IMAGE_PULL_POLICY",
                "INIT_APPDB_IMAGE_REPOSITORY",
                "INIT_APPDB_VERSION",
                "INIT_DATABASE_IMAGE_REPOSITORY",
                "INIT_DATABASE_VERSION",
                "INIT_OPS_MANAGER_IMAGE_REPOSITORY",
                "INIT_OPS_MANAGER_VERSION",
                "MANAGED_SECURITY_CONTEXT",
                "MONGODB_ENTERPRISE_DATABASE_IMAGE",
                "OPERATOR_ENV",
                "OPS_MANAGER_IMAGE_PULL_POLICY",
                "OPS_MANAGER_IMAGE_REPOSITORY",
                "WATCH_NAMESPACE"
            ],
            "paragraphs": "To provide optional settings, edit the   file that corresponds to\nyour deployment type in the directory where you cloned the\n  repository: If the setting that you want to add doesn't exist in the   file,\nadd it as a new array of key-value pair mappings in the\n spec.template.spec.containers.name.env.  collection: Vanilla   using  :  mongodb-enterprise.yaml OpenShift using  :  mongodb-enterprise-openshift.yaml Set the value of the  spec.template.spec.containers.name.env.name \nkey to the setting's name. Set the value of the  spec.template.spec.containers.name.env.value \nkey to the setting's value. Version of the image that contains the MongoDB Agent that the Application\nDatabase uses. The default value is 10.2.15.5958-1_4.2.11-ent. The default value varies based on whether you install the  \nto a vanilla   or an OpenShift environment:  of the repository from which the   downloads\nthe Application Database image. The default value is  quay.io/mongodb/mongodb-enterprise-appdb . The default value is  registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb . Version of the MongoDB Enterprise Database image that the  \ndeploys. Pull policy  for the MongoDB\nEnterprise database image the   deploys. The   accepts the following values:   Always ,\n IfNotPresent ,  Never . The default value is  Always .  of the repository from which the  initContainer  image that\ncontains the Application Database start-up scripts and the readiness\nprobe is downloaded. The default value is  quay.io/mongodb/mongodb-enterprise-appdb-init . The default value is  registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb-init . Version of the  initContainer  image that contains the Application\nDatabase start-up scripts and the readiness probe. The default value is  1.0.6 .  of the repository from which the  initContainer  image that\ncontains the MongoDB Agent start-up scripts and the readiness probe is\ndownloaded. The default value is  quay.io/mongodb/mongodb-enterprise-init-database . The default value is  registry.connect.redhat.com/mongodb/mongodb-enterprise-init-database . Version of the  initContainer  image that contains the MongoDB Agent\nstart-up scripts and the readiness probe. The default value is  1.0.2 .  of the repository from which the  initContainer  image that\ncontains the   start-up scripts and the readiness probe is\ndownloaded. The default value is  quay.io/mongodb/mongodb-enterprise-init-ops-manager . The default value is  registry.connect.redhat.com/mongodb/mongodb-enterprise-init-ops-manager . Version of the  initContainer  image that contains the  \nstart-up scripts and the readiness probe. The default value is  1.0.3 . Flag that determines whether or not the   inherits the\n securityContext  settings that your   cluster manages. This value must be  true  if you want to run the  \nin OpenShift or in a restrictive environment. The default value is  false . The default value is  true .  of the MongoDB Enterprise Database image that the  \ndeploys. The default value is  quay.io/mongodb/mongodb-enterprise-database . The default value is  registry.connect.redhat.com/mongodb/mongodb-enterprise-database . Label for the  s deployment environment. This value\naffects the default timeouts and the logging level and format: The default value is  prod . If the value is Log Level is set to Log Format is set to dev debug text prod info json Pull policy  for the\n  images the   deploys. The   accepts the following values:  Always ,\n IfNotPresent , and  Never . The default value is  Always .  of the repository from which the image for an  Ops\nManager resource  is downloaded. The default value is  quay.io/mongodb/mongodb-enterprise-ops-manager . The default value is  registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager . Namespaces that the   watches for  \nchanges. If this   differs from the default, ensure that\nthe   ServiceAccount  can access \nthis namespace. The default value is  <metadata.namespace> . To watch  all namespaces , specify  *  and assign the   to the\n mongodb-enterprise-operator  ServiceAccount that you use to run the\n . To watch a  subset of all namespaces , specify them in a\ncomma-separated list, escape each comma with a backslash,\nand surround the list in quotes, such as\n \"operator.watchNamespace=ns1\\,ns2\" . Watching a subset of namespaces is useful in deployments with\nmultiple   instances, where each   instance\nwatches a different subset of namespaces in your cluster. To deploy   and   to one or more   other\nthan the one where you deploy the  ,\nsee  Set Scope for   Deployment  for values you must use and\nadditional steps you might have to perform.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: APPDB_IMAGE_REPOSITORY\n              value: quay.io/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: APPDB_AGENT_VERSION\n              value: 10.2.15.5958-1_4.2.11-ent"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: APPDB_IMAGE_REPOSITORY\n              value: quay.io/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: APPDB_IMAGE_REPOSITORY\n              value: registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: DATABASE_VERSION\n              value: 2.0.0"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: IMAGE_PULL_POLICY\n              value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_APPDB_IMAGE_REPOSITORY\n              value: quay.io/mongodb/mongodb-enterprise-init-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_APPDB_IMAGE_REPOSITORY\n              value: registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb-init"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_APPDB_VERSION\n              value: 1.0.6"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n          - name: mongodb-enterprise-operator\n            image: <operatorVersionUrl>\n            imagePullPolicy: <policyChoice>\n            env:\n              - name: INIT_DATABASE_IMAGE_REPOSITORY\n                value:\n                quay.io/mongodb/mongodb-enterprise-init-database"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n          - name: mongodb-enterprise-operator\n            image: <operatorVersionUrl>\n            imagePullPolicy: <policyChoice>\n            env:\n              - name: INIT_DATABASE_IMAGE_REPOSITORY\n                value:\n                registry.connect.redhat.com/mongodb/mongodb-enterprise-init-database"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_DATABASE_VERSION\n              value: 1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_OPS_MANAGER_IMAGE_REPOSITORY\n              value: quay.io/mongodb/mongodb-enterprise-init-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_OPS_MANAGER_IMAGE_REPOSITORY\n              value: registry.connect.redhat.com/mongodb/mongodb-enterprise-init-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n        serviceAccountName: mongodb-enterprise-operator\n        containers:\n          - name: mongodb-enterprise-operator\n            image: <operatorVersionUrl>\n            imagePullPolicy: <policyChoice>\n            env:\n              - name: INIT_OPS_MANAGER_VERSION\n                value: 1.0.3"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: MANAGED_SECURITY_CONTEXT\n              value: false"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: MANAGED_SECURITY_CONTEXT\n              value: true"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n              value: quay.io/mongodb/mongodb-enterprise-database"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n              value: registry.connect.redhat.com/mongodb/mongodb-enterprise-database"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: OPERATOR_ENV\n              value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: OPS_MANAGER_IMAGE_PULL_POLICY\n              value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n          - name: OPS_MANAGER_IMAGE_REPOSITORY\n            value: quay.io/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n          - name: OPS_MANAGER_IMAGE_REPOSITORY\n            value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: WATCH_NAMESPACE\n              value: <testNamespace>"
                }
            ],
            "preview": "To provide optional settings, edit the  file that corresponds to\nyour deployment type in the directory where you cloned the\n repository:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/k8s-operator-om-specification",
            "title": "Ops Manager Resource Specification",
            "headings": [
                "Example",
                "Required  Resource Settings",
                "Optional  Resource Settings",
                "Prometheus Settings",
                "S3 Settings"
            ],
            "paragraphs": "The  MongoDB Enterprise Kubernetes Operator \ncreates a containerized   deployment from specification files\nthat you write. After you create or update an   resource specification, you\ndirect   to apply this specification to your  \nenvironment.   creates the services and custom  \nresources that   requires, then deploys   and its backing\napplication database in containers in your   environment. Each   resource uses an   specification in   to\ndefine the characteristics and settings of the deployment. The following example shows a resource specification for an  \ndeployment: This section describes settings that you must use for all  \nresources. Type : string Required . Version of the MongoDB   resource schema. Type : string Required . Kind of MongoDB Kubernetes resource to create. Set this\nto  MongoDBOpsManager . Type : string Required . Name of the MongoDB   resource you are creating. Resource names must be 44 characters or less. Type : integer Required . Number of   instances to run in parallel.\nThe minimum accepted value is  1 . For high availability, set this value to more than  1 .\n Multiple Ops Manager instances \ncan read from the same Application Database, ensuring failover if\none instance is unavailable and enabling you to update the\n  resource without downtime. Type : string Required . Version of   that you want to install\non this MongoDB   resource. Type : string Required . Name of the     you created for\nthe   admin user. When you deploy the   resource,\n  creates a user with these credentials. The admin user is granted the\n Global Owner \nrole. To avoid storing secrets in  , you can migrate all  \nto a  secret storage tool . Type : integer Required . Number of members in the\n Application Database \nreplica set. Type : string Required . Version of MongoDB installed on the  \n Application Database .\nYou must specify a compatible enterprise MongoDB version based on the tag in the\n container registry . Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. If you update this value to a later version, consider setting\n spec.featureCompatibilityVersion  to give yourself the\noption to downgrade if necessary.  resources can use the following settings: Type : collection   Application Database \nresource definition. The following settings from the\n replica set  resource specification are\noptional: spec.applicationDatabase. spec.additionalMongodConfig spec.applicationDatabase. spec.agent spec.applicationDatabase.agent. spec.agent.startupOptions spec.applicationDatabase.monitoringAgent. spec.agent.startupOptions spec.applicationDatabase. spec.featureCompatibilityVersion spec.applicationDatabase. spec.logLevel spec.applicationDatabase.podSpec.persistence. spec.podSpec.persistence.single spec.applicationDatabase.podSpec.persistence.multiple. spec.podSpec.persistence.multiple.data spec.applicationDatabase.podSpec.persistence.multiple. spec.podSpec.persistence.multiple.journal spec.applicationDatabase.podSpec.persistence.multiple. spec.podSpec.persistence.multiple.logs spec.applicationDatabase.podSpec. spec.podSpec.podTemplate All settings under  spec.applicationDatabase.agent  apply to both\nAutomation and Monitoring, unless you specify values for Automation\nand Monitoring separately in  spec.applicationDatabase.agent  and\n spec.applicationDatabase.monitoringAgent . Type : string Name of the  secret  that contains the\npassword for the   database user  mongodb-ops-manager .\n  uses this password to  authenticate to the Application\nDatabase . Type : string Name of the field in the  secret  that\ncontains the password for the   database user\n mongodb-ops-manager .   uses this password to\n authenticate to the Application Database . The default value is  password . Type : string Text to prefix to the name of the secret that contains the\napplication database's   certificate. Name the secret\n <prefix>-<metadata.name>-db-cert . Type : string Name of the     containing the   file for\nthe application database. This   signs the certificates that: spec.applicationDatabase.security.tls.ca  is required\nif you use a Custom Certificate Authority to sign your application\ndatabase   certificates. The   requires that the certificate is named\n ca-pem  in the ConfigMap. The   specified in this section is also used for\nconfiguring custom   certificates for   storage when either\n spec.backup.s3OpLogStores.customCertificate  or\n spec.backup.s3Stores.customCertificate  are set to\n true . the application database replica set members use to communicate\nwith one another, and  uses to communicate with the application database replica\nset. You must concatenate your custom   file and the entire\n  certificate chain from  downloads.mongodb.com  to prevent\n  from becoming inoperable if the application database\nrestarts. Type : string Text to prefix to the     that you\ncreated that contains your application database's   key and\ncertificate. You must name your secret  <prefix>-<metadata.name>-db-cert . To learn how to configure your   instance to run over\n , see  Deploy an   Resource . Encrypts communications using   certificates between   and\nthe application database. spec.security.applicationDatabase.tls.enabled  is\ndeprecated and will be removed in a future release. To enable\n , provide a value for the\n spec.security.applicationDatabase.certsSecretPrefix \nsetting. Type : boolean Flag that enables Backup for your   resource. When set to\n false , Backup is disabled. Default value is  true . Type : collection Configuration settings for the  head database .\n  creates a   with the specified configuration. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum size of   that should be mounted. This value is\nexpressed as an integer followed by a unit of storage in\n  notation. Default value is  30Gi . backup-hardware-requirements If the head database requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage specified in a  . You may create\nthis storage type as a   object before using it in this\n  specification. Make sure to set the    reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a   is removed. Type : array of strings Optional .   parameters passed to the   backup service\nin the container. This   parameter defaults to an empty list.  calculates the   memory heap values of the\nbackup service based on the container's memory. Changing the\n -Xms  and  -Xmx  values can cause issues with  . Type : integer Optional . Number of  backup daemon services \nto deploy in  . If not specified, defaults to  1 .\nTo ensure high availability for your backup service, deploy\n multiple backup daemons \nin  . Type : collection Required if you enable Backup. Array of  oplog stores  used\nfor Backup. Each item in the array references a MongoDB database\nresource deployed in the   cluster by the  . Type : string Required if you enable Backup. Name of the oplog store. Once specified, don't edit the name of the oplog store. Type : string Required if you enable Backup. Name of the MongoDB database resource that you create to store oplog\nslices. You must deploy this database resource in the same namespace\nas the   resource. The Oplog database only supports the  SCRAM  authentication mechanism.\nYou cannot enable other authentication mechanisms. If you enable  SCRAM  authentication on the oplog database, you\nmust: Create a MongoDB user resource to connect   to the oplog\ndatabase. Specify the  name \nof the user in the   resource definition. If a MongoDB database resource with this name doesn't exist, the\n backup  resource enters a  Pending  state. The  \nretries every 10 seconds until a MongoDB database resource with this\nname is created. The   begins to reconcile the   resource\nautomatically when you make security changes to the database\nresources you reference in this setting. The   updates\nthe  mongoURI  and  ssl  flags in the   configuration\nbased on your changes. Type : string Required if SCRAM authentication is enabled on the oplog\nstore database. Name of the MongoDB user resource used to connect to the oplog store\ndatabase. Deploy this user resource in the same\nnamespace as the   resource and with the\n readWriteAnyDatabase  and\n dbAdminAnyDatabase  roles. Type : collection Required if you enable Backup using a blockstore. Array of  blockstores  used\nfor Backup. Each item in the array references a MongoDB database\nresource deployed in the   cluster by the  . Type : string Required if you enable Backup using a blockstore. Name of the blockstore. Once specified, don't edit the name of the blockstore. Type : string Required if you enable Backup using a blockstore. Name of the MongoDB database resource that you create for the\nblockstore. You must deploy this database resource in the same\nnamespace as the   resource. The blockstore database only supports the  SCRAM  authentication\nmechanism. You cannot enable other authentication mechanisms. If you enable  SCRAM  authentication on the blockstore database,\nyou must: Create a MongoDB user resource to connect   to the\nblockstore database. Specify the  name \nof the user in the   resource definition. If a MongoDB database resource with this name doesn't exist, the\n backup  resource enters a  Pending  state. The  \nretries every 10 seconds until a MongoDB database resource with this\nname is created. The   begins to reconcile the   resource\nautomatically when you make security changes to the database\nresources you reference in this setting. The   updates\nthe  mongoURI  and  ssl  flags in the   configuration\nbased on your changes. Type : string Required if SCRAM authentication is enabled on the blockstore database. Name of the MongoDB user resource used to connect to the blockstore\ndatabase. Deploy this user resource in the same\nnamespace as the   resource and with the\n readWriteAnyDatabase  and\n dbAdminAnyDatabase  roles. Type : string Name of the secret that contains the  queryable.pem \nfile from   that you will use for accessing and querying backups\nbased on your deployment's   requirements.The PEM file contains\na public key certificate and its associated private key that are needed\nto access and run queries on backup snapshots in  .\nTo query backups, specify the value for this parameter. If not set,\nbackups are not affected, but you can't query them. Type : collection Specification for the   that the   creates\nfor the  backup daemon service . To review which fields you can add to\n spec.backup.statefulSet.spec , see\n StatefulSetSpec v1 apps  in the   documentation. Type : collection Template \nfor the   pods in the   that the   creates\nfor the  backup daemon service . The   doesn't validate the fields you provide\nin  spec.backup.statefulSet.spec.template . Type : collection Metadata for the   pods in the   that the\n  creates for the  backup daemon service . To review which fields you can add to\n spec.backup.statefulSet.spec.template.metadata , see\nthe  Kubernetes documentation . Type : collection Specifications of the   pods in the   that the\n  creates for the  backup daemon service . To review the complete list of fields you can add to\n spec.backup.statefulSet.spec.template.spec , see the\n Kubernetes documentation . The following example  spec.backup.statefulSet.spec.template.spec \ndefines minimum and maximum CPU and memory capacity for one\n backup daemon service  container the   deploys: Type : collection List of containers that belong to the   pods in the\n  that the   creates for the\n backup daemon service . To modify the specifications of the  backup daemon service  container,\nyou must provide the exact name of the container using the  name \nfield, as shown in the following example: When you add containers to\n spec.backup.statefulSet.spec.template.spec.containers ,\nthe   adds them to the   pod. These containers\nare appended to the  backup-daemon  containers in the pod. Type : string Minimum CPU capacity that must be available on a     to\nhost the  backup daemon service . The requested value must be less than or equal to\n spec.backup.statefulSet.spec.template.spec.containers.resources.limits.cpu . Type : string Maximum CPU capacity for the   being created to host\nthe  backup daemon service .\nIf omitted, this value is set to\n spec.backup.statefulSet.spec.template.spec.containers.resources.requests.cpu . Type : string Minimum memory capacity that must be available on a    \nto host the  backup daemon service  on  .\nThis value is expressed as an integer followed by a unit of memory\nin   notation. The requested value must be less than or equal to\n spec.backup.statefulSet.spec.template.spec.containers.resources.limits.memory . Set this value to at least  4.5Gi . Values of less than  4.5Gi \nmight result in an error. Type : string Maximum memory capacity for the   being created to host\nthe  backup daemon service . If omitted,\nthis value is set to  spec.backup.statefulSet.spec.template.spec.containers.resources.requests.memory . The   calculates and sets parameters for Java heap size\nbased on the container's memory. Setting this value to a value greater than 32 GB ( 32Gi ) can\ncause issues with the backup service. Excessive heaps can cause\nunpredictable results in  . Type : string  assigns each   a  . The   calculates\nthe   for each   using a provided  clusterDomain .\n  doesn't provide an   to query these hostnames. Type : string  assigns each   a  . The   calculates\nthe   for each   using a provided  clusterName .  \ndoesn't provide an   to query these hostnames. Use  spec.clusterDomain  instead. Type : collection  configuration properties.\nSee  Ops Manager Configuration Settings  for property names and descriptions.\nEach property takes a value of type  string . If   will manage MongoDB resources deployed outside of the\n  cluster it's deployed to, you must add the  mms.centralUrl \nsetting to  spec.configuration . Set the value to the URL by which   is exposed outside of the\n  cluster. Type : string The   service's  default server type . Accepted values are:  PRODUCTION_SERVER ,  TEST_SERVER ,  DEV_SERVER , and\n RAM_POOL . Type : collection Configuration object that enables external connectivity to  .\nIf provided, the   creates a    service  that allows traffic\noriginating from outside of the   cluster to reach the  \napplication. If not provided, the   doesn't create a   service.\nYou must create one manually or use a third-party solution that\nenables you to route external traffic to the   in your\n  cluster. Type : string The   service  ServiceType \nthat exposes   outside of  . Required  if  spec.externalConnectivity.type  is\npresent. Accepted values are:  LoadBalancer  and  NodePort .\n LoadBalancer  is recommended if your cloud provider supports it.\nUse  NodePort  for local deployments. Type : integer If  spec.externalConnectivity.type  is  NodePort , the\nport on the   service from which external traffic is routed to\nthe  . If  spec.externalConnectivity.type  is  LoadBalancer ,\nthe load balancer resource that your cloud provider creates routes\ntraffic to this port on the   service. You don't need to provide\nthis value.   uses an open port within the default range and\nhandles internal traffic routing appropriately. In both cases, if this value is not provided, the   service\nroutes traffic from an available port within the following default\nrange to the  :  30000 - 32767 . You must configure your network's firewall to allow traffic over\nthis port. Type : string The IP address the  LoadBalancer    service uses when the\n  creates it. This setting can only be used if your cloud provider supports it and\n spec.externalConnectivity.type  is  LoadBalancer . To\nlearn more about the\n Type LoadBalancer , see the\n  documentation. Type : string Routing policy for external traffic to the     service.\nThe service routes external traffic to node-local or cluster-wide\nendpoints depending the value of this setting. Accepted values are:  Cluster  and  Local . To learn which of\nvalues meet your requirements, see  Source IPs in Kubernetes  in the   documentation. If you select  Cluster , the  Source-IP  of your clients are\nlost during the network hops that happen at the  \nnetwork boundary. Type : collection Key-value pairs that allow you to provide cloud provider-specific\nconfiguration settings. To learn more about\n Annotations \nand\n TLS support on AWS ,\nsee the   documentation. Type : array of strings Optional .   parameters passed to the   in the\ncontainer. Any parameters given replace the default   parameters\nfor the  . This   parameter defaults to an empty list.  calculates its   memory heap values of the\n  based on the container's memory. Changing the\n -Xms  and  -Xmx  values can cause issues with  . Type : string Text to prefix to the     that you\ncreated that contain  's   key and certificate. You must name your secret  <prefix>-<metadata.name>-cert . To learn how to configure your   instance to run over\n , see  Deploy an   Resource . Name of the     that contains a custom  \nfile for  . This   signs the certificates that: spec.security.tls.ca  is required if you use a Custom\nCertificate Authority to sign your     certificates. The   requires that the certificate is named\n mms-ca.crt  in the ConfigMap. clients use to connect to the  , and agents in the application database   use to communicate\nwith  . You must concatenate your custom   file and the entire\n  certificate chain from  downloads.mongodb.com  to prevent\n  from becoming inoperable if the application database\nrestarts. Encrypts communications using   certificates between clients and\n . spec.security.tls.enabled  is\ndeprecated and will be removed in a future release. To enable\n , provide a value for the\n spec.security.certsSecretPrefix \nsetting. Type : collection Specification for the   that the   creates\nfor  . To review which fields you can add to\n spec.statefulSet.spec , see\n StatefulSetSpec v1 apps \nin the   documentation. Type : collection Template \nfor the   pods in the   that the   creates\nfor the  . The   doesn't validate the fields you provide\nin  spec.statefulSet.spec.template . Type : collection Metadata for the   pods in the   that the\n  creates for the  . To review which fields you can add to\n spec.statefulSet.spec.template.metadata , see\nthe  Kubernetes documentation . Type : collection Specifications of the   pods in the   that the\n  creates for the  . To review the complete list of fields you can add to\n spec.statefulSet.spec.template.spec , see the\n Kubernetes documentation . The following example  spec.statefulSet.spec.template.spec  defines\nminimum and maximum CPU and memory capacity for one  \ncontainer the   deploys: Type : collection List of containers that belong to the   pods in the\n  that the   creates for the\n . To modify the specifications of the   container,\nyou must provide the exact name of the container using the  name \nfield, as shown in the following example: When you add containers to\n spec.statefulSet.spec.template.spec.containers ,\nthe   adds them to the   pod. These containers\nare appended to the   containers in the pod. Type : string Minimum CPU capacity that must be available on a     to\nhost the  . The requested value must be less than or equal to\n spec.statefulSet.spec.template.spec.containers.resources.limits.cpu . Type : string Maximum CPU capacity for the   being created to host\nthe  . If omitted, this value is set to\n spec.statefulSet.spec.template.spec.containers.resources.requests.cpu . Type : string Minimum memory capacity that must be available on a    \nto host the   on  . This value is expressed as\nan integer followed by a unit of memory in   notation. The requested value must be less than or equal to\n spec.statefulSet.spec.template.spec.containers.resources.limits.memory . If   on   requires 6 gigabytes of memory, set\nthis value to  6Gi . MongoDB recommends setting this value to at least  5Gi . Type : string Maximum memory capacity for the   being created to host\nthe  . If omitted, this value is set to\n spec.statefulSet.spec.template.spec.containers.resources.requests.memory . The   calculates and sets parameters for Java heap size\nbased on the container's memory. Setting this value to a value greater than 32 GB ( 32Gi ) can\ncause issues with the backup service. Excessive heaps can cause\nunpredictable results in  . The following settings apply when you use Prometheus with your\napplication database: Type : array Optional List that contains the parameters for exposing metrics to Prometheus. Type : string Optional Default :  \"/metrics\" Human-readable string that indicates the path to the metrics\nendpoint. If you don't specify this setting, the default applies. Type : object Conditional Object that contains the details of the   for\nbasic HTTP authentication. If you want to use Prometheus with your\napplication database, you must specify this setting. Type : string Optional Default :  \"password\" Human-readable string that indentifies the key in the  \nthat stores the password for basic HTTP authentication. If you don't\nspecify this setting, the default applies. Type : string Conditional Human-readable label that identifies the   that contains\nthe password for basic HTTP authentication. If you want to use\nPrometheus with your application database, you must specify this\nsetting. Type : integer Optional Default:  9216 Number that identifies the port that the metrics endpoint will\nbind to. If you don't specify this setting, the default applies. Type : object Optional Object that contains the details of the   for  \nauthentication. Type : string Optional Default :  \"password\" Human-readable string that indentifies the key in the  \nthat stores the password for   authentication. If you don't\nspecify this setting, the default applies. Type : string Conditional Human-readable label that identifies the   that contains\nthe password for   authentication. If you want to use\nPrometheus with your application database and you want to use  \nauthentication, you must specify this setting. Type : string Conditional Human-readable label that identifies the user for basic HTTP\nauthentication. If you want to use Prometheus with your application\ndatabase, you must specify this setting. You can configure   to use   for storing oplogs and backup\nsnapshots, and secure connections to   with   using keys\nissued by custom  . To configure custom CA keys, use the ConfigMap with which you\nconfigured   for your application database as described on\nthe  TLS-Encrypted Connection (HTTPS)  tab of\n Deploy an   Resource .\nSet  spec.applicationDatabase.security.tls.ca  to this ConfigMap. You can use   for both   and your application database, or for\n  only. To use   for both, get certificates for both purposes from the\nsame  ca-pem  referenced in the ConfigMap. To use   for   only, don't define\n spec.security.applicationDatabase.certsSecretPrefix  in\nyour ConfigMap. Type : boolean Flag that indicates whether you use custom   certificates for\nyour   oplog store specified by\n spec.applicationDatabase.security.tls.ca .\nThe default is  False . Type : boolean Flag that enables using    IAM roles for service accounts \nin    EKS  to configure\nan   oplog store. The default is  False . If you aren't using\n  EKS, this flag has no effect. When set to  False , using  \nIAM roles for service accounts in EKS to configure an   oplog store\nis disabled. To learn more, see\n IAM roles for service accounts in EKS . Type : string Required to store the oplog using an S3 store. Name of the   oplog store. Type : string Name of the MongoDB database resource that you create to store\nmetadata for the   oplog store. You must deploy this database\nresource in the same namespace as the   resource. If you enable  SCRAM  authentication on this database, you must: Omit this setting to use the application database to store\nmetadata for the   oplog store. If you omit this setting, you must also omit the\n spec.backup.s3OpLogStores.mongodbUserRef.name  setting.\nThe   handles  SCRAM  user authentication\ninternally. Create a MongoDB user resource to connect   to the\ndatabase. Specify the\n name  of the\nuser in the   resource definition. Type : string Required if you created a MongoDB database resource to store\nS3 oplog metadata and SCRAM is enabled on this database. Name of the MongoDB user resource used to connect to the metadata\ndatabase of the   oplog store. Deploy this user resource in the\nsame namespace as the   resource and with the\n readWriteAnyDatabase  and\n dbAdminAnyDatabase  roles. Once specified, don't edit the name of the   metadata oplog\nstore username. Type : string Required to store the oplog using an S3 store. Name of the secret that contains the  accessKey  and\n secretKey  fields. The  backup daemon service  uses\nthe values of these fields as credentials to access your\n    or  -compatible bucket. To configure the   oplog\nstore, you must specify both keys in the secret. Type : boolean Indicates the style of the bucket endpoint URL. Default value is  true . Value Description Example true Path-style URL s3.amazonaws.com/<bucket> false Virtual-host-style URL <bucket>.s3.amazonaws.com Type : string Required to store the oplog using an S3 store. URL of the     bucket or  -compatible bucket that hosts the\noplog store. If your endpoint doesn't include a region in its  ,\nspecify the  s3RegionOverride \nfield. Type : string Required to store the oplog using an S3 store. Name of the     bucket or  -compatible bucket that hosts\nthe oplog store. Type : string Region where your  -compatible bucket resides. Use this\nfield only if your   oplog store's\n s3BucketEndpoint \ndoesn't support region scoping. Region scoping is when your endpoint doesn't include a region in its  . Don't use this field with     buckets. For more information, see\n S3 Blockstore Configuration . Type : boolean Flag that indicates whether you use custom   certificates for\nyour   snapshot store specified by\n spec.applicationDatabase.security.tls.ca .\nThe default is  False . Type : boolean Flag that enables using    IAM roles for service accounts \nin    EKS  to configure\nan   snapshot store. The default is  False . If you aren't using\n  EKS, this flag has no effect. When set to  False , using  \nIAM roles for service accounts in EKS to configure an   snapshot\nstore is disabled. To learn more, see\n IAM roles for service accounts in EKS . Type : string Required to store the oplog using an S3 store. Name of the   snapshot store. Once specified, don't edit the name of the   snapshot store. This change will likely fail if\nbackups use the old name. The consequences of\na successful change are unpredictable. Type : string Name of the MongoDB database resource that you create to store\nmetadata for the   snapshot store. You must deploy this database\nresource in the same namespace as the   resource. If you enable  SCRAM  authentication on this database, you must: Omit this setting to use the application database to store\nmetadata for the   snapshot store. If you omit this setting, you must also omit the\n spec.backup.s3Stores.mongodbUserRef.name  setting.\nThe   handles  SCRAM  user authentication\ninternally. Create a MongoDB user resource to connect   to the\ndatabase. Specify the\n name  of the\nuser in the   resource definition. Once specified, don't edit the name of the   snapshot store.\nThis change will likely fail if backups use the old name. The\nconsequences of a successful change are unpredictable. If a MongoDB database resource with this name doesn't exist, the\n backup  resource enters a  Pending  state. The  \nretries every 10 seconds until a MongoDB database resource with this\nname is created. The   begins to reconcile the   resource\nautomatically when you make security changes to the database\nresources you reference in this setting. The   updates\nthe  mongoURI  and  ssl  flags in the   configuration\nbased on your changes. Type : string Required if you created a MongoDB database resource to store\n|s3| snapshot metadata and SCRAM is enabled on this database. Name of the MongoDB user resource used to connect to the metadata\ndatabase of the   snapshot store. Deploy this user resource in the\nsame namespace as the   resource and with the\n readWriteAnyDatabase  and\n dbAdminAnyDatabase  roles. Once specified, don't edit the name of the   metadata snapshot\nstore username. Type : string Required if you enable Backup using an S3 store. Name of the secret that contains the  accessKey  and\n secretKey  fields. The  backup daemon service  uses\nthe values of these fields as credentials to access your\n    or  -compatible bucket. The   snapshot store\ncan't be configured if the secret is missing either key. Type : boolean Indicates the style of the bucket endpoint URL. Default value is  true . Value Description Example true Path-style URL s3.amazonaws.com/<bucket> false Virtual-host-style URL <bucket>.s3.amazonaws.com Type : string Required if you enable Backup using an S3 store. URL of the     bucket or  -compatible bucket that hosts the\nsnapshot store. If your endpoint doesn't include a region in its  ,\nspecify the  s3RegionOverride \nfield. Type : string Required if you enable Backup using an S3 store. Name of the     bucket or  -compatible bucket that hosts\nthe snapshot store. Type : string Region where your  -compatible bucket resides. Use this\nfield only if your   store's\n s3BucketEndpoint \ndoesn't support region scoping. Region scoping is when your endpoint doesn't include a region in its  . Don't use this field with     buckets. For more information, see\n S3 Blockstore Configuration .",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: om\nspec:\n replicas: 1\n version: \"5.0.0\"\n adminCredentials: ops-manager-admin\n configuration:\n  mms.fromEmailAddr: admin@example.com\n  mms.security.allowCORS: \"false\"\n backup:\n  enabled: true\n  headDB:\n   storage: \"30Gi\"\n   labelSelector:\n    matchLabels:\n     app: my-app\n  opLogStores:\n   - name: oplog1\n     mongodbResourceRef:\n      name: my-oplog-db\n     mongodbUserRef:\n      name: my-oplog-user\n  s3Stores:\n   - name: s3store1\n     mongodbResourceRef:\n      name: my-s3-metadata-db\n     mongodbUserRef:\n      name: my-s3-store-user\n     s3SecretRef:\n       name: my-s3-credentials\n     pathStyleAccessEnabled: true\n     s3BucketEndpoint: s3.region.amazonaws.com\n     s3BucketName: my-bucket\n\n applicationDatabase:\n   passwordSecretKeyRef:\n    name: om-db-user-secret\n    key: password\n   members: 3\n   version: \"4.4.18-ent\"\n"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  backup:\n    jvmParameters: [\"-XX:+UseStringCache\"]"
                },
                {
                    "lang": "yaml",
                    "value": "statefulSet:\n  spec:\n    template:\n      spec:\n        containers:\n        - name: mongodb-backup-daemon\n          resources:\n            requests:\n              cpu: \"0.50\"\n              memory: \"4500M\"\n            limits:\n              cpu: \"1\"\n              memory: \"6000M\""
                },
                {
                    "lang": "yaml",
                    "value": "backup:\n statefulSet:\n   spec:\n     template:\n       spec:\n         containers:\n         - name: mongodb-backup-daemon"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  jvmParameters: [\"-XX:+HeapDumpOnOutOfMemoryError\",\"-XX:HeapDumpPath=/tmp\"]"
                },
                {
                    "lang": "yaml",
                    "value": "statefulSet:\n  spec:\n    template:\n      spec:\n        containers:\n          - name: mongodb-ops-manager\n            resources:\n              requests:\n                cpu: \"0.70\"\n                memory: \"6Gi\"\n              limits:\n                cpu: \"1\"\n                memory: \"7000M\""
                },
                {
                    "lang": "yaml",
                    "value": "backup:\n statefulSet:\n   spec:\n     template:\n       spec:\n         containers:\n         - name: mongodb-ops-manager"
                }
            ],
            "preview": "Type: string",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/known-issues",
            "title": "Known Issues in the ",
            "headings": [
                "mongos Instances Fail to Reach Ready State After Disabling Authentication",
                "Update Google Firewall Rules to Fix WebHook Issues",
                "Configure Persistent Storage Correctly",
                "Remove Resources before Removing ",
                "Create Separate Namespaces for  and MongoDB Resources",
                "HTTPS Enabled After Deployment",
                "Unable to Update the MongoDB Agent on Application Database Pods",
                "Unable to Pull Enterprise Kubernetes Operator Images from IBM Cloud Paks",
                "Machine Memory vs. Container Memory"
            ],
            "paragraphs": "If you disable authentication by setting\n spec.security.auth.enabled  to   false , the   Pods\nnever reach a  ready  state. As a workaround, delete each   Pod in your deployment. Run the following command to list all of your Pods: For each Pod with a name that contains  mongos , delete it with the\nfollowing command: When you delete a Pod, Kubernetes recreates it. Each Pod that Kubernetes\nrecreates receives the updated configuration and can reach a  READY \nstate. To confirm that all of your   Pods are  READY , run the\nfollowing command: A response like the following indicates that all of your   Pods\nare  READY : This issue applies only to  sharded clusters \nthat meet the following criteria: Deployed using the   1.13.0 Use X.509 authentication Use  kubernetes.io/tls  secrets for  \ncertificates for the MongoDB Agent When you deploy   to   private clusters, the\n  or MongoDBOpsManager resource creation could time out.\nThe following message might appear in the logs: Error setting state to reconciling: Timeout: request did not\ncomplete within requested timeout 30s\". Google configures its firewalls to restrict access to your  \n . To use the webhook service,\n add a new firewall rule \nto grant   control plane access to your webhook service. The   webhook service runs on port 443. If there are no\n persistent volumes \navailable when you create a resource, the resulting   stays in\ntransient state and the Operator fails  (after 20 retries) with the\nfollowing error: To prevent this error, either: For testing only, you may also set  persistent : false . This\n must not be used in production , as data is not preserved between\nrestarts. Provide   or Set  persistent : false  for the resource Sometimes   can diverge from  . This mostly occurs when\n  resources are removed manually.   can keep displaying an\nAutomation Agent which has been shut down. If you want to remove deployments of MongoDB on  , use the\nresource specification to delete resources first so no dead Automation\nAgents remain. The best strategy is to create   and its resources in\ndifferent namespaces so that the following operations would work\ncorrectly: or If the   and resources sit in the same  mongodb \n , then operator would also be removed in the same operation.\nThis would mean that it could not clean the configurations, which\nwould have to be done in the  . We recommend that you enable    before  deploying your   resources.\nHowever, if you enable   after deployment,\nyour managed resources can no longer communicate with   and\nthe   reports your resources' status as  Failed . To resolve this issue, you must delete your   by\nrunning the following command for each Pod: After deletion,   automatically restarts the deleted Pods.\nDuring this period, the resource is unreachable and incurs\ndowntime. Configure   to Run over HTTPS Troubleshoot the  You can't use   to upgrade the MongoDB Agents that run on the\nApplication Database Pods. The MongoDB Agent version that runs on these\nPods is embedded in the Application Database Docker image. You can use the   to upgrade the MongoDB Agent version on\nApplication Database Pods as MongoDB publishes new images. APPDB_AGENT_VERSION appDb.version If you pull the   images from a container registry hosted in\n IBM Cloud Paks , the IBM Cloud Paks\nchanges the names of the images by adding a digest SHA to the official image\nnames. This action results in error messages from the   similar\nto the following: As a workaround, update the Ops Manager Application Database resource definition\nin  spec.applicationDatabase.podSpec. spec.podSpec.podTemplate  to\nspecify the new names for the   images that contain the digest SHAs,\nsimilar to the following example. The Automation Agent in   and   reports\nhost memory (RAM) usage instead of the   container memory usage.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl get pods"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete pod <podname>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                           READY   STATUS    RESTARTS   AGE\nmongodb-enterprise-operator-6495bdd947-ttwqf   1/1     Running   0          50m\nmy-sharded-cluster-0-0                         1/1     Running   0          12m\nmy-sharded-cluster-1-0                         1/1     Running   0          12m\nmy-sharded-cluster-config-0                    1/1     Running   0          12m\nmy-sharded-cluster-config-1                    1/1     Running   0          12m\nmy-sharded-cluster-mongos-0                    1/1     Running   0          11m\nmy-sharded-cluster-mongos-1                    1/1     Running   0          11m\nom-0                                           1/1     Running   0          42m\nom-db-0                                        2/2     Running   0          44m\nom-db-1                                        2/2     Running   0          43m\nom-db-2                                        2/2     Running   0          43m"
                },
                {
                    "lang": "sh",
                    "value": "Failed to update Ops Manager automation config: Some agents failed to register"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete pods --all"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete pod <replicaset-pod-name>"
                },
                {
                    "lang": "sh",
                    "value": "Failed to apply default image tag \"cp.icr.io/cp/cpd/ibm-cpd-mongodb-agent@\nsha256:10.14.24.6505-1\": couldn't parse image reference \"cp.icr.io/cp/cpd/\nibm-cpd-mongodb-agent@sha256:10.14.24.6505-1\": invalid reference format"
                },
                {
                    "lang": "sh",
                    "value": "applicationDatabase:\n  # The version specified must match the one in the image provided in the `mongod` field\n  version: 4.4.11-ent\n  members: 3\n  podSpec:\n    podTemplate:\n      spec:\n        containers:\n          - name: mongodb-agent\n            image: 'cp.icr.io/cp/cpd/ibm-cpd-mongodb-agent@sha256:689df23cc35a435f5147d9cd8a697474f8451ad67a1e8a8c803d95f12fea0b59'"
                }
            ],
            "preview": "If you disable authentication by setting\nspec.security.auth.enabled to  false, the  Pods\nnever reach a ready state.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-op-prerequisites",
            "title": "Prerequisites",
            "headings": [
                "Procedure",
                "Have a  solution available to use.",
                "Clone the MongoDB Enterprise Kubernetes Operator repository.",
                "Create a  for your  deployment.",
                "Optional: Have a running .",
                "Required for OpenShift Installs: Create a  that contains credentials authorized to pull images from the registry.connect.redhat.com repository."
            ],
            "paragraphs": "To install the MongoDB  , you must: If you need a   solution, see the  \n documentation on picking the right solution . You can use  Helm  to install the\n . To learn how to install Helm, see its\n documentation on GitHub . By default, The   uses the  mongodb  namespace. To\nsimplify your installation, consider creating a namespace labeled\n mongodb  using the following   command: If you do not want to use the  mongodb  namespace, you can label\nyour namespace anything you like: If you don't deploy an   resource with the\n , you must have an   running outside of your\n  cluster. If you will deploy an   resource in   with\nthe  , skip this prerequisite. Your   installation must run an active   service. If\nthe   host's clock falls out of sync, that host can't\ncommunicate with the  . To learn how to check your   service for your  \nhost, see the documentation for  RHEL . If you use the   to deploy MongoDB\nresources to  multiple namespaces  or with\na  cluster-wide scope , create the secret\nonly in the namespace where you intend to deploy the  . The\n  synchronizes the secret across all watched namespaces. If you have not already, obtain a Red Hat subscription. Create a  Registry Service Account . Click on your Registry Service Account, then click the\n Docker Configuration  tab. Download the  <account-name>-auth.json  file and open it in a\ntext editor. Copy the  registry.redhat.io  object, and paste another instance\nof this object into the file. Remember to add a comma after the\nfirst object. Rename the second object\n registry.connect.redhat.com , then save the file: Create an  openshift-pull-secret.yaml  file and add the contents\nof the modified  <account-name>-auth.json  file as\n stringData  named  .dockerconfigjson  to the\n openshift-pull-secret.yaml  secret file. The value you provide in the  metadata.name  field contains\nthe secret name. Provide this value when asked for the\n <openshift-pull-secret> . Create a   from the  openshift-pull-secret.yaml \nfile in the same namespace in which you will deploy the  .",
            "code": [
                {
                    "lang": "sh",
                    "value": "git clone https://github.com/mongodb/mongodb-enterprise-kubernetes.git"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace <namespaceName>"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"auths\": {\n    \"registry.redhat.io\": {\n      \"auth\": \"<encoded-string>\"\n    },\n    \"registry.connect.redhat.com\": {\n      \"auth\": \"<encoded-string>\"\n    }\n  }\n}"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: openshift-pull-secret\nstringData:\n  .dockerconfigjson: |\n      {\n        \"auths\": {\n          \"registry.redhat.io\": {\n            \"auth\": \"<encoded-string>\"\n          },\n          \"registry.connect.redhat.com\": {\n            \"auth\": \"<encoded-string>\"\n          }\n        }\n      }\ntype: kubernetes.io/dockerconfigjson"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f openshift-pull-secret.yaml -n <metadata.namespace>"
                }
            ],
            "preview": "To install the MongoDB , you must:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/edit-deployment",
            "title": "Edit a Database Resource",
            "headings": [
                "Considerations",
                "Prerequisites",
                "Procedure"
            ],
            "paragraphs": "You can modify a deployment's configuration and topology, including its\nMongoDB versions, storage engines, and numbers of hosts or shards. You\ncan make modifications at all levels of a deployment's topology from a\ntop-level  sharded cluster  or\n replica set  to lower levels, such as a replica set within a\nsharded cluster, or an individual process within a replica set. You can\nalso modify  standalone  processes. You can't change individual members of a replica set or sharded cluster.\nYou can change only the whole set or cluster. If a setting isn't available for a MongoDB resource, you can make changes\nusing only the  Ops Manager  or  Cloud Manager  application. MongoDB custom resources do not support all\n mongod  command line options. If you use an\nunsupported option in your object specification file, the backing\n MongoDB Agent \noverrides the unsupported options. For a complete list of options\nsupported by MongoDB custom resources, see  MongoDB Database Resource Specification . You can configure certain settings using only the  . To\nreview the list of settings, see  MongoDB   Exclusive Settings . Before you update a MongoDB  , complete the following procedures: Install and Configure the  Create Credentials for the  Create One Project using a ConfigMap Deploy a database Edit the   resource specification file. Modify or add any settings you need added or changed. Save your specification file. Invoke the following   command to update your resource.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <standalone-conf>.yaml"
                }
            ],
            "preview": "You can modify a deployment's configuration and topology, including its\nMongoDB versions, storage engines, and numbers of hosts or shards. You\ncan make modifications at all levels of a deployment's topology from a\ntop-level sharded cluster or\nreplica set to lower levels, such as a replica set within a\nsharded cluster, or an individual process within a replica set. You can\nalso modify standalone processes.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/resize-pv-storage",
            "title": "Resize Storage for One Database Resource",
            "headings": [
                "Prerequisites",
                "Storage Class Must Support Resizing",
                "Procedure",
                "Create or identify a persistent .",
                "Insert data to the database that the resource serves.",
                "Patch each persistence volume.",
                "Remove the StatefulSets.",
                "Update the database resource with a new storage value.",
                "Update the pods in a rolling fashion.",
                "Validate data exists on the updated ."
            ],
            "paragraphs": "Make sure the   and volume plugin provider that the  \nuse supports resize: If you don't have a StorageClass that supports resizing, ask your  \nadministrator to help. Use an existing database resource or create a new one with persistent\nstorage. Wait until the persistent volume gets to the  Running \nstate. A database resource with persistent storage would include: Start   in the   cluster. Insert data into the  test  database. Invoke the following commands for the entire replica set: Wait until each   gets to the following condition: Delete a   resource. This step removes the   only. The pods remain\nunchanged and running. Update the disk size. Open your preferred text editor and make\nchanges similar to this example: To update the disk size of the replica set to 2 GB, change the\n storage  value in database resource specification: Recreate a   resource with the new volume size. Wait until this StatefulSet achieves the  Running  state. Invoke the following command: The new pods mount the resized volume. If the   were reused, the data that you inserted in  Step\n2  can be found on the databases stored in  :",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl patch storageclass/<my-storageclass> --type='json' \\\n        -p='[{\"op\": \"add\", \"path\": \"/allowVolumeExpansion\", \"value\": true }]'"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.4.0\"\n  project: my-project\n  credentials: my-credentials\n  type: ReplicaSet\n  podSpec:\n    persistence:\n      single:\n        storage: \"1Gi\""
                },
                {
                    "lang": "sh",
                    "value": "$kubectl exec -it <my-replica-set>-0 \\\n         /var/lib/mongodb-mms-automation/mongodb-linux-x86_64-4.4.0/bin/mongo"
                },
                {
                    "lang": "javascript",
                    "value": "<my-replica-set>:PRIMARY> use test\n\nswitched to db test\n\n<my-replica-set>:PRIMARY> db.tmp.insertOne({\"foo\":\"bar\"})\n\n{\n  \"acknowledged\" : true,\n  \"insertedId\" : ObjectId(\"61128cb4a783c3c57ae5142d\")\n}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl patch pvc/\"data-<my-replica-set>-0\" -p='{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"2Gi\"}}}}'\nkubectl patch pvc/\"data-<my-replica-set>-1\" -p='{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"2Gi\"}}}}'\nkubectl patch pvc/\"data-<my-replica-set>-2\" -p='{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"2Gi\"}}}}'"
                },
                {
                    "lang": "yaml",
                    "value": "- lastProbeTime: null\n  lastTransitionTime: \"2019-08-01T12:11:39Z\"\n  message: Waiting for user to (re-)start a pod to finish file\n           system resize of volume on node.\n  status: \"True\"\n  type: FileSystemResizePending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete sts --cascade=false <my-replica-set>"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.4.0\"\n  project: my-project\n  credentials: my-credentials\n  type: ReplicaSet\n  podSpec:\n    persistence:\n      single:\n        storage: \"2Gi\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f my-replica-set-vol.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl rollout restart sts <my-replica-set>"
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl exec -it <my-replica-set>-1 \\\n          /var/lib/mongodb-mms-automation/mongodb-linux-x86_64-4.4.0/bin/mongo"
                },
                {
                    "lang": "javascript",
                    "value": "<my-replica-set>:PRIMARY> use test\nswitched to db test\n\n<my-replica-set>:PRIMARY> db.tmp.count()\n1"
                }
            ],
            "preview": "Make sure the  and volume plugin provider that the \nuse supports resize:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/create-project-using-configmap",
            "title": "Create One Project using a ConfigMap",
            "headings": [
                "Considerations",
                "Prerequisites",
                "Create One Project Using a ConfigMap",
                "Configure kubectl to default to your namespace.",
                "Invoke the following command to create a ConfigMap.",
                "Find the organization's ID.",
                "Copy and paste the orgId into the URL.",
                "Find the organization's ID.",
                "Copy and paste the orgId into the URL.",
                "Invoke the following  command to verify your .",
                "Connect to HTTPS-enabled Ops Manager Using a Custom CA",
                "Create a ConfigMap for the Certificate Authority certificate.",
                "Copy the highlighted section of the following example ConfigMap.",
                "Add the highlighted section to your project's ConfigMap.",
                "Specify the TLS settings",
                "Save your updated ConfigMap.",
                "Invoke the  command to verify your .",
                "Next Steps"
            ],
            "paragraphs": "The   uses a     to create or link your\n   Project . To create a\n  ConfigMap, you can edit a few lines of the\n example ConfigMap    file and apply\nthe ConfigMap. To view a full example, see the  project.yaml \nfile. Alternatively, you can use the    UI  or the  \n UI  to\nautomatically generate the ConfigMap YAML file, which you can then\napply to your Kubernetes environment. You can only deploy one MongoDB resource per project. See  Deploy a MongoDB Database Resource . You can use the   to deploy MongoDB resources with\n  and with   version 5.0.x or later. You can use the  Atlas Operator \nto deploy MongoDB resources to  . Kubernetes version 1.11 or later or Openshift version\n3.11 or later.  version 0.11 or later\n installed . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Key Type Description Example metadata.name string Name of the    . Resource names must be 44 characters or less.  documentation on  names .\nThis name must follow  RFC1123  naming\nconventions, using only lowercase alphanumeric\ncharacters,  -  or  . , and must start and end with an\nalphanumeric character. my-project metadata.namespace string    where the   creates this\n  and other  . mongodb data.projectName string Label for your  \n Project . The   creates the   project if it does\nnot exist. If you omit the  projectName , the  \ncreates a project with the same name as your\n  resource. To use an existing project in a  \norganization, locate\nthe  projectName  by clicking the  All Clusters \nlink at the top left of the   page, and\nsearching by name in the  Search \nbox, or scrolling to find the name in the list.\nEach card in this list represents the\ncombination of one    Organization  and  Project . myProjectName data.orgId string 24 character hex string that uniquely\nidentifies your\n   Organization . Depending on your    credentials , this field is: You can use the   to deploy MongoDB resources with\n  and with   version 5.0.x or later. You can use the  Atlas Operator \nto deploy MongoDB resources to  . Required  for  Global Programmatic API Keys . Optional  for  Organization Programmatic API Keys . You must specify an  existing Organization . Click  Settings  in the left navigation bar. Select your organization, view the current  \nin your browser and copy the value displayed in\nthe  <orgId>  placeholder as follows: https://ops.example.com:8443/ \n v2#/org/<orgId>/projects If specified, the   links to the organization. To find the  orgID  of your organization: If omitted,   creates an organization called\n projectName  that contains a project also called\n projectName . You must have the  Organization Project Creator \nrole to create a new project within an existing\n  organization. Click  Settings  in the left navigation bar. Select your organization, view the current  \nin your browser and copy the value displayed in\nthe  <orgId>  placeholder as follows: https://ops.example.com:8443/ \n v2#/org/<orgId>/projects 5b890e0feacf0b76ff3e7183 data.baseUrl string  to your   including the   and port\nnumber. If you deploy   with the   and   will\nmanage MongoDB database resources deployed  outside  of the  \ncluster it's deployed to, you must set  data.baseUrl  to the same\nvalue of the\n spec.configuration.mms.centralUrl \nsetting in the   resource specification. If you're using  , set the  data.baseUrl  value\nto  https://cloud.mongodb.com . https://ops.example.com:8443 This command returns a ConfigMap description in the shell: You might have chosen to use your own   certificate to enable\n  for your   instance. If you used a custom certificate,\nyou need to add the CA that signed that custom certificate to the\n . To add your custom CA, complete the following: The   requires the root   certificate of the\nCertificate Authority that issued the   host's certificate. Run\nthe following command to create a   containing the root\nCA certificate in the same namespace of your database pods: The   requires that the certificate is named\n mms-ca.crt  in the ConfigMap. Invoke the following command to edit your project's ConfigMap in\nthe default configured editor: Paste the highlighted section in the example   at\nthe end of the project ConfigMap. Change the following   keys: Key Type Description Example sslMMSCAConfigMap string Name of the   created in the first step\ncontaining the root   certificate used to sign the\n  host's certificate. This mounts the CA certificate\nto the   and database resources. my-root-ca sslRequireValidMMSServerCertificates boolean Forces the Operator to require a valid   certificate\nfrom  . The value must be enclosed in single quotes or the\noperator will throw an error. 'true' This command returns a ConfigMap description in the shell:  defaults to an empty namespace if you do not specify the\n -n  option, resulting in deployment failures. The\n ,  , and  s should run in the\nsame unique namespace. Now that you created your ConfigMap,  Create Credentials for the   before\nyou start  deploying MongoDB resources .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-project\n  namespace: mongodb\ndata:\n  projectName: myProjectName # this is an optional parameter\n  orgId: 5b890e0feacf0b76ff3e7183 # this is an optional parameter\n  baseUrl: https://ops.example.com:8443\n\nEOF"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe configmaps <configmap-name>"
                },
                {
                    "lang": "sh",
                    "value": "Name:           <configmap-name>\nNamespace:      <namespace>\nLabels:         <none>\nAnnotations:    <none>\n\nData\n====\nbaseUrl:\n----\n<myOpsManagerURL>\nEvents:  <none>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n <metadata.namespace> create configmap <root-ca-configmap-name> \\\n  --from-file=mms-ca.crt"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: <my-configmap>\n  namespace: <my-namespace>\ndata:\n  projectName: <my-ops-manager-project-name>\n  orgId: <org-id> # Optional\n  baseUrl: https://<my-ops-manager-URL>\n\n"
                },
                {
                    "lang": "yaml",
                    "value": "  sslMMSCAConfigMap: <root-ca-configmap-name>\n  sslRequireValidMMSServerCertificates: \u2018true\u2019\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl edit configmaps <my-configmap> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe configmaps <my-configmap> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Name:           <my-configmap>\nNamespace:      <namespace>\nLabels:         <none>\nAnnotations:    <none>\n\nData\n====\nsslMMSCAConfigMap:\n----\n<root-ca-configmap-name>\nsslRequireValidMMSServerCertificates:\n----\ntrue\nEvents:  <none>"
                }
            ],
            "preview": "The  uses a   to create or link your\n Project. To create a\n ConfigMap, you can edit a few lines of the\nexample ConfigMap  file and apply\nthe ConfigMap. To view a full example, see the project.yaml\nfile.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/k8s-operator-specification",
            "title": "MongoDB Database Resource Specification",
            "headings": [
                "Common Resource Settings",
                "Required",
                "Conditional",
                "Optional",
                "Deployment-Specific Resource Settings",
                "Standalone Settings",
                "Replica Set Settings",
                "Sharded Cluster Settings",
                "Prometheus Settings",
                "Security Settings",
                "Examples",
                "StatefulSet Settings"
            ],
            "paragraphs": "The  MongoDB Enterprise Kubernetes Operator \ncreates     from specification files that you\nwrote. MongoDB resources are created in Kubernetes as\n custom resources .\nAfter you create or update a   specification, you direct\n  to apply this specification to your   environment.\n  creates the defined  , services and\nother Kubernetes resources. After the Operator finishes creating those\nobjects, it updates the   deployment configuration to\nreflect changes. Each   uses an object specification in   to define the\ncharacteristics and settings of the MongoDB object: standalone,\n replica set , and  sharded cluster . At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . Deployment Type StatefulSets Size of StatefulSet Standalone 1 1  Replica Set 1 1   per member Sharded Cluster <numberOfShards> + 2 1   per  , shard, or config server member Every resource type must use the following settings: Every resource must use  one  of the following settings: Every resource type may use the following settings: Other settings you can and must use in a   specification\ndepend upon which MongoDB deployment item you want to create: Standalone Settings Replica Set Settings Sharded Cluster Settings All of the  Standalone Settings  also apply to replica set\nresources. The following settings apply only to replica set resource types: All of the  Standalone Settings  also apply to replica set\nresources. The following settings apply only to sharded cluster resource types: You can use Prometheus with your standalone resource, replica sets, or\nsharded clusters. To learn more, see  Deploy a Resource to Use with Prometheus . To view\nan example, see  MongoDB Resource with Prometheus . The following settings apply when you use Prometheus\nwith your MongoDB resource: Type : array Optional List that contains the parameters for exposing metrics to Prometheus. Type : string Optional Default :  \"/metrics\" Human-readable string that indicates the path to the metrics\nendpoint. If you don't specify this setting, the default applies. Type : object Conditional Object that contains the details of the   for\nbasic HTTP authentication. If you want to use Prometheus with your\nMongoDB resource, you must specify this setting. Type : string Optional Default :  \"password\" Human-readable string that indentifies the key in the  \nthat stores the password for basic HTTP authentication. If you don't\nspecify this setting, the default applies. Type : string Conditional Human-readable label that identifies the   that contains\nthe password for basic HTTP authentication. If you want to use\nPrometheus with your MongoDB resource, you must specify this setting. Type : integer Optional Default:  9216 Number that identifies the port that the metrics endpoint will\nbind to. If you don't specify this setting, the default applies. Type : object Optional Object that contains the details of the   for  \nauthentication. Type : string Optional Default :  \"password\" Human-readable string that indentifies the key in the  \nthat stores the password for   authentication. If you don't\nspecify this setting, the default applies. Type : string Conditional Human-readable label that identifies the   that contains\nthe password for   authentication. If you want to use\nPrometheus with your MongoDB resource and you want to use  \nauthentication, you must specify this setting. Type : string Conditional Human-readable label that identifies the user for basic HTTP\nauthentication. If you want to use Prometheus with your MongoDB\nresource, you must specify this setting. The following security settings apply only to replica set and sharded\ncluster resource types: The following example shows a resource specification for a\nstandalone deployment with every setting provided: The following example shows a resource specification for a\n replica set  with every setting provided: The following example shows a resource specification for a\n sharded cluster  with every setting provided: The following   settings apply only to replica set and\nsharded cluster resource types. Type : collection Specification for the   that the   creates\nfor  . Type : string Default :  <resource_name>-svc  and  <resource_name>-svc-external Name of the   service to be created or used for a\n . If the service with this name already exists, the\n  does not delete or recreate it. This setting lets\nyou create your own custom services and lets the  \nreuse them.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone\nspec:\n  version: \"4.2.2-ent\"\n  service: my-service\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  persistent: true\n  type: Standalone\n  additionalMongodConfig:\n    systemLog:\n      logAppend: true\n      verbosity: 4\n    operationProfiling:\n      mode: slowOp\n  podSpec:\n    persistence:\n      single:\n        storage: \"12Gi\"\n        storageClass: standard\n        labelSelector:\n          matchExpressions:\n          - {key: environment, operator: In, values: [dev]}\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/e2e-az-name\n            operator: In\n            values:\n            - e2e-az1\n            - e2e-az2\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: \"4.4.0-ent\"\n  service: my-service\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  persistent: true\n  type: ReplicaSet\n  podSpec:\n    persistence:\n      multiple:\n        data:\n          storage: \"10Gi\"\n        journal:\n          storage: \"1Gi\"\n          labelSelector:\n            matchLabels:\n              app: \"my-app\"\n        logs:\n          storage: \"500M\"\n          storageClass: standard\n    podAntiAffinityTopologyKey: nodeId\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/e2e-az-name\n            operator: In\n            values:\n            - e2e-az1\n            - e2e-az2\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n  security:\n    certsSecretPrefix: \"prefix\"\n    tls:\n      ca: custom-ca\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n  statefulSet:\n    spec:\n      serviceName: my-service\n  additionalMongodConfig:\n    net:\n      ssl:\n        mode: preferSSL\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.4.0-ent\"\n  service: my-service\n  type: ShardedCluster\n\n  ## Please Note: The default Kubernetes cluster name is\n  ## `cluster.local`.\n  ## If your cluster has been configured with another name, you can\n  ## specify it with the `clusterDomain` attribute.\n\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n\n  persistent: true\n  configSrvPodSpec:\n\n    # if \"persistence\" element is omitted then Operator uses the\n    # default size (5Gi) for mounting single Persistent Volume\n\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n  mongosPodSpec:\n    podAntiAffinityTopologyKey: rackId\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n  shardPodSpec:\n    persistence:\n      multiple:\n        # if the child of \"multiple\" is omitted then the default size will be used.\n        # 16GB for \"data\", 1GB for \"journal\", 3GB for \"logs\"\n        data:\n          storage: \"20Gi\"\n        logs:\n          storage: \"4Gi\"\n          storageClass: standard\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n  mongos:\n    additionalMongodConfig:\n      systemLog:\n        logAppend: true\n        verbosity: 4\n  configSrv:\n    additionalMongodConfig:\n      operationProfiling:\n        mode: slowOp\n  shard:\n    additionalMongodConfig:\n      storage:\n        journal:\n          commitIntervalMs: 50\n  security:\n    certsSecretPrefix: \"prefix\"\n    tls:\n     ca: custom-ca\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n  statefulSet:\n    spec:\n      serviceName: my-service\n...\n"
                }
            ],
            "preview": "Type: array",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-op-considerations",
            "title": "Considerations",
            "headings": [
                "Set MANAGED_SECURITY_CONTEXT for  OpenShift Deployments",
                "Understand the Default Permissions for  Objects",
                "Check Messages from the Validation Webhook",
                "Customize the CustomResourceDefinitions that the  Watches"
            ],
            "paragraphs": "When you deploy the   to OpenShift, you must set the\n MANAGED_SECURITY_CONTEXT  flag to  true . This value is set for\nyou in the \nfile. To learn more, see the  Operator installation  and choose the installation method you want to use. Objects in the   configuration use the following default\npermissions. These are the minumum permissions required for\nthe   to deploy and manage   and MongoDB resources\nin a   cluster: Kubernetes Resources Verbs Configmaps Require the following permissions: get ,  list ,  watch . The   reads the organization\nand project data from the specified  configmap . create ,  update . The   creates and updates  configmap \nobjects for configuring the  Application Database  instances. delete . The   needs the  delete   configmap  permission\nto support its  older versions .\nThis permission will be deleted when older versions reach their\nEnd of Life Date. Secrets Require the following permissions: get ,  list ,  watch . The   reads secret objects to\nretrieve sensitive data, such as  TLS  or\n X.509  access information. For example, it\nreads the credentials from a secret object to connect to the  . create ,  update . The   creates secret\nobjects holding  TLS  or\n X.509  access information. delete . The   deletes secret objects (containing passwords)\nrelated to the  Application Database . Services Require the following permissions: get ,  list ,  watch . The   reads and watches\nMongoDB services. For example, to communicate with the Ops Manager service,\nthe   needs  get ,  list  and  watch \npermissions to use the   service's URL. create ,  update . To communicate with services, the  \ncreates and updates service objects corresponding to  \nand MongoDB custom resources. StatefulSets Require the following permissions: get ,  list ,  watch . The   reacts to the changes in the\nStatefulSets it creates for the MongoDB custom resources. It also reads\nthe fields of  the StatefulSets it manages. create ,  update . The   creates and updates StatefulSets\ncorresponding to the mongoDB custom resources. delete . The   needs permissions to delete the StatefulSets\nwhen you delete the MongoDB custom resource. Pods Require the following permissions: get ,  list ,  watch . The   queries the\nApplication Database Pods to get information about its state. Namespaces Require the following permissions: list ,  watch . When you run the   in the cluster-wide mode,\nit needs  list  and  watch  permissions to all namespaces\nfor the MongoDB custom resources. The   uses a validation   to prevent users\nfrom applying invalid resource definitions. The webhook rejects invalid\nrequests. The   and   for the webhook are included in the default\nconfiguration files that you apply during the installation. To create\nthe role and binding, you must have  cluster-admin privileges . If you create an invalid resource definition, the webhook returns\na message similar to the following that describes the error to the shell: When the   reconciles each resource, it also validates that\nresource. The   doesn't require the validation webhook to\ncreate or update resources. If you omit the validation webhook, or if you remove the webhook's role\nand binding from the default configuration, or have insufficient\nprivileges to run the configuration, the   issues warnings,\nas these are not critical errors. If the   encounters a\ncritical error, it marks the resource as  Failed .  has a known issue with the webhook when deploying to private\nclusters. To learn more, see  Update Google Firewall Rules to Fix WebHook Issues . Earlier versions of the   would crash on startup if any\none of the MongoDB   was not present in the cluster. For\ninstance, you had to install the CustomResourceDefinition for  \neven if you did not plan to deploy it with the  . You can now specify which custom resources you want the  \nto watch. This allows you to install the CustomResourceDefinition for\nonly the resources that you want the   to manage. You must use  helm  to configure the   to watch only the\ncustom resources you specify. Follow the relevant  helm \n installation instructions ,\nbut make the following adjustments: Decide which CustomResourceDefinitions you want to install. You can\ninstall any number of the following: Value Description mongodb Install the CustomResourceDefinitions for database resources\nand watch those resources. mongodbusers Install the CustomResourceDefinitions for MongoDB user resources\nand watch those resources. opsmanagers Install the CustomResourceDefinitions for   resources\nand watch those resources. Install the Helm Chart and specify which\nCustomResourceDefinitions you want the\n  to watch. Separate each custom resource with a comma:",
            "code": [
                {
                    "lang": "sh",
                    "value": "error when creating \"my-ops-manager.yaml\":\nadmission webhook \"ompolicy.mongodb.com\" denied the request:\nshardPodSpec field is not configurable for application databases as\nit is for sharded clusters and appdb replica sets"
                },
                {
                    "lang": "sh",
                    "value": "helm install <deployment-name> mongodb/enterprise-operator \\\n  --set operator.watchedResources=\"{mongodb,mongodbusers}\" \\\n     --skip-crds"
                }
            ],
            "preview": "When you deploy the  to OpenShift, you must set the\nMANAGED_SECURITY_CONTEXT flag to true. This value is set for\nyou in the \nfile. To learn more, see the Operator installation and choose the installation method you want to use.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/manage-database-users-ldap",
            "title": "Manage Database Users Using LDAP Authentication",
            "headings": [
                "Considerations",
                "Supported Authentication Mechanisms",
                "Prerequisites",
                "Add a Database User",
                "Configure kubectl to default to your namespace.",
                "Copy the following example .",
                "Open your preferred text editor and paste the example  into a new text file.",
                "Change the lines for the following parameters, as needed.",
                "Add any additional roles for the user to the .",
                "Create the user.",
                "View the newly created user in .",
                "Delete a Database User"
            ],
            "paragraphs": "The   supports managing database users for deployments\nrunning with   and LDAP cluster authentication enabled. The configuration for users authenticated through   relies on the\nLDAP Query Templates and the mappings that MongoDB establishes. To learn more, see the following sections in the MongoDB Server\ndocumentation: LDAP Authorization LDAP Query Templates security.ldap.userToDNMapping The   supports SCRAM, LDAP, and X.509 authentication\nmechanisms in deployments it creates. In an  -created\ndeployment, you cannot use   to: Configure other authentication mechanisms for deployments. Manage users  not  using SCRAM, LDAP, or X.509 authentication. Before managing database users, you must deploy a\n replica set  or\n sharded cluster  with   enabled.\nenabled. Optionally, you can enable  . To learn more, see\n Secure a Database Resource . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Use the following table to guide you through changing the relevant\nlines in the|k8s-crd|. For a full list of LDAP user settings, see\n security settings  in the  \nMongoDB resource specification. Key Type Description Example metadata.name string The name of the resource for the MongoDB database user. Resource names must be 44 characters or less. ldap-user-1 spec.db string The name of the MongoDB database where users will be added. This\nvalue must be  $external . $external spec.mongodbResourceRef.name string The name of the  MongoDB resource \nto which this user is associated. my-resource spec.opsManager.configMapRef.name string The name of the project containing the MongoDB database\nwhere the user will be added. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. my-project spec.roles.db string The database the  role  can act on. admin spec.roles.name string The name of the  role  to grant the database user.\nThe role name can be any  built-in MongoDB role \nor  custom role  that\nexists in  . readWriteAnyDatabase spec.username string The authenticated username that is mapped to an LDAP Distinguished\nName (DN) according to\n spec.security.authentication.ldap.userToDNMapping .\nThe DN must already exist in your LDAP deployment.\nThis username must comply with the  RFC 2253 \nLDAPv3 Distinguished Name standard.  transformed To learn more, see\n LDAP Query Templates  in the\nMongoDB Manual. uid=mdb0,dc=example,dc=org You may grant additional roles to this user using the format defined\nin the following example: Invoke the following   command to create your database user: The following examples illustrate the connection string formats that you\ncan use when enabling authentication with LDAP in   MongoDB\ndeployments. These examples use the  mongodb  namespace and a replica\nset deployment named  replica-set-ldap . The examples are similar for\nsharded clusters. Using the previously-shown formats, you can connect to the MongoDB\ndatabase with the MongoDB Shell ( mongosh ), as in the following\nexample: You can use these credentials to\n connect to a MongoDB Database Resource from Inside Kubernetes . connectionString.standard : Standard connection string \nthat can connect you to the database as this database user. connectionString.standardSrv :  DNS seed list connection string  that\ncan connect you to the database as this database user. You can view the newly-created user in  : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. To delete a database user, pass the  metadata.name  from the user\n  to the following command:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: ldap-user-1\nspec:\n  username: \"uid=mdb0,dc=example,dc=org\"\n  db: \"$external\"\n  mongodbResourceRef:\n    name: ldap-replica-set\n  roles:\n    - db: \"admin\"\n      name: \"clusterAdmin\"\n    - db: \"admin\"\n      name: \"readWriteAnyDatabase\"\n    - db: \"admin\"\n      name: \"dbAdminAnyDatabase\"\n\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: ldap-user-1\nspec:\n  username: \"uid=mdb0,dc=example,dc=org\"\n  db: \"$external\"\n  mongodbResourceRef:\n    name: ldap-replica-set\n  roles:\n  - db: \"admin\"\n    name: \"clusterAdmin\"\n  - db: \"admin\"\n    name: \"readWriteAnyDatabase\"\n  - db: \"admin\"\n    name: \"dbAdminAnyDatabase\"\n\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "mongosh <connection-string> \\\n  --host <my-replica-set>/web1.example.com \\\n  --port 30907 \\\n  --authenticationMechanism PLAIN \\\n  --username cn=rob,cn=Users,dc=ldaps-01,dc=myteam,dc=com"
                },
                {
                    "lang": "sh",
                    "value": "mongodb://replica-set-ldap-0-0-svc.mongodb.svc.cluster.local/?connectTimeoutMS=20000&replicaSet=replica-set-ldap&serverSelectionTimeoutMS=20000&ssl=true&authSource=$external"
                },
                {
                    "lang": "sh",
                    "value": "mongodb+srv://replica-set-ldap-svc.mongodb.svc.cluster.local/?connectTimeoutMS=20000&replicaSet=replica-set-ldap&serverSelectionTimeoutMS=20000&ssl=true&authSource=$external"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mdbu <metadata.name>"
                }
            ],
            "preview": "The  supports managing database users for deployments\nrunning with  and LDAP cluster authentication enabled.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secret-storage",
            "title": "Configure Secret Storage",
            "headings": [
                "Supported Secret Storage Tools",
                "Secrets You Can Store",
                "Limitations",
                "Set the Secret Storage Tool",
                "Prerequisites",
                "Procedure",
                "Add the  policies for the  and its components.",
                "Bind the  roles to the  policies for the  and its components.",
                "Add the annotations to the  deployment file.",
                "Define the environment variable in .",
                "Create a file with the  configuration information.",
                "Replace the placeholders in the  configuration information.",
                "Create a ConfigMap with the  configuration.",
                "Manually migrate secrets that don't migrate automatically",
                "Next Steps"
            ],
            "paragraphs": "You can choose the   for  . The secret\nstorage tool is a secure place to store sensitive information for the components\nthat   manages. This includes secrets for MongoDB databases,  , and AppDB. Once you configure secret storage,   accesses the tool, retrieves\nthe secrets, and uses them to establish connections securely.  supports the following  : : store sensitive information as   (the built-in secret\nstorage for  ).    \nstore authentication credentials so that only   can access them. : store sensitive information in  , a third\nparty service for secret management. You can use any supported   for any secret in the  \ndocumentation except those listed in the  limitations . After configuration,   uses your selected  \nfor  all  secrets except those listed in the  limitations .\nYou can't mix and match  . The following limitations exist for the supported  : Some registries, such as OpenShift, require  imagePullSecrets \nto pull images from the repository. The   can't provide imagePullSecrets from\n . You can  specify a kubelet image credential provider \nto retrieve credentials for a container image registry using   instead. To set the  , select one of the following options: All tutorials in this   documentation use     by default.\nTo use     to store secrets for the  , proceed with\ninstallation of the   and follow the steps in the tutorials. To use   to store secrets for the\n , complete the following procedure. Before you begin, you must: Set up a   instance. The   cluster where the\n  is running must have access to the  \ninstance. Ensure that   is  not  running in  dev mode \nand that your   installation follows any applicable\n configuration recommendations . Enable  Kubernetes Authentication \nfor the   instance. This allows you to authenticate with\n . Deploy the Vault Agent sidecar injector \nin the   cluster. This allows you to inject secrets from\n  into your   pods. Download the four  Vault policy files \nfor the  , MongoDB database,  , and AppDB. Write the policies for  , MongoDB database,  , and AppDB resources\nto   using the following command, replacing the variables with the values in the table: Repeat the command for all the resources you're adding to  . Placeholder Description {PolicyName} Human-readable label that identifies the policy you're creating in  . {PathToPolicyFile} The absolute path to the policy file you downloaded. Bind   roles to the policies for  , MongoDB database,\n , and AppDB resources using the following four commands, replacing the\nvariables with the values in the table: These commands ensure that each component's pods have only the access specified in their\npolicy. Placeholder Description {OperatorPolicyName} A human-readable label that identifies the   policy in  . {DatabasePolicyName} A human-readable label that identifies the MongoDB database policy in  . {OpsManagerPolicyName} A human-readable label that identifies the   policy in  . {AppDBPolicyName} A human-readable label that identifies the AppDB policy in  . {ServiceAccountNamespace} Label that identifies the namespace for the service account bound to your pod. This step grants the   access to\n . To use   with applications that the\n  doesn't manage, you must write and bind   policies for those\napplications. You can adapt the commands in this step to bind other policies by\nreplacing the name of the  . To configure other\napplications to use  , replace the\n{ServiceAccountName} in the following command with the service account used\nfor the application's pod: Add the following highlighted lines to the  spec.template.metadata.annotations  section of your\n  deployment file. For most users, this file's name is  mongodb-enterprise.yaml  or\n mongodb-enterprise-openshift.yaml . If you're running   in   mode, you must also add the following\nhighlighted line to the file, replacing {TLSSecret} with the name of the secret\ncontaining a  ca.crt  entry. The content of the  ca.crt  entry must match\nthe certificate of the   used to generate the   TLS certificates. If you installed the   using Helm, the   already\nadded these annotations. You can proceed to the next step. Add the following highlighted lines to the  spec.env  section of your\n  deployment file. For most users, this file's name is  mongodb-enterprise.yaml  or\n mongodb-enterprise-openshift.yaml . This  defines the environment variable \nfor   in  . Using your preferred text editing application, create a file named  config .\nPaste the following text into the file: The paths in this file are the default paths. You can replace them with your\nbase paths if you customized your   configuration. If you're running   in   mode, you must also add the following\nhighlighted line to the file: Replace the placeholders in the  config  file with these values. Save\nthe file with a   file type by replacing the  .txt  file extension with\n .yaml . Placeholder Description {Namespace} The  namespace you created \nfor the  . The default namespace is  mongodb . {VaultServerAddress} The address that the   should use to connect to\n . {TLSSecret} Name of a secret containing a  ca.crt  entry. The content of the\n ca.crt  entry must match the certificate of the   used to generate\nthe   TLS certificates. Issue the following command to create a   containing the   information: This creates a   named  secret-configuration . This\n  contains the contents of the  config  file. You must manually migrate the following secrets to store them in  : To manually migrate or create new secrets,  add them to Vault .\nAfter you add them to  , you can remove them from  . All other secrets that the   creates migrate automatically, and   uses\n  for new secrets. User-created secrets must be  added to Vault . Any existing user-created secrets, including  Operator credentials stored as Kubernetes secrets ,\nif applicable The gen-key secret \n  creates The    admin credentials/admin key \n  creates TLS secrets cert-manager automatically recreates the     that\nit generates if you delete them from  . You must manually manage the\nremoval of these secrets or stop using cert-manager to avoid storing\nthe secrets in  . After you configure the   for the  , you can: Read the  Considerations . Complete the  Prerequisites . Install the Kubernetes Operator .",
            "code": [
                {
                    "lang": "sh",
                    "value": "vault policy write {PolicyName} {PathToPolicyFile}"
                },
                {
                    "lang": "sh",
                    "value": "vault write auth/kubernetes/role/{OperatorPolicyName}\nbound_service_account_names=enterprise-operator bound_service_account_namespaces={ServiceAccountNamespace}"
                },
                {
                    "lang": "sh",
                    "value": "vault write auth/kubernetes/role/{DatabasePolicyName}\nbound_service_account_names=mongodb-enterprise-database-pods bound_service_account_namespaces={ServiceAccountNamespace}"
                },
                {
                    "lang": "sh",
                    "value": "vault write auth/kubernetes/role/{OpsManagerPolicyName}\nbound_service_account_names=mongodb-enterprise-ops-manager bound_service_account_namespaces={ServiceAccountNamespace}"
                },
                {
                    "lang": "sh",
                    "value": "vault write auth/kubernetes/role/{AppDBPolicyName}\nbound_service_account_names=mongodb-enterprise-appdb bound_service_account_namespaces={ServiceAccountNamespace}"
                },
                {
                    "lang": "sh",
                    "value": "vault write auth/kubernetes/role/{PolicyName}\nbound_service_account_names={ServiceAccountName} bound_service_account_namespaces={ServiceAccountNamespace}"
                },
                {
                    "lang": "sh",
                    "value": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n   name: mongodb-enterprise-operator\n   namespace: production\nspec:\n   replicas: 1\n   template:\n      metadata:\n        annotations:\n          vault.hashicorp.com/agent-inject: \"true\"\n          vault.hashicorp.com/role: \"mongodbenterprise\""
                },
                {
                    "lang": "sh",
                    "value": "        annotations:\n          vault.hashicorp.com/agent-inject: \"true\"\n          vault.hashicorp.com/role: \"mongodbenterprise\"\n          vault.hashicorp.com/tls-secret: {TLSSecret}\n          vault.hashicorp.com/ca-cert: /vault/tls/ca.crt"
                },
                {
                    "lang": "sh",
                    "value": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n   name: mongodb-enterprise-operator\n   namespace: production\nspec:\n   env:\n   - name: OPERATOR_ENV\n     value: ENVIRONMENT_NAME\n   - name: SECRET_BACKEND\n     value: VAULT_BACKEND"
                },
                {
                    "lang": "sh",
                    "value": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: secret-configuration\n  namespace: {Namespace}\ndata:\n  VAULT_SERVER_ADDRESS: {VaultServerAddress}\n  OPERATOR_SECRET_BASE_PATH: mongodbenterprise/operator\n  DATABASE_SECRET_BASE_PATH: mongodbenterprise/database\n  OPS_MANAGER_SECRET_BASE_PATH: mongodbenterprise/opsmanager\n  APPDB_SECRET_BASE_PATH: mongodbenterprise/appdb"
                },
                {
                    "lang": "sh",
                    "value": "   OPS_MANAGER_SECRET_BASE_PATH: mongodbenterprise/opsmanager\n   APPDB_SECRET_BASE_PATH: mongodbenterprise/appdb\n   TLS_SECRET_REF: {TLSSecret}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap secret-configuration --from-file=config.yaml"
                }
            ],
            "preview": "You can choose the  for . The secret\nstorage tool is a secure place to store sensitive information for the components\nthat  manages. This includes secrets for MongoDB databases, , and AppDB.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/manage-database-users-x509",
            "title": "Manage Database Users Using X.509 Authentication",
            "headings": [
                "Supported Authentication Mechanisms",
                "Prerequisites",
                "Add a Database User",
                "Configure kubectl to default to your namespace.",
                "Copy the following example .",
                "Open your preferred text editor and paste the example  into a new text file.",
                "Change the lines for the following parameters, as needed.",
                "Add any additional roles for the user to the .",
                "Create the user.",
                "View the newly created user in .",
                "Delete a Database User"
            ],
            "paragraphs": "The   supports managing database users for deployments\nrunning with   and X.509 internal cluster authentication enabled. After enabling X.509 authentication, you can add X.509 users using the   interface or the  . The   supports SCRAM, LDAP, and X.509 authentication\nmechanisms in deployments it creates. In an  -created\ndeployment, you cannot use   to: Configure other authentication mechanisms for deployments. Manage users  not  using SCRAM, LDAP, or X.509 authentication. Before managing database users, you must deploy a\n replica set  or\n sharded cluster  with   and X.509\nenabled. If you need to generate X.509 certificates for your MongoDB users,\nsee  Generate X.509 Client Certificates . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Use the following table to guide you through changing the relevant\nlines in the  : Key Type Description Example metadata.name string The name of the database user resource. Resource names must be 44 characters or less. mms-user-1 spec.username string The subject line of the x509 client certificate signed\nby the     (Kube CA). To get the subject line of the X.509 certificate, run the\nfollowing command: The username must comply with the\n RFC 2253 \nLDAPv3 Distinguished Name standard. CN=mms-user,U=My Organizational Unit,O=My Org,L=New York,ST=New York,C=US spec.opsManager.configMapRef.name string The name of the project containing the MongoDB database\nwhere user will be added. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. my-project spec.roles.db string The database the  role  can act on. admin spec.mongodbResourceRef.name string The name of the  MongoDB resource  to\nwhich this user is associated. my-resource spec.roles.name string The name of the  role  to grant the database\nuser. The role name can be any\n built-in MongoDB role  or\n custom role  that exists\nin  . readWriteAnyDatabase You may grant additional roles to this user using the format defined\nin the following example: Invoke the following   command to create your database user: You can use these credentials to  Connect to a MongoDB Database Resource from Inside Kubernetes . When you create a new MongoDB database user,   automatically\ncreates a new    . The    \ncontains the following information about the new database user: username : Username for the database user password : Password for the database user connectionString.standard :  Standard connection string \nthat can connect you to the database as this database user. connectionString.standardSrv :  DNS seed list connection string  that can\nconnect you to the database as this database user. Alternatively, you can specify an optional\n spec.connectionStringSecretName  field in the\n MongoDBUser  custom resource to specify\nthe name of the connection string secret that the\n  creates. You can view the newly-created user in  : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. To delete a database user, pass the  metadata.name  from the user\n  to the following command:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: <resource-name>\nspec:\n  username: <rfc2253-subject>\n  db: \"$external\"\n  mongodbResourceRef:\n    name: '<MongoDB-Resource-name>'\n  roles:\n    - db: <database-name>\n      name: <role-name>\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "openssl x509 -noout \\\n  -subject -in <my-cert.pem> \\\n  -nameopt RFC2253"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: mms-user-1\nspec:\n  username: CN=mms-user,U=My Organizational Unit,O=My Org,L=New York,ST=New York,C=US\n  project: my-project\n  db: \"$external\"\n  roles:\n    - db: admin\n      name: backup\n    - db: admin\n      name: restore\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mdbu <metadata.name>"
                }
            ],
            "preview": "The  supports managing database users for deployments\nrunning with  and X.509 internal cluster authentication enabled.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/production-notes",
            "title": " Production Notes",
            "headings": [
                "Control Your Deployments with Policies Set in OPA Gatekeeper",
                "List of Sample OPA Gatekeeper Policies",
                "Deploy the Recommended Number of MongoDB Replica Sets",
                "Ensure Proper Persistence Configuration",
                "Name Your MongoDB Service with its Purpose",
                "Specify CPU and Memory Resource Requirements",
                "Set CPU and Memory Utilization Bounds for the  Pod",
                "Set CPU and Memory Utilization Bounds for MongoDB Pods",
                "Use Multiple Availability Zones",
                "Co-locate mongos Pods with Your Applications",
                "Use Labels to Differentiate Between Deployments",
                "Verify Permissions",
                "Enable HTTPS",
                "Enable TLS",
                "Enable Authentication",
                "Example Deployment CRD",
                "Example User CRD",
                "Configure Readiness Probe Overrides for Externally Accessed Kubernetes Operator Deployments"
            ],
            "paragraphs": "This page details system configuration recommendations for the\n  when running in production. All sizing and performance recommendations for common MongoDB deployments\nthrough the   in this section are subject to change. Do\nnot treat these recommendations as guarantees or limitations of any kind. These recommendations reflect performance testing findings and represent\nour suggestions for production deployments. We ran the tests on a cluster\ncomprised of seven AWS EC2 instances of type  t2.2xlarge  and a\nmaster node of type  t2.medium . The recommendations in this section don't discuss characteristics of\nany specific deployment. Your deployment's characteristics may differ\nfrom the assumptions made to create these recommendations. Contact\nMongoDB Support for further help with sizings. To control, audit, and debug your production deployments, you can use policies\nfor the  Gatekeeper \nOpen Policy Agent (OPA). Gatekeeper contains   for creating and extending\ndeployment constraints through the\n. The   offers a  list of Gatekeeper policies \nthat you can customize and apply to your deployments. Each Gatekeeper policy consists of: You can use binary and configurable Gatekeeper policies: To use and apply Gatekeeper sample policies with the  : <policy_name>.yaml  file constraints.yaml  file that is based on the  Binary policies allow or prevent specific configurations, such as\npreventing deployments that don't use TLS, or deploying only specific\nMongoDB or   versions. Configurable policies allow you to specify configurations, such as the\ntotal number of replica sets that will be deployed for a specific\nMongoDB or   custom resource.  on your Kubernetes cluster. Review the list of available constraint templates and constraints: Navigate to the policy directory, select a policy from the list and\napply it and its constraints file: Review the Gatekeeper policies that are currently applied: The   offers the following sample policies in this\n OPA examples \nGitHub directory: Location Policy Description Debugging Blocks all MongoDB and   resources. This allows you to use\nthe log output to craft your own policies. To learn more, see\n. mongodb_allow_replicaset Allows deploying only replica sets for MongoDB resources and\nprevents deploying sharded clusters. mongodb_allowed_versions Allows deploying only specific MongoDB versions. ops_manager_allowed_versions Allows deploying only specific   versions. mongodb_strict_tls Allows using strict TLS mode for MongoDB deployments. ops_manager_replica_members Allows deploying a specified number of   replica set and\nApplication Database members. ops_manager_wizardless Allows installing   in a non-interactive mode. We recommend that you use a single instance of the  \nto deploy up to 20 replica sets in parallel. You  may  increase this number to 50 and expect a reasonable\nincrease in the time that the   takes to download,\ninstall, deploy, and reconcile its resources. For 50 replica sets, the time to deploy varies and might take up to\n40 minutes. This time depends on the network bandwidth of the  \ncluster and the time it takes each MongoDB Agent to download MongoDB\ninstallation binaries from the Internet for each MongoDB cluster member. To deploy more than 50 MongoDB replica sets in parallel,\nuse multiple instances of the  . The   deployments orchestrated by the   are\nstateful. The   container uses   to maintain the\ncluster state between restarts. To satisfy the statefulness requirement, the   performs\nthe following actions: To meet your MongoDB cluster's storage needs, make the following\nchanges in your configuration for each replica set deployed with\nthe  : The following abbreviated example shows recommended persistent storage\nsizes. For a full example of persistent volumes configuration, see\n replica-set-persistent-volumes.yaml \nin the  Persistent Volumes Samples  directory. This\ndirectory also contains sample persistent volumes configurations for\nsharded clusters and standalone deployments. Creates   for your MongoDB deployment. Mounts storage devices to one or more directories\ncalled mount points. Creates one persistent volume for each MongoDB mount point. Sets the default path in each   container to  /data . Verify that persistent volumes are enabled in\n spec.persistent . This setting defaults to  true . Specify a sufficient amount of storage for the  \nto allocate for each of the volumes. The volumes store the data\nand the logs. To set multiple volumes, each for data, logs, and the  oplog , use\n spec.podSpec.persistence.multiple.data . To set a single volume to store data, logs, and the  oplog ,\nuse  spec.podSpec.persistence.single . spec.persistent spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data Set the  spec.service  parameter to a value that identifies\nthis deployment's purpose, as illustrated in the following example. spec.service In  , each Pod includes parameters that allow you\nto specify  CPU resources \nand  memory resources  for each\ncontainer in the Pod. To indicate resource bounds,   uses the  requests and limits \nparameters, where: The following sections illustrate how to: For the Pods hosting  , use the\n default resource limits configurations . request  indicates a lower bound of a resource. limit  indicates an upper bound of a resource. set CPU and Memory for the Operator Pod . set CPU and Memory for MongoDB Pods . When you deploy replica sets with the  , CPU usage for\nPod used to host the   is initially high during the\nreconciliation process, however, by the time the deployment completes,\nit lowers. For production deployments, to satisfy deploying up to 50 MongoDB\nreplica sets or sharded clusters in parallel with the  ,\nset the CPU and memory resources and limits for the   Pod\nas follows: If you don't include the unit of measurement for CPUs,   interprets\nit as the number of cores. If you specify  m , such as 500m,  \ninterprets it as  millis . To learn more, see\n Meaning of CPU . The following abbreviated example shows the configuration with\nrecommended CPU and memory bounds for the   Pod in your\ndeployment of 50 replica sets or sharded clusters. If you are\ndeploying fewer than 50 MongoDB clusters, you may use lower\nnumbers in the configuration file for the   Pod. For a full example of CPU and memory utilization resources and limits\nfor the   Pod that satisfy parallel deployment of up to\n50 MongoDB replica sets, see the  mongodb-enterprise.yaml \nfile. spec.template.spec.containers.resources.requests.cpu  to 500m spec.template.spec.containers.resources.limits.cpu  to 1100m spec.template.spec.containers.resources.requests.memory  to 200Mi spec.template.spec.containers.resources.limits.memory  to 1Gi Monitoring tools report the size of the   rather than the\nactual size of the container. Requests and Limits Assign CPU Resources to Containers and Pods The values for Pods hosting replica sets or sharded clusters map\nto the  requests field \nfor CPU and memory for the created Pod. These values are consistent\nwith  considerations \nstated for MongoDB hosts. The   uses its allocated memory for processing, for the\nWiredTiger cache, and for storing packages during the deployments. For production deployments, set the CPU and memory resources and limits\nfor the MongoDB Pod as follows: If you don't include the unit of measurement for CPUs,   interprets\nit as the number of cores. If you specify  m , such as 500m,  \ninterprets it as  millis . To learn more, see\n Meaning of CPU . The following abbreviated example shows the configuration with\nrecommended CPU and memory bounds for each Pod hosting a MongoDB\nreplica set member in your deployment. For a full example of CPU and memory utilization resources and limits\nfor Pods hosting MongoDB replica set members, see the\n replica-set-podspec.yaml \nfile in the  MongoDB Podspec Samples  directory. This directory also contains sample CPU and memory limits\nconfigurations for Pods used for: spec.podSpec.podTemplate.spec.containers.resources.requests.cpu  to 0.25 spec.podSpec.podTemplate.spec.containers.resources.limits.cpu  to 0.25 spec.podSpec.podTemplate.spec.containers.resources.requests.memory  to 512M spec.podSpec.podTemplate.spec.containers.resources.limits.memory  to 512M A sharded cluster, in the  sharded-cluster-podspec.yaml . Standalone MongoDB deployments, in the  standalone-podspec.yaml . spec.podSpec.podTemplate.spec Requests and Limits Assign CPU Resources to Containers and Pods Set the   and   to distribute all members\nof one replica set to different   to ensure high\navailability. The following abbreviated example shows affinity and multiple\navailability zones configuration. In this example, the   schedules the Pods deployment to\nthe nodes which have the label  kubernetes.io/e2e-az-name  in  e2e-az1  or\n e2e-az2  availability zones. Change  nodeAffinity  to\nschedule the deployment of Pods to the desired availability zones. See the full example of multiple availability zones configuration in\n replica-set-affinity.yaml \nin the  Affinity Samples \ndirectory. This directory also contains sample affinity and multiple zones\nconfigurations for sharded clusters and standalone MongoDB deployments. Running in Multiple Zones Node affinity You can run the lightweight  mongos  instance on the same  \nas your apps using MongoDB. The   supports standard  \n node affinity and anti-affinity \nfeatures. Using these features, you can force install the  mongos \non the same node as your application. The following abbreviated example shows affinity and multiple\navailability zones configuration. The  podAffinity  key determines whether to install an application\non the same Pod, node, or data center as another application. To specify Pod affinity: See the full example of multiple availability zones and node affinity\nconfiguration in\n replica-set-affinity.yaml \nin the  Affinity Samples \ndirectory. This directory also contains sample affinity and multiple\nzones configurations for sharded clusters and standalone\nMongoDB deployments. Add a label and value in the  spec.podSpec.podTemplate.metadata.labels \n  collection to tag the deployment. See\n spec.podSpec.podTemplate.metadata ,\nand the\n Kubernetes PodSpec v1 core API . Specify which label the  mongos  uses in the\n spec.mongosPodSpec.podAffinity \n .requiredDuringSchedulingIgnoredDuringExecution.labelSelector \n  collection. The  matchExpressions  collection defines the\n label  that the   uses to identify the Pod for hosting\nthe  mongos . Assigning Pods to Nodes Node affinity and anti-affinity Kubernetes PodSpec v1 core API Use the  Pod affinity \n  feature to: Separate different MongoDB resources, such as  test ,  staging ,\nand  production  environments. Place   on some specific nodes to take advantage of\nfeatures such as   support. Pod affinity Objects in the   configuration  use the following\ndefault permissions. Kubernetes Resources Verbs Configmaps Require the following permissions: get ,  list ,  watch . The   reads the organization\nand project data from the specified  configmap . create ,  update . The   creates and updates  configmap \nobjects for configuring the  Application Database  instances. delete . The   needs the  delete   configmap  permission\nto support its  older versions .\nThis permission will be deleted when older versions reach their\nEnd of Life Date. Secrets Require the following permissions: get ,  list ,  watch . The   reads secret objects to\nretrieve sensitive data, such as  TLS  or\n X.509  access information. For example, it\nreads the credentials from a secret object to connect to the  . create ,  update . The   creates secret\nobjects holding  TLS  or\n X.509  access information. delete . The   deletes secret objects (containing passwords)\nrelated to the  Application Database . Services Require the following permissions: get ,  list ,  watch . The   reads and watches\nMongoDB services. For example, to communicate with the Ops Manager service,\nthe   needs  get ,  list  and  watch \npermissions to use the   service's URL. create ,  update . To communicate with services, the  \ncreates and updates service objects corresponding to  \nand MongoDB custom resources. StatefulSets Require the following permissions: get ,  list ,  watch . The   reacts to the changes in the\nStatefulSets it creates for the MongoDB custom resources. It also reads\nthe fields of  the StatefulSets it manages. create ,  update . The   creates and updates StatefulSets\ncorresponding to the mongoDB custom resources. delete . The   needs permissions to delete the StatefulSets\nwhen you delete the MongoDB custom resource. Pods Require the following permissions: get ,  list ,  watch . The   queries the\nApplication Database Pods to get information about its state. Namespaces Require the following permissions: list ,  watch . When you run the   in the cluster-wide mode,\nit needs  list  and  watch  permissions to all namespaces\nfor the MongoDB custom resources.  Architecture in  The   supports configuring   to run over\n HTTPS . Enable   before deploying your   resources to avoid a situation\nwhere the   reports your resources' status as  Failed . HTTPS Enabled After Deployment The   supports   encryption.\nUse   with your MongoDB deployment to encrypt your data over\nthe network. The configuration in the following example enables   for the replica\nset. When   is enabled, all traffic between members of the replica\nset and clients is encrypted using   certificates. To learn more about securing your MongoDB deployments using  , see\n Deploy a Replica Set . The default   mode is  requireTLS . You can customize it using the\n spec.additionalMongodConfig.net.ssl.mode  configuration\nparameter, as shown in the following abbreviated example. See the full   configuration example in\n replica-set.yaml \nin the  TLS \nsamples directory. This directory also contains sample   configurations for\nsharded clusters and standalone deployments. The   supports  X.509 , LDAP,\nand  SCRAM  user authentication. You must create an additional   for your\nMongoDB users and the MongoDB Agent instances.\nThe   generates and distributes the certificate. See the full X.509 certificates configuration examples in the\n x509 Authentication  directory in\nthe  Authentication \nsamples directory. This directory also contains sample LDAP and SCRAM configurations. For LDAP configuration, see the\n spec.security.authentication.ldap.automationLdapGroupDN \nsetting. spec.security.authentication.ldap.automationLdapGroupDN Manage Database Users Using X.509 Authentication Manage Database Users Using SCRAM Authentication If you create custom services that require external access to MongoDB custom\nresources deployed by the  , and use readiness probes\nin  , set the  publishNotReadyAddresses  setting in   to  true . The  publishNotReadyAddresses  setting indicates that an agent that\ninteracts with endpoints for this service should disregard the service's\n ready \nstate. Setting  publishNotReadyAddresses  to  true  overrides the\nbehavior of the readiness probe configured for the Pod hosting your service. By default, the  publishNotReadyAddresses  setting is set to  false .\nIn this case, when the Pods that host the MongoDB custom resources in the\n  lose connectivity to   or  , the\nreadiness probes configured for these Pods fail.\nHowever, when you set the   publishNotReadyAddresses  setting to  true :  does not shut down the service whose readiness probe fails.  considers all endpoints as  ready \neven if the probes for the Pods hosting the services for these endpoints\nindicate that they aren't ready. MongoDB custom resources are still available for read and write operations.  and search for  publishNotReadyAddresses DNS for Services in Pods Configure Readiness Probes",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl get constrainttemplates\nkubectl get constraints"
                },
                {
                    "lang": "sh",
                    "value": "cd <policy_directory>\nkubectl apply -f <policy_name>.yaml\nkubectl apply -f constraints.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get constrainttemplates\nkubectl get contstraints"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-cluster\nspec:\n\n  ...\n  persistent: true\n\n\n  shardPodSpec:\n  ...\n    persistence:\n      multiple:\n        data:\n          storage: \"20Gi\"\n        logs:\n          storage: \"4Gi\"\n          storageClass: standard"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: \"4.4.0-ent\"\n  service: drilling-pumps-geosensors\n  featureCompatibilityVersion: \"4.0\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n name: mongodb-enterprise-operator\n namespace: mongodb\nspec:\n replicas: 1\n selector:\n  matchLabels:\n     app.kubernetes.io/component: controller\n     app.kubernetes.io/name: mongodb-enterprise-operator\n     app.kubernetes.io/instance: mongodb-enterprise-operator\n template:\n  metadata:\n   labels:\n     app.kubernetes.io/component: controller\n     app.kubernetes.io/name: mongodb-enterprise-operator\n     app.kubernetes.io/instance: mongodb-enterprise-operator\n   spec:\n     serviceAccountName: mongodb-enterprise-operator\n     securityContext:\n       runAsNonRoot: true\n       runAsUser: 2000\n     containers:\n     - name: mongodb-enterprise-operator\n       image: quay.io/mongodb/mongodb-enterprise-operator:1.9.2\n       imagePullPolicy: Always\n       args:\n        - \"-watch-resource=mongodb\"\n        - \"-watch-resource=opsmanagers\"\n        - \"-watch-resource=mongodbusers\"\n       command:\n        - \"/usr/local/bin/mongodb-enterprise-operator\"\n       resources:\n         limits:\n           cpu: 1100m\n           memory: 1Gi\n         requests:\n           cpu: 500m\n           memory: 200Mi"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\nname: my-replica-set\nspec:\n  members: 3\n  version: 4.0.0-ent\n  service: my-service\n  ...\n\n  persistent: true\n  podSpec:\n    podTemplate:\n      spec:\n        containers:\n        - name: mongodb-enterprise-database\n          resources:\n            limits:\n              cpu: \"0.25\"\n              memory: 512M"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: 4.2.1-ent\n  service: my-service\n  ...\n    podAntiAffinityTopologyKey: nodeId\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n        matchExpressions:\n        - key: security\n          operator: In\n          values:\n          - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n\n    nodeAffinity:\n       requiredDuringSchedulingIgnoredDuringExecution:\n         nodeSelectorTerms:\n         - matchExpressions:\n           - key: kubernetes.io/e2e-az-name\n           operator: In\n           values:\n           - e2e-az1\n           - e2e-az2"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: 4.2.1-ent\n  service: my-service\n\n  ...\n    podAntiAffinityTopologyKey: nodeId\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n        matchExpressions:\n        - key: security\n          operator: In\n          values:\n          - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n\n    nodeAffinity:\n       requiredDuringSchedulingIgnoredDuringExecution:\n         nodeSelectorTerms:\n         - matchExpressions:\n           - key: kubernetes.io/e2e-az-name\n           operator: In\n           values:\n           - e2e-az1\n           - e2e-az2"
                },
                {
                    "lang": "yaml",
                    "value": "mongosPodSpec:\n  podAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n        matchExpressions:\n        - key: security\n          operator: In\n          values:\n          - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\nname: my-tls-enabled-rs\nspec:\n  type: ReplicaSet\n  members: 3\n  version: 4.4.0-ent\n\n opsManager:\n   configMapRef:\n     name: my-project\n credentials: my-credentials\n\n security:\n   tls:\n     enabled: true\n     ca: <custom-ca>\n\n ...\n additionalMongodConfig:\n   net:\n     ssl:\n      mode: \"preferSSL\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-tls-enabled-rs\nspec:\n  type: ReplicaSet\n  members: 3\n  version: \"4.0.4-ent\"\n  project: my-project\n  credentials: my-credentials\n  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: user-with-roles\nspec:\n  username: \"CN=mms-user-1,OU=cloud,O=MongoDB,L=New York,ST=New York,C=US\"\n  db: \"$external\"\n  project: my-project\n  roles:\n    - db: \"admin\"\n      name: \"clusterAdmin\""
                }
            ],
            "preview": "This page details system configuration recommendations for the\n when running in production.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-op-container-images",
            "title": "Container Images",
            "headings": [
                " Image Operating System",
                "Resource Images"
            ],
            "paragraphs": "MongoDB rebuilds   images daily for the latest\noperating system and supporting library updates.  images are built with the latest versions of the\nfollowing operating systems: Registry Image OS Quay.io Red Hat UBI 8 Red Hat Catalog Red Hat UBI 8 When you install the  , it pulls the following images from\na container registry. To view all available versions for each image,\nsee the following links. Image Name Description mongodb-agent MongoDB Agent image. mongodb-enterprise-appdb-database The enterprise MongoDB image used for the Application Database. mongodb-enterprise-init-appdb initContainer  image that contains the Application Database\nstart-up scripts and the readiness probe. mongodb-enterprise-database MongoDB Database environment image. mongodb-enterprise-init-database initContainer  image that contains the MongoDB Agent start-up\nscripts and the readiness probe. mongodb-enterprise-ops-manager Ops Manager image. mongodb-enterprise-init-ops-manager initContainer  image that contains the Ops Manager start-up\nscripts and the readiness probe.",
            "code": [],
            "preview": " images are built with the latest versions of the\nfollowing operating systems:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/create-operator-credentials",
            "title": "Create Credentials for the ",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Create a new Kubernetes secret",
                "Verify your new Kubernetes secret"
            ],
            "paragraphs": "For the   to create or update   in your  \nProject, you need to store your  Programmatic API Key  in your  secret storage tool . Multiple secrets can exist in the same namespace. Each user should\nhave their own secret. You can follow the  Procedure  below to\nmanually store the  Programmatic API Key  as a    . Alternatively, you can: Use the    UI  or the\n   UI  to\nautomatically generate the Kubernetes secret YAML file, which you can\nthen apply to your Kubernetes environment. Store the  Programmatic API Key  as a\n  secret using the procedure to  Create a Vault Secret .\nTo use  , you must also  configure the secret storage . To create credentials for the  , you must: Have or create an  \n Organization . Have or generate a\n Programmatic API Key . Grant this new   the  Project Owner  role. Add the   or   block of any hosts that serve the\n  to the\n API Access List . To create your   secret: Ensure you have the Public and Private Keys for your desired\n   . Invoke the following   command to create your secret: The  -n  flag limits the   to which this secret applies.\nAll MongoDB   resources must exist in the same namespace as the\n  and  . The   doesn't use\neither the secrets or ConfigMaps. The deprecated version of this command specifies a  user  and  publicApiKey \ninstead of a  publicKey  and  privateKey .   accepts\neither version for authentication. Invoke the following   command to verify your secret: This command returns a secret description in the shell:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl -n <metadata.namespace> \\\ncreate secret generic <mycredentials> \\\n--from-literal=\"publicKey=<publicKey>\" \\\n--from-literal=\"privateKey=<privateKey>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe secrets/<mycredentials> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Name:         <mycredentials>\nNamespace:    <metadata.namespace>\nLabels:       <none>\nAnnotations:  <none>\n\nType:  Opaque\n\nData\n====\nprivateKey:  31 bytes\npublicKey:          22 bytes"
                }
            ],
            "preview": "For the  to create or update  in your \nProject, you need to store your Programmatic API Key in your secret storage tool.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/multi-cluster-connect-from-outside-k8s",
            "title": "Connect to Multi-Cluster Resource from Outside Kubernetes",
            "headings": [
                "Prerequisite",
                "Compatible MongoDB Versions",
                "Procedure",
                "Deploy a Multi-Cluster Replica Set.",
                "Secure the Multi-Cluster with TLS.",
                "Add Subject Alternate Names to your  certificates.",
                "Create a NodePort service for each of the Pods in different clusters.",
                "Verify the NodePort services.",
                "Update your replica set resource  file.",
                "Apply the updated replica set file.",
                "Test the connection to the replica set."
            ],
            "paragraphs": "The following procedure describes how to connect to a  MongoDBMulti \nresource deployed in   from outside of the   cluster. For your databases to be accessed outside of  , they must run\nMongoDB 4.2.3 or later. To connect to your  -deployed  MongoDBMulti  replica\nset resource from outside of the   cluster: Provide values for: The   secret in  spec.security.certsSecretPrefix . The custom   certificate in  spec.security.tls.ca . Add each external   name to the certificate  . When you create a  NodePort  service with  kubectl , it assigns a\nrandom port in the range from 30000 to 32767, inclusive. Create a NodePort service. To create a NodePort service that uses a randomly assigned port, run\nthe following command on each Pod in each cluster: To create a NodePort service that uses a deterministic port, on each\nPod in each cluster, create a  Nodeport  service definition YAML\nfile similar to the following example. Specify the port you want\nto use in the  spec.ports.NodePort  setting. This example\nconfigures a NodePort service on port 30007. Apply the YAML with  kubectl apply -f <nodeport-conf>.yaml . In each cluster, run this command to verify the NodePort services that\nyou created: The command returns results similar to the following example: Set the hostnames and ports in  spec.connectivity.replicaSetHorizons \nto the NodePort values that you created in the previous step. Confirm that you specified the correct external hostnames. External\nhostnames should match the   names of   worker nodes.\nThese can be  any  nodes in the   cluster. If the Pod runs on another\nnode,   nodes use internal routing. In each cluster, run this command to apply the updated replica set file: In the development environment, for each host in a replica set, run\nthe following command: In production, for each host in a replica set, specify the  \ncertificate and the   to securely connect to client tools or\napplications: If the connection succeeds, you should see: Don't use the  --sslAllowInvalidCertificates  flag in production.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl expose pod/<my-replica-set>-0 --type=\"NodePort\" --port 27017"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Service\nmetadata:\n  name: <my-replica-set>-0\n  labels:\n    controller: mongodb-enterprise-operator\nspec:\n  type: NodePort\n  selector:\n    controller: mongodb-enterprise-operator\n  ports:\n    port: 27017\n    targetPort: 27017\n    nodePort: 30007"
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl get svc <node_port_service_name>"
                },
                {
                    "lang": "sh",
                    "value": "NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\n<node_port_service_name>   NodePort    10.102.27.116   <none>        27017:30007/TCP   8m30s"
                },
                {
                    "lang": "sh",
                    "value": "apiVersion: mongodb.com/v1\n kind: MongoDBMulti\n metadata:\n  name: multi-cluster-replica-set\n  namespace: mongodb\n spec:\n  clusterSpecList:\n   clusterSpecs:\n   - clusterName: e2e.cluster1.mongokubernetes.com\n     members: 1\n   - clusterName: e2e.cluster2.mongokubernetes.com\n     members: 1\n   - clusterName: e2e.cluster3.mongokubernetes.com\n     members: 1\n  connectivity:\n   replicaSetHorizons:\n   - sample-horizon: web1.example.com:30907\n   - sample-horizon: web2.example.com:30907\n   - sample-horizon: web3.example.com:30907\n  credentials: my-credentials\n  duplicateServiceObjects: false\n  opsManager:\n   configMapRef:\n    name: my-project\n  persistent: true\n  security:\n   certsSecretPrefix: clustercert\n   tls:\n     ca: issuer-ca\n  type: ReplicaSet\n  version: 4.4.0-ent\""
                },
                {
                    "lang": "sh",
                    "value": "$ Kubectl apply -f <file_name.yaml>"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host <my-replica-set>/web1.example.com \\\n      --port 30907\n      --ssl \\\n      --sslAllowInvalidCertificates"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host <my-replica-set>/web1.example.com \\\n  --port 30907 \\\n  --tls \\\n  --tlsCertificateKeyFile server.pem \\\n  --tlsCAFile ca-pem"
                },
                {
                    "lang": "javascript",
                    "value": "Enterprise <my-replica-set> [primary]"
                }
            ],
            "preview": "The following procedure describes how to connect to a MongoDBMulti\nresource deployed in  from outside of the  cluster.",
            "tags": "split-horizon DNS",
            "facets": null
        },
        {
            "slug": "tutorial/install-k8s-operator",
            "title": "Install the ",
            "headings": [
                "Prerequisites and Considerations",
                "Install with ",
                "Navigate to the directory in which you cloned the MongoDB Enterprise Kubernetes Operator repository.",
                "Install the  for MongoDB deployments using the following  command:",
                "Optional: Customize the   before installing it.",
                "Install the  using the following  command:",
                "Add the  repository to Helm.",
                "Install the :",
                "Use docker to request the files on a host connected to the Internet.",
                "Export the  images as .tar archive files:",
                "Copy these .tar files to the host running the  docker daemon.",
                "Import the .tar files into docker.",
                "Add the  repository to Helm.",
                "Install the  with modified pull policy values using the following helm command:",
                "Install with OpenShift",
                "Navigate to the directory in which you cloned the MongoDB Enterprise Kubernetes Operator repository.",
                "Install the  for MongoDB deployments.",
                "Optional: Customize the   before installing it.",
                "Add your <openshift-pull-secret> to the ServiceAccount definitions in the   before installing it.",
                "Install the  using the following  command:",
                "Add the  repository to Helm.",
                "Install the  using helm.",
                "Use docker to request the files on a host connected to the Internet.",
                "Export the  images as .tar archive files:",
                "Copy these .tar files to the host running the  docker daemon.",
                "Import the .tar files into docker.",
                "Add the  repository to Helm.",
                "Install the  with modified pull policy values.",
                "Verify the Installation",
                "Install a Specific Daily Build with Helm",
                "Next Steps"
            ],
            "paragraphs": "Before you install the  , make sure you\n plan for your installation : Choose a  deployment topology . Read the  Considerations . Complete the  Prerequisites . This tutorial presumes some knowledge of   and links to\nrelevant   documentation. If you are unfamiliar\nwith  , please review that documentation first. The installation procedure varies based on how you want to configure your\nenvironment: The following examples assume that you created a  \nusing the default   namespace of  mongodb .\nIf you specified a different label for your namespace when\nyou  created it , change all\nvalues for  metadata.namespace  to that namespace. To change the label for the namespace for the following\ndeployment to  production , edit all values for\n metadata.namespace  in  mongodb-enterprise.yaml : For example, if you cloned the repository in your home directory, run: Invoke the following   command: To learn about optional   installation settings,\nsee  Operator kubectl and oc Installation Settings . Invoke the following   command: Use the . You can install the   with  Helm 3 . Install .\nThe following command installs the   and the  \nin the current namespace named  default . By default, the\n  uses the  default  namespace. The following command installs the    in the  mongodb \nnamespace with the optional  --create-namespace  option. To learn about optional   installation settings, see\n Operator Helm Installation Settings . Use the . To install the   on a host not connected to the Internet: You can install the   with  Helm 3 . Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the version of the MongoDB Enterprise Database\nimage that you want to use. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the version of the MongoDB Enterprise Database\nimage that you want to use. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the version of the MongoDB Enterprise Database\nimage that you want to use. Install \nand set the value of  registry.pullPolicy  to  IfNotPresent .\nTo learn about optional   installation settings, see\n Operator Helm Installation Settings . The installation procedure varies based on how you want to configure your\nenvironment: The following examples assume that you created a  \nusing the default   namespace of  mongodb .\nIf you specified a different label for your namespace when\nyou  created it , change all\nvalues for  metadata.namespace  to that namespace. To change the label for the namespace for the following\ndeployment to  production , edit all values for\n metadata.namespace  in  mongodb-enterprise-openshift.yaml : For example, if you cloned the repository in your home directory, run: Invoke the following   command: To learn about optional   installation settings,\nsee  Operator kubectl and oc Installation Settings . To learn more, see the  registry.imagePullSecrets  setting in the\n Helm installation settings . Invoke the following   command: Use the . You can install the   with  Helm 3 . Install : Use the \nsettings. To learn about optional   installation settings,\nsee  Operator Helm Installation Settings . Use the . To install the   on a host not connected to the Internet: You can install the   with  Helm 3 . Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the version of the MongoDB Enterprise Database\nimage that you want to use. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the version of the MongoDB Enterprise Database\nimage that you want to use. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the version of the MongoDB Enterprise Database\nimage that you want to use. Install : Use the \nsettings,  registry.pullPolicy=IfNotPresent , and\n registry.imagePullSecrets=<openshift-pull-secret> . To learn\nabout optional   installation settings, see\n Operator Helm Installation Settings . To verify that the   installed correctly, run the\nfollowing command and verify the output: By default, deployments exist in the  mongodb  namespace. If the\nfollowing error message appears, ensure you use the correct\nnamespace: To troubleshoot your  , see  Review Logs from the  \nand other  troubleshooting topics . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . MongoDB rebuilds   images every day to integrate the\nlatest security and OS updates. By default,  helm  installs the latest build for the version of\nthe   you specify. To install an earlier build, specify the build ID as a parameter with\n --set build=<build-id> . Build IDs are always in the format\n -b<YYYYMMDD>T000000Z , where  <YYYYMMDD>  is the date that the\nbuild you want to use was created. This example shows how to install the   with the latest\nimage: This example shows how to install the   with the image\ncreated at midnight on February 5th, 2021: MongoDB recommends using the default (latest) build. After installing the  , you can: Create an instance of Ops Manager Configure the Kubernetes Operator to deploy MongoDB resources",
            "code": [
                {
                    "lang": "yaml",
                    "value": "##---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-operator\n  namespace: production\n##---\n# Source: mongodb-enterprise-operator/templates/operator.yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-enterprise-operator\n  namespace: production\n\n---\n# Example truncated\n---\n..."
                },
                {
                    "lang": "sh",
                    "value": "cd ~/mongodb-enterprise-kubernetes"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f crds.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f mongodb-enterprise.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm repo add mongodb https://mongodb.github.io/helm-charts"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator \\\n  --namespace mongodb \\\n  --create-namespace"
                },
                {
                    "lang": "sh",
                    "value": "docker pull quay.io/mongodb/mongodb-enterprise-operator:<op-version>; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-database:<db-version>; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-ops-manager:<om-version>; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-appdb:10.2.15.5958-1_4.2.11-ent; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-init-ops-manager:1.0.3; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-init-appdb:1.0.6; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-init-database:1.0.2;"
                },
                {
                    "lang": "sh",
                    "value": "docker save quay.io/mongodb/mongodb-enterprise-operator:<op-version> -o mongodb-enterprise-operator.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-database:<db-version> -o mongodb-enterprise-database.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-ops-manager:<om-version> -o mongodb-enterprise-ops-manager.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-appdb:10.2.15.5958-1_4.2.11-ent -o mongodb-enterprise-appdb.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-init-ops-manager:1.0.3 -o mongodb-enterprise-init-ops-manager.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-init-appdb:1.0.6 -o mongodb-enterprise-init-appdb.tar;\ndocker save quay.io/mongodb/mongodb-enterprise-init-database:1.0.2 -o mongodb-enterprise-init-database.tar;"
                },
                {
                    "lang": "sh",
                    "value": "docker load -i mongodb-enterprise-operator.tar; \\\ndocker load -i mongodb-enterprise-database.tar; \\\ndocker load -i mongodb-enterprise-ops-manager.tar; \\\ndocker load -i mongodb-enterprise-appdb.tar; \\\ndocker load -i mongodb-enterprise-init-ops-manager.tar; \\\ndocker load -i mongodb-enterprise-init-appdb.tar; \\\ndocker load -i mongodb-enterprise-init-database.tar;"
                },
                {
                    "lang": "sh",
                    "value": "helm repo add mongodb https://mongodb.github.io/helm-charts"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator \\\n  --set registry.pullPolicy='IfNotPresent'"
                },
                {
                    "lang": "yaml",
                    "value": "##---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n  namespace: production\n##---\n# Source: mongodb-enterprise-operator/templates/operator.yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: enterprise-operator\n  namespace: production\n\n---\n# Example truncated\n---\n..."
                },
                {
                    "lang": "sh",
                    "value": "cd ~/mongodb-enterprise-kubernetes"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-appdb\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-database-pods\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f mongodb-enterprise-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm repo add mongodb https://mongodb.github.io/helm-charts"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator \\\n  --values https://raw.githubusercontent.com/mongodb/helm-charts/main/charts/enterprise-operator/values-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "docker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-operator:<op-version>; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-database:<db-version>; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager:<om-version>; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb:10.2.15.5958-1_4.2.11-ent; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-init-ops-manager:1.0.3; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-init-appdb:1.0.6; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-init-database:1.0.2;"
                },
                {
                    "lang": "sh",
                    "value": "docker save registry.connect.redhat.com/mongodb/mongodb-enterprise-operator:<op-version> -o mongodb-enterprise-operator.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-database:<db-version> -o mongodb-enterprise-database.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager:<om-version> -o mongodb-enterprise-ops-manager.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb:10.2.15.5958-1_4.2.11-ent -o mongodb-enterprise-appdb.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-init-ops-manager:1.0.3 -o mongodb-enterprise-init-ops-manager.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-init-appdb:1.0.6 -o mongodb-enterprise-init-appdb.tar;\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-init-database:1.0.2 -o mongodb-enterprise-init-database.tar;"
                },
                {
                    "lang": "sh",
                    "value": "docker load -i mongodb-enterprise-operator.tar; \\\ndocker load -i mongodb-enterprise-database.tar; \\\ndocker load -i mongodb-enterprise-ops-manager.tar; \\\ndocker load -i mongodb-enterprise-appdb.tar; \\\ndocker load -i mongodb-enterprise-init-ops-manager.tar; \\\ndocker load -i mongodb-enterprise-init-appdb.tar; \\\ndocker load -i mongodb-enterprise-init-database.tar;"
                },
                {
                    "lang": "sh",
                    "value": "helm repo add mongodb https://mongodb.github.io/helm-charts"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator \\\n  --set registry.pullPolicy='IfNotPresent' \\\n  --set registry.imagePullSecrets='<openshift-pull-secret>' \\\n  --values https://raw.githubusercontent.com/mongodb/helm-charts/main/charts/enterprise-operator/values-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe deployments mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Error from server (NotFound): deployments.apps \"mongodb-enterprise-operator\" not found"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator \\\n  --set build=-b20210205T000000Z"
                }
            ],
            "preview": "Before you install the , make sure you\nplan for your installation:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-client-connections",
            "title": "Secure Client Connections",
            "headings": [],
            "paragraphs": "Configure   for client authentication. Configure X.509 for client authentication. Configure X.509 for internal authentication.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-replica-set",
            "title": "Deploy a Replica Set",
            "headings": [
                "Considerations",
                "Deploy a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Create the secret for your replica set's  certificate.",
                "Create the  for your agent's TLS certificate.",
                "Create the  to link your  with your deployment.",
                "Copy the sample replica set resource.",
                "Paste the copied example to create a new replica set resource.",
                "Change the highlighted settings to your preferred values.",
                "Configure the TLS settings for your replica set resource using a Custom Certificate Authority.",
                "Add any additional accepted settings for a replica set deployment.",
                "Save this replica set config file with a .yaml extension.",
                "Start your replica set deployment.",
                "Track the status of your replica set deployment.",
                "Renew TLS Certificates for a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Renew the  for your TLS certificates.",
                "Configure kubectl to default to your namespace.",
                "Copy the sample replica set resource.",
                "Paste the copied example to create a new replica set resource.",
                "Change the highlighted settings to your preferred values.",
                "Add any additional accepted settings for a replica set deployment.",
                "Save this replica set config file with a .yaml extension.",
                "Start your replica set deployment.",
                "Track the status of your replica set deployment."
            ],
            "paragraphs": "A  replica set  is a group of MongoDB deployments that maintain\nthe same data set. Replica sets provide redundancy and high\navailability and are the basis for all production deployments. To learn more about replica sets, see the\n Replication Introduction  in\nthe MongoDB manual. Use this procedure to deploy a new replica set that   manages.\nAfter deployment, use   to manage the replica set, including such\noperations as adding, removing, and reconfiguring members. At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . You can use the   to deploy MongoDB resources with\n  and with   version 5.0.x or later. You can use the  Atlas Operator \nto deploy MongoDB resources to  .  doesn't support  arbiter nodes . When you deploy your replica set via the  , you must\nchoose whether to encrypt connections using   certificates. The following procedure for  TLS-Encrypted  connections: The following procedure for  Non-Encrypted Connections : To set up   encryption for a sharded cluster, see\n Deploy a Sharded Cluster . Select the appropriate tab based on whether you want to encrypt your\nreplica set connections with  . Establishes  -encrypted connections between MongoDB hosts in the\nreplica set. Establishes  -encrypted connections between client applications\nand MongoDB deployments. Requires valid certificates for   encryption. Doesn't encrypt connections between MongoDB hosts in the\nreplica set. Doesn't encrypt connections between client applications\nand MongoDB deployments. Has fewer setup requirements than a deployment with  -encrypted\nconnections. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create a new   that stores\nthe replica set's certificate: You must prefix your secrets with  <prefix>-<metadata.name> . If you call your deployment  my-deployment  and you set the\nprefix to  mdb , you must name the   secret for the\nclient   communications  mdb-my-deployment-cert . Also,\nyou must name the   secret for internal cluster authentication\n(if enabled)  mdb-my-deployment-clusterfile . If you're using   as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. To learn about your options for secret\nstorage, see  Configure Secret Storage . Run this  kubectl  command to create a new   that stores\nthe agent's TLS certificate: If you're using   as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. Run this  kubectl  command to link your   to your replica\nset: Change the settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\ninto a new text file. Key Type Description Example metadata.name string Label for this    replica set   . Resource names must be 44 characters or less. metadata.name Kubernetes documentation on\n names . myproject spec.members integer Number of members of the  replica set . 3 spec.version string Version of MongoDB that this  replica set  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. 3.6.7 string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value must exist on the  same  namespace as the resource\nyou want to create. The   tracks any changes to the ConfigMap and\nreconciles the state of the  . <myconfigmap> spec.credentials string Name of the secret you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . The      Secret  object\nholding the Credentials must exist on the  same  Namespace as\nthe resource you want to create. The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. ReplicaSet spec.persistent string Optional. Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. If this value is  true , then\n spec.podSpec.persistence.single  is set to its\ndefault value of  16Gi . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: If you want one   for each  , configure the\n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs Grant your containers permission to write to your  .\nThe   sets  fsGroup = 2000 ,  runAsUser = 2000 ,\nand  runAsNonRoot = true  in  securityContext .  \nsets  fsgroup  equal to  runAsUser  to make the volume writable\nfor a user that runs the main process in the container. To learn\nmore, see  Configure a\nSecurity Context for a Pod or Container  and the related\n discussion  in\nthe   documentation. If redeploying the resource doesn't fix\nissues with your Persistent Volume, contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example string Required Add the  's name that stores the custom  \nthat you used to sign your deployment's   certificates. <custom-ca> string Required Add the  <prefix>  of the secret\nname that contains your MongoDB deployment's   certificates. If you call your deployment  my-deployment  and you set the\nprefix to  mdb , you must name the   secret for the\nclient   communications  mdb-my-deployment-cert . Also,\nyou must name the   secret for internal cluster authentication\n(if enabled)  mdb-my-deployment-clusterfile . devDb You can also add any of the following optional settings to the\n  specification file for a  replica set  deployment: spec.additionalMongodConfig spec.backup.mode spec.clusterDomain spec.connectivity.replicaSetHorizons spec.featureCompatibilityVersion spec.logLevel spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs spec.podSpec.podAffinity spec.podSpec.podAntiAffinityTopologyKey spec.podSpec.nodeAffinity spec.podSpec.podTemplate.metadata spec.podSpec.podTemplate.spec You must set  spec.clusterDomain  if your   cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n  might not function as expected. In any directory, invoke the following   command to create your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. After you encrypt your database resource with  , you can secure the\nfollowing: Client authentication with LDAP Client authentication with X.509 Internal authentication with X.509 Renew your   certificates periodically\nusing the following procedure: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to renew an existing   that\nstores the replica set's certificates: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\ninto a new text file. Key Type Description Example metadata.name string Label for this    replica set   . Resource names must be 44 characters or less. metadata.name Kubernetes documentation on\n names . myproject spec.members integer Number of members of the  replica set . 3 spec.version string Version of MongoDB that this  replica set  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. 3.6.7 string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value must exist on the  same  namespace as the resource\nyou want to create. The   tracks any changes to the ConfigMap and\nreconciles the state of the  . <myconfigmap> spec.credentials string Name of the secret you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . The      Secret  object\nholding the Credentials must exist on the  same  Namespace as\nthe resource you want to create. The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. ReplicaSet spec.persistent string Optional. Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. If this value is  true , then\n spec.podSpec.persistence.single  is set to its\ndefault value of  16Gi . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: If you want one   for each  , configure the\n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs Grant your containers permission to write to your  .\nThe   sets  fsGroup = 2000 ,  runAsUser = 2000 ,\nand  runAsNonRoot = true  in  securityContext .  \nsets  fsgroup  equal to  runAsUser  to make the volume writable\nfor a user that runs the main process in the container. To learn\nmore, see  Configure a\nSecurity Context for a Pod or Container  and the related\n discussion  in\nthe   documentation. If redeploying the resource doesn't fix\nissues with your Persistent Volume, contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n  specification file for a  replica set  deployment: spec.additionalMongodConfig spec.backup.mode spec.clusterDomain spec.connectivity.replicaSetHorizons spec.featureCompatibilityVersion spec.logLevel spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs spec.podSpec.podAffinity spec.podSpec.podAntiAffinityTopologyKey spec.podSpec.nodeAffinity spec.podSpec.podTemplate.metadata spec.podSpec.podTemplate.spec You must set  spec.clusterDomain  if your   cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n  might not function as expected. In any directory, invoke the following   command to create your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-cert \\\n  --cert=<replica-set-tls-cert> \\\n  --key=<replica-set-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap custom-ca --from-file=ca-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      ca: <custom-ca>\n    certsSecretPrefix: <prefix>\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-cert \\\n  --cert=<replica-set-tls-cert> \\\n  --key=<replica-set-tls-key> \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "A replica set is a group of MongoDB deployments that maintain\nthe same data set. Replica sets provide redundancy and high\navailability and are the basis for all production deployments.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-om-container-remote-mode",
            "title": "Configure an  Resource to use Remote Mode",
            "headings": [
                "Prerequisites and Considerations",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Create a  for Nginx.",
                "Deploy Nginx to your  cluster.",
                "Create a  service to make Nginx accessible from other pods in your cluster.",
                "Copy and update the highlighted fields of this  resource.",
                "Paste the copied example section into your existing  resource.",
                "Save your  config file.",
                "Apply changes to your  deployment.",
                "Track the status of your  instance.",
                "Deploy a MongoDB Database Resource."
            ],
            "paragraphs": "In a default configuration, the MongoDB Agents and Backup Daemons\naccess MongoDB installation archives over the Internet from  You can configure   to run in  Remote Mode  with the\n  if the nodes in your   cluster don't have access to\nthe Internet. The Backup Daemons and managed MongoDB resources download\ninstallation archives only from  , which proxies download\nrequests to an HTTP endpoint on a local web server or S3-compatible\nstore deployed to your   cluster. This procedure covers deploying an Nginx HTTP server to your  \ncluster to host the MongoDB installation archives. Deploy an   Resource . The following procedure shows you how to\nupdate your       to enable Remote Mode. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : The ConfigMap in this tutorial configures Nginx to: Run an HTTP server named  localhost  listening on port  80  on a\nnode in your   cluster, and Route HTTP requests for specific resources to locations that serve\nthe the MongoDB Server and MongoDB Database Tools installation\narchives. Paste the following example Nginx ConfigMap into a text editor: Save this file with a  .yaml  file extension. Create the Nginx ConfigMap by invoking the following\n kubectl  command on the ConfigMap file you created: The Nginx resource configuration in this tutorial: Deploys one Nginx replica, Creates volume mounts to store MongoDB Server and MongoDB Database\nTools installation archives, and Defines  init containers  that use  curl \ncommands to download the installation archives that Nginx serves to\nMongoDB Database resources you deploy in your   cluster. Paste the following example Nginx resource configuration\ninto a text editor: Modify the lines highlighted in the example to specify the\nMongoDB Server versions that you want to install. For example, to replace MongoDB version  4.0.2  with\na different database version, update the following block: Update this block to modify the MongoDB Database Tools\nversion: to the appropriate initContainer for each version you want\nNginx to serve. For example, to configure Nginx to serve MongoDB  5.0.12 \nand  6.0.1 : Save this file with a  .yaml  file extension. Deploy Nginx by invoking the following  kubectl \ncommand on the Nginx resource file you created: Paste the following example Nginx resource configuration\ninto a text editor: Modify the lines highlighted in the example to specify the\nMongoDB Server versions that you want to install. For example, to replace MongoDB version  4.0.2  with\na different database version, update the following block: Update this block to modify the MongoDB Database Tools\nversion: To load multiple versions, append  curl  commands\nto the appropriate initContainer for each version you want\nNginx to serve. For example, to configure Nginx to serve MongoDB  4.2.0 \nand  4.4.0 : Save this file with a  .yaml  file extension. Deploy Nginx by invoking the following  oc \ncommand on the Nginx resource file you created: The service in this tutorial exposes Nginx to traffic from other nodes\nin your   cluster over port  80 . This allows the MongoDB\nDatabase resource pods you deploy using the   to download\nthe installation archives from Nginx. Run the following command to create a service your Nginx deployment: Paste the following example service into a text editor: Save this file with a  .yaml  file extension. Create the service by invoking the following\n kubectl  command on the service file you created: The highlighted section uses the following   configuration\nsettings: automation.versions.source: remote  in\n spec.configuration  to enable Remote Mode. automation.versions.download.baseUrl  in\n spec.configuration  to provide the base URL of the\nHTTP resources that serve the MongoDB installation archives. Update this line to replace  <namespace>  with the namespace to\nwhich you deploy resources with the  . automation.versions.download.baseUrl.allowOnlyAvailableBuilds:\n\"false\"  in  spec.configuration  to help ensure\nenterprise builds have no issues. Open your preferred text editor and paste the  \nspecification into the appropriate location in your resource file. Invoke the following  kubectl  command on the filename of the\n  resource  definition: To check the status of your   resource, invoke the following\ncommand: See  Troubleshoot the   for information about the\nresource deployment statuses. After the   resource completes the  Reconciling  phase, the\ncommand returns output similar to the following: Copy the value of the  status.opsManager.url  field, which states\nthe resource's connection  . You use this value when you create a\n  later in the procedure. MongoDB Agents running in MongoDB database resource containers that\nyou create with the   download the installation archives\nfrom   via Nginx instead of from the Internet. If you have not done so already, complete the following\nprerequisites: Create Credentials for the  Create One Project using a ConfigMap Deploy a  MongoDB Database resource \nin the same namespace to which you deployed  .\nEnsure that you: Match the  spec.opsManager.configMapRef.name  of the resource\nto the  metadata.name  of your ConfigMap. Match the  spec.credentials  of the resource to the name of\nthe secret you created that contains an   programmatic\nAPI key pair.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-conf\ndata:\n  nginx.conf: |\n    events {}\n    http {\n      server {\n        server_name localhost;\n        listen 80;\n        location /linux/ {\n          alias /mongodb-ops-manager/mongodb-releases/linux/;\n        }\n        location /tools/ {\n          alias /tools/;\n        }\n      }\n    }\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <nginix-configmap>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - image: nginx:1.14.2\n          imagePullPolicy: IfNotPresent\n          name: nginx\n          ports:\n            - containerPort: 80\n          volumeMounts:\n            - mountPath: /mongodb-ops-manager/mongodb-releases/linux\n              name: mongodb-versions\n            - mountPath: /tools/db/\n              name: mongodb-tools\n            - name: nginx-conf\n              mountPath: /etc/nginx/nginx.conf\n              subPath: nginx.conf\n      initContainers:\n        - name: setting-up-rhel-mongodb\n          image: curlimages/curl:latest\n          command:\n            - curl\n            - -L\n            - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel80-6.0.1.tgz\n            - -o\n            - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel80-6.0.1.tgz\n          volumeMounts:\n            - name: mongodb-versions\n              mountPath: /mongodb-ops-manager/mongodb-releases/linux\n        - name: setting-up-rhel-mongodb-tools\n          image: curlimages/curl:latest\n          command:\n            - curl\n            - -L\n            - https://fastdl.mongodb.org/tools/db/mongodb-database-tools-rhel80-x86_64-100.6.0.tgz\n            - -o\n            - /tools/db/mongodb-database-tools-rhel80-x86_64-100.6.0.tgz\n          volumeMounts:\n            - name: mongodb-tools\n              mountPath: /tools/db/\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      volumes:\n        - name: mongodb-versions\n          emptyDir: {}\n        - name: mongodb-tools\n          emptyDir: {}\n        - configMap:\n            name: nginx-conf\n          name: nginx-conf\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "- name: setting-up-rhel-mongodb\n  image: curlimages/curl:latest\n  command:\n    - curl\n    - -L\n    - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel80-5.0.12.tgz\n    - -o\n    - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel80-5.0.12.tgz\n    - &&\n    - curl\n    - -L\n    - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel80-6.0.1.tgz\n    - -o\n    - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel80-6.0.1.tgz"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <nginix>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - image: nginx:1.14.2\n          imagePullPolicy: IfNotPresent\n          name: nginx\n          ports:\n            - containerPort: 80\n          volumeMounts:\n            - mountPath: /mongodb-ops-manager/mongodb-releases/linux\n              name: mongodb-versions\n            - mountPath: /tools/db/\n              name: mongodb-tools\n            - name: nginx-conf\n              mountPath: /etc/nginx/nginx.conf\n              subPath: nginx.conf\n      initContainers:\n        - name: setting-up-rhel-mongodb\n          image: curlimages/curl:latest\n          command:\n            - curl\n            - -L\n            - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel<version>-4.2.0.tgz\n            - -o\n            - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel<version>-4.2.0.tgz\n          volumeMounts:\n            - name: mongodb-versions\n              mountPath: /mongodb-ops-manager/mongodb-releases/linux\n        - name: setting-up-rhel-mongodb-tools\n          image: curlimages/curl:latest\n          command:\n            - curl\n            - -L\n            - https://fastdl.mongodb.org/tools/db/mongodb-database-tools-rhel<version>-x86_64-100.1.0.tgz\n            - -o\n            - /tools/db/mongodb-database-tools-rhel<version>-x86_64-100.1.0.tgz\n          volumeMounts:\n            - name: mongodb-tools\n              mountPath: /tools/db/\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      volumes:\n        - name: mongodb-versions\n          emptyDir: {}\n        - name: mongodb-tools\n          emptyDir: {}\n        - configMap:\n            name: nginx-conf\n          name: nginx-conf\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "- name: setting-up-rhel-mongodb\n  image: curlimages/curl:latest\n  command:\n    - curl\n    - -L\n    - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel80-5.0.12.tgz\n    - -o\n    - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel80-5.0.12.tgz\n    - &&\n    - curl\n    - -L\n    - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel80-6.0.1.tgz\n    - -o\n    - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel80-6.0.1.tgz"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f <nginix>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: Service\nmetadata:\n name: nginx-svc\n labels:\n   app: nginx\nspec:\n ports:\n - port: 80\n   protocol: TCP\n selector:\n   app: nginx\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <nginix-service>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: ops-manager-localmode\nspec:\n replicas: 1\n version: \"6.0.0\"\n adminCredentials: ops-manager-admin-secret\n configuration:\n   # this enables local mode in Ops Manager\n   automation.versions.source: remote\n   automation.versions.download.baseUrl: \"http://nginx-svc.<namespace>.svc.cluster.local:8080\"\n\n backup:\n   enabled: false\n\n applicationDatabase:\n   members: 3\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2020-05-15T16:20:22Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: \"4.2.11-ent\"\n  backup:\n    phase: \"\"\n  opsManager:\n    lastTransition: \"2020-05-15T16:20:26Z\"\n    phase: Running\n    replicas: 1\n    url: http://ops-manager-localmode-svc.mongodb.svc.cluster.local:8080\n    version: \"5.0.0\"\n"
                }
            ],
            "preview": "In a default configuration, the MongoDB Agents and Backup Daemons\naccess MongoDB installation archives over the Internet from ",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/modify-resource-image",
            "title": "Modify  or MongoDB Kubernetes Resource Containers",
            "headings": [
                "Define a Volume Mount for a MongoDB Kubernetes Resource",
                "Tune MongoDB Kubernetes Resource Docker Images with an InitContainer",
                "Build Custom Images with Dockerfile Templates",
                "MongoDB Dockerfile Templates",
                "Context Images",
                "docker build Example"
            ],
            "paragraphs": "You can modify the containers in the   in which   and\nMongoDB database resources run using the  template  or\n podTemplate  setting that applies to your deployment: To review which fields you can add to a  template  or a\n podTemplate , see the  Kubernetes documentation . When you create containers with a  template  or  podTemplate , the\n  handles container creation differently based on the\n name  you provide for each container in the  containers  array: MongoDB database:  spec.podSpec.podTemplate :  spec.statefulSet.spec.template backup-daemon :  spec.backup.statefulSet.spec.template If the  name  field  matches  the name of the applicable resource\nimage, the   updates the   or MongoDB database\ncontainer in the   to which the  template  or\n podTemplate  applies: :  mongodb-enterprise-ops-manager backup-daemon :  mongodb-backup-daemon MongoDB database:  mongodb-enterprise-database Application Database:  mongodb-enterprise-appdb If the  name  field  does not match  the name of the applicable\nresource image, the   creates a new container in each\n  to which the  template  or  podTemplate  applies. On-disk files in containers in   don't survive container\ncrashes or restarts. Using the  spec.podSpec.podTemplate \nsetting, you can add a  volume mount \nto persist data in a MongoDB database resource for the life of the\n . To create a volume mount for a MongoDB database resource: Update the MongoDB database resource definition to include a volume\nmount for containers in the database pods that the  \ncreates. Use  spec.podSpec.podTemplate  to define a volume mount: Apply the updated resource definition:  Docker images run on RHEL and use RHEL's default\nsystem configuration. To tune the underlying RHEL system\nconfiguration in the   containers, add a privileged\nInitContainer\n init container \nusing one of the following settings: To tune Docker images for a MongoDB database resource container:  adds a privileged InitContainer to each   that the\n  creates using the   definition. Open a shell session to a running container in your database resource\n  and verify your changes. spec.podSpec.podTemplate : add a privileged InitContainer\nto a MongoDB database resource container. spec.statefulSet.spec.template : add a privileged\nInitContainer to an   resource container. MongoDB database resource Docker images use the RHEL default\n keepalive  time of  7200 . MongoDB recommends a shorter\n keepalive  time of  120  for database deployments. You can tune the  keepalive  time in the database resource Docker\nimages if you experience network timeouts or socket errors in\ncommunication between clients and the database resources. Update the MongoDB database resource definition to append a\nprivileged InitContainer to the database pods that the\n  creates. Change  spec.podSpec.podTemplate  the  keepalive \nvalue to the recommended value of  120 : Apply the updated resource definition: To follow the previous  keepalive  example, invoke the following\ncommand to get the current  keepalive  value: You can modify MongoDB Dockerfile templates to create custom\n  images that suit your use case. To build a\ncustom image, you need: Your custom Dockerfile, modified from a MongoDB template. The MongoDB-provided context image for your template. The Dockerfiles used to build container images are publicly\navailable from the\n MongoDB Enterprise Kubernetes GitHub repository . The Dockerfile directory is organized by resource name, version and\ndistribution: Copy the template you want to use to your own Dockerfile and modify as\ndesired. To build an image from any MongoDB Dockerfile template, you must supply\nits context image. Each Dockerfile template has one associated context image, retrievable\nfrom the same  Quay.io  registry as the original\nimages. Context image are always tagged in the format\n quay.io/mongodb/<resource-name>:<image-version>-context . To supply a context image to  docker build , include the\n --build-arg  option with the  imagebase  variable set to a\nQuay.io tag, where  <resource-name>  and  <image-version>  match\nyour Dockerfile template. If you want to build the  mongodb-enterprise-database  version\n2.0.0 image for any distribution, include: The Ubuntu distribution for  mongodb-enterprise-operator  version\n1.9.1 is based on  ubuntu:1604  by default. In this example, that\nbase Dockerfile template is modified to use  ubuntu:1804  and\nsaved as  myDockerfile . The following command builds the custom image and gives it the tag\n 1.9.1-ubuntu-1804 : Include a hyphen ( - ) at the end of  docker build  to read\nthe output of  cat myDockerfile  instead of providing a\nlocal directory as build context. To learn more about  docker build , see the\n Docker documentation .",
            "code": [
                {
                    "lang": "yaml",
                    "value": "podSpec:\n  podTemplate:\n    spec:\n      containers:\n      - name: mongodb-enterprise-database\n        volumeMounts:\n        - mountPath: </new/mount/path>\n          name: survives-restart\n      volumes:\n      - name: survives-restart\n        emptyDir: {}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-resource-conf>.yaml -n <metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  podSpec:\n    podTemplate:\n      spec:\n        initContainers:\n        - name: \"adjust-tcp-keepalive\"\n          image: \"busybox:latest\"\n          securityContext:\n            privileged: true\n          command: [\"sysctl\", \"-w\", \"net.ipv4.tcp_keepalive_time=120\"]"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-resource-conf>.yaml -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "> kubectl exec -n <metadata.namespace> -it <pod-name> -- cat /proc/sys/net/ipv4/tcp_keepalive_time\n\n\n> 120"
                },
                {
                    "lang": "sh",
                    "value": "\u251c\u2500\u2500 <resource name>\n\u2502   \u2514\u2500\u2500 <image version>\n\u2502       \u2514\u2500\u2500 <base distribution>\n\u2502           \u2514\u2500\u2500 Dockerfile template"
                },
                {
                    "lang": "sh",
                    "value": "--build-arg imagebase=quay.io/mongodb/mongodb-enterprise-database:2.0.0-context"
                },
                {
                    "lang": "sh",
                    "value": "cat myDockerfile | docker build --build-arg imagebase=quay.io/mongodb/mongodb-enterprise-operator:1.9.1-context \\\n--tag mongodb-enterprise-operator:1.9.1-ubuntu-1804 -"
                }
            ],
            "preview": "You can modify the containers in the  in which  and\nMongoDB database resources run using the template or\npodTemplate setting that applies to your deployment:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/mdb-resources-arch",
            "title": "MongoDB Database Architecture in ",
            "headings": [
                "The MongoDB Custom Resource Definition",
                "Standalone",
                "Replica Set",
                "Sharded Cluster",
                "Reconciling the MongoDB Custom Resource",
                "Diagram of a Replica Set Reconciliation",
                "Diagram of a Sharded Cluster Reconciliation",
                "Reconciliation Workflow",
                "Reconciling the MongoDBUser Custom Resource"
            ],
            "paragraphs": "You can use the   and   to deploy MongoDB database\nresources to a   cluster. You can use an existing  , or deploy\n  in   to manage your databases. The   uses   to manage the following MongoDB database custom resources: Your   specifications define these resources in the  .\nThe   monitors these resources. When you update the\nresource's specification, the   pushes these changes to\n , which make changes to the MongoDB deployment's configuration. MongoDB MongoDBUser The   manages MongoDB database deployments\nwhich are defined by MongoDB custom resources. The MongoDB database   specification defines the\nfollowing types of the MongoDB database custom resources: The following diagram illustrates the composition of each type of the MongoDB\nresource in the  . Standalone ReplicaSet ShardedCluster  doesn't support  arbiter nodes . For the  Standalone  type of the MongoDB database resource, the  \ndeploys a replica set with a single member to the   cluster as a\n . The   creates the StatefulSet, which contains the Pod\nspecification with the number of Pods to create. The  \nrelies on the   StatefulSet Controller to create a Pod for this\nstandalone MongoDB database instance. In  , a  Standalone  resource is equivalent to a  ReplicaSet \nresource with only one member. We recommend that you deploy\na  ReplicaSet  with one member instead of a  Standalone \nbecause a replica set allows you to add members to it in\nthe future. For the  ReplicaSet  type of the MongoDB resource, the  \ndeploys a replica set to the   cluster as a  , with\na number of members equal to the value of  spec.members . The   relies on the   StatefulSet Controller to create\none Pod in the StatefulSet for each member of the replica set. Each Pod in the StatefulSet runs a MongoDB Agent instance. The  ShardedCluster  type of the MongoDB resource consists of one or more\nConfig Servers,   instances, and shard members. For the  ShardedCluster  resource, the   deploys: The   relies on the   StatefulSet Controller to\ncreate one Pod in each of the StatefulSets created for the sharded cluster. One StatefulSet for all Config Servers One StatefulSet for all   instances One StatefulSet for each Shard Member When you apply a MongoDB custom resource specification,\nthe   deploys each resource as a  \nto the   cluster. The  : Watches the custom resource's specification and associated\n  or secrets stored in your\n secret storage tool . Validates the changes when the specification file, the ConfigMap,\nor the secret change. Makes the appropriate updates to the MongoDB database resources\nin the   cluster. Pushes the changes to  , which make changes to the MongoDB\ndeployment's configuration. The following diagram describes how the   behaves\nif you make changes to a replica set's: MongoDB  custom resource specifications Associated  Associated secrets stored in your\n secret storage tool The following diagram describes how the   behaves if\nyou make changes to a sharded cluster's: MongoDB  custom resource specifications Associated  Associated secrets stored in your\n secret storage tool When you create or change a MongoDB resource specification, or when you make\nchanges to an associated   or secret, the  \nperforms the following actions to reconcile the changes: Reads the required organization and project configuration\nfrom the  ConfigMap \nthat you used to create or connect to a project in the  . If you change your resource specification, the   identifies\nthat the change took place, and checks the specification for the ConfigMap\nspecified in  spec.opsManager.configMapRef.name . When you configure the   for MongoDB resources,\nyou  create a ConfigMap  to connect\nor create your   project. The MongoDB Agent uses this ConfigMap\nto start or make changes to the deployment for the MongoDB resource. Reads the authentication configuration for   from\nthe secret specified in either: This secret stores the\n Cloud Manager API keys \nor the  Ops Manager API Keys \nrequired for the   to authenticate to  . spec.credentials  in the resource specification Your  secret storage tool When you configure the   for MongoDB resources,\nyou either  create this secret in Kubernetes  or store this\nsecret in your  secret storage tool . The   connects to   and performs the following actions: Reads the organization specified in the  OrgId  field in the ConfigMap,\nor uses the organization that has the same name as the project in  . Reads a project name specified in the  projectName  field in the\nConfigMap, or creates this project in   if it doesn't exist. Checks that the  <project-id>-group-secret  secret\ncreated by the   for the MongoDB Agent exists.\nThe   reads the secret from your  secret storage tool ,\nor creates it with\n Ops Manager API keys \nor  Cloud Manager API keys . Registers itself as a watcher of the ConfigMap and this secret.\nThis enables the   to react to changes that you make\nto the ConfigMap or the secret. The   verifies any  TLS and X.509 certificates . If  TLS  is enabled for a replica set, the\n  looks for certificates provided in the\n <prefix>-<resource-name>-cert  secret or your\n secret storage tool . If  TLS  is enabled for a sharded cluster, the\n  looks for certificates in these secrets: <prefix>-<resource-name>-x-cert  for each shard member. <prefix>-<resource-name>-config-cert  for all config servers. <prefix>-<resource-name>-mongos-cert  for all   instances. Your  secret storage tool . If  X.509  or\n internal authentication with X.509 and  TLS \nare enabled, the   checks that their certificates\ncontain the required configuration. The   locates and updates the necessary StatefulSets,\nor creates new StatefulSets if they don't exist. The number of\nStatefulSets depends on the type of the MongoDB resource. For  ReplicaSet  or  Standalone  resources,\nthe   creates a single StatefulSet. For a  ShardedCluster  resource, the   creates: At this point, each Pod runs at least one MongoDB Agent instance,\nbut does not yet contain   instances. One StatefulSet for all config servers. One StatefulSet for all   instances. One StatefulSet for each shard member. Each MongoDB Agent instance starts polling   to receive the\nMongoDB automation configuration. When the MongoDB Agent receives the configuration for the first\ntime, it downloads the MongoDB binaries with the version\nspecified in  spec.version  from the Internet, or\nfrom  , if the MongoDB Agent is configured in the local mode. After the MongoDB Agent receives the automation configuration, it starts a\n  instance on the corresponding Pod. For each Pod of each StatefulSet that the MongoDB custom resource creates,\nexcept for   StatefulSets, the   generates a  .\nYou can override this behavior by setting  spec.persistent  to\n false  in the resource specification. The   updates the automation configuration it received from the\nMongoDB Agent with changes from the specifications and sends it to  . Each MongoDB Agent for each Pod polls   again and receives the\nupdated automation configuration. If you change any field in the specification, the  \nperforms a   of the StatefulSets to start new Pods\nmatching the new specification. The   waits for each MongoDB Agent to report that it reached\nthe ready state. If you change the  security configuration \nof a database resource, or  scale down \nan existing StatefulSet, the   runs step 6 before it\nruns  step 5. The   updates the   services, or for a new MongoDB\nresource, creates the services required for each new StatefulSet. For the    ClusterIP , the   sets\n ClusterIP  to  None , and performs these actions: Creates this service if it doesn't exist. For  ReplicaSet  or  Standalone  resources, the  \nnames the service with the custom resource's name  with  -svc \nappended to it. For a  ShardedCluster  resource, the   uses these\nnaming conventions: For   instances, the   uses the name specified in\n spec.service , or the resource's name  with  -svc \nappended to it. For the config servers, the   uses the resource's name\nwith  -cs  appended to it. For each shard, the   uses the resource's name\nwith  -sh  appended to it. For the port, the   uses the default port 27017, or\nthe  .net.port \nspecified in  spec.additionalMongodConfig . If the user authentication method is set to  SCRAM ,\nthe  MongoDBUser  custom resource depends\non the  secret storage tool  that stores the\nuser credentials.  If you are using a    , you specify the\nsecret in the  spec.passwordSecretKeyRef  settings in the  MongoDBUser \nresource specification. The   watches the secret for changes. If you make changes\nto the secret's configuration, the   reconciles the\nchanges. It takes the following actions: The following diagram describes how the   behaves if you make\nchanges to the user secret or the  MongoDBUser \ncustom resource specification. Determines the MongoDB user's resource based on the value\nspecified in the  spec.MongoDBResourceRef.name  setting in the\n MongoDBUser  resource specification. Connects to  : Reads the organization's name from  OrgId  in the ConfigMap, or\nuses the same name for the organization as the project's name in  . Reads a project's name from  projectName  in the ConfigMap,\nor creates this project in   if it doesn't exist. Checks that the  <project-id>-group-secret  created by the\n  for the MongoDB Agent exists.\nThe   reads the secret from your  secret storage tool ,\nor creates it with\n Ops Manager API keys \nor  Cloud Manager API keys . Updates the user's credentials in  , or creates a new user if it doesn't exist. If the user authentication method is  SCRAM ,\nreads the password from the secret. Reads the user name. If the user name has changed, the  \nremoves the old name and adds a new one. Ensures that the user exists in  .",
            "code": [],
            "preview": "You can use the  and  to deploy MongoDB database\nresources to a  cluster. You can use an existing , or deploy\n in  to manage your databases.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/connect-from-outside-k8s",
            "title": "Connect to a MongoDB Database Resource from Outside Kubernetes",
            "headings": [
                "Prerequisite",
                "Compatible MongoDB Versions",
                "Procedure",
                "Open your standalone resource  file.",
                "Copy the sample standalone resource.",
                "Paste the copied example section into your existing standalone resource.",
                "Change the highlighted settings to your preferred values.",
                "Save your standalone config file.",
                "Update and restart your standalone deployment.",
                "Discover the dynamically assigned NodePorts.",
                "Test the connection to the standalone.",
                "Deploy a replica set with the .",
                "Add Subject Alternate Names to your  certificates.",
                "Create a NodePort for each .",
                "Discover the dynamically assigned NodePorts.",
                "Open your replica set resource  file.",
                "Copy the sample replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Change the highlighted settings to your preferred values.",
                "Confirm the external hostnames and NodePort values in your replica set resource.",
                "Save your replica set config file.",
                "Update and restart your replica set deployment.",
                "Test the connection to the replica set.",
                "Deploy a replica set with the .",
                "Configure services to ensure connectivity.",
                "Configure routes to ensure  terminination passthrough.",
                "Add Subject Alternate Names to your  certificates.",
                "Open your replica set resource  file.",
                "Configure your replica set resource  file.",
                "Change the settings to your preferred values.",
                "Save your replica set config file.",
                "Create the necessary  certificates and  secrets.",
                "Update and restart your replica set deployment.",
                "Test the connection to the replica set.",
                "Open your sharded cluster resource  file.",
                "Copy the sample sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Change the highlighted settings to your preferred values.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Discover the dynamically assigned NodePorts.",
                "Test the connection to the sharded cluster."
            ],
            "paragraphs": "The following procedure describes how to connect to a MongoDB resource\ndeployed in   from outside of the   cluster. For your databases to be accessed outside of  , they must run\nMongoDB 4.2.3 or later. How you connect to a MongoDB resource that the   deployed\nfrom outside of the   cluster depends on the resource. This procedure uses the following example: To connect to your  -deployed MongoDB\nstandalone resource from outside of the   cluster: Change the settings of this   file to match your\ndesired  standalone  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. Key Type Necessity Description Example spec.exposedExternally Boolean Optional Set this value to  true  to allow external services to connect\nto the MongoDB deployment. This results in   creating a\n NodePort service . true In any directory, invoke the following   command to update and\nrestart your {k8sResource}}: Discover the dynamically assigned NodePort: The list output should contain an entry similar to the following:  exposes   on port  27017  within the  \ncontainer. The NodePort service exposes the  mongod  via port  30994 .\nNodePorts range from 30000 to 32767, inclusive. To connect to your deployment from outside of the   cluster, run\nthe   command with the external   of a   as the\n --host  flag. If a node in the   cluster has an external   of\n ec2-54-212-23-143.us-west-2.compute.amazonaws.com , you can\nconnect to this standalone instance from outside of the  \ncluster using the following command: To obtain the external  DNS (Domain Name System)  of your\nKubernetes cluster, you can run the following command: This command displays the external DNS in the\n Addresses.ExternalDNS  section of the output. Alternatively, you can output the external DNS directly by running: To connect to your  -deployed MongoDB replica\nset resource from outside of the   cluster: This procedure explains the least complicated way to\nenable external connectivity. Other utilities can be\nused in production. If you haven't deployed a replica set, follow the instructions to\n deploy one . You must enable   for the replica set by providing a value for\nthe  spec.security.certsSecretPrefix  setting. The replica\nset must use a custom   certificate stored with\n spec.security.tls.ca . Add each external   name to the certificate  . Invoke the following commands to create the NodePorts: Discover the dynamically assigned NodePorts: NodePorts range from 30000 to 32767, inclusive. Change the settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. Key Type Necessity Description Example collection Conditional Add this parameter and values if you need your database to be\naccessed outside of  . This setting allows you to provide\ndifferent   settings within the   cluster and to the\n  cluster. The   uses split horizon   for\nreplica set members. This feature allows communication both\nwithin the   cluster and from outside  . You may add multiple external mappings per host. Make sure that each value in this array is unique. Make sure that the number of entries in this array matches\nthe value given in  spec.members . Provide a value for the\n spec.security.certsSecretPrefix  setting to\nenable  . This method to use split horizons requires the\nServer Name Indication extension of the   protocol. See Setting string Required Add the  <prefix>  of the secret\nname that contains your MongoDB deployment's   certificates. devDb Confirm that the external hostnames in the\n spec.connectivity.replicaSetHorizons  setting are correct. External hostnames should match the   names of   worker nodes.\nThese can be  any  nodes in the   cluster.   nodes use internal\nrouting if the pod runs on another node. Set the ports in  spec.connectivity.replicaSetHorizons  to\nthe NodePort values that you discovered. In any directory, invoke the following   command to update and\nrestart your {k8sResource}}: In the development environment, for each host in a replica set, run\nthe following command: In production, for each host in a replica set, specify the  \ncertificate and the   to securely connect to client tools or\napplications: If the connection succeeds, you should see: Don't use the  --sslAllowInvalidCertificates  flag in production. To connect to your  -deployed MongoDB replica\nset resource from outside of the   cluster with OpenShift: If you haven't deployed a replica set, follow the instructions to\n deploy one . You must enable   for the replica set by providing a value for\nthe  spec.security.certsSecretPrefix  setting. The replica\nset must use a custom   certificate stored with\n spec.security.tls.ca . Paste the following example services into a text editor: If the  spec.selector  has entries that target headless\nservices or applications, OpenShift may create a software\nfirewall rule explicitly dropping connectivity. Review the\nselectors carefully and consider targeting the stateful set pod\nmembers directly as seen in the example. Routes in OpenShift\noffer port 80 or port 443. This example service uses\nport 443. Change the settings to your preferred values. Save this file with a  .yaml  file extension. To create the services, invoke the following  kubectl  command\non the services file you created: Paste the following example routes into a text editor: To ensure the     negotiation with   necessary\nfor   to respond with the correct horizon replica set\ntopology for the drivers to use, you must set  \ntermination passthrough. Change the settings to your preferred values. Save this file with a  .yaml  file extension. To create the routes, invoke the following  kubectl  command on\nthe routes file you created: Add each external   name to the certificate  . Use the following example to edit your replica set resource  \nfile: OpenShift clusters require localhost horizons if you intend to use\nthe   to create each  . If you manually create\nyour   certificates, ensure you include localhost in\nthe   list. Key Type Necessity Description Example collection Conditional Add this parameter and values if you need your database to be\naccessed outside of  . This setting allows you to provide\ndifferent   settings within the   cluster and to the\n  cluster. The   uses split horizon   for\nreplica set members. This feature allows communication both\nwithin the   cluster and from outside  . You may add multiple external mappings per host. Make sure that each value in this array is unique. Make sure that the number of entries in this array matches\nthe value given in  spec.members . Provide a value for the\n spec.security.certsSecretPrefix  setting to\nenable  . This method to use split horizons requires the\nServer Name Indication extension of the   protocol. See Setting string Required Add the  <prefix>  of the secret\nname that contains your MongoDB deployment's   certificates. devDb Configure TLS for your replica set . Create one secret for the MongoDB replica set\nand one for the certificate authority. The   uses these\nsecrets to place the   files in the pods for MongoDB to use. In any directory, invoke the following   command to update and\nrestart your {k8sResource}}: The   should deploy the MongoDB replica set,\nconfigured with the horizon routes created for ingress. After\nthe   completes the deployment, you may connect with the\nhorizon using   connectivity.  If the certificate authority is\nnot present on your workstation, you can view and copy it from a\nMongoDB pod using the following command: To test the connections, run the following command: In production, for each host in a replica set, specify the  \ncertificate and the   to securely connect to client tools or\napplications: If the connection succeeds, you should see: In the following example, for each member of the replica set, use\nyour replica set names and replace  {redacted}  with the domain\nthat you manage. Don't use the  --tlsAllowInvalidCertificates  flag in production. For this procedure, you must deploy a  -enabled sharded MongoDB\ncluster in the  .\nProvide the external   names ( s) for each member of\nthe MongoDB sharded cluster. The   for each MongoDB hosts corresponds to: Each   certificate requires the   ( ) that\ncorresponds to the   that this host has outside the\nsharded cluster deployed with the  . To connect to your  -deployed MongoDB sharded\ncluster resource from outside of the   cluster: Change the settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. Key Type Necessity Description Example spec.exposedExternally Boolean Optional Set this value to  true  to allow external services to connect\nto the MongoDB deployment. This results in   creating a\n NodePort service . true collection Optional List of every domain that should be added to   certificates\nto each pod in this deployment. When you set this parameter,\nevery   that the   transforms into a  \ncertificate includes a   in the form  <pod\nname>.<additional cert domain> . true string Required Add the  <prefix>  of the secret\nname that contains your MongoDB deployment's   certificates. devDb In any directory, invoke the following   command to update and\nrestart your {k8sResource}}: Discover the dynamically assigned NodePort: The list output should contain an entry similar to the following:  exposes   on port  27017  within the  \ncontainer. The NodePort service exposes the  mongod  via port  30078 .\nNodePorts range from 30000 to 32767, inclusive. To connect to your deployment from outside of the   cluster, run\nthe   command with the external   of a   as the\n --host  flag. If a node in the   cluster has an external   of\n ec2-54-212-23-143.us-west-2.compute.amazonaws.com , you can\nconnect to this sharded cluster instance from outside of the  \ncluster using the following command: To obtain the external  DNS (Domain Name System)  of your\nKubernetes cluster, you can run the following command: This command displays the external DNS in the\n Addresses.ExternalDNS  section of the output. Alternatively, you can output the external DNS directly by running:",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\nspec:\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true\n  exposedExternally: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\nspec:\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  exposedExternally: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <standalone-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get services -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\n\n<my-standalone>           NodePort    10.102.27.116   <none>        27017:30994/TCP   8m30s"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host ec2-54-212-23-143.us-west-2.compute.amazonaws.com \\\n  --port 30994"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe nodes"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get nodes -o jsonpath='{ $.items[*].status.addresses[?(@.type==\"ExternalDNS\")].address }'"
                },
                {
                    "lang": "sh",
                    "value": "kubectl expose pod/<my-replica-set>-0 --type=\"NodePort\" --port 27017\nkubectl expose pod/<my-replica-set>-1 --type=\"NodePort\" --port 27017\nkubectl expose pod/<my-replica-set>-2 --type=\"NodePort\" --port 27017"
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl get svc | grep <my-replica-set>\n<my-replica-set>-0     NodePort   172.30.39.228   <none>  27017:30907/TCP  16m\n<my-replica-set>-1     NodePort   172.30.185.136  <none>  27017:32350/TCP  16m\n<my-replica-set>-2     NodePort   172.30.84.192   <none>  27017:31185/TCP  17m\n<my-replica-set>-svc   ClusterIP  None            <none>  27017/TCP        38m"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  type: ReplicaSet\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:30907\"\n      - \"example-website\": \"web2.example.com:32350\"\n      - \"example-website\": \"web3.example.com:31185\"\n..."
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:30907\"\n      - \"example-website\": \"web2.example.com:32350\"\n      - \"example-website\": \"web3.example.com:31185\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host <my-replica-set>/web1.example.com \\\n      --port 30907\n      --ssl \\\n      --sslAllowInvalidCertificates"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host <my-replica-set>/web1.example.com \\\n  --port 30907 \\\n  --tls \\\n  --tlsCertificateKeyFile server.pem \\\n  --tlsCAFile ca-pem"
                },
                {
                    "lang": "javascript",
                    "value": "Enterprise <my-replica-set> [primary]"
                },
                {
                    "lang": "",
                    "value": "---\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-external-0\nspec:\n  ports:\n    - name: mongodb\n      protocol: TCP\n      port: 443\n      targetPort: 27017\n  selector:\n    statefulset.kubernetes.io/pod-name: my-external-0\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-external-1\nspec:\n  ports:\n    - name: mongodb\n      protocol: TCP\n      port: 443\n      targetPort: 27017\n  selector:\n    statefulset.kubernetes.io/pod-name: my-external-1\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-external-2\nspec:\n  ports:\n    - name: mongodb\n      protocol: TCP\n      port: 443\n      targetPort: 27017\n  selector:\n    statefulset.kubernetes.io/pod-name: my-external-2\n\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <my-external-services>.yaml"
                },
                {
                    "lang": "",
                    "value": "---\napiVersion: v1\nkind: Route\nmetadata:\n  name: my-external-0\nspec:\n  host: my-external-0.{redacted}\n  to:\n    kind: Service\n    name: my-external-0\n  tls:\n    termination: passthrough\n---\napiVersion: v1\nkind: Route\nmetadata:\n  name: my-external-1\nspec:\n  host: my-external-1.{redacted}\n  to:\n    kind: Service\n    name: my-external-1\n  tls:\n    termination: passthrough\n---\napiVersion: v1\nkind: Route\nmetadata:\n  name: my-external-2\nspec:\n  host: my-external-2.{redacted}\n  to:\n    kind: Service\n    name: my-external-2\n  tls:\n    termination: passthrough\n\n...\n "
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <my-external-routes>.yaml"
                },
                {
                    "lang": "",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-external\n  namespace: mongodb\nspec:\n  type: ReplicaSet\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: {redacted}\n  credentials: {redacted}\n  persistent: false\n  security:\n    tls:\n      # TLS must be enabled to allow external connectivity\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"SCRAM\",\"X509\"]\n  connectivity:\n    # The \"localhost\" routes are included to enable the creation of localhost\n    # TLS SAN in the CSR, per OpenShift route requirements.\n    # \"ocroute\" is the configured route in OpenShift.\n    replicaSetHorizons:\n      - \"ocroute\": \"my-external-0.{redacted}:443\"\n        \"localhost\": \"localhost:27017\"\n      - \"ocroute\": \"my-external-1.{redacted}:443\"\n        \"localhost\": \"localhost:27018\"\n      - \"ocroute\": \"my-external-2.{redacted}:443\"\n        \"localhost\": \"localhost:27019\"\n\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "oc exec -it my-external-0 -- cat /mongodb-automation/ca.pem"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host my-external/my-external-0.{redacted} \\\n      --port 443\n      --ssl \\\n      --tlsAllowInvalidCertificates"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host my-external/my-external-0.{redacted} \\\n  --port 443 \\\n  --tls \\\n  --tlsCertificateKeyFile server.pem \\\n  --tlsCAFile ca-pem"
                },
                {
                    "lang": "javascript",
                    "value": "Enterprise <my-replica-set> [primary]"
                },
                {
                    "lang": "sh",
                    "value": "<mdb-resource-name><shard><pod-index>.<external-domain>\n<mdb-resource-name><config><pod-index>.<external-domain>\n<mdb-resource-name><mongos><pod-index>.<external-domain>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  credentials: my-secret\n  type: ShardedCluster\n  persistent: true\n  exposedExternally: true\n  security:\n    tls:\n      enabled: true\n      additionalCertificateDomains:\n        - \"additional-cert-test.com\"\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  credentials: my-secret\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  exposedExternally: true\n  security:\n    tls:\n      enabled: true\n      additionalCertificateDomains:\n        - \"additional-cert-test.com\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get services -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\n\n<my-sharded cluster>      NodePort    10.106.44.30    <none>        27017:30078/TCP   10s"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host ec2-54-212-23-143.us-west-2.compute.amazonaws.com \\\n  --port 30078"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe nodes"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get nodes -o jsonpath='{ $.items[*].status.addresses[?(@.type==\"ExternalDNS\")].address }'"
                }
            ],
            "preview": "The following procedure describes how to connect to a MongoDB resource\ndeployed in  from outside of the  cluster.",
            "tags": "split-horizon DNS",
            "facets": null
        },
        {
            "slug": "tutorial/secure-ldap-auth",
            "title": "Secure Client Authentication with LDAP",
            "headings": [
                "Considerations",
                "General Prerequisites",
                "Configure LDAP Client Authentication for a Replica Set",
                "Copy the sample replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the LDAP settings for your replica set resource.",
                "Configure the LDAP settings for the MongoDB Agent.",
                "Save your replica set config file.",
                "Apply your changes to your replica set deployment.",
                "Track the status of your deployment.",
                "Configure LDAP Client Authentication for a Sharded Cluster",
                "Copy the sample sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the LDAP settings for your sharded cluster resource.",
                "Configure the LDAP settings for the MongoDB Agent.",
                "Save your sharded cluster config file.",
                "Apply your changes to your sharded cluster deployment.",
                "Track the status of your deployment."
            ],
            "paragraphs": "You can use the   to configure LDAP to authenticate your\nclient applications that connect to your MongoDB deployments. This guide\ndescribes how to configure LDAP authentication from client applications\nto your MongoDB deployments. MongoDB Enterprise \nsupports: To learn more, see the  LDAP Proxy Authentication \nand  LDAP Authorization  sections\nin the MongoDB Server documentation. Proxying authentication requests to a Lightweight Directory Access\nProtocol (LDAP) service. Simple and SASL binding to LDAP servers. MongoDB Enterprise can bind\nto an LDAP server via  saslauthd  or through the operating system\nlibraries. To configure   in  , use the parameters under the\n spec.security.authentication.ldap  and other\n security LDAP settings  specific to the\nMongoDB Agent, from the   MongoDB resource specification.\nThe procedures in this section describe the required settings and\nprovide examples of LDAP configuration. To improve security, consider deploying a\n TLS-encrypted replica set  or a\n TLS-encrypted sharded cluster .\nEncryption with   is optional. By default,   traffic is sent\nas plain text. This means that username and password are exposed to\nnetwork threats. Many modern directory services, such as Microsoft\nActive Directory, require encrypted connections. Consider using\n  over   to encrypt vauthentication requests in your\n  MongoDB deployments. Before you configure LDAP authentication for your MongoDB deployments,\ncomplete the following tasks: Ensure that you deploy the MongoDB Enterprise database resource.\nMongoDB Community databases don't support LDAP authentication. Deploy the replica set  or\n deploy the sharded cluster \nwhose client authentication you want to secure with  . Change the settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: The resulting configuration may look similar to the following\nexample: For a full list of LDAP settings, see  security settings  in the   MongoDB resource specification.\nAlso see the  spec.security.authentication.agents.automationUserName \nsetting for the MongoDB Agent user in your LDAP-enabled  \ndeployment. Key Type and necessity Description Example Set to  true  to enable LDAP authentication. true Specify the LDAP Distinguished Name to which MongoDB binds when\nconnecting to the LDAP server. cn=admin,dc=example,dc=org Specify the name of the   that contains the\nLDAP Bind Distinguished Name's password with which MongoDB binds\nwhen connecting to an LDAP server. <secret-name> Add the  's name that stores the custom  \nthat you used to sign your deployment's   certificates. <configmap-name> Add the field name that stores the   which validates the\nLDAP server's   certificate. <configmap-key> Specify the list of  hostname:port  combinations of one or more\nLDAP servers. For each server, use a separate line. <example.com:636> Set to  tls  to use LDAPS (LDAP over  ). Leave blank if\nyour LDAP server doesn't accept TLS. You must enable TLS when you\ndeploy the database resource to use this setting. tls Specify the mapping that maps the username provided to\n mongod  or  mongos  for authentication\nto an LDAP Distinguished Name (DN). To learn more, see  security.ldap.userToDNMapping \nand  LDAP Query Templates  in the\nMongoDB Server documentation. <match: \"(.+)\",substitution: \"uid={0},ou=groups,dc=example,dc=org\"> Set to  LDAP  to enable authentication through LDAP. LDAP Update your MongoDB resource \nwith  security settings  specific to the Agent,\nfrom the   MongoDB resource specification. The resulting\nconfiguration may look similar to the following example: Invoke the following   command to update your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. Change the settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: The resulting configuration may look similar to the following\nexample: For a full list of LDAP settings, see  security settings  in the   MongoDB resource specification.\nAlso see the  spec.security.authentication.agents.automationUserName \nsetting for the MongoDB Agent user in your LDAP-enabled  \ndeployment. Key Type and necessity Description Example Set to  true  to enable LDAP authentication. true Specify the LDAP Distinguished Name to which MongoDB binds when\nconnecting to the LDAP server. cn=admin,dc=example,dc=org Specify the name of the   that contains the\nLDAP Bind Distinguished Name's password with which MongoDB binds\nwhen connecting to an LDAP server. <secret-name> Add the  's name that stores the custom  \nthat you used to sign your deployment's   certificates. <configmap-name> Add the field name that stores the   which validates the\nLDAP server's   certificate. <configmap-key> Specify the list of  hostname:port  combinations of one or more\nLDAP servers. For each server, use a separate line. <example.com:636> Set to  tls  to use LDAPS (LDAP over  ). Leave blank if\nyour LDAP server doesn't accept TLS. You must enable TLS when you\ndeploy the database resource to use this setting. tls Specify the mapping that maps the username provided to\n mongod  or  mongos  for authentication\nto an LDAP Distinguished Name (DN). To learn more, see  security.ldap.userToDNMapping \nand  LDAP Query Templates  in the\nMongoDB Server documentation. <match: \"(.+)\",substitution: \"uid={0},ou=groups,dc=example,dc=org\"> Set to  LDAP  to enable authentication through LDAP. LDAP Update your MongoDB resource \nwith  security settings  specific to the Agent,\nfrom the   MongoDB resource specification. The resulting\nconfiguration may look similar to the following example: Invoke the following   command to update your\n sharded cluster : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": ""
                },
                {
                    "lang": "yaml",
                    "value": "security:\n authentication:\n   enabled: true\n   # Enabled LDAP Authentication Mode\n   modes:\n     - \"LDAP\"\n     - \"SCRAM\"\n     # LDAP related configuration\n   ldap:\n   # Specify the hostname:port combination of one or\n   # more LDAP servers\n     servers:\n       - \"ldap1.example.com:636\"\n       - \"ldap2.example.com:636\"\n\n   # Set to \"tls\" to use LDAP over TLS. Leave blank if\n   # the LDAP server doesn't accept TLS. You must enable TLS when you deploy the database resource to use this setting.\n   transportSecurity: \"tls\"\n\n   # If TLS is enabled, add a reference to a ConfigMap that\n   # contains a CA certificate that validates the LDAP server's\n   # TLS certificate.\n   caConfigMapRef:\n     name: \"<configmap-name>\"\n     key: \"<configmap-entry-key>\"\n\n   # Specify the LDAP Distinguished Name to which\n   # MongoDB binds when connecting to the LDAP server\n   bindQueryUser: \"cn=admin,dc=example,dc=org\"\n\n   # Specify the password with which MongoDB binds\n   # when connecting to an LDAP server. This is a\n   # reference to a Secret Kubernetes Object containing\n   # one \"password\" key.\n   bindQueryPasswordSecretRef:\n     name: \"<secret-name>\""
                },
                {
                    "lang": "yaml",
                    "value": "security:\n  authentication:\n    agents:\n      automationPasswordSecretRef:\n        key: automationConfigPassword\n        name: automation-config-password\n      automationUserName: mms-automation-agent\n      clientCertificateSecretRef:\n        name: agent-client-cert\n      mode: LDAP\n    enabled: true\n    ldap:\n      bindQueryPasswordSecretRef:\n        name: bind-query-password\n      bindQueryUser: cn=admin,dc=example,dc=org\n      servers:\n        - openldap.namespace.svc.cluster.local:389\n      userToDNMapping: '[{match: \"(.+)\",substitution: \"uid={0},ou=groups,dc=example,dc=org\"}]'\n    modes:\n      - LDAP\n      - SCRAM\n    requireClientTLSAuthentication: false"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": ""
                },
                {
                    "lang": "yaml",
                    "value": "security:\n authentication:\n   enabled: true\n   # Enabled LDAP Authentication Mode\n   modes:\n     - \"LDAP\"\n     - \"SCRAM\"\n     # LDAP related configuration\n   ldap:\n   # Specify the hostname:port combination of one or\n   # more LDAP servers\n     servers:\n       - \"ldap1.example.com:636\"\n       - \"ldap2.example.com:636\"\n\n   # Set to \"tls\" to use LDAP over TLS. Leave blank if\n   # the LDAP server doesn't accept TLS. You must enable TLS when you deploy the database resource to use this setting.\n   transportSecurity: \"tls\"\n\n   # If TLS is enabled, add a reference to a ConfigMap that\n   # contains a CA certificate that validates the LDAP server's\n   # TLS certificate.\n   caConfigMapRef:\n     name: \"<configmap-name>\"\n     key: \"<configmap-entry-key>\"\n\n   # Specify the LDAP Distinguished Name to which\n   # MongoDB binds when connecting to the LDAP server\n   bindQueryUser: \"cn=admin,dc=example,dc=org\"\n\n   # Specify the password with which MongoDB binds\n   # when connecting to an LDAP server. This is a\n   # reference to a Secret Kubernetes Object containing\n   # one \"password\" key.\n   bindQueryPasswordSecretRef:\n     name: \"<secret-name>\""
                },
                {
                    "lang": "yaml",
                    "value": "security:\n  authentication:\n    agents:\n      automationPasswordSecretRef:\n        key: automationConfigPassword\n        name: automation-config-password\n      automationUserName: mms-automation-agent\n      clientCertificateSecretRef:\n        name: agent-client-cert\n      mode: LDAP\n    enabled: true\n    ldap:\n      bindQueryPasswordSecretRef:\n        name: bind-query-password\n      bindQueryUser: cn=admin,dc=example,dc=org\n      servers:\n        - openldap.namespace.svc.cluster.local:389\n      userToDNMapping: '[{match: \"(.+)\",substitution: \"uid={0},ou=groups,dc=example,dc=org\"}]'\n    modes:\n      - LDAP\n      - SCRAM\n    requireClientTLSAuthentication: false"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "You can use the  to configure LDAP to authenticate your\nclient applications that connect to your MongoDB deployments. This guide\ndescribes how to configure LDAP authentication from client applications\nto your MongoDB deployments.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/create-x509-client-certs",
            "title": "Generate X.509 Client Certificates",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Generate an X.509 client certificate.",
                "Configure kubectl to default to your namespace.",
                "Copy and save the following example .",
                "Create the X.509 MongoDB user.",
                "Verify your newly created user",
                "Find the mount location of the .",
                "Use your X.509 user to connect to the MongoDB deployment"
            ],
            "paragraphs": "The   can deploy MongoDB instances with\n X.509 authentication \nenabled. If X.509 authentication has been enabled for the deployment,\nyou must generate and use an X.509 certificate to connect to the\ndeployment. This new client certificate must be signed by the\nsame   that signs the server certificates for the MongoDB\ndeployment to accept it. Use the procedure outlined in this document to use an X.509 certificate\nto connect to your X.509-enabled MongoDB deployment. If you're using   as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. A full description of Transport Layer Security (TLS), Public Key Infrastructure (PKI)\ncertificates, and Certificate Authorities is beyond the scope of this\ndocument. This page assumes prior knowledge of   and\nX.509 authentication. To complete this tutorial, you must have the  \ninstalled. For instructions on installing the  ,\nsee  Install the  . This tutorial assumes you have a MongoDB deployment which\nrequires X.509 authentication. For instructions on deploying\nMongoDB resources, see  Deploy a MongoDB Database Resource . First create the client certificate. Then create a MongoDB user\nand connect to the X.509-enabled deployment. For production use, your MongoDB deployment should use valid\ncertificates generated and signed by a  . You or your\norganization can generate and maintain an independent   using\n -native tools such as\n cert-manager . Obtaining and managing certificates is beyond the scope of this\ndocumentation. To learn about the properties that your client certificates must have,\nsee  Client Certificate Requirements \nin the MongoDB Manual. You must concatenate your client's   certificate and the\ncertificate's key in a  .pem  file. You must present this\n .pem  file when you connect to your X.509-enabled MongoDB\ndeployment. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Save the following ConfigMap as  x509-mongodb-user.yaml : This ConfigMap  .yaml  file describes a  MongoDBUser  custom object. You\ncan use these custom objects to create MongoDB users. In this example, the ConfigMap describes the user as an X.509\nuser that the client can use to connect to MongoDB with the\ncorresponding X.509 certificate. Run the following command to apply the ConfigMap and create the\nX.509 MongoDB user: You should see an output similar to the following: Run the following command to check the state of the  new-x509-user : You should see an output similar to the following: Run the following command to find where in each pod the  \nmounted the   secret: In the output, find the  secret-ca  mount: In the following step when you connect to your database deployment,\nappend  secret-ca  to the  mountPath , which forms the full path: Once you have created your X.509 user, try to connect to the\ndeployment using the MongoDB Shell ( mongosh ):",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "none",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: new-x509-user\nspec:\n  username: \"CN=my-x509-authenticated-user,OU=organizationalunit,O=organization\"\n  db: \"$external\"\n  mongodbResourceRef:\n    name: '<name of the MongoDB resource>'\n  roles:\n    - db: \"admin\"\n      name: \"readWriteAnyDatabase\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f x509-mongodb-user.yaml"
                },
                {
                    "lang": "sh",
                    "value": "mongodbuser.mongodb.com/new-x509-user created"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdbu/new-x509-user -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "NAME            CREATED AT\nnew-x509-user   8m"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulset <metadata.name> -o yaml"
                },
                {
                    "lang": "yaml",
                    "value": "volumeMounts:\n  - mountPath: /opt/scripts\n    name: database-scripts\n    readOnly: true\n  - mountPath: /var/lib/mongodb-automation/secrets/ca\n    name: secret-ca\n    readOnly: true\n  - mountPath: /var/lib/mongodb-automation/secrets/certs\n    name: secret-certs\n    readOnly: true"
                },
                {
                    "lang": "sh",
                    "value": "/var/lib/mongodb-automation/secrets/ca/secret-ca"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host {host} --port {port} --tls \\\n  --tlsCAFile </path/to/secret-ca> \\\n  --tlsCertificateKeyFile <your-cert>.pem \\\n  --authenticationMechanism MONGODB-X509  \\\n  --authenticationDatabase '$external'"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host {host} --port {port} --ssl \\\n  --sslCAFile </path/to/secret-ca> \\\n  --sslPEMKeyFile <your-cert>.pem \\\n  --authenticationMechanism MONGODB-X509 \\\n  --authenticationDatabase '$external'"
                }
            ],
            "preview": "The  can deploy MongoDB instances with\nX.509 authentication\nenabled. If X.509 authentication has been enabled for the deployment,\nyou must generate and use an X.509 certificate to connect to the\ndeployment. This new client certificate must be signed by the\nsame  that signs the server certificates for the MongoDB\ndeployment to accept it.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/scale-resources",
            "title": "Scale a Deployment",
            "headings": [
                "Considerations",
                "Examples"
            ],
            "paragraphs": "You can scale your  replica set  and  sharded cluster \ndeployments up or down to match your desired configuration. Scaling up\nincreases the number of members and/or shards in the deployment,\nthereby improving your deployment's redundancy and availability. The\nscale of your deployment is configured in its corresponding\n . To scale your replica set deployment, set the  spec.members \nsetting to the desired number of replica set members. To learn more\nabout replication, see  Replication  in the\nMongoDB manual. To scale your sharded cluster deployment, set the following settings\nas desired: To learn more about sharded cluster configurations, see\n Sharded Cluster Components  in the MongoDB manual. Setting Description spec.shardCount Number of  shards  in the sharded cluster. spec.mongodsPerShardCount Number of members per shard. spec.mongosCount Number of Shard Routers. spec.configServerCount Number of members in the Config Server. The   does not support modifying deployment types.\nFor example, you cannot convert a standalone deployment to a\nreplica set. To modify the type of a deployment,\nwe recommend the following procedure: Create the new deployment with the desired configuration. Back up the data  from\nyour current deployment. Restore the data  from your current\ndeployment to the new deployment. Test your application connections to the new deployment as needed. Once you have verified that the new deployment contains the\nrequired data and can be reached by your application(s), bring\ndown the old deployment. Select the desired tab based on the deployment configuration you\nwant to scale: Consider a replica set resource with the following\n : To scale up this replica set and add more members: Adjust the  spec.members  setting to the desired\nnumber of members: Reapply the configuration to  : Consider a sharded cluster resource with the following\n : To scale up this sharded cluster: Adjust the following settings to the desired values: spec.shardCount spec.mongodsPerShardCount spec.mongosCount spec.configServerCount Reapply the configuration to  :",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-secure-replica-set>\nspec:\n  members: 4\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <repl-set-config>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-secure-sharded-cluster>\nspec:\n  shardCount: 3\n  mongodsPerShardCount: 3\n  mongosCount: 3\n  configServerCount: 4\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-config>.yaml"
                }
            ],
            "preview": "You can scale your replica set and sharded cluster\ndeployments up or down to match your desired configuration. Scaling up\nincreases the number of members and/or shards in the deployment,\nthereby improving your deployment's redundancy and availability. The\nscale of your deployment is configured in its corresponding\n.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/upgrade-mdb-version",
            "title": "Upgrade MongoDB Version and FCV",
            "headings": [
                "Example"
            ],
            "paragraphs": "You can upgrade the major, minor, or feature compatibility versions of\nyour MongoDB resource. Configure these settings in your  MongoDB Database Resource Specification . To upgrade your resource's major or minor versions, set the\n spec.version  setting to the desired MongoDB version. To modify your resource's\n feature compatibility version ,\nset the  spec.featureCompatibilityVersion  setting to the\ndesired version. If you update  spec.version  to a later version, consider setting\n spec.featureCompatibilityVersion  to the current working\nMongoDB version to give yourself the option to downgrade if\nnecessary. To learn more about feature compatibility, see\n setFeatureCompatibilityVersion  in the MongoDB Server\nDocumentation. To upgrade the standalone deployment's MongoDB version from  4.2.2-ent \nto  4.4.18-ent , complete the steps in the following syntactic example. If you update  spec.version  to a later version without setting the\n spec.featureCompatibilityVersion  to any value, the Feature\nCompatibility Version (FCV) upgrades to the  same version  that you specify\nin  spec.version . However, you can explicitly specify a previous\nversion for the FCV. The following example illustrates this use case.\nIt sets  spec.version  to  4.4.18-ent  and\n spec.featureCompatibilityVersion  to  4.2 . This resource has a MongoDB version of \u200b. The\nfollowing steps upgrade the deployment's MongoDB version to\n 4.4.18-ent :  automatically reconfigures your deployment with the new\nspecifications. You can see these changes reflected in the   or\n Cloud Manager  application. Perform the following modifications to the resource's  : Set  spec.version  to the desired MongoDB version. Set  spec.featureCompatibilityVersion  to the current\nworking MongoDB version: Reapply the configuration to  :",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone-downgrade\nspec:\n  version: \"4.2.2-ent\"\n  type: Standalone\n  project: my-project\n  credentials: my-credentials\n  persistent: false\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone-downgrade\nspec:\n  version: \"4.4.18-ent\"\n  featureCompatibilityVersion: \"4.2\"\n  type: Standalone\n  project: my-project\n  credentials: my-credentials\n  persistent: false\n..."
                },
                {
                    "lang": "none",
                    "value": "kubectl apply -f <standalone-config>.yaml"
                }
            ],
            "preview": "You can upgrade the major, minor, or feature compatibility versions of\nyour MongoDB resource. Configure these settings in your MongoDB Database Resource Specification.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/multi-cluster-secure-client-connections",
            "title": "Secure Client Connections in Multi-Cluster Deployments",
            "headings": [],
            "paragraphs": "Configure   for client authentication in  . Configure X.509 for client authentication in  . Configure X.509 for internal authentication in  .",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/migrate-k8s-images",
            "title": "Migrate  from Ubuntu-based Images to UBI-based Images",
            "headings": [],
            "paragraphs": "To migrate   from Ubuntu-based images to UBI-based images, edit\nyour Kubernetes Operator\n configuration file  to pull\nimages from the appropriate UBI repositories by suffixing the existing\nimage repository path with  -ubi . You don't need to perform this\nprocedure if you are using OpenShift, as you are already using UBI\nimages. The following example compares a default configuration for the\n INIT_APPDB_IMAGE_REPOSITORY  setting with an updated configuration\nthat pulls a UBI image. After saving the changes, reapply your configuration file. For users running vanilla Kubernetes: For users running OpenShift: Repeat this procedure for the following repository\nconfigurations by applying the same  -ubi  suffix, saving the changes\nand reapplying the configuration each time to migrate the images\nseparately: After adding the necessary suffix, your configuration should match the\nconfiguration below: AGENT_IMAGE APPDB_IMAGE_REPOSITORY INIT_DATABASE_IMAGE_REPOSITORY INIT_OPS_MANAGER_IMAGE_REPOSITORY MONGODB_ENTERPRISE_DATABASE_IMAGE OPS_MANAGER_IMAGE_REPOSITORY",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f mongodb-enterprise.yaml"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f mongodb-enterprise-openshift.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_APPDB_IMAGE_REPOSITORY\n              value: quay.io/mongodb/mongodb-enterprise-init-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_APPDB_IMAGE_REPOSITORY\n              value: quay.io/mongodb/mongodb-enterprise-init-appdb-ubi"
                }
            ],
            "preview": "To migrate  from Ubuntu-based images to UBI-based images, edit\nyour Kubernetes Operator\nconfiguration file to pull\nimages from the appropriate UBI repositories by suffixing the existing\nimage repository path with -ubi. You don't need to perform this\nprocedure if you are using OpenShift, as you are already using UBI\nimages.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-internal-auth",
            "title": "Secure Internal Authentication with X.509",
            "headings": [
                "General Prerequisites",
                "Configure X.509 Internal Authentication for a Replica Set",
                "Prerequisites",
                "Enable X.509 Internal Authentication",
                "Create the  for your X.509 certificate.",
                "Copy the sample replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the general X.509 settings for your replica set resource.",
                "Configure the internal X.509 settings for your replica set resource.",
                "Save your replica set config file.",
                "Apply your changes to your replica set deployment.",
                "Track the status of your deployment.",
                "Renew Internal Authentication X.509 Certificates for a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Renew the  for your TLS certificates.",
                "Renew the  for your X.509 certificate.",
                "Renew the  for your agents' X.509 certificates.",
                "Configure X.509 Internal Authentication for a Sharded Cluster",
                "Prerequisites",
                "Enable X.509 Internal Authentication",
                "Create the  for your Shards' X.509 certificates.",
                "Create the  for your config servers' X.509 certificate.",
                "Create the  for your mongos server's X.509 certificates.",
                "Copy the sample sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the general X.509 settings for your sharded cluster resource.",
                "Configure the internal X.509 settings for your sharded cluster resource.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Track the status of your deployment.",
                "Renew Internal Authentication X.509 Certificates for a Sharded Cluster",
                "Configure kubectl to default to your namespace.",
                "Renew the  for your Shards' TLS certificates.",
                "Renew the  for your config server's TLS certificates.",
                "Renew the  for your mongos server's TLS certificates.",
                "Renew the  for your Shards' X.509 certificates.",
                "Renew the  for your config servers' X.509 certificate.",
                "Renew the  for your mongos server's X.509 certificates.",
                "Renew the  for your agents' X.509 certificates."
            ],
            "paragraphs": "This guide instructs you on how to configure: The   doesn't support other authentication schemes between\nMongoDB nodes in a cluster. X.509 internal authentication between MongoDB nodes in a cluster. X.509 authentication from clients to your MongoDB instances. Before you secure any of your MongoDB deployments using  \nencryption, complete the following: Install the Kubernetes Operator Create Credentials for the Kubernetes Operator Enabling X.509 authentication at the project level configures all\nagents to use X.509 client authentication when communicating with\nMongoDB deployments. X.509 client authentication requires one of the following:  4.1.7 or later  4.0.11 or later Before you secure your replica set using X.509,\n deploy a TLS-encrypted replica set . Run this  kubectl  command to create a new   that stores\nthe replica set's certificate: You must prefix your secrets with  <prefix>-<metadata.name> . If you call your deployment  my-deployment  and you set the\nprefix to  mdb , you must name the   secret for the\nclient   communications  mdb-my-deployment-cert . Also,\nyou must name the   secret for internal cluster authentication\n(if enabled)  mdb-my-deployment-clusterfile . Change the settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Required Set this value to  true  to enable authentication on the\nMongoDB deployment. true array Conditional Set this value to  [\"X509\"] . [\"X509\"] To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example string Required Use this setting to enable\n X.509 internal cluster authentication . Once internal cluster authentication is enabled, it can't\nbe disabled. X509 Invoke the following   command to update your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. If you have already created certificates, we recommend that you renew\nthem periodically using the following procedure. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to renew an existing   that\nstores the replica set's certificates: Run this  kubectl  command to renew an existing   that\nstores the replica set's certificate: Run this  kubectl  command to renew an existing   that\nstores the agents' X.509 certificates: Before you secure your sharded cluster using X.509,\n deploy a TLS-encrypted sharded cluster . Run this  kubectl  command to create a new   that stores\nthe sharded cluster shards' certificates: Run this  kubectl  command to create a new   that stores\nthe sharded cluster config server's certificates: Run this  kubectl  command to create a new   that stores\nthe sharded cluster   certificates: Change the settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Required Set this value to  true  to enable authentication on the\nMongoDB deployment. true array Conditional Set this value to  [\"X509\"] . [\"X509\"] To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example string Required Use this setting to enable\n X.509 internal cluster authentication . Once internal cluster authentication is enabled, it can't\nbe disabled. X509 In any directory, invoke the following   command to update and\nrestart your {k8sResource}}: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. If you have already created certificates, we recommend that you renew\nthem periodically using the following procedure. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to renew an existing   that\nstores the sharded cluster shards' certificates: Run this  kubectl  command to renew an existing   that\nstores the sharded cluster config server's certificates: Run this  kubectl  command to renew an existing   that\nstores the sharded cluster   certificates: Run this  kubectl  command to renew an existing   that stores\nthe sharded cluster shards' certificates: Run this  kubectl  command to renew an existing   that stores\nthe sharded cluster config servers' certificate: Run this  kubectl  command to renew an existing   that stores\nthe sharded cluster   certificates: Run this  kubectl  command to renew an existing   that\nstores the agents' X.509 certificates:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-clusterfile \\\n  --cert=<replica-set-clusterfile-tls-cert> \\\n  --key=<replica-set-clusterfile-tls-key>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      ca: <custom-ca>\n    certsSecretPrefix: <prefix>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-cert \\\n  --cert=<replica-set-tls-cert> \\\n  --key=<replica-set-tls-key> \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-clusterfile \\\n  --cert=<replica-set-clusterfile-tls-cert> \\\n  --key=<replica-set-clusterfile-tls-key> \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key> \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-0-clusterfile \\\n  --cert=<shard-0-clusterfile-tls-cert> \\\n  --key=<shard-0-clusterfile-tls-cert>\n\nkubectl -n mongodb create secret tls <prefix>-<metadata.name>-1-clusterfile \\\n  --cert=<shard-1-clusterfile-tls-cert> \\\n  --key=<shard-1-clusterfile-tls-cert>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-config-clusterfile \\\n  --cert=<config-clusterfile-tls-cert> \\\n  --key=<config-clusterfile-tls-cert>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-mongos-clusterfile \\\n  --cert=<mongos-clusterfile-tls-cert> \\\n  --key=<mongos-clusterfile-tls-cert>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      ca: <custom-ca>\n    certsSecretPrefix: <prefix>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-0-cert \\\n  --cert=<shard-0-tls-cert> \\\n  --key=<shard-0-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -\n\nkubectl -n mongodb create secret tls <prefix>-<metadata.name>-1-cert \\\n  --cert=<shard-1-tls-cert> \\\n  --key=<shard-1-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-config-cert \\\n  --cert=<config-tls-cert> \\\n  --key=<config-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-mongos-cert \\\n  --cert=<mongos-tls-cert> \\\n  --key=<mongos-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-0-clusterfile \\\n  --cert=<shard-0-clusterfile-tls-cert> \\\n  --key=<shard-0-clusterfile-tls-cert> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -\n\nkubectl -n mongodb create secret tls <prefix>-<metadata.name>-1-clusterfile \\\n  --cert=<shard-1-clusterfile-tls-cert> \\\n  --key=<shard-1-clusterfile-tls-cert> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-config-clusterfile \\\n  --cert=<config-clusterfile-tls-cert> \\\n  --key=<config-clusterfile-tls-cert> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-mongos-clusterfile \\\n  --cert=<mongos-clusterfile-tls-cert> \\\n  --key=<mongos-clusterfile-tls-cert> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key> \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                }
            ],
            "preview": "This guide instructs you on how to configure:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-standalone",
            "title": "Deploy a Standalone MongoDB Instance",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the following example standalone  .",
                "Open your preferred text editor and paste the  specification into a new text file.",
                "Configure the settings highlighted in the preceeding step as follows.",
                "Add any additional accepted settings for a Standalone deployment.",
                "Save this file with a .yaml file extension.",
                "Start your Standalone deployment.",
                "Track the status of your standalone deployment."
            ],
            "paragraphs": "You can deploy a  standalone  MongoDB instance for   to\nmanage. Use standalone instances for testing and development.\n Do not  use these deployments for production systems as they lack\nreplication and high availability. For all production deployments\nuse replica sets. To learn about replica sets, see\n Deploy a Replica Set . At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . You can use the   to deploy MongoDB resources with\n  and with   version 5.0.x or later. You can use the  Atlas Operator \nto deploy MongoDB resources to  . To deploy a  standalone  using an  , you must: Have or create an  Ops Manager instance  or a  Cloud Manager organization . Have or install the  MongoDB Enterprise Kubernetes Operator . Create or generate a  Kubernetes Operator ConfigMap . Create  credentials for the Kubernetes Operator  or\nconfigure  a different secret storage tool . To avoid storing secrets in  , you can migrate all  \nto a  secret storage tool . To troubleshoot your sharded cluster, see: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : This is a   file that you can modify to meet your desired\nconfiguration. Change the highlighted settings to match your desired\nstandalone configuration. Key Type Description Example metadata.name string Label for this   standalone  . Resource names must be 44 characters or less. metadata.name  documentation on  names . my-project spec.version string Version of MongoDB that is installed on this\nstandalone. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. For best results, use the\n latest available enterprise MongoDB version \nthat is  compatible  with your\n  version. string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value must exist on the  same  namespace as the resource\nyou want to create. <myproject> spec.credentials string Name of the secret you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . The      Secret  object\nholding the Credentials must exist on the  same  Namespace as\nthe resource you want to create. The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. Standalone spec.persistent string Optional. If this value is  true , then\n spec.podSpec.persistence.single  is set to its\ndefault value of  16Gi . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: If you want one   for each  , configure the\n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs Grant your containers permission to write to your  .\nThe   sets  fsGroup = 2000 ,  runAsUser = 2000 ,\nand  runAsNonRoot = true  in  securityContext .  \nsets  fsgroup  equal to  runAsUser  to make the volume writable\nfor a user that runs the main process in the container. To learn\nmore, see  Configure a\nSecurity Context for a Pod or Container  and the related\n discussion  in\nthe   documentation. If redeploying the resource doesn't fix\nissues with your Persistent Volume, contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n  specification file for a Standalone deployment: spec.additionalMongodConfig spec.exposedExternally spec.logLevel spec.featureCompatibilityVersion spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs spec.podSpec.podAffinity spec.podSpec.podTemplate spec.podSpec.nodeAffinity Invoke the following   command to create your standalone: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. Find a Specific Pod Review Logs from Specific Pod",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\nspec:\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <standalone-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "You can deploy a standalone MongoDB instance for  to\nmanage. Use standalone instances for testing and development.\nDo not use these deployments for production systems as they lack\nreplication and high availability. For all production deployments\nuse replica sets. To learn about replica sets, see\nDeploy a Replica Set.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/om-arch",
            "title": " Architecture in ",
            "headings": [
                "The MongoDBOpsManager Custom Resource Definition",
                "Application Database",
                "",
                "Backup Daemon",
                "Reconciling the MongoDBOpsManager Custom Resource"
            ],
            "paragraphs": "You can use the   to deploy   and MongoDB resources\nto a   cluster. The   manages the lifecycle of each of\nthese deployments differently. All     that the  \ncreates can later be migrated to a different  secret storage tool \nto avoid storing secrets in  . The   manages   deployments using the\n MongoDBOpsManager   . The   watches\nthe custom resource's specification for changes. When the\nspecification changes, the   validates the changes and\nmakes the appropriate updates to the resources in the   cluster. MongoDBOpsManager   s specification defines the\nfollowing   components: the Application Database, the   application, and the Backup Daemon. For the Application Database, the   deploys a MongoDB\nreplica set as a   to the   cluster.   creates\none Pod in the StatefulSet for each member\nthat comprises your Application Database replica set. Each Pod in\nthe StatefulSet runs a  mongod  and the MongoDB Agent. To enable each MongoDB Agent to start   on its\nPod in the StatefulSet, you must specify a specific MongoDB Server\nversion for the Application Database using the\n spec.applicationDatabase.version  setting. The version\nthat you specify in this setting must correspond to the tag in the\n container registry . Each MongoDB Agent\nstarts the Application Database with the specified version on its Pod\nin the StatefulSet. After each MongoDB Agent starts  s on its Application Database\nPod, the MongoDB Agents add all   processes to the Application\nDatabase replica set. You configure the number of replicas in and other\nconfiguration options for the Application Database replica set in the\n spec.applicationDatabase  collection in the\n MongoDBOpsManager  custom resource. The   passes\nthis configuration to the MongoDB Agents using a   that the\n  mounts to each Pod in the Application Database StatefulSet. Each time that you update\nthe  spec.applicationDatabase  collection, the\n  applies the changes to the MongoDB Agent configuration and\nthe StatefulSet specification, if applicable. If the StatefulSet\nspecification changes,   upgrades the Pods in a rolling\nfashion and restarts each Pod. The   creates a   with  clusterIp=none  to\nprovide connectivity to each Application Database Pod from within the\n  cluster. You can customize the   for the Application Database Pods using\nthe  spec.applicationDatabase.podSpec.persistence.single  or\n spec.applicationDatabase.podSpec.persistence.multiple  options. Depending on the   or the environment to which you deploy the\n ,   might create the   using\n dynamic volume provisioning . After the Application Database reaches a  Running  state, the\n  starts the  . For  , the\n  deploys a StatefulSet to the   cluster.  \ncreates one Pod in the StatefulSet for each   replica that\nyou want to deploy. Each Pod contains one   process. The   creates a   with  clusterIp=none  to\nallow clients deployed to the   cluster to connect to  . To\nallow clients external to the   cluster to connect to  ,\nconfigure the  spec.externalConnectivity  collection in the\nspecification for your   deployment. Deploy  multiple   replicas to\nmake your deployment highly available in the event of an   Pod\nfailure. If  spec.backup.enabled  is  true , the  \nstarts the Backup Daemon after the   reaches a  Running \nstage. For the Backup Daemon,   deploys a StatefulSet\nto the   cluster.   creates one pod in the\nStatefulSet for the Backup Daemon. If you enable backup, you must provide additional fields in the\n spec.backup  collection to configure:\nthe  oplog store  and a  blockstore  or an    snapshot store . If you enable backup, the   creates a   for the\nBackup Daemon's  head database . You can\nconfigure the head database using the  spec.backup.headDB \nsetting. The   invokes   APIs to ensure that the\n 's backup configuration matches the one that you define in\nthe custom resource definition. The following diagram describes how the   reconciles\nchanges to the  MongoDBOpsManager   . The   creates or updates the\n <om_resource_name>-db-config  secret. This secret contains\nthe configurations that the MongoDB Agent uses to start the\nApplication Database replica set. The   creates or updates the  <om_resource_name>-db \nApplication Database StatefulSet. This StatefulSet contains at\nleast three  . Each Pod runs one MongoDB Agent instance. Each MongoDB Agent starts a\n  instance on its Pod. The   mounts the  <om_resource_name>-db-config \nsecret to each Pod. The MongoDB Agent uses this secret to\nconfigure the Application Database replica set. The   creates or updates the  <om_resource_name> \nStatefulSet. This StatefulSet contains one Pod for each\n  replica. Each   replica connects to the Application\nDatabase. Most changes to the  MongoDBOpsManager   \ntrigger a rolling upgrade of the Pods in the\n <om_resource_name>  StatefulSet.  Enabling TLS for the\nApplication Database  also triggers a rolling\nrestart because the connection string to the Application Database\nchanges. Changes to the  spec.backup \n MongoDBOpsManager    collection don't\ntrigger a rolling upgrade. The   invokes   APIs to create an admin user.\nThe   saves this admin user's credentials in the\n <om_resource_name>-admin-key  secret. The  \nuses these credentials for all other   API invocations. This reconciliation step happens only once: when you use the\n  to create an   resource. The\n  skips this step when it updates the resource. The   performs a rolling upgrade of the Pods in the\n <om_resource_name>-db  Application Database StatefulSet\nto enable   to monitor it. This reconciliation step happens only when you enable Monitoring\nfor an application database for the first time. This happens most\noften when you deploy a new   resource. If  spec.backup.enabled  is  true , the  \ncreates the  <om_resource_name>-backup-daemon  StatefulSet or\nverifies that it is running. The   mounts a   for\nthe head database. The Backup Daemon connects to the same Application Database as the\n  deployment. If  spec.backup.enabled  is  true , the  \ninvokes   APIs to ensure that the  's backup\nconfiguration matches the one that you define in the custom resource\ndefinition.",
            "code": [],
            "preview": "You can use the  to deploy  and MongoDB resources\nto a  cluster. The  manages the lifecycle of each of\nthese deployments differently.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/configure-file-store",
            "title": "Configure File System Backup Store with ",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Create a PersistentVolumeClaim object.",
                "Create and apply an Ops Manager Custom Resource Definition specifying your PersistentVolumeClaim.",
                "Apply changes to your  deployment.",
                "Configure your oplog store.",
                "Create a File System Snapshot Store in ."
            ],
            "paragraphs": " supports storage of filesystem snapshots. To configure file system snapshot storage, your  \ndeployment must have a\n storage class \nconfigured with the  ReadWriteMany  method. To configure file system snapshot storage: Create a  PersistentVolumeClaim \nobject, and allocate storage as needed. Set\n accessModes \nto  ReadWriteMany : Create an     that specifies your\n PersistentVolumeClaim  object and the  backup.fileSystemStores \nfield, which is the name of your file system snapshot store. The following example creates a   file named\n ops-manager-fs.yaml , for the MongoDB\n oplog store  with a  kube-user . Invoke the following  kubectl  command on the filename of your\n  resource definition: Wait for your   object to report its state as  Running , then\nconfigure your oplog store as described in the\n Configure Backup Settings  step of the\n Deploy an Ops Manager Resource  procedure. Log into your   instance and navigate to:  Admin \n   Backup   \n Snapshot Store   \n Create New File System Store . Set the name to the value you set for  backup.fileSystemStores \nin your  . Set the other values as appropriate, then click\n Create . Your   object will report a  BACKUP  state of  Pending \nafter you create the new file system store.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: snapshot-store-ops-manager\nspec:\n  storageClassName: managed-nfs-storage #SC that supports(RWX)\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 10G\n    ..."
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\n  kind: MongoDBOpsManager\n  metadata:\n    name: ops-manager\n  spec:\n    replicas: 1\n    version: 6.0.7\n    adminCredentials: ops-manager-admin-secret\n    statefulSet:\n      spec:\n        template:\n          spec:\n            volumes:\n              - name: snapshot-store\n                persistentVolumeClaim:\n                  claimName: snapshot-store-ops-manager\n            containers:\n              - name: mongodb-ops-manager\n                volumeMounts:\n                  - name: snapshot-store\n                    mountPath: /snapshot_store\n    backup:\n      enabled: true\n      fileSystemStores:\n        - name: filesystem1\n       assignmentLabels: [\"test1\", \"test2\"]\n      opLogStores:\n        - name: oplog1\n          mongodbResourceRef:\n            name: oplog-db\n          # mongodbUserRef:\n          #   name: kube-user\n      statefulSet:\n        spec:\n          template:\n            spec:\n              volumes:\n                - name: snapshot-store\n                  persistentVolumeClaim:\n                    claimName: snapshot-store-ops-manager\n              containers:\n                - name: mongodb-backup-daemon\n                  volumeMounts:\n                    - name: snapshot-store\n                      mountPath: /snapshot_store\n    applicationDatabase:\n      members: 3\n      version: 5.0.7-ent"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                }
            ],
            "preview": " supports storage of filesystem snapshots.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/upgrade-om-version",
            "title": "Upgrade Ops Manager and Backing Database Versions",
            "headings": [
                "Prerequisites",
                "Procedure"
            ],
            "paragraphs": "Update the major and minor versions of your   instance and  backing databases \nin the  Ops Manager Resource Specification  that the   uses to manage your deployment. To maintain existing settings and availability, back up the following in your current   instance: Your  conf-mms.properties  to a secure location. The  conf-mms.properties \nstores settings for the   instance. Your  gen.key  files to a secure location. The  gen.key \nprovides details to encrypt and decrypt  's backing databases\nand user credentials.   might delete these files as part of the upgrade process. Your  application database . If the upgrade fails,\nyou need a current backup to restore your   instance. Upgrade   by following the considerations, prerequisites, and procedure in  upgrade-om . Reference  compatible-mdb-versions  to ensure your  backing databases \nuse a MongoDB version that is compatible with the new   version. If you need to upgrade your backing databases to a compatible MongoDB version,\nsee  Upgrade MongoDB Version and FCV . To update   from 5.0 to 6.0 and the application database to MongoDB\n 4.4.18-ent , complete the following steps: For example, the following resource updates   from 4.0 to 5.0 and the application database to MongoDB  4.2.11-ent . Reapply the configuration to  :  automatically reconfigures your deployment with the new specifications. You can see these changes reflected in your   or  Cloud Manager  application.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: om\nspec:\n replicas: 1\n version: \"5.0.0\"\n adminCredentials: ops-manager-admin\n configuration:\n  mms.fromEmailAddr: admin@example.com\n  mms.security.allowCORS: \"false\"\n backup:\n  enabled: true\n  headDB:\n   storage: \"30Gi\"\n   labelSelector:\n    matchLabels:\n     app: my-app\n  opLogStores:\n   - name: oplog1\n     mongodbResourceRef:\n      name: my-oplog-db\n     mongodbUserRef:\n      name: my-oplog-user\n  s3Stores:\n   - name: s3store1\n     mongodbResourceRef:\n      name: my-s3-metadata-db\n     mongodbUserRef:\n      name: my-s3-store-user\n     s3SecretRef:\n       name: my-s3-credentials\n     pathStyleAccessEnabled: true\n     s3BucketEndpoint: s3.region.amazonaws.com\n     s3BucketName: my-bucket\n\n applicationDatabase:\n   passwordSecretKeyRef:\n    name: om-db-user-secret\n    key: password\n   members: 3\n   version: \"4.4.18-ent\"\n"
                },
                {
                    "lang": "none",
                    "value": "kubectl apply -f <om-resource-specification>.yaml"
                }
            ],
            "preview": "Update the major and minor versions of your  instance and backing databases\nin the Ops Manager Resource Specification that the  uses to manage your deployment.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-om-container-local-mode",
            "title": "Configure an  Resource to use Local Mode",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Delete the  that manages your  Pods.",
                "Copy the fields of this  resource.",
                "Paste the copied example section into your existing  resource.",
                "Save your  config file.",
                "Apply changes to your  deployment.",
                "In a rolling fashion, delete your old  Pods.",
                "Track the status of your  instance.",
                "Download the MongoDB installation archive to your local machine.",
                "Copy the MongoDB archive to the  Persistent Volume.",
                "Deploy a MongoDB Database Resource."
            ],
            "paragraphs": "In a default configuration, the MongoDB Agents and Backup Daemons\naccess MongoDB installation archives over the Internet from  You can configure   to run in  Local Mode  with the\n  if the nodes in your   cluster don't have access to\nthe Internet. The Backup Daemons and managed MongoDB resources download\ninstallation archives only from a   that you create for\nthe   StatefulSet. This procedure covers uploading installation archives to  . Configuring   to use Local Mode in   is not recommended.\nConsider  configuring Ops Manager to use Remote Mode  instead. Deploy an   Resource . The following procedure shows you how to\nupdate your       to enable Local Mode. To avoid downtime when you enable Local Mode, ensure that you set\n spec.replicas  to a value greater than  1  in your\n  resource definition. If you updated your   resource definition to make  \nhighly available, apply your changes before you begin this tutorial: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : In this tutorial, you update the StatefulSet that manages the  \nPods in your   cluster. You must first delete the   StatefulSet so that   can apply\nthe updates that Local Mode requires. Find the name of your   StatefulSet: The entry in the response that matches the\n metadata.name  of your Your   StatefulSet is the entry in the response that matches\nthe  metadata.name  in your   resource\ndefinition. Delete the   StatefulSet: Ensure that you include the  --cascade=false  flag when you\ndelete your   StatefulSet. If you don't include this\nflag,   also deletes your   Pods. Copy Lines 9-31 of this example to: Use the   configuration setting\n automation.versions.source: local  in\n spec.configuration  to enable Local Mode. Define a   for the   StatefulSet to store the\nMongoDB installation archive. MongoDB Agents running in MongoDB\ndatabase resource containers that you create with the  \ndownload the installation archives from   instead of from the\nInternet. Open your preferred text editor and paste the  \nspecification into the appropriate location in your resource file. Invoke the following  kubectl  command on the filename of the\n  resource definition:  creates a new   StatefulSet when you apply the changes\nto your   resource definition. Before proceeding to the next\nstep, run the following command to ensure that the   StatefulSet\nexists: The new   StatefulSet should show 0 members ready: When the new Pod is ready, the output is similar to the following example: List the   Pods in your   cluster: Delete one   Pod:  recreates the   Pod you deleted. Continue to get the\nstatus of the new Pod until it is ready: When the new Pod is initializing, the output is similar to the\nfollowing example: Repeat Steps  b  and  c  until you've deleted all of your\n  Pods and confirmed that all of the new Pods are ready. To check the status of your   resource, invoke the following\ncommand: See  Troubleshoot the   for information about the\nresource deployment statuses. After the   resource completes the  Reconciling  phase, the\ncommand returns output similar to the following: Copy the value of the  status.opsManager.url  field, which states\nthe resource's connection  . You use this value when you create a\n  later in the procedure. The installers that you download depend on the environment to which\nyou deployed the operator: Download the RHEL installation tarball for the MongoDB Server version\nyou want the   to deploy. For example, to download the\n 6.0.1  release: The following example includes a link that allows you to download\nthe specified version of MongoDB Community Edition.\nTo download any other version of MongoDB Community Edition, visit the  MongoDB Community Edition Download Center .\nTo download MongoDB Enterprise Edition, visit the  MongoDB Enterprise Download Center . Copy the MongoDB archive for each MongoDB version you intend to deploy\nto the   Persistent Volume. The commands that you use depend on the environment to which you\ndeployed the  : If you deployed more than one  \n replica , copy only the MongoDB\ninstallation  tarball  packages to  Replica 1  and\nbeyond. To copy the MongoDB installation archive to the\n  PersistentVolume: Copy the MongoDB Server installation tarball to the\n  PersistentVolume. For example, to copy the  6.0.1 \nrelease: To copy the MongoDB installation archive to the\n  PersistentVolume, copy the MongoDB Server installation  tarball  to the\n  PersistentVolume. For example, to copy the  6.0.1 \nrelease: MongoDB Agents run in MongoDB database resource containers that\nyou create with the  . Download the installation archives\nfrom   instead of downloading them from the Internet. If you have not done so already, complete the following\nprerequisites: Create Credentials for the  Create One Project using a ConfigMap Deploy a  MongoDB database resource \nin the same namespace to which you deployed  .\nEnsure that you: Match the  spec.opsManager.configMapRef.name  of the resource\nto the  metadata.name  of your ConfigMap. Match the  spec.credentials  of the resource to the name of\nthe secret you created that contains an   programmatic\nAPI key pair.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulsets"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulsets -n mongodb\nNAME                       READY   AGE\nops-manager-localmode      2/2     2m31s\nops-manager-localmode-db   3/3     4m46s"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete statefulset --cascade=false <ops-manager-statefulset>"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: ops-manager-localmode\nspec:\n replicas: 2\n version: \"6.0.0\"\n adminCredentials: ops-manager-admin-secret\n configuration:\n   # this enables local mode in Ops Manager\n   automation.versions.source: local\n statefulSet:\n   spec:\n     # the Persistent Volume Claim will be created for each Ops Manager Pod\n     volumeClaimTemplates:\n      - metadata:\n          name: mongodb-versions\n        spec:\n          accessModes: [ \"ReadWriteOnce\" ]\n          resources:\n            requests:\n              storage: \"20Gi\"\n   template:\n     spec:\n       containers:\n         - name: mongodb-ops-manager\n           volumeMounts:\n           - name: mongodb-versions\n             # this is the directory in each Pod where all MongoDB\n             # archives must be put\n             mountPath: /mongodb-ops-manager/mongodb-releases\n backup:\n  enabled: false\n applicationDatabase:\n  members: 3"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulsets"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulsets -n mongodb\nNAME                       READY   AGE         ops-manager-localmode      0/2     2m31s\nops-manager-localmode-db   3/3     4m46s"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                          READY   STATUS    RESTARTS   AGE\nmongodb-enterprise-operator-5648d4c86-k5brh   1/1     Running   0          5m24s\nops-manager-localmode-0                       1/1     Running   0          3m55s\nops-manager-localmode-1                       1/1     Running   0          5m45s\nops-manager-localmode-db-0                    1/1     Running   0          5m19s\nops-manager-localmode-db-1                    1/1     Running   0          4m54s\nops-manager-localmode-db-2                    1/1     Running   0          4m12s"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete pod <om-pod-0>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                          READY   STATUS    RESTARTS   AGE\nmongodb-enterprise-operator-5648d4c86-k5brh   1/1     Running   0          5m24s\nops-manager-localmode-0                       0/1     Running   0          0m55s\nops-manager-localmode-1                       1/1     Running   0          5m45s\nops-manager-localmode-db-0                    1/1     Running   0          5m19s\nops-manager-localmode-db-1                    1/1     Running   0          4m54s\nops-manager-localmode-db-2                    1/1     Running   0          4m12s"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2020-05-15T16:20:22Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: \"4.2.11-ent\"\n  backup:\n    phase: \"\"\n  opsManager:\n    lastTransition: \"2020-05-15T16:20:26Z\"\n    phase: Running\n    replicas: 1\n    url: http://ops-manager-localmode-svc.mongodb.svc.cluster.local:8080\n    version: \"5.0.0\"\n"
                },
                {
                    "lang": "sh",
                    "value": "curl -OL https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel80-6.0.1.tgz"
                },
                {
                    "lang": "sh",
                    "value": "kubectl cp mongodb-linux-x86_64-rhel80-6.0.1.tgz \\\n\"ops-manager-localmode-0:/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-rhel80-6.0.1.tgz\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl cp mongodb-linux-x86_64-rhel80-6.0.1.tgz \\\n\"ops-manager-localmode-1:/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-rhel80-6.0.1.tgz\""
                },
                {
                    "lang": "sh",
                    "value": "oc rsync  \"ops-manager-localmode-0:/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-rhel80-6.0.1.tgz\" \\\nmongodb-linux-x86_64-rhel80-6.0.1.tgz"
                },
                {
                    "lang": "sh",
                    "value": "oc rsync  \"ops-manager-localmode-1:/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-rhel80-6.0.1.tgz\" \\\nmongodb-linux-x86_64-rhel80-6.0.1.tgz"
                }
            ],
            "preview": "In a default configuration, the MongoDB Agents and Backup Daemons\naccess MongoDB installation archives over the Internet from ",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-operator-install",
            "title": "Plan your  Installation",
            "headings": [],
            "paragraphs": "Use the   to deploy: To deploy MongoDB resources with the  , you need an\n  instance. Deploy this instance to   using the Operator or\noutside   using\n traditional installation methods . The\nOperator uses     methods to deploy and then manage MongoDB\nresources. Ops Manager resources MongoDB standalone, replica set, and sharded cluster resources Review the architecture of the custom resources in the  :\nthe   and the MongoDB database. Review compatible versions of  , OpenShift, MongoDB, and  . Review container image details. Set the scope for the   deployment by configuring which\ntype of namespace the   should use. Review   deployment scopes and other preparation\ninformation. Review the prerequisites before you install the  .",
            "code": [],
            "preview": "Use the  to deploy:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/multi-cluster-secure-x509",
            "title": "Secure Multi-Cluster Deployments with X.509",
            "headings": [
                "Prerequisites",
                "Enable X.509 Authentication for a MongoDBMulti Resource",
                "Create the secret for your agent's X.509 certificate of your MongoDBMulti custom resource.",
                "Update your MongoDBMulti custom resource to enable X509 authentication.",
                "Verify that the MDB resources are running.",
                "Renew X.509 Certificates for a MongoDBMulti Resource",
                "Renew the  for a MongoDBMulti resource.",
                "Renew the secret for your agent's X.509 certificates."
            ],
            "paragraphs": "You can configure the   to use X.509 certificates to authenticate\nyour client applications in a multi-cluster deployment. To secure your multi-cluster deployment with X.509 certificates,\nyou run all actions on the  central cluster .\nThe   propagates the X.509 configuration to each member cluster\nand updates the   configuration on each member cluster. Before you secure your   using  \nencryption, complete the following tasks: Follow the steps in the  Multi-Cluster Quick Start Prerequisites . Deploy a  TLS-encrypted multi-cluster . Create credentials for the Kubernetes Operator  for the  . Enabling X.509 authentication at the project level configures all\nagents to use X.509 client authentication when communicating with\nMongoDB deployments. X.509 client authentication requires one of the following:  5.0.7 or later Run the  kubectl  command to create a new secret that stores the agent's X.509 certificate: Update your MongoDB multi-cluster resource \nwith  security settings  from the  \nMongoDB resource specification. The resulting configuration should look as\nfollows: The   copies the ConfigMap with the   created in\nthe central cluster to each member cluster, generates a concatenated\n  secret, and distributes it to the member clusters. For member clusters, run the following commands to verify that\nthe MongoDB Pods are in the running state: In the central cluster, run the following commands to verify that\nthe  MongoDBMulti  custom resource is in the running state: If you have already created X.509 certificates, renew them periodically using\nthe following procedure. Run this  kubectl  command to renew an existing   that\nstores the  MongoDBMulti  resource's certificates: Run the  kubectl  command to renew an existing secret that stores\nthe  MongoDBMulti  resource's agent certificates:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key>"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBMulti\nmetadata:\n name: multi-replica-set\nspec:\n version: 5.0.0-ent\n type: ReplicaSet\n persistent: false\n duplicateServiceObjects: true\n credentials: my-credentials\n opsManager:\n   configMapRef:\n     name: my-project\n security:\n   tls:\n     ca: custom-ca\n   certsSecretPrefix: <prefix>\n authentication:\n   enabled: true\n   modes: [\"X509\"]\n   agents:\n     mode: \"X509\"\n clusterSpecList:\n   clusterSpecs:\n   - clusterName: ${MDB_CLUSTER_1_FULL_NAME}\n     members: 3\n   - clusterName: ${MDB_CLUSTER_2_FULL_NAME}\n     members: 2\n   - clusterName: ${MDB_CLUSTER_3_FULL_NAME}\n     members: 3"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_1_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_2_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_3_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context=$MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace mongodb \\\n  get mdbm multi-replica-set -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n--namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-cert \\\n--cert=<resource-tls-cert> \\\n--key=<resource-tls-key> \\\n--dry-run=client \\\n-o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key> \\\n  --dry-run=client \\\n  -o yaml | kubectl apply -f -"
                }
            ],
            "preview": "You can configure the  to use X.509 certificates to authenticate\nyour client applications in a multi-cluster deployment.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-sharded-cluster",
            "title": "Deploy a Sharded Cluster",
            "headings": [
                "Considerations",
                "Do Not Deploy Monitoring Agents Inside and Outside ",
                "Choose Whether to Encrypt Connections",
                "Deploy a Sharded Cluster",
                "Configure kubectl to default to your namespace.",
                "Create the secret for your Shards' TLS certificates.",
                "Create the secret for your config servers' TLS certificate.",
                "Create the secret for your mongos servers' TLS certificate.",
                "Create the  for your agent's TLS certificate.",
                "Create the  to link your  with your deployment.",
                "Copy the sample sharded cluster resource.",
                "Paste the copied example to create a new sharded cluster resource.",
                "Configure the settings highlighted in the preceding step as follows.",
                "Configure the TLS settings for your sharded cluster resource using a Custom Certificate Authority.",
                "Add any additional accepted settings for a sharded cluster deployment.",
                "Save this file with a .yaml file extension.",
                "Start your sharded cluster deployment.",
                "Track the status of your sharded cluster deployment.",
                "Renew TLS Certificates for a Sharded Cluster",
                "Configure kubectl to default to your namespace.",
                "Renew the  for your Shards' TLS certificates.",
                "Renew the  for your config server's TLS certificates.",
                "Renew the  for your mongos server's TLS certificates.",
                "Configure kubectl to default to your namespace.",
                "Copy the sample sharded cluster resource.",
                "Paste the copied example to create a new sharded cluster resource.",
                "Configure the settings highlighted in the preceding step as follows.",
                "Add any additional accepted settings for a sharded cluster deployment.",
                "Save this file with a .yaml file extension.",
                "Start your sharded cluster deployment.",
                "Track the status of your sharded cluster deployment."
            ],
            "paragraphs": "Sharded clusters  provide horizontal scaling\nfor large data sets and enable high throughput operations by\ndistributing the data set across a group of servers. To learn more about sharding, see\n Sharding Introduction  in the\nMongoDB manual. Use this procedure to deploy a new sharded cluster that   manages.\nLater, you can use   to add shards and perform other maintenance\noperations on the cluster. At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . You can use the   to deploy MongoDB resources with\n  and with   version 5.0.x or later. You can use the  Atlas Operator \nto deploy MongoDB resources to  .  doesn't support  arbiter nodes . Due to   network translation, a monitoring agent outside  \ncannot monitor MongoDB instances inside  . For this reason, k8s\nand non-k8s deployments in the same project are not supported. Use\nseparate projects. When you deploy your sharded cluster via the  , you must\nchoose whether to encrypt connections using   certificates. The following procedure for  TLS-Encrypted  connections: The following procedure for  Non-Encrypted Connections : To set up   encryption for a replica set, see\n Deploy a Replica Set . Select the appropriate tab based on whether you want to encrypt your\nreplica set connections with  . Establishes  -encrypted connections between cluster shards. Establishes  -encrypted connections between client applications\nand MongoDB deployments. Requires valid certificates for   encryption. Doesn't encrypt connections between cluster shards. Doesn't encrypt connections between client applications\nand MongoDB deployments. Has fewer setup requirements than a deployment with  -encrypted\nconnections. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create a new   that stores\nthe sharded cluster shards' certificates: If you're using   as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. Run this  kubectl  command to create a new secret that stores\nthe sharded cluster config servers' certificate: If you're using   as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. Run this  kubectl  command to create a new secret that stores\nthe sharded cluster   certificate: If you're using   as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. Run this  kubectl  command to create a new   that stores\nthe agent's TLS certificate: If you're using   as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. Run this  kubectl  command to link your   to your sharded\ncluster: Change the settings of this   file to match your\ndesired  sharded cluster  configuration. Change the settings to match your desired\n sharded cluster  configuration. Open your preferred text editor and paste the   specification\ninto a new text file. Key Type Description Example metadata.name string Label for this    sharded cluster   . Resource names must be 44 characters or less. metadata.name  documentation on  names . myproject spec.shardCount integer Number of shards to deploy. 2 spec.mongodsPerShardCount integer Number of shard members per shard. 3 spec.mongosCount integer Number of shard routers to deploy. 2 spec.configServerCount integer Number of members of the config server replica set. 3 spec.version string Version of MongoDB that this  sharded cluster  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. For best results, use the\n latest available enterprise MongoDB version \nthat is  compatible  with your\n  version. spec.opsManager.configMapRef.name string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value must exist on the  same  namespace as the resource\nyou want to create. The   tracks any changes to the ConfigMap and\nreconciles the state of the  . <myproject> spec.credentials string Name of the secret you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . The      Secret  object\nholding the Credentials must exist on the  same  Namespace as\nthe resource you want to create. The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. ShardedCluster spec.persistent string Optional. Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. If this value is  true , then the following values are set\nto their default value of  16Gi : To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: spec.shardPodSpec.persistence.single spec.configSrvPodSpec.persistence.single If you want one   for each  , configure the\n spec.shardPodSpec.persistence.single  and\n spec.configSrvPodSpec.persistence.single \ncollections. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: In the  spec.configSrvPodSpec.persistence.multiple \ncollection:\n-  .data \n-  .journal \n-  .logs In the  spec.configSrvPodSpec.persistence.multiple  collection:\n-  .data \n-  .journal \n-  .logs Grant your containers permission to write to your  .\nThe   sets  fsGroup = 2000 ,  runAsUser = 2000 ,\nand  runAsNonRoot = true  in  securityContext .  \nsets  fsgroup  equal to  runAsUser  to make the volume writable\nfor a user that runs the main process in the container. To learn\nmore, see  Configure a\nSecurity Context for a Pod or Container  and the related\n discussion  in\nthe   documentation. If redeploying the resource doesn't fix\nissues with your Persistent Volume, contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example string Required Add the  's name that stores the custom  \nthat you used to sign your deployment's   certificates. <custom-ca> string Required Add the  <prefix>  of the secret\nname that contains your MongoDB deployment's   certificates. If you call your deployment  my-deployment  and you set the\nprefix to  mdb , you must name the   secret for the\nclient   communications  mdb-my-deployment-cert . Also,\nyou must name the   secret for internal cluster authentication\n(if enabled)  mdb-my-deployment-clusterfile . devDb You can also add any of the following optional settings to the\n  specification file for a  sharded cluster \ndeployment: For config server For shard routers For shard members spec.backup.mode spec.clusterDomain spec.connectivity.replicaSetHorizons spec.exposedExternally spec.featureCompatibilityVersion spec.logLevel You must set  spec.clusterDomain  if your   cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n  might not function as expected. spec.configSrv.additionalMongodConfig spec.configSrvPodSpec.persistence.single spec.configSrvPodSpec.persistence.multiple.data spec.configSrvPodSpec.persistence.multiple.journal spec.configSrvPodSpec.persistence.multiple.logs spec.configSrvPodSpec.nodeAffinity spec.configSrvPodSpec.podAffinity spec.configSrvPodSpec.podAntiAffinityTopologyKey spec.configSrvPodSpec.podTemplate.metadata spec.configSrvPodSpec.podTemplate.spec spec.mongos.additionalMongodConfig spec.mongosPodSpec.nodeAffinity spec.mongosPodSpec.podAffinity spec.mongosPodSpec.podAntiAffinityTopologyKey spec.mongosPodSpec.podTemplate.metadata spec.mongosPodSpec.podTemplate.spec spec.shard.additionalMongodConfig spec.shardPodSpec.nodeAffinity spec.shardPodSpec.persistence.single spec.shardPodSpec.persistence.multiple.data spec.shardPodSpec.persistence.multiple.journal spec.shardPodSpec.persistence.multiple.logs spec.shardPodSpec.podAffinity spec.shardPodSpec.podAntiAffinityTopologyKey spec.shardPodSpec.podTemplate.metadata spec.shardPodSpec.podTemplate.spec Invoke the following   command to create your\n sharded cluster : Check the log  after running this\ncommand. If the creation was successful, you should see a message\nsimilar to the following: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. After you encrypt your database resource with  , you can secure the\nfollowing: Client authentication with LDAP Client authentication with X.509 Internal authentication with X.509 Renew your   certificates periodically\nusing the following procedure: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to renew an existing   that\nstores the sharded cluster shards' certificates: Run this  kubectl  command to renew an existing   that\nstores the sharded cluster config server's certificates: Run this  kubectl  command to renew an existing   that\nstores the sharded cluster   certificates: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the settings of this   file to match your\ndesired  sharded cluster  configuration. This is a   file that you can modify to meet your desired\nconfiguration. Change the settings to match your desired\n sharded cluster  configuration. Open your preferred text editor and paste the   specification\ninto a new text file. Key Type Description Example metadata.name string Label for this    sharded cluster   . Resource names must be 44 characters or less. metadata.name  documentation on  names . myproject spec.shardCount integer Number of shards to deploy. 2 spec.mongodsPerShardCount integer Number of shard members per shard. 3 spec.mongosCount integer Number of shard routers to deploy. 2 spec.configServerCount integer Number of members of the config server replica set. 3 spec.version string Version of MongoDB that this  sharded cluster  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. For best results, use the\n latest available enterprise MongoDB version \nthat is  compatible  with your\n  version. spec.opsManager.configMapRef.name string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value must exist on the  same  namespace as the resource\nyou want to create. The   tracks any changes to the ConfigMap and\nreconciles the state of the  . <myproject> spec.credentials string Name of the secret you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . The      Secret  object\nholding the Credentials must exist on the  same  Namespace as\nthe resource you want to create. The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. ShardedCluster spec.persistent string Optional. Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. If this value is  true , then the following values are set\nto their default value of  16Gi : To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: spec.shardPodSpec.persistence.single spec.configSrvPodSpec.persistence.single If you want one   for each  , configure the\n spec.shardPodSpec.persistence.single  and\n spec.configSrvPodSpec.persistence.single \ncollections. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: In the  spec.configSrvPodSpec.persistence.multiple \ncollection:\n-  .data \n-  .journal \n-  .logs In the  spec.configSrvPodSpec.persistence.multiple  collection:\n-  .data \n-  .journal \n-  .logs Grant your containers permission to write to your  .\nThe   sets  fsGroup = 2000 ,  runAsUser = 2000 ,\nand  runAsNonRoot = true  in  securityContext .  \nsets  fsgroup  equal to  runAsUser  to make the volume writable\nfor a user that runs the main process in the container. To learn\nmore, see  Configure a\nSecurity Context for a Pod or Container  and the related\n discussion  in\nthe   documentation. If redeploying the resource doesn't fix\nissues with your Persistent Volume, contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n  specification file for a  sharded cluster \ndeployment: For config server For shard routers For shard members spec.backup.mode spec.clusterDomain spec.connectivity.replicaSetHorizons spec.exposedExternally spec.featureCompatibilityVersion spec.logLevel You must set  spec.clusterDomain  if your   cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n  might not function as expected. spec.configSrv.additionalMongodConfig spec.configSrvPodSpec.persistence.single spec.configSrvPodSpec.persistence.multiple.data spec.configSrvPodSpec.persistence.multiple.journal spec.configSrvPodSpec.persistence.multiple.logs spec.configSrvPodSpec.nodeAffinity spec.configSrvPodSpec.podAffinity spec.configSrvPodSpec.podAntiAffinityTopologyKey spec.configSrvPodSpec.podTemplate.metadata spec.configSrvPodSpec.podTemplate.spec spec.mongos.additionalMongodConfig spec.mongosPodSpec.nodeAffinity spec.mongosPodSpec.podAffinity spec.mongosPodSpec.podAntiAffinityTopologyKey spec.mongosPodSpec.podTemplate.metadata spec.mongosPodSpec.podTemplate.spec spec.shard.additionalMongodConfig spec.shardPodSpec.nodeAffinity spec.shardPodSpec.persistence.single spec.shardPodSpec.persistence.multiple.data spec.shardPodSpec.persistence.multiple.journal spec.shardPodSpec.persistence.multiple.logs spec.shardPodSpec.podAffinity spec.shardPodSpec.podAntiAffinityTopologyKey spec.shardPodSpec.podTemplate.metadata spec.shardPodSpec.podTemplate.spec Invoke the following   command to create your\n sharded cluster : Check the log  after running this\ncommand. If the creation was successful, you should see a message\nsimilar to the following: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-0-cert \\\n  --cert=<shard-0-tls-cert> \\\n  --key=<shard-0-tls-key>\n\nkubectl -n mongodb create secret tls <prefix>-<metadata.name>-1-cert \\\n  --cert=<shard-1-tls-cert> \\\n  --key=<shard-1-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-config-cert \\\n  --cert=<config-tls-cert> \\\n  --key=<config-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-mongos-cert \\\n  --cert=<mongos-tls-cert> \\\n  --key=<mongos-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap custom-ca --from-file=ca-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      ca: <custom-ca>\n    certsSecretPrefix: <prefix>\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "2018-06-26T10:30:30.346Z INFO operator/shardedclusterkube.go:52 Created! {\"sharded cluster\": \"my-sharded-cluster\"}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-0-cert \\\n  --cert=<shard-0-tls-cert> \\\n  --key=<shard-0-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -\n\nkubectl -n mongodb create secret tls <prefix>-<metadata.name>-1-cert \\\n  --cert=<shard-1-tls-cert> \\\n  --key=<shard-1-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-config-cert \\\n  --cert=<config-tls-cert> \\\n  --key=<config-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-mongos-cert \\\n  --cert=<mongos-tls-cert> \\\n  --key=<mongos-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "2018-06-26T10:30:30.346Z INFO operator/shardedclusterkube.go:52 Created! {\"sharded cluster\": \"my-sharded-cluster\"}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "Sharded clusters provide horizontal scaling\nfor large data sets and enable high throughput operations by\ndistributing the data set across a group of servers.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/multi-cluster-secure-internal-auth",
            "title": "Secure Internal Authentication in Multi-Cluster Deployments with X.509",
            "headings": [
                "Prerequisites",
                "Configure X.509 Internal Authentication for a MongoDBMulti Resource",
                "Create the secret for your agent's X.509 certificate of your MongoDBMulti custom resource.",
                "Create the secret for the member cluster's internal X.509 certificate.",
                "Update your MongoDBMulti custom resource to enable X509 authentication.",
                "Verify that the MDB resources are running.",
                "Renew Internal Authentication X.509 Certificates for a MongoDBMulti Resource",
                "Renew the  for a MongoDBMulti resource.",
                "Renew the secret for your agent's X.509 certificates.",
                "Renew the secret for internal members's X.509 certificates of the MongoDBMulti resource."
            ],
            "paragraphs": "This guide instructs you on how to configure: X.509 internal authentication between MongoDB nodes in each cluster in\nyour  . X.509 authentication from clients to your MongoDB instances. Before you secure your   using  \nencryption, complete the following tasks: Follow the steps in the  Multi-Cluster Quick Start Prerequisites . Deploy a   TLS-encrypted multi-cluster . Create credentials for the Kubernetes Operator \nfor the  . Enabling X.509 authentication at the project level configures all\nagents to use X.509 client authentication when communicating with\nMongoDB deployments. X.509 client authentication requires one of the following:  5.0.7 or later Run the  kubectl  command to create a new secret that stores the agent's X.509 certificate: Run the  kubectl  command to create a new secret that stores the internal\ncluster member's X.509 certificate. The member clusters are defined in\nyour  MongoDBMulti  custom resource. Update your MongoDB multi-cluster resource \nwith  security settings  from the  \nMongoDB resource specification. Add the  internalCluster  setting,\nunder  spec.authentication , and set it to  \"X509\" . The resulting\nconfiguration should look as follows: The   copies the ConfigMap with the   created in\nthe central cluster to each member cluster, generates a concatenated\n  secret, and distributes it to the member clusters. For member clusters, run the following commands to verify that\nthe MongoDB Pods are in the running state: In the central cluster, run the following commands to verify that\nthe  MongoDBMulti  custom resource is in the running state: If you have already created certificates, we recommend that you renew\nthem periodically using the following procedure. Run this  kubectl  command to renew an existing   that\nstores the  MongoDBMulti  resource's certificates: Run the  kubectl  command to renew an existing secret that stores\nthe  MongoDBMulti  resource's agent certificates: Run the  kubectl  command to renew an existing secret that stores\nX.509 certificates for internal members of the  MongoDBMulti  resource:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-clusterfile \\\n  --cert=<resource-clusterfile-tls-cert> \\\n  --key=<resource-clusterfile-tls-key>"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBMulti\nmetadata:\n name: multi-replica-set\nspec:\n version: 5.0.0-ent\n type: ReplicaSet\n persistent: false\n duplicateServiceObjects: true\n credentials: my-credentials\n opsManager:\n   configMapRef:\n     name: my-project\n security:\n   tls:\n     ca: custom-ca\n   certsSecretPrefix: <prefix>\n authentication:\n   enabled: true\n   modes: [\"X509\"]\n   agents:\n     mode: \"X509\"\n   internalCluster: \"X509\"\n clusterSpecList:\n   clusterSpecs:\n   - clusterName: ${MDB_CLUSTER_1_FULL_NAME}\n     members: 3\n   - clusterName: ${MDB_CLUSTER_2_FULL_NAME}\n     members: 2\n   - clusterName: ${MDB_CLUSTER_3_FULL_NAME}\n     members: 3"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_1_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_2_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_3_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context=$MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace mongodb \\\n  get mdbm multi-replica-set -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n--namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-cert \\\n--cert=<resource-tls-cert> \\\n--key=<resource-tls-key> \\\n--dry-run=client \\\n-o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key> \\\n  --dry-run=client \\\n  -o yaml | kubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-clusterfile \\\n  --cert=<resource-clusterfile-tls-cert> \\\n   --key=<resource-clusterfile-tls-key> \\\n   --dry-run=client \\\n   -o yaml | kubectl apply -f -"
                }
            ],
            "preview": "This guide instructs you on how to configure:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-om-container",
            "title": "Deploy an  Resource",
            "headings": [
                "Considerations",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Create secrets for your certificates.",
                "If necessary, validate your TLS certificates.",
                "Copy the following example   .",
                "Open your preferred text editor and paste the  specification into a new text file.",
                "Configure the settings highlighted in the prior example.",
                "Optional: Configure Backup settings",
                "Optional: Configure any additional settings for an  deployment.",
                "Save this file with a .yaml file extension.",
                "Create your  instance.",
                "Track the status of your  instance.",
                "Access the  application.",
                "Create credentials for the Kubernetes Operator.",
                "Create a project using a .",
                "Deploy MongoDB database resources to complete the Backup configuration.",
                "Confirm that the  resource is running.",
                "Configure kubectl to default to your namespace.",
                "Copy the following example   .",
                "Open your preferred text editor and paste the  specification into a new text file.",
                "Configure the settings highlighted in the prior example.",
                "Optional: Configure Backup settings.",
                "Optional: Configure any additional settings for an  deployment.",
                "Save this file with a .yaml file extension.",
                "Create your  instance.",
                "Track the status of your  instance.",
                "Access the  application.",
                "Optional: Create credentials for the Kubernetes Operator.",
                "Optional: Create a project using a .",
                "Optional: Deploy MongoDB database resources to complete the Backup configuration.",
                "Optional: Confirm that the  resource is running."
            ],
            "paragraphs": "You can deploy   in a container with the  . When you configure your   deployment, you must choose whether to run connections over   or  . The following   procedure: The following   procedure: When running over  ,   runs on port  8443  by\ndefault. Select the appropriate tab based on whether you want to encrypt\nyour   and application database connections with  . Establishes  -encrypted connections to/from the  \napplication. Establishes  -encrypted connections between the application\ndatabase's replica set members. Requires valid certificates for   encryption. Doesn't encrypt connections to or from the   application. Doesn't encrypt connections between the\napplication database's replica set members. Has fewer setup requirements. Follow these steps to deploy the   resource to run over\n  and secure the application database using  . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : If you're using   as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. To learn about your options for secret\nstorage, see  Configure Secret Storage . Once you have your   certificates and private keys, run the\nfollowing command to create a   that stores  's\n  certificate: Run the following command to create a new   that stores\nthe application database's   certificate: If your     certificate or your application database\n  certificate is signed by a Custom Certificate\nAuthority, you must provide a  CA (Certificate Authority) \ncertificate to validate the   certificate(s). To validate the\n  certificate(s), create a   to hold the\n CA (Certificate Authority)  certificate: You must concatenate your custom   file and the entire\n  certificate chain from  downloads.mongodb.com  to prevent\n  from becoming inoperable if the application database\nrestarts. The   requires that: Your   certificate is named  mms-ca.crt  in the\nConfigMap. Your application database certficate is named  ca-pem  in\nthe ConfigMap. Obtain the entire   certificate chain for both   and\nthe application database from\n downloads.mongodb.com . The following  openssl  command\noutputs each certificate in the chain to your current working\ndirectory, in  .crt  format: Concatenate your  's certificate file for  \nwith the entire   certificate chain from\n downloads.mongodb.com  that\nyou obtained in the previous step: Concatenate your  's certificate file for the application\ndatabase with the entire   certificate chain from\n downloads.mongodb.com  that\nyou obtained in the previous step: Create the   for  : Create the   for the application database: Change the highlighted settings to match your desired\n  and application database configuration. Key Type Description Example metadata.name string Name for this      . Resource names must be 44 characters or less. metadata.name  documentation on  names . om spec.replicas number Number of   instances to run in parallel. The minimum valid value is  1 . For high availability, set this value to more than  1 .\n Multiple Ops Manager instances \ncan read from the same Application Database, ensuring failover if\none instance is unavailable and enabling you to update the\n  resource without downtime. 1 spec.version string Version of   to be installed. The format should be  X.Y.Z .\nTo view available   versions, view the\n container registry . \u200b spec.adminCredentials string Name of the   you  created \nfor the   admin user. Configure the secret to use the same   as the\n  resource. om-admin-secret string Required . Text to prefix to the name of the secret that contains\n s   certificates. om-prod string Name of the   you created to verify your\n   \ncertificates signed using a Custom Certificate Authority. This field is required if you signed your\n   \ncertificates using a Custom Certificate Authority. om-http-cert-ca string The   service  ServiceType \nthat exposes   outside of  . Exclude the\n spec. externalConnectivity  setting\nand its children if you don't want the   to\ncreate a   service to route external traffic to the\n  application. LoadBalancer integer Number of members of the  mms-application-database \nreplica set. 3 string Required . Version of MongoDB that the  mms-application-database \nshould run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the  Enterprise edition . To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. For best results, use the\n latest available enterprise MongoDB version \nthat is  compatible  with your\n  version. string Required . Text to prefix to the name of the secret that contains\nthe application database's   certificates. appdb-prod string Name of the   you created to verify your\napplication database  \ncertificates signed using a Custom Certificate Authority. This field is required if you signed your\napplication database  \ncertificates using a Custom Certificate Authority. ca The   mounts the   you add using the\n spec.applicationDatabase.security.tls.ca  setting to\nboth the   and the Application Database pods. If you want to enable backup for your   instance, you must\nconfigure all of the following settings: You must also configure an  S3 snapshot store \nor a  blockstore . To configure a snapshot store, configure the following settings: To configure a blockstore, configure the following settings: Key Type Description Example boolean Flag that indicates that Backup is enabled for your You must\nspecify  spec.backup.enabled: true  to configure settings\nfor the head database, oplog store, and snapshot store. true string Name of the oplog store. oplog1 string Name of the MongoDB database resource for the oplog store. my-oplog-db If you deploy both an  S3 snapshot store \nand a  blockstore ,  \nrandomly choses one to use for Backup. Key Type Description Example string Name of the   snapshot store. s3store1 string Name of the   that contains the  accessKey  and\n secretKey  fields. The  backup-daemon  uses the\nvalues of these fields as credentials to access the   or\n -compatible bucket. my-s3-credentials string  of the   or  -compatible bucket that\n stores  the\ndatabase Backup snapshots. s3.us-east-1.amazonaws.com string Name of the   or  -compatible bucket that stores the\ndatabase Backup snapshots. my-bucket string Region where your  -compatible bucket resides. Use this\nfield only if your   store's\n s3BucketEndpoint \ndoesn't include a region in its  . Don't use this field with     buckets. us-east-1 Key Type Description Example string Name of the blockstore. blockStore1 string Name of the MongoDB database resource that you create for the\nblockstore. You must deploy this database resource in the same\nnamespace as the   resource. my-mongodb-blockstore Add any  optional settings  that you\nwant to apply to your deployment to the   specification file. Invoke the following  kubectl  command on the filename of the\n  resource definition: To check the status of your   resource, invoke the following\ncommand: The command returns the following output under the  status  field\nwhile the resource deploys: The   reconciles the resources in the following order: The   doesn't reconcile a resource until the preceding\none enters the  Running  phase. After the   resource completes the  Reconciling  phase, the\ncommand returns the following output under the  status  field if you\nenabled backup: Backup remains in a  Pending  state until you configure the Backup\ndatabases. After the resource completes the  Reconciling  phase, the command\nreturns the following output under the  status  field: Backup remains in a  Pending  state until you configure the Backup\ndatabases. Application Database. . Backup. The  status.opsManager.url  field states the resource's\nconnection  . Using this  , you can reach   from\ninside the   cluster or  create a project using a\nConfigMap . The  status.opsManager.url  field states the resource's\nconnection  . Using this  , you can reach   from\ninside the   cluster or  create a project using a\nConfigMap . The steps you take differ based on how you are routing traffic to the\n  application in  . If you configured the   to\ncreate a   service for you, or you created a   service\nmanually, use one of the following methods to access the  \napplication: To learn how to access the   application using a third-party\nservice, refer to the documentation for your solution. Query your cloud provider to get the   of the load\nbalancer service. See your cloud provider's documentation\nfor details. Open a browser window and navigate to the  \napplication using the   and port number of your load\nbalancer service. Log in to   using the  admin user credentials . Set your firewall rules to allow access from the Internet to\nthe  spec.externalConnectivity. port \non the host on which your   cluster is running. Open a browser window and navigate to the  \napplication using the   and the\n spec.externalConnectivity. port . Log in to   using the  admin user credentials . To configure credentials, you must create an   organization,\ngenerate programmatic API keys, and create a  . These\nactivities follow the prerequisites and procedure on the\n Create Credentials for the   page. To create a project, follow the prerequisites and procedure on the\n Create One Project using a ConfigMap  page. Set the following fields in your project ConfigMap: Set  data.baseUrl  in the ConfigMap to the  's  .\nTo find this  , invoke the following command: The command returns the URL of the   in the\n status.opsManager.url  field. If you deploy   with the   and   will\nmanage MongoDB database resources deployed  outside  of the  \ncluster it's deployed to, you must set  data.baseUrl  to the same\nvalue of the\n spec.configuration.mms.centralUrl \nsetting in the   resource specification. Set  data.sslMMSCAConfigMap  to the name of your\n  containing the root  CA (Certificate\nAuthority)  certificate used to sign the   host's\ncertificate. The   requires this name to be\n mms-ca.crt . By default,   enables  mms-backup-functional-overview .\nCreate a MongoDB database resource for the oplog and snapshot stores\nto complete the configuration. Deploy a  MongoDB database resource  for the oplog store in the same\nnamespace as the   resource. Match the  metadata.name  of the resource with the\n spec.backup.opLogStores.mongodbResourceRef.name \nthat you specified in your   resource definition. Create this database as a three-member  replica set . Deploy a  MongoDB database resource  for the   snapshot store in the\nsame namespace as the   resource. Match the  metadata.name  of the resource to the\n spec.backup.s3Stores.mongodbResourceRef.name \nthat you specified in your   resource definition. Create the   snapshot store as a replica set. To check the status of your   resource, invoke the following\ncommand: When   is running, the command returns the following\noutput under the  status  field: See  Troubleshoot the   for information about the\nresource deployment statuses. Follow these steps to deploy the   resource to run over\n : If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the highlighted settings to match your desired\n  configuration. Key Type Description Example metadata.name string Name for this      . Resource names must be 44 characters or less. metadata.name  documentation on  names . om spec.replicas number Number of   instances to run in parallel. The minimum valid value is  1 . For high availability, set this value to more than  1 .\n Multiple Ops Manager instances \ncan read from the same Application Database, ensuring failover if\none instance is unavailable and enabling you to update the\n  resource without downtime. 1 spec.version string Version of   to be installed. The format should be  X.Y.Z .\nFor the list of available   versions, view the\n container registry . \u200b spec.adminCredentials string Name of the secret you  created \nfor the   admin user. Configure the secret to use the same   as the\n  resource. om-admin-secret string Optional . The   service  ServiceType \nthat exposes   outside of  . Exclude the\n spec. externalConnectivity  setting\nand its children if you don't want the   to\ncreate a   service to route external traffic to the\n  application. LoadBalancer integer Number of members of the  mms-application-database \nreplica set. 3 string Required . Version of MongoDB that the  mms-application-database \nshould run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the  Enterprise edition . To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. For best results, use the\n latest available enterprise MongoDB version \nthat is  compatible  with your\n  version. If you want to enable backup, you must configure all of the following\nsettings: You must also configure an  S3 snapshot store \nor a  blockstore . To configure a snapshot store, configure the following settings: To configure a blockstore, configure the following settings: Key Type Description Example boolean Flag that indicates that Backup is enabled. You must specify\n spec.backup.enabled: true  to configure settings\nfor the head database, oplog store, and snapshot store. true string Name of the oplog store. oplog1 string Name of the MongoDB database resource for the oplog store. my-oplog-db If you deploy both an    snapshot store \nand a  blockstore ,  \nrandomly choses one to use for Backup. Key Type Description Example string Name of the   snapshot store. s3store1 string Name of the secret that contains the  accessKey  and\n secretKey  fields. The  backup-daemon  uses the\nvalues of these fields as credentials to access the   or\n -compatible bucket. my-s3-credentials string  of the   or  -compatible bucket that\n stores  the\ndatabase Backup snapshots. s3.us-east-1.amazonaws.com string Name of the   or  -compatible bucket that stores the\ndatabase Backup snapshots. my-bucket string Region where your  -compatible bucket resides. Use this\nfield only if your   store's\n s3BucketEndpoint \ndoesn't include a region in its  . Don't use this field with     buckets. us-east-1 Key Type Description Example string Name of the blockstore. blockStore1 string Name of the MongoDB database resource that you create for the\nblockstore. You must deploy this database resource in the same\nnamespace as the   resource. my-mongodb-blockstore Add any  optional settings  that you\nwant to apply to your deployment to the   specification file. Invoke the following  kubectl  command on the filename of the\n  resource definition: To check the status of your   resource, invoke the following\ncommand: The command returns the following output under the  status  field\nwhile the resource deploys: The   reconciles the resources in the following order: The   doesn't reconcile a resource until the preceding\none enters the  Running  phase. After the   resource completes the  Reconciling  phase, the\ncommand returns the following output under the  status  field if you\nenabled backup: Backup remains in a  Pending  state until you configure the Backup\ndatabases. Application Database. . Backup. The  status.opsManager.url  field states the resource's\nconnection  . Using this  , you can reach   from\ninside the   cluster or  create a project using a\nConfigMap . The steps you take differ based on how you are routing traffic to the\n  application in  . If you configured the   to\ncreate a   service for you, or you created a   service\nmanually, use one of the following methods to access the  \napplication: To learn how to access the   application using a third-party\nservice, refer to the documentation for your solution. Query your cloud provider to get the   of the load\nbalancer service. See your cloud provider's documentation\nfor details. Open a browser window and navigate to the  \napplication using the   and port number of your load\nbalancer service. Log in to   using the  admin user credentials . Set your firewall rules to allow access from the Internet to\nthe  spec.externalConnectivity. port \non the host on which your   cluster is running. Open a browser window and navigate to the  \napplication using the   and the\n spec.externalConnectivity. port . Log in to   using the  admin user credentials . If you enabled Backup, you must create an   organization,\ngenerate programmatic API keys, and create a secret in your  secret-storage-tool .\nThese activities follow the prerequisites and procedure on the\n Create Credentials for the   page. If you enabled Backup, create a project by following the prerequisites\nand procedure on the  Create One Project using a ConfigMap  page. You must set  data.baseUrl  in the ConfigMap to the  's  . To find this  , invoke the following command: The command returns the URL of the   in the\n status.opsManager.url  field. If you deploy   with the   and   will\nmanage MongoDB database resources deployed  outside  of the  \ncluster it's deployed to, you must set  data.baseUrl  to the same\nvalue of the\n spec.configuration.mms.centralUrl \nsetting in the   resource specification. If you enabled  mms-backup-functional-overview ,\ncreate a MongoDB database resource for the oplog and snapshot stores\nto complete the configuration. Deploy a  MongoDB database resource  for the oplog store in the same\nnamespace as the   resource. Match the  metadata.name  of the resource with the\n spec.backup.opLogStores.mongodbResourceRef.name \nthat you specified in your   resource definition. Create this database as a  replica set . Choose one of the following: Deploy a  MongoDB database resource  for the blockstore in the\nsame namespace as the   resource. Match the  metadata.name  of the resource to the\n spec.backup.blockStores.mongodbResourceRef.name \nthat you specified in your   resource definition. Configure an   bucket to use as the   snapshot store. Ensure that you can access the   bucket using the details\nthat you specified in your   resource definition. If you enabled backup, check the status of your   resource by\ninvoking the following command: When   is running, the command returns the following\noutput under the  status  field: See  Troubleshoot the   for information about the\nresource deployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-cert \\\n  --cert=<om-tls-cert> \\\n  --key=<om-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-db-cert \\\n  --cert=<appdb-tls-cert> \\\n  --key=<appdb-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "openssl s_client -showcerts -verify 2 \\\n-connect downloads.mongodb.com:443 -servername downloads.mongodb.com < /dev/null \\\n| awk '/BEGIN/,/END/{ if(/BEGIN/){a++}; out=\"cert\"a\".crt\"; print >out}'"
                },
                {
                    "lang": "sh",
                    "value": "cat cert1.crt cert2.crt cert3.crt cert4.crt  >> mms-ca.crt"
                },
                {
                    "lang": "sh",
                    "value": "cat cert1.crt cert2.crt cert3.crt cert4.crt  >> ca-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap om-http-cert-ca --from-file=\"mms-ca.crt\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap ca --from-file=\"ca-pem\""
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: <myopsmanager>\nspec:\n  replicas: 1\n  version: <opsmanagerversion>\n  adminCredentials: <adminusercredentials> # Should match metadata.name\n                                           # in the Kubernetes secret\n                                           # for the admin user\n\n  externalConnectivity:\n    type: LoadBalancer\n  security:\n      certsSecretPrefix: <prefix> # Required. Text to prefix \n                                  # the name of the secret that contains\n                                  # Ops Manager's TLS certificate.\n      tls:\n        ca: \"om-http-cert-ca\"  # Optional. Name of the ConfigMap file\n                               # containing the certificate authority that\n                               # signs the certificates used by the Ops\n                               # Manager custom resource.\n\n  applicationDatabase:\n    members: 3\n    version: \"4.4.0-ent\"\n    security:\n      certsSecretPrefix: <prefix> # Required. Text to prefix to the \n                                  # name of the secret that contains the Application\n                                  # Database's TLS certificate. Name the secret \n                                  # <prefix>-<metadata.name>-db-cert.\n      tls:\n        ca: \"appdb-ca\" # Optional. Name of the ConfigMap file\n                       # containing the certicate authority that\n                       # signs the certificates used by the\n                       # application database.\n\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n applicationDatabase:\n  lastTransition: \"2020-04-01T09:49:22Z\"\n  message: AppDB Statefulset is not ready yet\n  phase: Reconciling\n  type: \"\"\n  version: \"\"\n backup:\n  phase: \"\"\n opsManager:\n  phase: \"\""
                },
                {
                    "lang": "yaml",
                    "value": " status:\n   applicationDatabase:\n     lastTransition: \"2020-04-01T09:50:20Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: \"4.2.0\"\n  backup:\n   lastTransition: \"2020-04-01T09:57:42Z\"\n   message: The MongoDB object <namespace>/<oplogresourcename>\n     doesn't exist\n   phase: Pending\n   opsManager:\n     lastTransition: \"2020-04-01T09:57:40Z\"\n     phase: Running\n     replicas: 1\n     url: https://om-svc.cloudqa.svc.cluster.local:8443\n     version: \"5.0.0\""
                },
                {
                    "lang": "yaml",
                    "value": " status:\n   applicationDatabase:\n     lastTransition: \"2019-12-06T18:23:22Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: \"4.2.11-ent\"\n   opsManager:\n     lastTransition: \"2019-12-06T18:23:26Z\"\n     message: The MongoDB object namespace/oplogdbname doesn't exist\n     phase: Pending\n     url: https://om-svc.dev.svc.cluster.local:8443\n     version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "https://ops.example.com:8443"
                },
                {
                    "lang": "sh",
                    "value": "https://ops.example.com:30036"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "  status:\n    applicationDatabase:\n      lastTransition: \"2019-12-06T18:23:22Z\"\n      members: 3\n      phase: Running\n      type: ReplicaSet\n      version: \"4.2.11-ent\"\n    opsManager:\n      lastTransition: \"2019-12-06T18:23:26Z\"\n      message: The MongoDB object namespace/oplogdbname doesn't exist\n      phase: Pending\n      url: https://om-svc.dev.svc.cluster.local:8443\n      version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2019-12-06T17:46:15Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: \"4.2.11-ent\"\n  opsManager:\n    lastTransition: \"2019-12-06T17:46:32Z\"\n    phase: Running\n    replicas: 1\n    url: https://om-backup-svc.dev.svc.cluster.local:8443\n    version: \"5.0.0\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: <myopsmanager>\nspec:\n  replicas: 1\n  version: <opsmanagerversion>\n  adminCredentials: <adminusercredentials> # Should match metadata.name\n                                           # in the secret\n                                           # for the admin user\n  externalConnectivity:\n    type: LoadBalancer\n\n  applicationDatabase:\n    members: 3\n    version: <mongodbversion>\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n applicationDatabase:\n  lastTransition: \"2020-04-01T09:49:22Z\"\n  message: AppDB Statefulset is not ready yet\n  phase: Reconciling\n  type: \"\"\n  version: \"\"\n backup:\n  phase: \"\"\n opsManager:\n  phase: \"\""
                },
                {
                    "lang": "yaml",
                    "value": " status:\n   applicationDatabase:\n     lastTransition: \"2020-04-01T09:50:20Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: \"4.2.11-ent\"\n  backup:\n   lastTransition: \"2020-04-01T09:57:42Z\"\n   message: The MongoDB object <namespace>/<oplogresourcename>\n     doesn't exist\n   phase: Pending\n   opsManager:\n     lastTransition: \"2020-04-01T09:57:40Z\"\n     phase: Running\n     replicas: 1\n     url: http://om-svc.cloudqa.svc.cluster.local:8080\n     version: \"5.0.0\""
                },
                {
                    "lang": "sh",
                    "value": "http://ops.example.com:8080"
                },
                {
                    "lang": "sh",
                    "value": "http://ops.example.com:30036"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": " status:\n   applicationDatabase:\n     lastTransition: \"2020-04-01T10:00:32Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: \"4.2.11-ent\"\n  backup:\n   lastTransition: \"2020-04-01T09:57:42Z\"\n   message: The MongoDB object <namespace>/<oplogresourcename>\n     doesn't exist\n   phase: Pending\n   opsManager:\n     lastTransition: \"2020-04-01T09:57:40Z\"\n     phase: Running\n     replicas: 1\n     url: http://om-svc.cloudqa.svc.cluster.local:8080\n     version: \"5.0.0\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2020-04-01T10:00:32Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: \"4.2.11-ent\"\n  backup:\n    lastTransition: \"2020-04-01T10:00:53Z\"\n    phase: Running\n    version: \"4.2.8\"\n  opsManager:\n    lastTransition: \"2020-04-01T10:00:34Z\"\n    phase: Running\n    replicas: 1\n    url: http://om-svc.cloudqa.svc.cluster.local:8080\n    version: \"5.0.0\""
                }
            ],
            "preview": "You can deploy  in a container with the .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/upgrade-k8s-operator",
            "title": "Upgrade the ",
            "headings": [
                "Upgrade using ",
                "Upgrade the  for MongoDB deployments.",
                "Optional: Customize the   before upgrading it.",
                "If you are upgrading the Operator to version 1.13.0 or later, specify the spec.opsManager.configMapRef.name or the spec.cloudManager.configMapRef.name settings:",
                "Upgrade to the new version of the .",
                "If you are upgrading the Operator to version 1.13.0 or later, specify the spec.opsManager.configMapRef.name or the spec.cloudManager.configMapRef.name settings:",
                "Optional: Customize your Helm Chart before upgrading it.",
                "Upgrade the .",
                "If you are upgrading the Operator to version 1.13.0 or later, specify the spec.opsManager.configMapRef.name or the spec.cloudManager.configMapRef.name settings:",
                "Optional: Customize your Helm Chart before upgrading it.",
                "Upgrade to the latest version of the .",
                "Upgrade using OpenShift",
                "Upgrade the  for MongoDB deployments.",
                "Optional: Customize the   before upgrading it.",
                "If you are upgrading the Operator to version 1.13.0 or later, specify the spec.opsManager.configMapRef.name or the spec.cloudManager.configMapRef.name settings:",
                "Upgrade to the new version of the .",
                "If you are upgrading the Operator to version 1.13.0 or later, specify the spec.opsManager.configMapRef.name or the spec.cloudManager.configMapRef.name settings:",
                "Optional: Customize your Helm Chart before upgrading it.",
                "Upgrade the .",
                "If you are upgrading the Operator to version 1.13.0 or later, specify the spec.opsManager.configMapRef.name or the spec.cloudManager.configMapRef.name settings:",
                "Optional: Customize your Helm Chart before upgrading it.",
                "Upgrade to the latest version of the ."
            ],
            "paragraphs": "The following procedure outlines how to upgrade the  \nto its latest version.  version  1.15.1  fixes an issue\nthat prevented the   upgrade when managing a\nTLS-enabled Application Database whose TLS certificate is stored in an\n Opaque \nsecret. We recommend that you upgrade to   version 1.15.1\nor later. We strongly advise against upgrading to   version 1.14.0\nor 1.15.0. If you have a broken Application Database after upgrading to\n  version 1.14.0 or 1.15.0, see\n  in Failed State . The following steps depend on how your environment is configured: Invoke the following   command: To learn about optional   installation settings,\nsee  Operator kubectl and oc Installation Settings . To learn more, see  Changes to the MongoDB Resource . Open the  MongoDB Database Resource Specification  in the editor of your choice. Add the value to the  spec.opsManager.configMapRef.name \nsetting or the\n spec.cloudManager.configMapRef.name  setting and save\nthe specification. Invoke the following   command: To learn more, see  Changes to the MongoDB Resource . Open the  MongoDB Database Resource Specification  in the editor of your choice. Add the value to the  spec.opsManager.configMapRef.name \nsetting or the\n spec.cloudManager.configMapRef.name  setting and save\nthe specification. To learn about optional   installation settings, see\n Operator Helm Installation Settings . Invoke the following  helm  command: To upgrade the   on a host not connected to the\nInternet: To learn more, see  Changes to the MongoDB Resource . Open the  MongoDB Database Resource Specification  in the editor of your choice. Add the value to the  spec.opsManager.configMapRef.name \nsetting or the\n spec.cloudManager.configMapRef.name  setting and save\nthe specification. To learn about optional   installation settings, see\n Operator Helm Installation Settings . Invoke the following  helm upgrade  command.\nUse the  registry.pullPolicy=IfNotPresent  setting. To learn\nabout optional   installation settings, see\n Operator Helm Installation Settings . The following steps depend on how your environment is configured: Invoke the following   command: To learn about optional   installation settings,\nsee  Operator kubectl and oc Installation Settings . To learn more, see  Changes to the MongoDB Resource . Open the  MongoDB Database Resource Specification  in the editor of your choice. Add the value to the  spec.opsManager.configMapRef.name \nsetting or the\n spec.cloudManager.configMapRef.name  setting and save\nthe specification. Invoke the following   command: To learn more, see  Changes to the MongoDB Resource . Open the  MongoDB Database Resource Specification  in the editor of your choice. Add the value to the  spec.opsManager.configMapRef.name \nsetting or the\n spec.cloudManager.configMapRef.name  setting and save\nthe specification. To learn about optional   installation settings, see\n Operator Helm Installation Settings . Invoke the following  helm upgrade  command.\nUse  settings. To learn\nabout optional   installation settings, see\n Operator Helm Installation Settings . To upgrade the   on a host not connected to the\nInternet: To learn more, see  Changes to the MongoDB Resource . Open the  MongoDB Database Resource Specification  in the editor of your choice. Add the value to the  spec.opsManager.configMapRef.name \nsetting or the\n spec.cloudManager.configMapRef.name  setting and save\nthe specification. To learn about optional   installation settings, see\n Operator Helm Installation Settings . Invoke the following  helm upgrade  command: Use the \nsettings,  registry.pullPolicy=IfNotPresent , and\n registry.imagePullSecrets=<openshift-pull-secret> . To learn\nabout optional   installation settings, see\n Operator Helm Installation Settings . To troubleshoot your  , see  Review Logs from the  \nand other  troubleshooting topics . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl replace -f crds.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f mongodb-enterprise.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade enterprise-operator mongodb/enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade enterprise-operator mongodb/enterprise-operator \\\n  --set registry.pullPolicy='IfNotPresent'"
                },
                {
                    "lang": "sh",
                    "value": "oc replace -f crds.yaml"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f mongodb-enterprise-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade enterprise-operator mongodb/enterprise-operator \\\n  --values https://raw.githubusercontent.com/mongodb/helm-charts/main/charts/enterprise-operator/values-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade enterprise-operator mongodb/enterprise-operator \\\n    --set registry.pullPolicy='IfNotPresent' \\\n    --set registry.imagePullSecrets='<openshift-pull-secret>' \\\n    --values https://raw.githubusercontent.com/mongodb/helm-charts/main/charts/enterprise-operator/values-openshift.yaml"
                }
            ],
            "preview": "The following procedure outlines how to upgrade the \nto its latest version.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-op-compatibility",
            "title": " Compatibility",
            "headings": [
                " and OpenShift Versions",
                "Other Distributions of ",
                "Supported Platforms and MongoDB Versions",
                " and  Versions"
            ],
            "paragraphs": "The   is compatible with the following   and OpenShift\nversions. Unless otherwise noted, each   version listed\nspans the full release series starting from the listed version.  Release Series  Version OpenShift Version 1.17.x 1.21, 1.22, 1.23 4.6 EUS, 4.8-4.10 1.16.1+ 1.21, 1.22, 1.23 4.6 EUS, 4.8-4.10 1.16.0 1.21, 1.22, 1.23 4.9 1.15.x 1.21, 1.22, 1.23 4.9 1.14.x 1.19, 1.20, 1.21, 1.22 4.7, 4.8, 4.9 The   is also compatible with all\n CNCF-certified distributions \nof   and core   features. The distribution version\nmust match one of the base   versions supported by the  . To learn which   versions are supported by your   series,\nsee the preceding  version table . The   is compatible with different versions of MongoDB\ndepending on the base image of the MongoDB database resource. The   is compatible with actively supported versions of\nMongoDB. The   isn't compatible with MongoDB versions that\nhave reached end of life, or that are listed on the  MongoDB Alerts  page. To learn more about\nwhich MongoDB versions your base image supports, see\n Platform Support \nin the MongoDB Manual. Unless otherwise noted, each   version listed spans the\nfull release series starting from the listed version. Starting with the   1.17\nrelease, Ubuntu base images\nare deprecated and will be removed in the   1.19 release. MongoDB will continue to support the Red Hat UBI 8 base images. To provide feedback about this change, use the  MongoDB Feedback Engine .  Release Series Base Image 1.17.x Ubuntu 20.04 (deprecated), Red Hat UBI 8 Base Image 1.16.x Ubuntu 20.04, Red Hat UBI 8 Base Image 1.15.x Ubuntu 20.04, Red Hat UBI 8 Base Image 1.14.x Ubuntu 20.04, Red Hat UBI 8 Base Image The   is compatible with   and with the\nfollowing   versions. Unless otherwise noted, each  \nversion listed spans the full release series starting from the listed\nversion.  Release Series  Version 1.17.x 5.0, 6.0 1.16.x 5.0 1.15.x 5.0 1.14.x 5.0",
            "code": [],
            "preview": "The  is compatible with the following  and OpenShift\nversions. Unless otherwise noted, each  version listed\nspans the full release series starting from the listed version.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/create-vault-secret",
            "title": "Create Secrets in ",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Obtain the  public and private Keys.",
                "Create the secret in .",
                "Verify the  secret creation was successful."
            ],
            "paragraphs": "After you  set your secret storage tool  to\n , you must also create secrets in  . This applies\nwhen you're manually migrating your existing    \nor you're creating secrets for the first time. For a list of secrets that you must manually migrate to  , see the\n  section of  Configure Secret Storage . The following tutorial stores your  Programmatic API Key  in  . You can adapt the commands in this\nprocedure to add other secrets to   by changing the base path, the\nnamespace, and the secret name. To learn more about  , see  Configure Secret Storage . To create credentials for the   in  , you must: Have or create an  \n Organization . Have or generate a\n Programmatic API Key . Grant this new   the  Project Owner  role. Add the   or   block of any hosts that serve the\n  to the\n API Access List . Set up a   instance and  enable Vault . Ensure that   is  not  running in  dev mode \nand that your   installation follows any applicable\n configuration recommendations . To create your secret in  : Make sure you have the public and private keys for your desired\n   . Invoke the following   command to create your secret, replacing\nthe variables with the values in the table: The path in this command is the default path. You can replace  mongodbenterprise/operator  with\nyour base path if you customized your   configuration. Placeholder Description {Namespace} Label that identifies the namespace where you deployed  . {SecretName} Human-readable label that identifies the secret you're creating in  . {PublicKey} The public key for your desired    . {PrivateKey} The private key for your desired    . Invoke the following   command to verify your secret, replacing\nthe variables with the values in the following table: This command returns a secret description in the shell: Placeholder Description {Namespace} Label that identifies the namespace where you deployed  . {SecretName} Human-readable label that identifies the secret you're creating in  .",
            "code": [
                {
                    "lang": "sh",
                    "value": "vault kv put secret/data/mongodbenterprise/operator/{Namespace}/{SecretName} publicKey={PublicKey} privateKey={PrivateKey}"
                },
                {
                    "lang": "sh",
                    "value": "vault kv get secret/data/mongodbenterprise/operator/{Namespace}/{SecretName}"
                },
                {
                    "lang": "sh",
                    "value": "====== Metadata ======\nKey              Value\n---              -----\ncreated_time     2021-12-15T17:20:22.985303Z\ndeletion_time    n/a\ndestroyed        false\nversion          1\n\n======= Data =======\nKey          Value\n---          -----\npublicKey    {PublicKey}\nprivateKey   {PrivateKey}"
                }
            ],
            "preview": "After you set your secret storage tool to\n, you must also create secrets in . This applies\nwhen you're manually migrating your existing  \nor you're creating secrets for the first time.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/connect-from-inside-k8s",
            "title": "Connect to a MongoDB Database Resource from Inside Kubernetes",
            "headings": [
                "Considerations",
                "Procedure",
                "Open the Topology view for your deployment.",
                "Click  for the deployment to which you want to connect.",
                "Click Connect to this instance.",
                "Copy the connection command displayed in the Connect to your Deployment dialog.",
                "Run the connection command in a terminal to connect to the deployment.",
                "Run the command to view the Kubernetes secret file.",
                "Copy the connectionString.standard value displayed in the Kubernetes secret file.",
                "Run the connection command.",
                "(Optional) Mount the Kubernetes secret in your pod."
            ],
            "paragraphs": "The following procedure describes how to connect to a MongoDB resource\ndeployed by   from inside of the   cluster. You must be able to connect to the host and port where you deployed your\n  resource. To learn more about connecting to your deployment, see\n Connect to a MongoDB Process . Retrieve and run the connection command for your deployment. You can retrieve\nthe connection command from the   or\n Cloud Manager \napplication, depending on where your clusters are hosted. You can also retrieve the connection command from\nthe     that the   creates automatically when you\n add a MongoDB user with SCRAM authentication  or X509. The procedure for connecting to a MongoDB Database resource varies based\non how you want to retrieve your connection string: Perform the following steps in the   or  Cloud Manager \napplication, depending on where your clusters are hosted: When connecting to a resource from inside of  , the\nhostname to which you connect has the following form: Click  Deployment  in the left navigation. To connect to a sharded cluster resource named\n shardedcluster , you might use the following connection\nstring: Perform the following steps to view the credentials and\nuse the connection string to connect to MongoDB: When you create a new MongoDB database user,   automatically\ncreates a new    . The    \ncontains the following information about the new database user: username : Username for the database user password : Password for the database user connectionString.standard :  Standard connection string \nthat can connect you to the database as this database user. connectionString.standardSrv :  DNS seed list connection string  that can\nconnect you to the database as this database user. Alternatively, you can specify an optional\n spec.connectionStringSecretName  field in the\n MongoDBUser  custom resource to specify\nthe name of the connection string secret that the\n  creates. Run the following command in a terminal to view the  , replacing\nthe variables with the values in the table: If this command returns an error, you can verify the name of the   by\nrunning the following command and retrieving the correct name: Placeholder Description {MongoDB-Resource-Name} Human-readable label that identifies the MongoDB resource. {User-Name} Human-readable label that identifies the MongoDB user. Use the  connectionString.standard  value within a  connection string \nto connect to the deployment. You can  mount the secret in your pod \nto ensure that your applications can access the credentials.",
            "code": [
                {
                    "lang": "sh",
                    "value": "<k8s-pod-name>.<k8s-internal-service-name>.<k8s-namespace>.<cluster-name>"
                },
                {
                    "lang": "none",
                    "value": "mongosh --host shardedcluster-mongos-0.shardedcluster-svc.mongodb.svc.cluster.local \\\n  --port 27017"
                },
                {
                    "lang": "none",
                    "value": "kubectl get secret {MongoDB-Resource-Name}-{User-Name}-admin -o jsonpath='{.data}'"
                },
                {
                    "lang": "none",
                    "value": "kubectl get secrets"
                },
                {
                    "lang": "none",
                    "value": "mongosh {connectionString.standard}"
                }
            ],
            "preview": "The following procedure describes how to connect to a MongoDB resource\ndeployed by  from inside of the  cluster.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/configure-om-queryable-backups",
            "title": "Configure Queryable Backups for  Resources",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Create the PEM file for backups.",
                "Create a secret containing the PEM file.",
                "Configure  custom resource to use the secret.",
                "Save your  config file.",
                "Apply changes to your  deployment.",
                "Track the status of the mounted volumes and Secrets."
            ],
            "paragraphs": "You can configure  queryable backups \nfor   resources that you deploy in the  . Queryable backups allow you to  run queries \non specific backup snapsnots from your   resources. Querying  \nbackups helps you compare data from different snapshots and identify the\nbest snapshot to use for  restoring data . In the following procedure you: Once the   deploys the updated configuration for its custom\nresource,   can read the secret from the  spec.backup.queryableBackupSecretRef.name \nparameter. You can now access the backup snapshots and run queries on them. In the   documentation, queryable backups are also\nreferred to as queryable snapshots, or queryable restores. Create the  queryable.pem \nfile that holds the certificates for accessing the backup snapshots that you intend to query. Create the secret containing the  queryable.pem  file. Configure an   custom resource to use the secret for queryable backups. Save the   custom resource configuration and apply it. Before you configure queryable backups, complete the following tasks: Install the Kubernetes Operator . Deploy the Ops Manager application . Configure Backup Settings for the Ops Manager Resource .\nIn the linked procedures, see the steps for configuring backups. After you configure queryable backups, you can  query them \nto select the best backup snapshot to use for  restoring data . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Create the  Ops Manager queryable.pem \nfile that you will use for accessing and querying backups based on\nyour deployment's   requirements. The PEM file contains a public\nkey certificate and its associated private key that are needed to\naccess and run queries on backup snapshots in  . To learn more about the PEM file's requirements, see\n Authorization and Authentication Requirements in Ops Manager . Run the following command to create a secret with the\n queryable.pem \nfile that you created in the previous step: If you're using   as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. To learn about your options for secret\nstorage, see  Configure Secret Storage . Configure  spec.backup.queryableBackupSecretRef.name  to\nreference the  queryable.pem \nsecret. Invoke the following  kubectl  command on the filename of the\n  resource definition: When you apply the changes to your   resource\ndefinition,   updates the   StatefulSet,\ncreates the volumes, and mounts the Secrets. Obtain the list of persistent volume claims: Obtain the Secrets: Check the status of your   resources: The  -w  flag means \"watch\". With the \"watch\" flag set, the\noutput refreshes immediately when the configuration changes until\nthe status phase achieves the  Running  state. To learn more about the resource deployment statuses, see\n Troubleshoot the  .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic queryable-pem --from-file=./queryable.pem"
                },
                {
                    "lang": "yaml",
                    "value": "  apiVersion: mongodb.com/v1\n  kind: MongoDBOpsManager\n  metadata:\n    name: ops-manager\n  spec:\n    replicas: 1\n    version: 6.0.0\n    adminCredentials: ops-manager-admin-secret\n    backup:\n      enabled: true\n      queryableBackupSecretRef:\n        name: om-queryable-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pvc"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get secrets"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om <resource-name> -o yaml -w"
                }
            ],
            "preview": "You can configure queryable backups\nfor  resources that you deploy in the .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/multi-cluster-secure-ldap-auth",
            "title": "Secure Client Authentication with LDAP in Multi-Cluster Deployments",
            "headings": [
                "Considerations",
                "General Prerequisites",
                "Configure LDAP Client Authentication for a Multi-Cluster Replica Set",
                "Update your MongoDBMulti custom resource to enable  authentication.",
                "Verify that the MDB resources are running."
            ],
            "paragraphs": "You can use the   to configure LDAP to authenticate your\nclient applications that connect to your  . This guide\ndescribes how to configure LDAP authentication from client applications\nto your  . MongoDB Enterprise \nsupports: To learn more, see the  LDAP Proxy Authentication \nand  LDAP Authorization  sections\nin the MongoDB Server documentation. Proxying authentication requests to a Lightweight Directory Access\nProtocol (LDAP) service. Simple and SASL binding to LDAP servers. MongoDB Enterprise can bind\nto an LDAP server via  saslauthd  or through the operating system\nlibraries. To configure   in  , use the parameters under the\n spec.security.authentication.ldap  and other\n security LDAP settings  specific to the\nMongoDB Agent, from the   MongoDB resource specification.\nThe procedures in this section describe the required settings and\nprovide examples of   configuration. To improve security, consider deploying a\n TLS-encrypted multi-cluster .\nEncryption with   is optional. By default,   traffic is sent\nas plain text. This means that username and password are exposed to\nnetwork threats. Many modern directory services, such as Microsoft\nActive Directory, require encrypted connections. Consider using\n  over   to encrypt vauthentication requests in your\n  MongoDB deployments. Before you secure your   using  \nencryption, complete the following tasks: Follow the steps in the  Multi-Cluster Quick Start Prerequisites . Deploy a multi-cluster using a  Multi-Cluster Quick Start . Update your MongoDBMulti custom resource \nwith  security settings  from the  \nMongoDB resource specification. To enable   in your deployment, configure the following\nsettings in your   object: The resulting configuration may look similar to the following\nexample: For a full list of LDAP settings, see  security settings  in the   MongoDB resource specification.\nAlso see the  spec.security.authentication.agents.automationUserName \nsetting for the MongoDB Agent user in your LDAP-enabled  \ndeployment. Key Type and necessity Description Example Set to  true  to enable LDAP authentication. true Specify the LDAP Distinguished Name to which MongoDB binds when\nconnecting to the LDAP server. cn=admin,dc=example,dc=org Specify the name of the   that contains the\nLDAP Bind Distinguished Name's password with which MongoDB binds\nwhen connecting to an LDAP server. <secret-name> Add the  's name that stores the custom  \nthat you used to sign your deployment's   certificates. <configmap-name> Add the field name that stores the   which validates the\nLDAP server's   certificate. <configmap-key> Specify the list of  hostname:port  combinations of one or more\nLDAP servers. For each server, use a separate line. <example.com:636> Set to  tls  to use LDAPS (LDAP over  ). Leave blank if\nyour LDAP server doesn't accept TLS. You must enable TLS when you\ndeploy the database resource to use this setting. tls Specify the mapping that maps the username provided to\n mongod  or  mongos  for authentication\nto an LDAP Distinguished Name (DN). To learn more, see  security.ldap.userToDNMapping \nand  LDAP Query Templates  in the\nMongoDB Server documentation. <match: \"(.+)\",substitution: \"uid={0},ou=groups,dc=example,dc=org\"> Set to  LDAP  to enable authentication through LDAP. LDAP For member clusters, run the following commands to verify that\nthe MongoDB Pods are in the running state: In the central cluster, run the following commands to verify that\nthe  MongoDBMulti  custom resource is in the running state:",
            "code": [
                {
                    "lang": "yaml",
                    "value": "security:\n authentication:\n   enabled: true\n   # Enabled LDAP Authentication Mode\n   modes:\n     - \"LDAP\"\n     - \"SCRAM\"\n     # LDAP related configuration\n   ldap:\n   # Specify the hostname:port combination of one or\n   # more LDAP servers\n     servers:\n       - \"ldap1.example.com:636\"\n       - \"ldap2.example.com:636\"\n\n   # Set to \"tls\" to use LDAP over TLS. Leave blank if\n   # the LDAP server doesn't accept TLS. You must enable TLS when\n   # you deploy the multi-cluster resource to use this setting.\n   transportSecurity: \"tls\"\n\n   # If TLS is enabled, add a reference to a ConfigMap that\n   # contains a CA certificate that validates the LDAP server's\n   # TLS certificate.\n   caConfigMapRef:\n     name: \"<configmap-name>\"\n     key: \"<configmap-entry-key>\"\n\n   # Specify the LDAP Distinguished Name to which\n   # MongoDB binds when connecting to the LDAP server\n   bindQueryUser: \"cn=admin,dc=example,dc=org\"\n\n   # Specify the password with which MongoDB binds\n   # when connecting to an LDAP server. This is a\n   # reference to a Secret Kubernetes Object containing\n   # one \"password\" key.\n   bindQueryPasswordSecretRef:\n     name: \"<secret-name>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_1_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_2_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_3_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context=$MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace mongodb \\\n  get mdbm multi-replica-set -o yaml -w"
                }
            ],
            "preview": "You can use the  to configure LDAP to authenticate your\nclient applications that connect to your . This guide\ndescribes how to configure LDAP authentication from client applications\nto your .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/manage-database-users-scram",
            "title": "Manage Database Users Using SCRAM Authentication",
            "headings": [
                "Considerations",
                "Supported SCRAM Implementations",
                "Supported Authentication Mechanisms",
                "Prerequisites",
                "Add a Database User",
                "Create User Secret",
                "Configure kubectl to default to your namespace.",
                "Copy the following example .",
                "Create a new User Secret YAML file.",
                "Change the highlighted lines.",
                "Save the User Secret file with a .yaml extension.",
                "Create MongoDBUser",
                "Copy the following example MongoDBUser.",
                "Create a new MongoDBUser file.",
                "Change the highlighted lines.",
                "Add any additional roles for the user to the MongoDBUser.",
                "Save the MongoDBUser file with a .yaml extension.",
                "Create the user.",
                "View the newly created user in .",
                "Delete a Database User",
                "Change Authentication Mechanism"
            ],
            "paragraphs": "The   supports managing database users using SCRAM\nauthentication on MongoDB deployments. When you specify  SCRAM  as the authentication mechanism, the\nimplementation of SCRAM used depends upon: The version of MongoDB and If the database is the Application Database or another database. MongoDB Version Database SCRAM Implementation 3.6 or earlier Any except Application Database SCRAM-SHA-1 4.0 or later Any except Application Database SCRAM-SHA-256 Any Application Database SCRAM-SHA-1 After enabling SCRAM authentication, you can add SCRAM users using the\n  interface or the MongoDBUser  . The   supports SCRAM, LDAP, and X.509 authentication\nmechanisms in deployments it creates. In an  -created\ndeployment, you cannot use   to: Configure other authentication mechanisms for deployments. Manage users  not  using SCRAM, LDAP, or X.509 authentication. Before managing database users, you must deploy a\n standalone ,\n replica set , or\n sharded cluster . For  , you must deploy replica sets.\nSee  Deploy Multiple Clusters (Beta) . You cannot assign the same database user\nto more than one MongoDB\n standalone ,\n replica set , or\n sharded cluster .\nThis includes database users with\n admin  roles. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : You can choose to use a cleartext password: or you can choose to use a Base64-encoded password: Make sure to copy the desired password configuration. Plaintext\npasswords use  stringData.password  and Base64-encoded\npasswords use  data.password Open your preferred text editor. Paste this User Secret into a new text file. If you're using   as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. To learn about your options for secret\nstorage, see  Configure Secret Storage . Use the following table to guide you through changing the highlighted\nlines in the Secret: Key Type Description Example metadata.name string Name of the database password secret. Resource names must be 44 characters or less. mms-scram-user-1-password stringData.password string Plaintext password for the desired user. Use this option and value  or   data.password . You\ncan't use both. <my-plain-text-password> data.password string Base64-encoded password for the desired user. Use this option and value  or   stringData.password .\nYou can't use both. You must encode your password into Base64 yourself then\npaste the resulting value with this option. There are\ntools for most every platform and multiple web-based\ntools as well. <my-base64-encoded-password> Open your preferred text editor. Paste this MongoDBUser into a new YAML file. Use the following table to guide you through changing the highlighted\nlines in the MongoDBUser YAML file: Key Type Description Example metadata.name string Name of the database user resource. Resource names must be 44 characters or less. mms-scram-user-1 spec.username string Name of the database user. mms-scram-user-1 spec.passwordSecretKeyRef.name string metadata.name  value of the secret that stores the\nuser's password. my-resource spec.mongodbResourceRef.name string Name of the  MongoDB resource \nthis user is associated with. my-resource spec.roles.db string Database on which the  role  can act. admin spec.roles.name string Name of the  role  to grant the database\nuser. The role name can be any\n built-in MongoDB role  or\n custom role  that\nexists in  . readWriteAnyDatabase You may grant additional roles to this user. Invoke the following   command to create your database user: You can use these credentials to\n Connect to a MongoDB Database Resource from Inside Kubernetes . When you create a new MongoDB database user,   automatically\ncreates a new    . The    \ncontains the following information about the new database user: username : Username for the database user password : Password for the database user connectionString.standard :  Standard connection string \nthat can connect you to the database as this database user. connectionString.standardSrv :  DNS seed list connection string  that can\nconnect you to the database as this database user. Alternatively, you can specify an optional\n spec.connectionStringSecretName  field in the\n MongoDBUser  custom resource to specify\nthe name of the connection string secret that the\n  creates. You can view the newly-created user in  : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. To delete a database user, pass the  metadata.name  from the user\nMongoDBUser to the following command: To change your user authenication from SCRAM to X.509: Disable authentication. Under  spec.security.authentication , change  enabled  to\n false . Reapply the user's resource definition. Wait for the MongoDBResource to reach the  running  state. Enable SCRAM authentication. Under  spec.security.authentication , change  enabled  to\n true  and set  modes  to  [\"SCRAM\"] . Reapply the MongoDBUser resource. Wait for the MongoDBResource to reach the  running  state.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: <mms-user-1-password>\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\nstringData:\n  password: <my-plain-text-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key\ndata:\n  password: <base-64-encoded-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: <mms-user-1-password>\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\nstringData:\n  password: <my-plain-text-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key\ndata:\n  password: <base-64-encoded-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: <mms-scram-user-1>\nspec:\n  passwordSecretKeyRef:\n    name: <mms-user-1-password>\n    # Match to metadata.name of the User Secret\n    key: password\n  username: \"<mms-scram-user-1>\"\n  db: \"admin\" #\n  mongodbResourceRef:\n    name: \"<my-replica-set>\"\n    # Match to MongoDB resource using authenticaiton\n  roles:\n    - db: \"admin\"\n      name: \"clusterAdmin\"\n    - db: \"admin\"\n      name: \"userAdminAnyDatabase\"\n    - db: \"admin\"\n      name: \"readWrite\"\n    - db: \"admin\"\n      name: \"userAdminAnyDatabase\"\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mdbu <metadata.name>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  security:\n    authentication:\n      enabled : false"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  security:\n    authentication:\n      enabled : true\n      modes: [\"SCRAM\"]"
                }
            ],
            "preview": "The  supports managing database users using SCRAM\nauthentication on MongoDB deployments.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-x509-auth",
            "title": "Secure Client Authentication with X.509",
            "headings": [
                "General Prerequisites",
                "Configure X.509 Client Authentication for a Replica Set",
                "Prerequisites",
                "Enable X.509 Client Authentication",
                "Copy the sample replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the general X.509 settings for your replica set resource.",
                "Save your replica set config file.",
                "Apply your changes to your replica set deployment.",
                "Track the status of your deployment.",
                "Renew X.509 Certificates for a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Renew the  for your TLS certificates.",
                "Renew the  for your agents' X.509 certificates.",
                "Configure X.509 Client Authentication for a Sharded Cluster",
                "Prerequisites",
                "Enable X.509 Client Authentication",
                "Copy the sample sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the general X.509 settings for your sharded cluster resource.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Track the status of your deployment.",
                "Renew X.509 Certificates for a Sharded Cluster",
                "Configure kubectl to default to your namespace.",
                "Renew the  for your Shards' TLS certificates.",
                "Renew the  for your config server's TLS certificates.",
                "Renew the  for your mongos server's TLS certificates.",
                "Renew the  for your agents' X.509 certificates."
            ],
            "paragraphs": "The   can use X.509 certificates to authenticate your\nclient applications to your MongoDB deployments. This guide instructs you on how to configure X.509 authentication from\nclients to your MongoDB instances. Before you secure your MongoDB deployment using   encryption,\ncomplete the following: Install the Kubernetes Operator Create Credentials for the Kubernetes Operator Enabling X.509 authentication at the project level configures all\nagents to use X.509 client authentication when communicating with\nMongoDB deployments. X.509 client authentication requires one of the following:  4.1.7 or later  4.0.11 or later Before you secure your replica set using X.509,\n deploy a TLS-encrypted replica set . Change the settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Required Set this value to  true  to enable authentication on the\nMongoDB deployment. true array Conditional Set this value to  [\"X509\"] . [\"X509\"] Invoke the following   command to update your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. If you have already created certificates, we recommend that you renew\nthem periodically using the following procedure. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to renew an existing   that\nstores the replica set's certificates: Run this  kubectl  command to renew an existing   that\nstores the agents' X.509 certificates: Before you secure your sharded cluster using X.509,\n deploy a TLS-encrypted sharded cluster . Change the settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Required Set this value to  true  to enable authentication on the\nMongoDB deployment. true array Conditional Set this value to  [\"X509\"] . [\"X509\"] In any directory, invoke the following   command to update and\nrestart your {k8sResource}}: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. If you have already created certificates, we recommend that you renew\nthem periodically using the following procedure. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to renew an existing   that\nstores the sharded cluster shards' certificates: Run this  kubectl  command to renew an existing   that\nstores the sharded cluster config server's certificates: Run this  kubectl  command to renew an existing   that\nstores the sharded cluster   certificates: Run this  kubectl  command to renew an existing   that\nstores the agents' X.509 certificates:",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      ca: <custom-ca>\n    certsSecretPrefix: <prefix>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-cert \\\n  --cert=<replica-set-tls-cert> \\\n  --key=<replica-set-tls-key> \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key> \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      ca: <custom-ca>\n    certsSecretPrefix: <prefix>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-0-cert \\\n  --cert=<shard-0-tls-cert> \\\n  --key=<shard-0-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -\n\nkubectl -n mongodb create secret tls <prefix>-<metadata.name>-1-cert \\\n  --cert=<shard-1-tls-cert> \\\n  --key=<shard-1-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-config-cert \\\n  --cert=<config-tls-cert> \\\n  --key=<config-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-mongos-cert \\\n  --cert=<mongos-tls-cert> \\\n  --key=<mongos-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key> \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                }
            ],
            "preview": "The  can use X.509 certificates to authenticate your\nclient applications to your MongoDB deployments.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-op-architecture",
            "title": " Architecture",
            "headings": [],
            "paragraphs": "The   provides a container image for the MongoDB Agent in  . This\nallows you to manage and deploy MongoDB database clusters with full monitoring,\nbackups, and automation provided by  . The   container serves as a host on which   orchestrates the\ninstallation of   processes and deploys the cluster configuration. As part of deployment, the   creates   for\nthe   StatefulSets. The   container uses   to maintain the cluster state\nbetween restarts. The   architecture consists of: An Ops Manager custom resource . Through this resource, the  \ndeploys   components: the application database, the  \napplication, and the Backup Daemon in the   containers. After the deployment\nis operational, the   components reconcile updates that you make to\nthe MongoDB cluster configuration. To learn more, see   Architecture in  . MongoDB database custom resources . The   deploys the  MongoDB \ndatabase and the  MongoDBUser   . After the deployment is\noperational, these resources reconcile updates that you make to the\nuser or the MongoDB cluster configuration. To learn more, see  MongoDB Database Architecture in  .",
            "code": [],
            "preview": "The  provides a container image for the MongoDB Agent in . This\nallows you to manage and deploy MongoDB database clusters with full monitoring,\nbackups, and automation provided by .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-om-resource",
            "title": "Plan Your Ops Manager Resource",
            "headings": [
                "Architecture",
                "Considerations",
                "Encryption Key",
                "Application Database",
                "Topology",
                "Monitoring",
                "Authentication",
                "Offline Deployments",
                "Streamlined Configuration",
                "Backup",
                "Oplog Store",
                "Blockstore",
                "S3 Snapshot Store",
                "S3 Oplog Store",
                "Disable Backup",
                "Configure  to Run over HTTPS",
                "Ops Manager Application Access",
                "Deploying  in Remote or Local Mode",
                "Managing External MongoDB Deployments",
                "Prerequisites"
            ],
            "paragraphs": "MongoDB   is an enterprise application that manages, backs\nup, and monitors MongoDB deployments. With  , you can scale and\nupgrade MongoDB, optimize queries, perform point-in-time restores,\nreceive performance alerts, and more. To easily manage and maintain\n  and its underlying database, you can use the   to run\n  as a container on  . Before you deploy an   resource, make sure you read the\n considerations  and complete\nthe  prerequisites . For   resource architecture details, see   Architecture in  . The   generates an encryption key to protect sensitive\ninformation in the  mms-application-database . The  \nsaves this key in a   in the same namespace as the  \nresource. The   names the secret\n <om-resource-name>-gen-key . If you remove the   resource, the key remains stored in the\nsecret on the   cluster. If you stored the Application Database in\na   and you create another   resource with the same name,\nthe   reuses the secret. If you create an  \nresource with a different name, then   creates a new\nsecret and Application Database, and the old secret isn't reused. To avoid storing secrets in  , you can migrate all  \nto a  secret storage tool . When you create an instance of   through the  , the\n mms-application-database  is deployed as a  replica set .\nYou can't configure the Application Database as a  standalone \ndatabase or  sharded cluster . If you have concerns about\nperformance or size requirements for the Application Database, contact\n MongoDB Support . The   automatically configures   to monitor the\nApplication Database that backs the  . The  \ncreates a project named  <ops-manager-deployment-name>-db  for you to\nmonitor the Application Database deployment.  monitors the Application Database deployment, but   does\nnot manage it. You cannot change the Application Database's\nconfiguration in the  . The   UI might display warnings in the\n <ops-manager-deployment-name>-db  project stating that the\nagents for the Application Database are out of date. You can safely\nignore these warnings. The   enforces  SCRAM-SHA-256 \n authentication  on\nthe Application Database. The   creates the database user which   uses to\nconnect to the Application Database. This database user has the\nfollowing attributes: You can't modify the   database user's name and roles. You\n create a secret  to set the database user's\npassword. You edit the secret to update the password. If you don't\ncreate a secret or delete an existing secret, the  \ngenerates a password and stores it. To learn about other options for secret\nstorage, see  Configure Secret Storage . Username mongodb-ops-manager Authentication Database admin Roles readWriteAnyDatabase dbAdminAnyDatabase clusterMonitor The   requires that you specify the MongoDB Enterprise version\nfor the  Application Database  image to enable any\ndeployments of   resources, including offline deployments. After you deploy  , you need to configure it. The regular\nprocedure involves setting up   through the\n configuration wizard . If you\nset some essential settings in your object specification before you\ndeploy, you can bypass the configuration wizard. In the  spec.configuration  block of your   object\nspecification, you need to: Add  mms.ignoreInitialUiSetup  and set to\n true . Add the  minimum configuration settings  to\nallow the   instance to start without errors. To disable the   configuration wizard, configure the\nfollowing settings in your  spec.configuration  block: Replace the example values with the values you want your   to\nuse.  enables  mms-backup-functional-overview  by\ndefault. The   deploys a   comprised of\none pod to host the  backup-daemon , and then creates a  \nand   for the Backup Daemon's  head database . The\n  uses the  Ops Manager API  to\nenable the Backup Daemon and configure the head database. To configure Backup, you must create MongoDB database resources for\nthe  oplog store  and for one of the\nfollowing: If you deploy both an  \n snapshot store  and a\n blockstore ,   chooses\none to use for Backup at random. The   resource remains in a  Pending  state until you configure these Backup resources.   snapshot store . blockstore . You must deploy a three-member replica set to store your\n oplog slices . The Oplog database only supports the  SCRAM  authentication mechanism.\nYou cannot enable other authentication mechanisms. If you enable  SCRAM  authentication on the oplog database, you\nmust: Create a MongoDB user resource to connect   to the oplog\ndatabase. Specify the  name \nof the user in the   resource definition. To configure a  blockstore , you\nmust deploy a replica set to store snapshots. To configure an    snapshot store , you\nmust create an     or  -compatible bucket to store your\ndatabase Backup  snapshots . The default configuration stores snapshot metadata in the Application\nDatabase. You can also deploy a replica set to store snapshot metadata,\nthen configure it using the\n spec.backup.s3Stores.mongodbResourceRef.name  settings in\nthe   resource definition. You can update any additional  \n configuration settings \nthat   doesn't manage through the  . To configure an   oplog store, you must create an     or\n -compatible bucket to store your database Backup Oplog. You can configure storing the oplog using the\n spec.backup.s3OpLogStores.mongodbResourceRef.name  setting\nin the   resource definition. To disable backup after you enabled it: Set the        spec.backup.enabled \nsetting to  false . Disable backups  in the\n . Delete the  backup-daemon   : The   and   for the Backup Daemon's  head\ndatabase  are not deleted when you delete the  backup-daemon \n . You can retrieve stored data before you delete\nthese   resources. To learn about reclaiming  , see the\n Kubernetes documentation . You can configure your   instance created through the  \nto run over   instead of  . To configure your   instance to run over  : For detailed instructions, see  Deploy an   Resource . Create a secret that contains the   certificate and private key. Add this secret to the   configuration object. If you have existing deployments, you must restart them manually\nafter enabling  . To avoid restarting your deployments,\nconfigure   before deploying your managed resources. To learn more, see  HTTPS Enabled After Deployment . By default, the   doesn't create a   service to route\ntraffic originating from outside of the   cluster to the  \napplication. To access the   application, you can: The simplest method is configuring the   to create a  \nservice that routes external traffic to the   application. The\n  deployment procedure instructs you to add the following\nsettings to the   specification that configures the\n  to create a service: Configure the   to create a   service. Create a   service manually. MongoDB recommends using a\n LoadBalancer    service if your cloud provider supports it. If you're using OpenShift, use\n routes . Use a third-party service, such as Istio. spec. externalConnectivity spec.externalConnectivity. type You can use the   to configure   to operate in\n Local  or  Remote  mode if your environment prevents granting hosts\nin your   cluster access to the Internet. In these modes, the Backup\nDaemons and managed MongoDB resources download installation archives\nfrom   instead of from the Internet: Configure an   Resource to use Remote Mode :   reads the\ninstallation archives from HTTP endpoints on a web server\nor S3-compatible file store deployed to your   cluster. Configure an   Resource to use Local Mode :   reads the instalation\narchives from a   that you create for the   StatefulSet. When you deploy   with the  ,   can manage\nMongoDB database resources deployed: If   manages MongoDB database resources deployed to different\n  clusters than   or outside of   clusters, you must: To the same   cluster as  . Outside of   clusters. Add the  mms.centralUrl  setting to  spec.configuration  in the\n  resource specification. Set the value to the URL by which   is exposed outside of the\n  cluster: Update the ConfigMaps  referenced by\nall MongoDB database resources inside the   cluster that you\ndeployed with the  . Set  data.baseUrl  to the same value of the\n spec.configuration.mms.centralUrl \nsetting in the   resource specification. This includes the ConfigMaps that the  MongoDB database resources\nfor the oplog and snapshot stores reference . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Install  the   1.6.x or\nlater. Ensure that the host on which you want to deploy   has a\nminimum of five gigabytes of memory. Create a     for an admin user in the same  \nas the   resource. When you deploy the   resource,   creates a user with\nthese credentials and grants it the  Global Owner  role.\nUse these credentials to log in to   for the first time. Once\nyou deploy  , change the password or remove this secret. If you're using   as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. To learn about your options for secret\nstorage, see  Configure Secret Storage . The admin user's password must adhere to the  \n password complexity requirements . ( Optional ) To set the password for the   database user,\ncreate a   in the same   as the   resource. The   creates the database user that   uses to\nconnect to the  mms-application-database . You can set the\npassword for this database user by invoking the following command to\ncreate a secret: If you don't create a secret, then the   automatically\ngenerates a password and stores it internally. To learn more,\nsee  Authentication . If you're using   as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. If you choose to create a secret for the   database user,\nyou must specify the secret's\n name \nin the   resource definition. By default, the\n  looks for the password value in the  password \nkey. If you stored the password value in a different key, you\nmust also specify that\n key \nname in the   resource definition. ( Optional ). To configure Backup to an   snapshot store, create\na   in the same namespace as the   resource. This secret stores your   credentials so that the  \ncan connect   to your     or  -compatible bucket.\nThe secret must contain the following key-value pairs: To create the secret, invoke the following command: To learn more about managing   snapshot storage, see the\n Prerequisites . If you're using   as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. Key Value accessKey Unique identifer of the   user who owns the   or\n -compatible bucket. secretKey Secret key of the   user who owns the   or\n -compatible bucket.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "spec:\n  configuration:\n    mms.ignoreInitialUiSetup: \"true\"\n    automation.versions.source: \"remote\"\n    mms.adminEmailAddr: cloud-manager-support@mongodb.com\n    mms.fromEmailAddr: cloud-manager-support@mongodb.com\n    mms.mail.hostname: email-smtp.us-east-1.amazonaws.com\n    mms.mail.port: \"465\"\n    mms.mail.ssl: \"true\"\n    mms.mail.transport: smtp\n    mms.minimumTLSVersion: TLSv1.2\n    mms.replyToEmailAddr: cloud-manager-support@mongodb.com\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete statefulset <metadata.name> -backup-daemon \\\n -n <metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  configuration:\n    mms.centralUrl: https://a9a8f8566e0094380b5c257746627b82-1037623671.us-east-1.elb.example.com:8080/"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) \\\n  -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <adminusercredentials> \\\n  --from-literal=Username=\"<username>\" \\\n  --from-literal=Password=\"<password>\" \\\n  --from-literal=FirstName=\"<firstname>\" \\\n  --from-literal=LastName=\"<lastname>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <om-db-user-secret-name> \\\n  --from-literal=password=\"<om-db-user-password>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <my-aws-s3-credentials> \\\n  --from-literal=accessKey=\"<AKIAIOSFODNN7EXAMPLE>\" \\\n  --from-literal=secretKey=\"<wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY>\""
                }
            ],
            "preview": "MongoDB  is an enterprise application that manages, backs\nup, and monitors MongoDB deployments. With , you can scale and\nupgrade MongoDB, optimize queries, perform point-in-time restores,\nreceive performance alerts, and more. To easily manage and maintain\n and its underlying database, you can use the  to run\n as a container on .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/set-scope-k8s-operator",
            "title": "Set Scope for  Deployment",
            "headings": [
                " Deployment Scopes",
                "Operator Uses the Same Single Namespace as Resources",
                "Operator Uses a Subset of Namespaces",
                "Operator Uses Cluster-Wide Scope",
                "Next Steps"
            ],
            "paragraphs": "Before you install the  , you can set the scope of the\n  deployment. The scopes depend on the namespaces in\nwhich you choose to deploy   and  . You can set one of these scopes: Operator Uses the Same Single Namespace as Resources   (Default) Operator Uses a Subset of Namespaces Operator Uses Cluster-Wide Scope You can set the scope for the   to use the same   as\nresources. In this case, the   watches   and\n  in that same  . When you  install  the  , it\nuses the default namespace. You can set the scope for the   to use one or more  \nthat differ from the namespace used by the   resources.\nIn this case, the   watches   and  \nin a subset of   that you specify. To install the   instances with this\nscope, use  helm  with the  operator.watchNamespace  parameter. Follow the relevant  installation instructions  for  helm , but specify one or more namespaces\nin the  operator.watchNamespace  parameter for the   to\nwatch: When installing the   to watch resources in one or more\nnamespaces other than the namespace in which the   is\ndeployed: The following example illustrates how the   and   work\ntogether in the cluster. Suppose you create a ServiceAccount in the  mongodb  namespace, and\nthen install the   in this namespace. The  \nuses this ServiceAccount. To set the   scope to watch namespaces  ns1  and  ns2 : See also  operator.watchNamespace . Watching a subset of namespaces is useful in deployments with\nmultiple   instances, where each   instance\nwatches a different subset of namespaces in your cluster. Create the following resources: A   with access to multiple resources. For the full resource\ndefinition, see the\n operator-roles.yaml \nexample. This is a cluster-scoped resource. Create a   to link   with ServiceAccount. This\n clusterRoleBinding  will bind the  clusterRole  that you\ncreated  with the ServiceAccount that the   is using\non the namespace where you install it. Include the   and  \nin the default configuration files that you apply during the\ninstallation. Obtain  cluster-admin privileges . Using these privileges, create a cluster-wide, non-namespaced  . Create a   in three namespaces:  mongodb ,  ns1 \nand  ns2 . This   will bind the\n  to the ServiceAccount in the  mongodb  namespace.\nThe  clusterRoleBinding  will allow the   deployed in\nthe  mongodb  namespace to access the resources described in the\n clusterRole  of the target namespace, that is, in  mongodb ,\n ns1  and  ns2 . You can set the scope for the   to the   cluster.\nIn this case, the   watches   and  \nin all   in the   cluster. To set a cluster-wide scope for the  , follow the\ninstructions for your preferred installation method. You can deploy only one instance of the   with a\ncluster-wide scope per   cluster. Use the  mongodb-enterprise.yaml \nsample   file from the  MongoDB Enterprise Kubernetes Operator GitHub repository . Set the  spec.template.spec.containers.name.env.name:WATCH_NAMESPACE \nin  mongodb-enterprise.yaml \nto  \"*\" . You must include the double quotation marks\n( \" ) around the asterisk ( * ) in the   file. In  mongodb-enterprise.yaml ,\nchange: to: Add the following code to the  ClusterRole  that you\nhave just modified: In  mongodb-enterprise.yaml ,\nchange: to: In the  mongodb-enterprise.yaml  file, change the\n <namespace>  value to the namespace where you want\nthe   to deploy resources and apply the\n  fle. Create local    : For each namespace, create some or all of the following\nlocal    : Copy and paste the applicable examples and replace the  <namespace> \nvalue with the label that identifies the namespace. If you want to deploy a MongoDB instance in the\nnamespace, use  mongodb-enterprise-database-pods . If you want to deploy   in the namespace, use\n mongodb-enterprise-appdb  and  mongodb-enterprise-ops-manager . Before you deploy the  , configure the following\nitems: Configure the   to watch all namespaces: Create local    : For each namespace, create some or all of the following\nlocal    : Copy and paste the applicable examples and replace the  <namespace> \nvalue with the label that identifies the namespace. If you want to deploy a MongoDB instance in the\nnamespace, use  mongodb-enterprise-database-pods . If you want to deploy   in the namespace, use\n mongodb-enterprise-appdb  and  mongodb-enterprise-ops-manager . Before you deploy the  , configure the following\nitems: Use the  mongodb-enterprise-openshift.yaml \nsample   file from the  MongoDB Enterprise Kubernetes Operator GitHub repository . Set the  spec.template.spec.containers.name.env.name:WATCH_NAMESPACE  in\n mongodb-enterprise-openshift.yaml \nto  \"*\" . You must include the double quotation marks\n( \" ) around the asterisk ( * ) in the   file. Create the corresponding roles for these accounts. In\n mongodb-enterprise-openshift.yaml ,\nchange: to: Add the following code to the  ClusterRole  that you\nhave just modified: In  mongodb-enterprise-openshift.yaml ,\nchange: to: Create the secret only in the namespace where you will\ndeploy the  .\nIf you deploy MongoDB resources in  multiple namespaces  or with a  cluster-wide\nscope , the  \nsynchronizes the secret across all watched namespaces.\nTo learn more, see the  registry.imagePullSecrets \nsetting in the  Helm installation settings . In the  mongodb-enterprise.yaml  file, replace\n <namespace>  with the namespace in which you want to\ninstall the  . Use   or the OpenShift\nContainer Platform UI to apply the resulting   file. Create local    : For each namespace, create some or all of the following\nlocal    : Copy and paste the applicable examples and replace the  <namespace> \nvalue with the label that identifies the namespace. If you want to deploy a MongoDB instance in the\nnamespace, use  mongodb-enterprise-database-pods . If you want to deploy   in the namespace, use\n mongodb-enterprise-appdb  and  mongodb-enterprise-ops-manager . Before you deploy the  , configure the following\nitems: Configure the   to watch all namespaces: Create the secret only in the namespace where you will\ndeploy the  .\nIf you deploy MongoDB resources in  multiple namespaces  or with a  cluster-wide\nscope , the  \nsynchronizes the secret across all watched namespaces.\nTo learn more, see the  registry.imagePullSecrets \nsetting in the  Helm installation settings . In the  mongodb-enterprise.yaml  file, replace  <namespace> \nwith the namespace in which you want to install the\n . Use   or the OpenShift Container\nPlatform UI to apply the resulting   file. Create local    : For each namespace, create some or all of the following\nlocal    : Copy and paste the applicable examples and replace the  <namespace> \nvalue with the label that identifies the namespace. If you want to deploy a MongoDB instance in the\nnamespace, use  mongodb-enterprise-database-pods . If you want to deploy   in the namespace, use\n mongodb-enterprise-appdb  and  mongodb-enterprise-ops-manager . After setting up the scope for the  , you can: Read the  Considerations . Complete the  Prerequisites . Install the Kubernetes Operator .",
            "code": [
                {
                    "lang": "sh",
                    "value": "WATCH_NAMESPACE: \"*\""
                },
                {
                    "lang": "sh",
                    "value": "kind:  Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: mongodb-enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "kind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: mongodb-enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "- apiGroups:\n  - \"\"\n  resources:\n  - namespaces\n  verbs:\n  - list\n  - watch"
                },
                {
                    "lang": "sh",
                    "value": "kind:  RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: mongodb-enterprise-operator\n namespace: mongodb\nroleRef:\n apiGroup: rbac.authorization.k8s.io\n kind: Role\n name: mongodb-enterprise-operator\nsubjects:\n - kind: ServiceAccount\n name: mongodb-enterprise-operator\n namespace: mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kind:  ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: mongodb-enterprise-operator\n namespace: mongodb\nroleRef:\n apiGroup: rbac.authorization.k8s.io\n kind: ClusterRole\n name: mongodb-enterprise-operator\nsubjects:\n - kind: ServiceAccount\n name: mongodb-enterprise-operator\n namespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-database-pods\n  namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-ops-manager\n  namespace: <namespace>\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - secrets\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - patch\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: mongodb-enterprise-appdb\nsubjects:\n  - kind: ServiceAccount\n    name: mongodb-enterprise-appdb\n    namespace: <namespace>\n...\n\n"
                },
                {
                    "lang": "sh",
                    "value": "---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n name: mongodb-enterprise-database-pods\n namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n name: mongodb-enterprise-appdb\n namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n name: mongodb-enterprise-ops-manager\n namespace: <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator \\\n  --set operator.watchNamespace=\"*\""
                },
                {
                    "lang": "sh",
                    "value": "helm template mongodb/enterprise-operator \\\n  --set namespace=<metadata.namespace>\n  --show-only templates/database-roles.yaml | kubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "WATCH_NAMESPACE: \"*\""
                },
                {
                    "lang": "sh",
                    "value": "kind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "kind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "- apiGroups:\n  - \"\"\n  resources:\n  - namespaces\n  verbs:\n  - list\n  - watch"
                },
                {
                    "lang": "sh",
                    "value": "kind:  RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: enterprise-operator\n namespace: mongodb\nroleRef:\n apiGroup: rbac.authorization.k8s.io\n kind: Role\n name: enterprise-operator\nsubjects:\n - kind: ServiceAccount\n name: enterprise-operator\n namespace: mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kind:  ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: enterprise-operator\n namespace: mongodb\nroleRef:\n apiGroup: rbac.authorization.k8s.io\n kind: ClusterRole\n name: enterprise-operator\nsubjects:\n - kind: ServiceAccount\n name: enterprise-operator\n namespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-database-pods\n  namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-ops-manager\n  namespace: <namespace>\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - secrets\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - patch\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: mongodb-enterprise-appdb\nsubjects:\n  - kind: ServiceAccount\n    name: mongodb-enterprise-appdb\n    namespace: <namespace>\n...\n\n"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator \\\n  --set operator.watchNamespace=\"*\" \\"
                },
                {
                    "lang": "yaml",
                    "value": "---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-database-pods\n  namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-ops-manager\n  namespace: <namespace>\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - secrets\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - patch\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: mongodb-enterprise-appdb\nsubjects:\n  - kind: ServiceAccount\n    name: mongodb-enterprise-appdb\n    namespace: <namespace>\n...\n\n"
                },
                {
                    "lang": "sh",
                    "value": "helm template mongodb/enterprise-operator \\\n  --set namespace=<metadata.namespace>\n  --show-only templates/database-roles.yaml | oc apply -f -"
                }
            ],
            "preview": "Before you install the , you can set the scope of the\n deployment. The scopes depend on the namespaces in\nwhich you choose to deploy  and .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-prometheus",
            "title": "Deploy a Resource to Use with Prometheus",
            "headings": [
                "Quick Start",
                "Prerequisites",
                "Install the Prometheus Operator",
                "Install the ",
                "Create a MongoDB Resource",
                "Optional: Enable TLS on the Prometheus Endpoint",
                "Install Cert-Manager",
                "Enable TLS on the MongoDB CRD",
                "Update ServiceMonitor",
                "mongodb-prometheus-sample.yaml",
                "Examples",
                "MongoDB Resource with Prometheus",
                "ServiceMonitor",
                "Endpoint Credentials"
            ],
            "paragraphs": "You can use the  mongodb-prometheus-sample.yaml  file to deploy a MongoDB resource in your\n  cluster, with a  ServiceMonitor \nto indicate to Prometheus how to consume metrics data from\nit. The sample specifies a simple MongoDB resource with one user,\nand the  spec.prometheus  attribute with basic HTTP\nauthentication and no  . The sample lets you test\nthe metrics that MongoDB sends to Prometheus. We tested this setup with version 0.54 of the\n Prometheus Operator . Kubernetes 1.16+ Helm 3+ You can install the Prometheus Operator using Helm. To learn\nmore, see the  installation instructions . To install the Prometheus Operator using Helm, run the\nfollowing commands: Run the following command to install the   and create a\nnamespace to contain the   and resources: To learn more, see  Install the  . You can use the  mongodb-prometheus-sample.yaml  file to deploy a MongoDB resource in your\n  cluster, with a  ServiceMonitor \nto indicate to Prometheus how to consume metrics data from\nit. You can apply the sample directly with the following command: This command creates two   that contain authentication\nfor a new MongoDB user and basic HTTP authentication for the\nPrometheus endpoint. The command creates both   in the\n mongodb  namespace. This command also creates a  ServiceMonitor  that\nconfigures Prometheus to consume this resource's metrics. This command\ncreates the  ServiceMonitor  in the  prometheus-system \nnamespace. Specify the full path to the  mongodb-prometheus-sample.yaml  file. Ensure you specify\n spec.credentials  and\n spec.cloudManager.configMapRef.name . To install  cert-manager  using Helm,\nsee the  cert-manager installation documentation . To create a cert-manager  Issuer , see the\n cert-manager configuration documentation To create a certificate, see the  cert-manager usage documentation . To enable  , you must add a new entry to the\n spec.prometheus  section of the MongoDB custom resource. Run\nthe following  patch \noperation to add the needed entry. The following response appears: After a few minutes, the MongoDB resource should return to the\nRunning phase. Now you must configure the Prometheus\n ServiceMonitor \nto point to the HTTPS endpoint. Do  NOT  use this configuration in Production\nenvironments! A security expert should advise you about how to\nconfigure  . tlsSecretKeyRef.name  points at a   of type\n kubernetes.io/tls  that holds a  Server certificate . To update the  ServiceMonitor , run\nthe following command to patch the resource again: The following reponse appears: With these changes, the new  ServiceMonitor \npoints to the HTTPS endpoint (defined in\n /spec/endpoints/0/scheme ). You also set\n spec/endpoints/0/tlsConfig/insecureSkipVerify  to  true ,\nso that Prometheus doesn't verify the   certificates on\nMongoDB's end. Prometheus should now be able to scrape the MongoDB target\nusing HTTPS. Create the following  mongodb-prometheus-sample.yaml  file to deploy\na MongoDB resource in your   cluster, with a\n ServiceMonitor \nto indicate to Prometheus how to consume metrics data from\nit. This sample file specifies a simple MongoDB resource with one user,\nand the  spec.prometheus  attribute with basic HTTP\nauthentication and no  . The sample lets you test\nthe metrics that MongoDB sends to Prometheus. To learn more, see  Prometheus Settings . The following examples show the resource definitions required to use\nPrometheus with your MongoDB resource. To learn more, see  Prometheus Settings .",
            "code": [
                {
                    "lang": "sh",
                    "value": "helm repo add prometheus-community https://prometheus-community.github.io/helm-charts"
                },
                {
                    "lang": "sh",
                    "value": "helm repo update"
                },
                {
                    "lang": "sh",
                    "value": "helm install prometheus prometheus-community/kube-prometheus-stack \\\n  --namespace <prometheus-system> \\\n  --create-namespace"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator \\\n  --namespace <mongodb> --create-namespace"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <mongodb-prometheus-sample.yaml>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl patch mdbc mongodb --type='json' \\\n  -p='[{\"op\": \"add\", \"path\": \"/spec/prometheus/tlsSecretKeyRef\", \"value\":{\"name\": \"prometheus-target-cert\"}}]' \\\n  --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "mongodbenterprise.mongodbenterprise.mongodb.com/mongodb patched"
                },
                {
                    "lang": "sh",
                    "value": "kubectl patch servicemonitors mongodb-sm --type='json' \\\n    -p='\n[\n    {\"op\": \"replace\", \"path\": \"/spec/endpoints/0/scheme\", \"value\": \"https\"},\n    {\"op\": \"add\",     \"path\": \"/spec/endpoints/0/tlsConfig\", \"value\": {\"insecureSkipVerify\": true}}\n]\n' \\\n    --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "servicemonitor.monitoring.coreos.com/mongodb-sm patched"
                },
                {
                    "lang": "sh",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: 5.0.6-ent\n\n  cloudManager:\n    configMapRef:\n      name: <project-configmap>\n\n  credentials: <credentials-secret>\n  type: ReplicaSet\n\n  persistent: true\n\n  prometheus:\n    passwordSecretRef:\n      # SecretRef to a Secret with a 'password' entry on it.\n      name: metrics-endpoint-password\n\n    # change this value to your Prometheus username\n    username: prometheus-username\n\n    # Enables HTTPS on the prometheus scrapping endpoint\n    # This should be a reference to a Secret type kuberentes.io/tls\n    # tlsSecretKeyRef:\n    #   name: <prometheus-tls-cert-secret>\n\n    # Port for Prometheus, default is 9216\n    # port: 9216\n    #\n    # Metrics path for Prometheus, default is /metrics\n    # metricsPath: '/metrics'\n\n---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n\n  # This needs to match `spec.ServiceMonitorSelector.matchLabels` from your\n  # `prometheuses.monitoring.coreos.com` resouce.\n  labels:\n    release: prometheus\n\n  name: mongodb-sm\n\n  # Make sure this namespace is the same as in `spec.namespaceSelector`.\n  namespace: mongodb\nspec:\n  endpoints:\n\n  # Configuring a Prometheus Endpoint with basic Auth.\n  # `prom-secret` is a Secret containing a `username` and `password` entries.\n  - basicAuth:\n      password:\n        key: password\n        name: metrics-endpoint-creds\n      username:\n        key: username\n        name: metrics-endpoint-creds\n\n    # This port matches what we created in our MongoDB Service.\n    port: prometheus\n\n    # If using HTTPS enabled endpoint, change scheme to https\n    scheme: http\n\n    # Configure different TLS related settings. For more information, see:\n    # https://github.com/prometheus-operator/prometheus-operator/blob/main/pkg/apis/monitoring/v1/types.go#L909\n    # tlsConfig:\n    #    insecureSkipVerify: true\n\n  # What namespace to watch\n  namespaceSelector:\n    matchNames:\n    # Change this to the namespace the MongoDB resource was deployed.\n    - mongodb\n\n  # Service labels to match\n  selector:\n    matchLabels:\n      app: my-replica-set-svc\n\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: metrics-endpoint-creds\n  namespace: mongodb\ntype: Opaque\nstringData:\n  password: 'Not-So-Secure!'\n  username: prometheus-username\n\n..."
                },
                {
                    "lang": "sh",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: 5.0.6-ent\n  cloudManager:\n    configMapRef:\n      name: <project-configmap>\n  credentials: <credentials-secret>\n  type: ReplicaSet\n  persistent: true\n  prometheus:\n    passwordSecretRef:\n      name: metrics-endpoint-password\n    username: prometheus-username\n\n..."
                },
                {
                    "lang": "sh",
                    "value": "---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  labels:\n    release: prometheus\n  name: mongodb-sm\n  namespace: mongodb\nspec:\n  endpoints:\n  - basicAuth:\n      password:\n        key: password\n        name: metrics-endpoint-creds\n      username:\n        key: username\n        name: metrics-endpoint-creds\n    port: prometheus\n    scheme: http\n  namespaceSelector:\n    matchNames:\n    - mongodb\n  selector:\n    matchLabels:\n      app: my-replica-set-svc\n\n..."
                },
                {
                    "lang": "sh",
                    "value": "---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: metrics-endpoint-creds\n  namespace: mongodb\ntype: Opaque\nstringData:\n  password: 'Not-So-Secure!'\n  username: prometheus-username\n\n..."
                }
            ],
            "preview": "You can use the mongodb-prometheus-sample.yaml file to deploy a MongoDB resource in your\n cluster, with a ServiceMonitor\nto indicate to Prometheus how to consume metrics data from\nit.",
            "tags": null,
            "facets": null
        }
    ]
}