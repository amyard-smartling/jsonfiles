{
    "url": "http://mongodb.com/docs/atlas/app-services",
    "includeInGlobalSearch": true,
    "documents": [
        {
            "slug": "logs",
            "title": "App Log Types",
            "headings": [],
            "paragraphs": "App Services stores logs for the following types of events: Authentication Logs Change Stream Logs Device Sync Logs Endpoint Logs Function Logs Schema Logs Service Logs Trigger Logs Trigger Error Handler Logs",
            "code": [],
            "preview": "App Services stores logs for the following types of events:",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "rules",
            "title": "Define Data Access Permissions",
            "headings": [
                "Overview",
                "Key Concepts"
            ],
            "paragraphs": "In traditional applications, an application server exposes an API to client\napplications and handles database queries on their behalf. To prevent malicious,\nimproper, or incorrect read and write operations, clients don't query the\ndatabase directly. Atlas App Services provides a configurable and dynamic permissions engine that\nenables you to run MongoDB and Device Sync queries from client applications\nwhile transparently preventing unauthorized reads and writes. Permissions are\ndefined for entire collections in a linked MongoDB Atlas cluster and apply to\nindividual documents in the collection dynamically based on the\n application user  that issues a query or uses Device Sync. You\ncan also define default roles that serve as a fallback for any collection that\ndoesn't have its own roles. Federated data sources  do not support rules or schemas . You can only access a Federated data source\nfrom a system function. To understand how the permissions engine works and how to configure it for your\nuse case, see  Role-based Permissions . You define permissions with  Rule Expressions , a\ndomain-specific language built on JSON. You can optimize the performance of the permissions engine with filters. See\n Filter Incoming Queries . If you're using Device Sync (Flexible Mode), permissions work a bit differently.\nSee  Device Sync-Compatible Permissions  for\nimportant considerations. The default or \"Basic Mode\" rules editor in the App Services UI covers the\nmajority of use cases for collection rules. However, there are times when you\nneed more fine-grained control than the UI interface provides. In those times,\nsee  Configure Advanced Rules . Finally, to help you put these concepts into practice, we have provided some\n Data Access Role Examples .",
            "code": [],
            "preview": "In traditional applications, an application server exposes an API to client\napplications and handles database queries on their behalf. To prevent malicious,\nimproper, or incorrect read and write operations, clients don't query the\ndatabase directly.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "apps",
            "title": "Develop & Deploy Apps",
            "headings": [
                "Overview",
                "What is an App Services App?",
                "How are Apps Deployed?",
                "App Administration Tools",
                "Automate Deployment",
                "Deployment History"
            ],
            "paragraphs": "An App Services App is a collection of managed services that run your\napplication's backend operations on MongoDB Atlas. You configure the\nApp's services using declarative configuration files and write\nJavaScript code to implement custom behavior. An App sits between your application's frontend client and your data\nstored in MongoDB Atlas. Apps have built-in tools for modeling and\nworking with data in Atlas, including role-based access permissions and\ndocument schema validation. You can use these to power services like\n Device Sync ,  Triggers , and the\n Data API . To learn how to create an App, see  Create an App . Apps use a \"serverless\" architecture, which means that you as the\ndeveloper don't directly manage or think about the servers that run your\nApp. Instead, you deploy your App to a one or more cloud regions around\nthe world. Your services run on-demand on servers in\nthat region managed by MongoDB Atlas. To learn more, see  Deployment Models & Regions . App Services includes several tools that you can use to create,\nconfigure, and deploy Apps. You can accomplish most administration tasks\nusing any of these tools, so choose the tool that best suits your\ndevelopment workflow. The following administration tools are available: App Services UI : A browser-based GUI that's available within MongoDB\nAtlas. You can use it to develop and manage your Apps with rich\neditors and forms. App Services CLI : A command-line interface that you can access from a\nterminal or shell script. The CLI lets you work directly with your\nApp's configuration files in a local environment or CI/CD pipeline. App Services Admin API : A REST-ful API that you can access using\nany standard HTTPS client. You can use the Admin API to integrate App\nServices with automation tools like CI/CD pipelines. Many pages in this documentation include tabs that let you choose a\nspecific admin tool. Make sure to look out for these tabs and to\nchoose your preferred tool. You can use the App Services admin tools on an ad hoc basis to manually\ndevelop and deploy changes. Alternatively, you can automate the\ndeployment process. This is particularly useful for Apps with testing\nand staging environments or that are developed by teams. There are two approaches to automatic deployment: App Services GitHub Integration : The built-in GitHub integration\nlets you link your App to a GitHub repository and automatically syncs\nyour deployed App with configuration files stored in the repo. To\nlearn more, see  Deploy Automatically with GitHub . Custom Automation Workflows : You can create, modify, and delete\nApps in your own CI/CD pipelines using a code-first admin tool like\nthe App Services CLI or Admin API. To learn more, see  Set Up a\nCI/CD Pipeline . App Services logs every deployment of an application and stores a list\nof the 25 most recent deployments. You can\n export  or  rollback  to\nany version stored in the deployment history. You can access the deployment history for an App from the\n Deployment  screen in the App Services UI or by calling the\n List recent deployments  Admin API endpoint.",
            "code": [],
            "preview": "App Services includes several tools that you can use to create,\nconfigure, and deploy Apps. You can accomplish most administration tasks\nusing any of these tools, so choose the tool that best suits your\ndevelopment workflow.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference",
            "title": "Reference",
            "headings": [],
            "paragraphs": "App Configuration Files App Metrics App Services Admin API App Services CLI Billing Service Limitations Template Apps Third-Party Licenses Upgrade a Shared Tier Cluster Partition-Based Sync Mode Push Notifications [Deprecated] Third-Party Services [Deprecated]",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "schemas",
            "title": "Schemas",
            "headings": [
                "Overview",
                "What is a Schema?",
                "Why Define a Schema?",
                "Define a Schema",
                "How App Services Enforces Schemas",
                "App Services Schema vs Built-In Schema Validation"
            ],
            "paragraphs": "A schema is a JSON object that defines the structure and contents of your\ndata. You can use Atlas App Services' BSON schemas, which extend the  JSON Schema  standard, to define your application's data model\nand validate documents whenever they're created, changed, or deleted. Schemas represent  types  of data rather than specific values. App Services supports\nmany built-in  schema types . These include primitives, like\nstrings and numbers, as well as structural types, like objects and arrays, which\nyou can combine to create schemas that represent custom  object types . For example, this is a basic schema for data about cars and some car objects\nthat conform to the schema: Schemas are the specification for your application's data model. Once you've\ndefined a schema, App Services provides you with additional tools and services to work\nwith data that conforms to the schema. App Services uses schemas in many application services: Atlas Device Sync  uses schemas to sync data between realms and MongoDB\nAtlas. App Services can also generate idiomatic SDK object models for you based on\nyour schemas. Data Access Rules  validate that data conforms to your schema\nbefore and after every request. If any document fails validation, App Services\nprevents or rolls back the entire request. A root-level collection schema can contain additional schemas that describe the\ntype's properties. Each root-level schema is an  object  schema that has the\nfollowing form: You can use any of the supported  schema types  to\nconfigure the object's properties: Object Array String Number Boolean UUID ObjectId Binary Data Mixed Set Dictionary To learn how to configure and deploy a schema in your app, see\n Enforce a Schema . App Services validates all write operations (inserts, updates, and deletes) on a\nMongoDB collection against its collection schema. It checks every document\nbefore and after every request to ensure that all properties conform to the\nschema and that no invalid changes occured. App Services evaluates the result of all document writes and compares them against the\nschema before committing the writes to your cluster. If the result of any write\noperation in a request does not match the schema, App Services returns an error\nto the user without applying any changes in the request. A collection has the following schema: A user with permission to read and write all fields wants to update\nthe  name  field of a particular document. They issue the following\nquery: The query attempts to set the value of  name  to the number  42 ,\nbut the schema requires the value to be a  string . App Services will\nreject this write operation even though the user had permission to\nupdate the document because the write result does not conform to the\nschema. A schema in App Services is  not the same  as  MongoDB's built-in\nschema validation . Both use the JSON\nschema standard with additional support for BSON types. However, App\nServices does not use your cluster's built-in schema and may interact\nwith your cluster in a way that is incompatible with a built-in schema. If you want to use App Services schemas  and  your cluster's built-in\nschema validation at the same time, consider the following: If you need help in working with both schema validation layers\nsimultaneously, contact  MongoDB Support . Set your cluster's schema validation level to \"warn\" initially. Then,\nmonitor activity and address existing warnings. Once you're\ncomfortable that both schema validation layers are compatible, you can\nupgrade the validation level to \"error\". If you're using  Device Sync , avoid required fields for\nembedded documents and arrays of embedded documents. The sync protocol\nmay break a valid embedded object write into multiple equivalent\nwrites that do not individually include all required fields. If you're using Device Sync, avoid distinguishing between\n undefined ,  null , empty arrays, and embedded objects with no\nfields. The sync protocol treats these values as functional\nequivalents.",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"car\",\n  \"required\": [\n     \"_id\",\n     \"year\",\n     \"make\",\n     \"model\",\n     \"miles\"\n  ],\n  \"properties\": {\n    \"_id\": { \"bsonType\": \"objectId\" },\n    \"year\": { \"bsonType\": \"string\" },\n    \"make\": { \"bsonType\": \"string\" },\n    \"model\": { \"bsonType\": \"string\" },\n    \"miles\": { \"bsonType\": \"number\" }\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": ObjectId(\"5af712eff26b29dc5c51c60f\"),\n  \"year\": \"2017\",\n  \"make\": \"Honda\",\n  \"model\": \"Civic\",\n  \"miles\": 117424\n}\n{\n  \"_id\": ObjectId(\"5af714eff24b294c5251cf04\"),\n  \"year\": \"2020\",\n  \"make\": \"Ford\",\n  \"model\": \"Mustang\",\n  \"miles\": 13579\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"bsonType\": \"object\",\n  \"title\": \"<Type Name>\",\n  \"required\": [\"<Required Field Name>\", ...],\n  \"properties\": {\n    \"<Field Name>\": <Schema>\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"person\",\n  \"properties\": {\n    \"_id\": { \"bsonType\": \"objectId\" },\n    \"name\": { \"bsonType\": \"string\" }\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "collection.updateOne(\n  { \"_id\": BSON.ObjectId(\"5ae782e48f25b9dc5c51c4d0\") },\n  { \"$set\": { \"name\": 42 } }\n)"
                }
            ],
            "preview": "A root-level collection schema can contain additional schemas that describe the\ntype's properties. Each root-level schema is an object schema that has the\nfollowing form:",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli",
            "title": "realm-cli [Deprecated]",
            "headings": [],
            "paragraphs": "The MongoDB Realm Command Line Interface ( realm-cli ) allows you to\nprogrammatically manage your App Services Apps. With  realm-cli , you\ncan create or update Apps from a local directory as well as export\nexisting applications to a local directory. There are two versions of  realm-cli . Find the appropriate version\nfor your use case below: realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: realm-cli@^1 realm-cli@^2",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                }
            ],
            "preview": "The MongoDB Realm Command Line Interface (realm-cli) allows you to\nprogrammatically manage your App Services Apps. With realm-cli, you\ncan create or update Apps from a local directory as well as export\nexisting applications to a local directory.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "values-and-secrets",
            "title": "Values & Secrets",
            "headings": [
                "Introduction",
                "Concepts",
                "Value",
                "Secret",
                "Environment Values",
                "Summary"
            ],
            "paragraphs": "Atlas App Services Values  and  App Services Secrets  are static, server-side constants that you can access or link to\nfrom other components of your application. You can  access Values  directly from  Atlas\nFunctions  and  rule expressions  and link\nSecrets to configuration values for  authentication providers . A  value  is a named reference to a piece of static data stored by App Services\nthat you can access in  functions  and  rule expression . Values provide an alternative to hardcoding configuration\nconstants directly into your functions and rules. In other words, values allow\nyou to separate deployment-specific configuration data from the business logic\nof your app. Values can resolve to two types of data: A  plain text  Value resolves to a regular JSON object,\narray, or string that you define. A  secret  Value resolves to the value of a\n Secret . A  Secret  is a private value that is stored on the\nApp Services backend, hidden from users, and not included in exported\napplications. Secrets are useful for storing sensitive\ninformation such as an API key or an internal identifier. You cannot directly read the value of a Secret after\ndefining it. Instead, you link to the Secret by name in\nauthentication provider and service configurations. If you\nneed to access the Secret from a Function or\nRule, you can link the Secret to a\n Value . A Secret value has a maximum character length of 500 characters. An  environment value  is similar to a regular text  Value \nbut can dynamically resolve to one of multiple values depending on an\napplication-wide environment tag. You can specify different values for each of\nthe following supported environment tags: You can access defined environment values in functions with\n context.environment.values  and in\nrule expressions with  %%environment . For more information, see  Configure an App Environment . \"\" \"development\" \"testing\" \"qa\" \"production\" App Services Values and App Services Secrets are server-side constants you can use in\nyour app. App Services Values allow you to separate deployment-specific configuration\ndata from the business logic of your app. App Services Secrets store sensitive data. You cannot export or directly read a\nSecret. Instead, you access a Secret indirectly\nvia a named App Services Value.",
            "code": [],
            "preview": "Atlas App Services Values and App Services Secrets are static, server-side constants that you can access or link to\nfrom other components of your application. You can access Values directly from Atlas\nFunctions and rule expressions and link\nSecrets to configuration values for authentication providers.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "hosting",
            "title": "Host Files [Deprecated]",
            "headings": [
                "Introduction",
                "Concepts",
                "Static Content",
                "Hosting Domain",
                "Resource Paths",
                "Constraints"
            ],
            "paragraphs": "Static Hosting is deprecated.  Learn More . Atlas Hosting allows you to host, manage, and serve your application's\nstatic media and document files. You can use Hosting to store individual\npieces of content or to upload and serve your entire client application. To get started,  enable hosting  for your\napplication then start  uploading content . To enable static hosting, you must have a paid-tier (i.e.  M2  or\nhigher) Atlas cluster linked to your app as a data source. For more\ninformation on Atlas cluster tiers, see  Create a Cluster . A file is  static  if you can serve it directly to clients without\nrequiring additional processing or logic to generate it. Static content\nincludes document files, such as HTML, JavaScript, and CSS, as well as\nmedia files like images, audio, and videos. App Services hosts your application's content behind a unique domain\nname. By default, App Services uses domains of the following form: You can configure App Services to host content at a  custom domain\nname  that you own in addition to\nthe default hosting domain. A resource path is a string that uniquely identifies an uploaded file\nbased on its position in the file tree. You can access a hosted file\nthrough a unique URL that consists of your application's hosting domain\nfollowed by the file's resource path. For example, a PNG image of a company's logo hosted in a directory named\n images  would have the resource path  /images/logo.png . You could\naccess this image at a URL similar to the following: App Services enforces constraints on static hosting across several dimensions\nfor each app. The following constraints apply to all\nstatic content hosted by App Services: Dimension Constraint Maximum File Size 25 megabytes Maximum Total Storage 1 gigabyte Maximum Total Files 20,000 files Maximum CDN Flushes 25 flushes per hour Maximum Bandwidth 100 gigabytes",
            "code": [
                {
                    "lang": "none",
                    "value": "<Your App ID>.mongodbstitch.com"
                },
                {
                    "lang": "none",
                    "value": "myapp-abcde.mongodbrealm.com/images/logo.png"
                }
            ],
            "preview": "Atlas Hosting allows you to host, manage, and serve your application's\nstatic media and document files. You can use Hosting to store individual\npieces of content or to upload and serve your entire client application.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "edge-server",
            "title": "Atlas Edge Server",
            "headings": [
                "Key Features",
                "Edge Server Setup",
                "Create a Device Sync-Enabled App",
                "Install and Configure the Edge Server",
                "Connect to the Edge Server from a Client",
                "Secure, Powerful, Fast"
            ],
            "paragraphs": "You have a warehouse, retail store, or another premises where devices\nneed to seamlessly sync their data. Those locations may or may not be\nonline at any given time. Lack of a network connection shouldn't prevent\nthe devices from sharing data with each other. Edge Server runs on-premises and handles sync between local devices,\nand bi-directional sync between the Edge Server and Atlas. Edge Server is currently in Private Preview. Learn more about previewing\nthe\n Atlas Device Sync Edge Server . Edge server is a local instance of MongoDB paired with a Device Sync\nserver that runs on-premises. It brings real-time sync, disconnection\ntolerance, and conflict resolution to the location's devices. It\nseamlessly syncs with Atlas when it has a network connection. Delta Sync ensures that only fields with changes in a document are\nsent to Atlas. Mission-critical data is transferred efficiently while\nminimizing network congestion. Edge clients can sync and access essential data independently, even\nwhen disconnected from the central database. Applications and devices\nin a connected infrastructure continue to function seamlessly,\ndespite intermittent connectivity. Create an  App Services App  with\n Device Sync enabled . To get started quickly,\n create a template app . Once your App is created, coordinate with your Product or Account\nRepresentative to enable Edge Server. Configure hardware on-site or a cloud server to host your Edge Server.\nInstall the Edge Server on your hardware, complete a basic\n config.json  with your configuration details, and install required\ndependencies. Add an authorization provider, and start the Edge Server. Configure your client to connect to the Edge Server instead of directly\nto your Atlas App. Local clients can sync data through the Edge Server\neven without network connectivity. When the Edge Server connects\nwith Atlas, it syncs data from the local clients. Built-in user authentication providers include anonymous,\nemail/password, API key, and Custom JWT. A user-based permissions\nsystem lets you control who can access which data. Edge Server uses the Device Sync engine to handle conflicts. You don't\nhave to write complex custom code to resolve conflicting writes from\nmultiple clients. Having a local data layer enables rapid synchronization of critical\ndata to edge devices. Reduce latency and enable immediate actions\nbased on real-time insights.",
            "code": [],
            "preview": "Create an App Services App with\nDevice Sync enabled. To get started quickly,\ncreate a template app.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "migrate-hosting-graphql",
            "title": "Migrate Static Hosting and GraphQL From App Services",
            "headings": [
                "GraphQL Providers",
                "Apollo",
                "Hasura",
                "Static Hosting Providers",
                "Netlify",
                "Vercel",
                "AWS S3 Blob Storage"
            ],
            "paragraphs": "As of  March 12, 2024  , GraphQL and Static Hosting are deprecated for Atlas App\nServices. GraphQL and Static Hosting services will be discontinued after one\nyear on March 12, 2025. If you use GraphQL or Static Hosting, you should migrate to other providers\nbefore the services are discontinued. Below, you can find migration guides\nfor official, trusted MongoDB partners that offer best-in-class alternative\nsolutions. These migration guides offer step by step guidance for how to\ntransition off of our services, and have been created in collaboration with our\npartners. GraphQL and Static Hosting are deprecated. The following GraphQL providers can help you keep your projects running. Apollo provides a developer platform and open-source tools and SDKs\u2014including\nApollo Server\u2014to unify your data and services. Apollo Server is a\nspec-compliant, production-ready server library that can use data from any\nsource, including MongoDB Atlas. Refer to  Migrate GraphQL to Apollo  for details. Hasura empowers developers to rapidly build and deploy GraphQL and REST APIs on\nMongoDB and many other data sources. By radically cutting down API development\ntimes, Hasura enables rapid access to data, reduces friction across teams and\nservices, and enables enterprises to shorten time to market on data-powered\nproducts and features. Refer to  Migrate GraphQL to Hasura  for details. The following static hosting providers can help you keep your projects running. Netlify provides developers with the platform and workflow to focus on building\nwebsites and apps without dedicating time and resources on labor-intensive\noperations. Developers are enabled to build and deploy future-proof web apps and\nwebsites with modern, composable tooling that works with all modern frontend\nframeworks. With support for static site generation (SSG), server-side rendering\n(SSR) and incremental site regeneration (ISR), Netlify's managed frontend cloud\nand CI/CD can support the growing needs of your App Services application. Refer to  Migrate Static Hosting to Netlify  for details. Vercel is a frontend cloud for hosting web sites and apps, including static\nsites, single-page applications (SPAs), dynamic server-rendered applications,\nand more. It is compatible with your App Services application. Refer to  Migrate Static Hosting to Vercel  for details. If you are solely using the MongoDB Atlas Hosting service as a blob store for\nstatic content and are not hosting a client application, follow the steps below\nto migrate from using Atlas Hosting to using your own S3 bucket. Refer to  Migrate to Your Own S3 Bucket  for details.",
            "code": [],
            "preview": "Learn how to migrate GraphQL and your static assets away from Atlas App Services.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli",
            "title": "App Services CLI",
            "headings": [
                "Overview",
                "Installation",
                "Authentication",
                "Generate an API Key",
                "Navigate to MongoDB Cloud Access Manager",
                "Create an API Key",
                "Configure Your API Access List",
                "Authenticate with an API Key",
                "Authenticate a CLI User",
                "The .mdb Directory",
                "CLI Profiles",
                "Options",
                "Related Commands"
            ],
            "paragraphs": "The Atlas App Services Command Line Interface ( appservices ) allows you\nto programmatically manage your Applications. With the App Services CLI, you can create or update Apps from a local directory\nas well as export existing applications to a local directory. App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: To use the App Services CLI, you must authenticate. To authenticate, you must\ngenerate an API Key. The  MongoDB Cloud Access Manager \nallows you to manage access to your project for users, teams, and API\nKeys. Use the Project Access Manager by clicking the\n Project Access  tab on the  access manager\ndropdown  on your screen's top left-hand side. Project Users can log in with a Project API Key. Create a project\nAPI Key by clicking the grey  Create API Key  button on\nthe right-hand side of the Project Access Manager. Clicking this button navigates you to the \"Create API Key\" screen. Set a\ndescription for your key. For write access, the CLI requires an API key with \"Project Owner\"\npermissions. For read-only access, you can use \"Project Read Only\". Use the\n Project Permissions  dropdown to select the appropriate permissions\nfor your use case. Copy the public key to use later in order to log in. Click  next  to\ncontinue configuring your key details. Copy your Private Key to a safe location for later use. For security,\nthe Private Key will not be visible again after initialization.\nAnother security feature is the API Access List. Creating an API\nAccess List entry ensures that API calls originate from permitted IPs. The IP Address of the user who will be using the API Key is required\nto use the key. Click the  Add Access List Entry  button.\nType in the IP Address or click the  Use Current IP Address \nbuttton and click save.  Finally, click the done button on your screen's\nlower right-hand to finish setting up your API key. Using your newly created public and private key, log in by running the\ncommand below. You should see the following result: When you use the App Services CLI to push or pull configuration files, the CLI\nstores information about the App you're working with in the  .mdb \ndirectory of your application config. This allows the CLI to remember a\nspecific deployment that your configuration files are associated with\nacross multiple commands. This directory is machine generated and you typically should not\nmanually modify it. If you delete the  .mdb  directory, the CLI will\nno longer be able to associate your configuration files with a specific\ndeployment. The CLI creates a new  .mdb  directory when you run a\ncommand that targets a specific deployment. The CLI stores identifiers and configuration metadata in the\n .mdb/meta.json  file, which has the following format: Field Description The configuration file format version that all configuration\nfiles in the directory conform to. This is used to ensure that\nthe CLI can read the configuration files. The App's internal ObjectId value. The Atlas Project ID that the App is associated with. The human-readable Client App ID. The CLI stores information about its users in a profile. This lets you\nrun commands in a given context. For example, when you log in with\nan Atlas Admin API Key, the CLI stores the API Key and the current\nsession access token. Then it reuses that token for subsequent commands\nuntil it expires. You can set up multiple named profiles and choose a profile to use for\nany given CLI command. If you don't specify one, the CLI uses the\n default profile , which is a profile named  default . To specify a profile, add the  --profile  argument on any command. For\nexample, to log in with a new profile named  my-profile ,\nrun the following: Once logged in, you can run other commands with the same profile: You can list all profiles on your system with a CLI command: The CLI stores profiles on your computer in individual configuration\nfiles named after the profile. The location of the profile definitions\ndepends on your system: Operating System Profile Directory Unix / Linux $XDG_CONFIG_HOME/<profile>.yaml  or  $HOME/.config/<profile>.yaml macOS $HOME/Library/Application\\ Support/appservices-cli/<profile>.yaml Windows %AppData%/<profile>.yaml Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts -h, --help false help for appservices appservices accessList  - Manage the allowed IP addresses and CIDR blocks of your app (aliases: accesslist, access-list) appservices apps  - Manage the App Service Apps associated with the current user (alias: app) appservices deploy  - Manage automatic deployments of your app appservices function  - Interact with the Functions of your app (alias: functions) appservices login  - Log the CLI into App Services using a MongoDB Cloud API Key appservices logout  - Log the CLI out of App Services appservices logs  - Interact with the Logs of your app (alias: log) appservices pull  - Exports the latest version of your app into your local directory (alias: export) appservices push  - Imports and deploys changes from your local directory to your app (alias: import) appservices schema  - Manage the Schemas of your app (alias: schemas) appservices secrets  - Manage the Secrets of your app (alias: secret) appservices users  - Manage the Users of your app (alias: user) appservices whoami  - Display information about the current user",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": "shell",
                    "value": "appservices login --api-key=\"<my api key>\" --private-api-key=\"<my private api key>\""
                },
                {
                    "lang": "shell",
                    "value": "you have successfully logged in as <your public key>"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"config_version\": 20230101,\n  \"app_id\": \"42249d526d97af5a287c1eae\",\n  \"group_id\": \"4b2cf422930196872221a2d4\",\n  \"client_app_id\": \"myapp-abcde\"\n}"
                },
                {
                    "lang": null,
                    "value": "appservices login --profile my-profile"
                },
                {
                    "lang": null,
                    "value": "appservices pull --remote=myapp-abcde --profile my-profile"
                },
                {
                    "lang": "shell",
                    "value": "appservices profiles list"
                },
                {
                    "lang": "text",
                    "value": "Found 2 profile(s)\n  Profile     API Key\n  ----------  -----------------------------------------------\n  my-profile  rjxerfwi (********-****-****-****-f00b471ec015)\n  default     xkwwtlmj (********-****-****-****-f03b321dae23)"
                }
            ],
            "preview": "The Atlas App Services Command Line Interface (appservices) allows you\nto programmatically manage your Applications.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "get-started",
            "title": "Get Started",
            "headings": [
                "Template Apps",
                "Tutorials"
            ],
            "paragraphs": "The resources on this page are designed to help you begin using Atlas App\nServices. To explore a working codebase that showcases App Services'\nfeatures, check out a Template App. For a more guided experience in\nlearning how to develop with App Services and Realm, start with a\nTutorial. Bootstrap App Services with a  Template App .\nTemplate apps bring together many of the building blocks available in\nAtlas App Services and start you off with a prebuilt app that you can\ncustomize. The following template apps are available: To learn more: Todo list mobile apps written with Realm SDKs that sync data with App\nServices using Device Sync An Event-driven  Database Trigger \ntemplate that updates a view in a separate collection. Watch our  Using Template Apps in the Atlas UI \nLearning Byte. Check out the template apps in the\n Atlas UI Follow along with one of the tutorials below to build a mobile app\nbased on the template app. Tutorials provide a detailed step-by-step guide to developing\napps that use App Services features. Develop a cross-platform to-do list app that\nsyncs data using the React Native SDK and Device\nSync. Develop an iOS to-do list app that syncs data using the\nSwift SDK and Device Sync. Develop an Android to-do list app that syncs data using\nthe Kotlin SDK and Device Sync. Develop a cross-platform to-do list app that\nsyncs data using the .NET SDK and Device Sync. Develop a multi-platform to-do list app that\nsyncs data using the Flutter SDK and Device Sync. Build a serverless app to track GitHub activity.",
            "code": [],
            "preview": "The resources on this page are designed to help you begin using Atlas App\nServices. To explore a working codebase that showcases App Services'\nfeatures, check out a Template App. For a more guided experience in\nlearning how to develop with App Services and Realm, start with a\nTutorial.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "authentication",
            "title": "Authentication Providers",
            "headings": [
                "Authentication Providers",
                "User Metadata",
                "Summary"
            ],
            "paragraphs": "Users log in to your client application using authentication providers. Apps provide several authentication provider options: Use a single provider when all users authenticate in the same way. For\nmore flexibility, you can enable multiple providers. Link user accounts\nfrom one provider to another with client SDKs. Anonymous Authentication  allows\nusers to view or edit data without creating an account. The  Email/Password  and\n API Key  providers let you register new\nuser accounts and API keys that are unique to your App. The OAuth providers for  Facebook ,\n Google , and  Apple  allow users to log in with their existing\naccounts using the OAuth 2.0 standard. The  Custom JWT  and  Custom\nFunction  providers allow you to\nintegrate with any authentication system using standard JSON Web\nTokens or custom code that you write. A blog or news service app might use multiple authentication providers.\nA reader might authenticate anonymously with no need to register.\nJournalists would sign in through an account with authorization to\npublish content. In this example, an anonymous user might have read-only\naccess. The journalist with an account would have write access. Atlas App Services provides these authentication providers for user login: Authentication Provider Description Authenticate without credentials. Allow users to create and interact\nwith data without creating an identity. This is great for a read-only\nuser, or one who does not need to persist important data. You can\nlater link the data from the Anonymous session with a permanent\nidentity. See the client SDK documentation for details. Authenticate with an email address and password. Client applications\nmust implement email confirmation and password reset functionality.\nRealm SDKs provide methods to simplify or customize this\nimplementation. Log in with API keys generated in the App Services admin console\nor by your end users. Use OAuth2 to log in with an Apple ID. Use OAuth2 to log in with an\n existing Google account . Use OAuth2 to log in with an\n existing Facebook account . Log in with JWT-based credentials generated by a service external\nto App Services. Log in with arbitrary credentials according to custom authentication\nlogic that you define. Each authentication provider can associate metadata fields with an\napplication user. Some providers, such as Email/Password, always add\nspecific fields. Others allow you to configure the data to associate with\neach user. To learn more, see  Authentication Provider Metadata . Authentication Provider Details Anonymous Anonymous users have no metadata. Email/Password Email/Password users always have an  email  field. This contains\nthe user's email address. API Key (Server & User) API Key users always have a  name  field. When you create an API\nkey, you give it a name. When users authenticate with API keys,\nthe  name  field contains the key name. OAuth 2.0 ( Facebook  &  Google ) OAuth 2.0 authentication services can provide user metadata. Specify\nthe metadata you want to access in the provider's\n Metadata Fields  configuration. Each user must grant\nyour app permission to access the requested data. Custom Function Custom Function authentication users do not have metadata. Custom JWT You can add metadata to JWT users. This metadata comes from data\nin the JWT that the authentication system returns. Use the\n Metadata Fields  configuration to specify the expected\nmetadata. This configuration maps fields in the JWT to fields in\nthe user object. App Services's  authentication providers  allow users to log in\nto your app. You can link a specific user across\nmultiple providers. Each authentication provider has metadata about a user's identity.",
            "code": [],
            "preview": "Users log in to your client application using authentication providers.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "graphql",
            "title": "Atlas GraphQL API [Deprecated]",
            "headings": [
                "Overview",
                "Why GraphQL?",
                "How App Services Creates GraphQL Schemas",
                "GraphQL Operations",
                "Queries",
                "Mutations",
                "Limitations"
            ],
            "paragraphs": "GraphQL is deprecated.  Learn More . The Atlas  GraphQL  API allows client applications to access\ndata stored in a linked MongoDB Atlas cluster using any standard GraphQL client. Atlas App Services automatically creates GraphQL types for every linked\ncollection that has a defined  schema  and evaluates\n role-based permissions  for all GraphQL requests.\nTo learn how to make data available through the GraphQL API,\nsee  Expose Data in a Collection . To learn about the generated types and operations that you can use with the Atlas\nGraphQL API, see  GraphQL Types & Resolvers . To extend the generated GraphQL API's functionality with custom queries and mutations,\nsee  Define a Custom Resolver . The GraphQL API lets you access data that you have stored in a\n MongoDB Atlas  cluster or\nFederated database instance. To get started, create a free cluster and  link it to\nyour App . If you don't have any data yet but you still want to explore the\nGraphQL API, consider adding a  sample data set  to your cluster. GraphQL is a declarative, strongly-typed query language for client\napplications. Clients define the exact data shape and contents that they\nneed in a single request which eliminates over-fetching problems and\ncircumvents the need for multiple costly round trips to the server. To learn more about GraphQL, check out the  official GraphQL\ntutorial . Using App Services, you generate the GraphQL schema and resolvers from JSON schemas\nfor MongoDB collections. This differs from the traditional code-first and schema-first approaches\nto GraphQL schema development. To define your GraphQL schema with App Services: Define a JSON schema  for a MongoDB collection in your MongoDB Atlas cluster.\nYou can  enforce the shape of the collection schema \nbased on custom definitions or use a generated schema based on the documents in the collection. Generate the GraphQL schema and resolvers  based on your collection JSON schema. Optionally extend the functionality of your generated GraphQL schema with\n custom resolvers . App Services automatically generates types and resolvers for data that you\n expose to the GraphQL API . The generated\ntypes and operations are all named after the base type name for each\nexposed collection. If you don't define a type name, App Services uses the\ncollection name instead. For more information on how to expose a collection and name its data\ntype, see  Expose Data in a Collection . GraphQL mutation and custom resolver requests use MongoDB\ntransactions to ensure correctness across multiple database\noperations. If any operation in a request fails, then the entire\ntransaction fails and no operations are committed to the database. A GraphQL  query  is a read operation that requests specific fields from one\nor more types. App Services automatically generates query types for\ndocuments in each collection that has a defined  schema . For more information and examples, including a list of all automatically\ngenerated query types, see  Query Resolvers . A GraphQL  mutation  is a write operation that creates, modifies, or deletes\none or more documents. App Services automatically generates mutation types\nfor documents in each collection that has a defined  schema .\nApp Services uses transactions to ensure safe writes via mutations. For more information and examples, including a list of all automatically\ngenerated mutation types, see  Mutation Resolvers . The GraphQL API can process a maximum of ten\nresolvers for a given query or mutation. If an operation specifies\nmore than ten resolvers, the entire operation\nfails with the error message  \"max number of queries reached\" . The GraphQL API can resolve  relationships  to a\nmaximum depth of five for a given query\nor mutation. If an operation specifies a relationship deeper than\nfive resolvers, the entire operation\nfails with the error message  \"max relationship depth exceeded\" . The GraphQL API expects collection schemas to have unique titles and\nraises a warning if your data model contains duplicate titles. You can safely ignore this warning if: The title conflicts only involve embedded objects. Every schema with a given title uses an identical definition,\nincluding relationships. The GraphQL API does not currently support relationships for fields\ninside arrays of embedded objects. You can use a  custom resolver  to manually look up and resolve embedded\nobject array relationships.",
            "code": [
                {
                    "lang": "graphql",
                    "value": "# Find a single movie by name\nquery {\n  movie(query: { title: \"The Matrix\" }) {\n    _id\n    title\n    year\n    runtime\n  }\n}\n\n# Find all movies from the year 2000\nquery {\n  movies(query: { year: 2000 }) {\n    _id\n    title\n    year\n    runtime\n  }\n}\n\n# Find the ten longest movies from the year 2000\nquery {\n  movies(\n    query: { year: 2000 }\n    sortBy: RUNTIME_DESC\n    limit: 10\n  ) {\n    _id\n    title\n    year\n    runtime\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "# Insert a new movie\nmutation {\n  insertOneMovie(data: {\n    title: \"Little Women\"\n    director: \"Greta Gerwig\"\n    year: 2019\n    runtime: 135\n  }) {\n    _id\n    title\n  }\n}\n\n# Update the year of a movie\nmutation {\n  updateOneMovie(\n    query: { title: \"The Matrix\" }\n    set: { year: 1999 }\n  ) {\n    _id\n    title\n  }\n}\n\n# Delete a movie\nmutation {\n  deleteOneMovie(query: { title: \"The Room\" }) {\n    _id\n    title\n  }\n}\n\n# Delete multiple movies\nmutation {\n  deleteManyMovies(query: { director: \"Tommy Wiseau\" }) {\n    _id\n    title\n  }\n}"
                }
            ],
            "preview": "The Atlas GraphQL API allows client applications to access\ndata stored in a linked MongoDB Atlas cluster using any standard GraphQL client.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "introduction",
            "title": "Introduction to Atlas App Services for Backend and Web Developers",
            "headings": [
                "What Problem Does App Services Solve?",
                "User Authentication and Management",
                "Schema Validation and Data Access Rules",
                "Event-Driven Serverless Functions",
                "Secure Client-Side Queries",
                "Client-Side MongoDB Queries",
                "Synchronize Data Across Devices",
                "Get Started with App Services",
                "Prebuilt Applications",
                "Tutorials",
                "Summary"
            ],
            "paragraphs": "As a backend developer, you can use Atlas App Services to rapidly develop cloud-based\napplications. App Services Apps can react to changes in your  MongoDB Atlas  data,\nconnect that data to other systems, and scale to meet demand. App Services\ndoes this without the need to manage database and server infrastructure. As a web developer, you have to deal with lots of overhead when sending\ndata from a server to the browser. This includes things like authentication\nand data validation. And then the application has to be scalable and secure.\nMany libraries and frameworks exist to provide these services.\nSelecting the best ones for your project requires deep understanding of the tradeoffs. As a mobile app developer, syncing data across devices poses challenges.\nYou might write a lot of custom conflict resolution code yourself,\nor you might use a platform-specific backend like CloudKit that's not designed\nfor developing cross-platform applications. Atlas Device Sync provides an\noffline-first, cross-platform solution for syncing data between a backend and\nmobile devices. When you combine this with App Services's authentication and data\nvalidation, you solve a host of common mobile app development issues. App Services is a serverless application backend that streamlines solving these\ncommon challenges. App Services provides configurable functions, integrated data access, and security rules.\nWith App Services, you can focus on building unique features instead of boilerplate backend code. Watch our  Overview of App Services \nLearning Byte to learn more. It takes time, resources, and expertise to build, administer, and maintain\nbackend infrastructure. However, a lot of these features are similar across applications.\nApp Services manages the backend infrastructure and user management for you.\nThis frees you to focus on the features that makes your app special, not boilerplate code. App Services is: Deploy server-side applications without having to set up and manage server infrastructure.\nApp Services includes provisioning, deployment, operating systems, web servers, logging, backups, and redundancy. Run cloud-based Atlas Functions that can crunch data or interact with Node.js modules. Skip writing an authentication API on top of your App Services instance.\nAuthenticated clients can interact with parts of the database based on their permissions. React to data changes in MongoDB Atlas, process data from HTTPS endpoints,\nor run Atlas Functions on a schedule with\nAtlas Triggers. Get up and running quickly for free, then scale according to the demands of your application. Pay for and receive only the exact amount of compute you need at any given time\nwith usage-based pricing. Usage under a certain amount per month is always free. Let users log in with familiar authentication providers that are built into App Services. Control which users may read and write data on a per-field basis with\n role-based permissions  that you can define declaratively or\nwith a function. Enforce data integrity by defining  validation logic . Seamlessly sync data across devices with Atlas Device Sync . Avoid writing complex synchronization logic with the  Atlas Device SDK . Secure user authentication is essential for most apps. Some apps authenticate\nusers with traditional email/password combinations. Others use API keys or third-party\nauthentication providers like Facebook and Google. These methods often require a lot of\ncomplex boilerplate code. App Services provides built-in user management and authentication methods. These\nfeatures encapsulate complexity and make it easy to integrate third-party\nauthentication providers. You can enable authentication providers in your\nserver-side configuration then immediately log in from any client application\nusing a Realm SDK. To learn more about authentication in App Services, see  Authenticate & Manage Users . Modern applications require that data is available using consistent types and formats.\nData consistency guarantees that all the application's components can work together.\nIt's also important to make sure that any given piece of data is only accessible\nto an authorized user.  For example, you might grant a user access\nto only their own data. App Services validates data with a schema you define in the\n JSON schema  standard.\nA schema serves as the source of truth for defining data types in your application.\nApp Services also uses the schema to map data between your application and a\n MongoDB Atlas collection . You can also configure App Services to secure data with role-based data access rules.\nThese rules determine each user's access to every document for every request.\nA user can only insert, read, or modify data if both they and the document\nmeet pre-defined conditions. To learn more about defining a schema and access rules for your data, see\n collection rules . Many applications require that some business logic runs on a server. Usually this\nlogic deals with sensitive user data or interacting with external services. Apps might\nalso need to respond in real time when something happens in the app or in an external service. App Services lets you define serverless JavaScript functions called Atlas Functions.\nYou can use Atlas Functions for the following common backend use cases: To learn more about defining and using serverless functions, see\n Atlas Functions  and  Atlas Triggers . Call from client applications with the Atlas Device SDK for Web. Execute Atlas Functions in response to trigger events, like an update to a document in MongoDB or at a scheduled time. HTTPS endpoints that execute a function in response to external applications. Developers need efficient, secure data access.\nIf the data is on a server, you need an API to access the data and prevent unauthorized access. The SDKs provide a secure MongoDB client to interact with data\nin your  MongoDB data sources . App Services's server-side rules ensures that users only read and modify permitted documents.\nRules consist of one or more user roles for each collection. A role determines if\na given user has permission to access and edit a document. App Services determines a role\nfor each document in every request. It then enforces the role's permissions before responding. Traditionally developers query the database on the server and access the server\nfrom the client through a HTTP API. This requires more developer resources and lacks\nthe flexibility of directly querying a database. App Services and the Atlas Device SDK includes MongoDB Data Access,\nan API that lets you access MongoDB Atlas from the client using\nthe  Query API .\nYou can query Atlas like you're using a database driver and App Services enforces\n data access rules  for all requests. The following SDKs let you query Atlas with MongoDB Data Access from client apps: Query MongoDB - Java SDK Query MongoDB - .NET SDK Query MongoDB - Node.js SDK Query MongoDB - React Native SDK Query MongoDB - Swift SDK Query MongoDB - Web SDK Synchronizing data is a hard problem for mobile app developers. Device Sync\nprovides offline-first sync capabilities for your mobile application.\nClient applications use  an offline-first persistence layer  to persist data to the device. When the device\nhas a network connection, Sync seamlessly sends the data to the App. The App\nstores the data to a linked  MongoDB Atlas data source ,\nand updates the data on other client devices. Device Sync provides  conflict resolution  to\nhandle multiple users updating the same data. When combined with  user\nauthentication and management  and  schema\nvalidation and data access , Device Sync provides a secure\nway for your mobile clients to sync data across devices and with the cloud. To explore a prebuilt app that you can customize, create a  template\napp . Template applications are working apps that use App Services\nservices to do things like: Template apps are working apps you can run and change to experiment with\nApp Services. These apps are a good choice for developers who prefer to learn by\nexperimentation, versus the tutorials, which provide a deeper, step-by-step\nprocess for building apps. Use Sync to synchronize data between mobile clients and the linked\nMongoDB Atlas collection. Host a Todo web app using  Atlas Device SDK for Web Manage event-driven  Database Triggers \nto update views in a separate collection Check out our  tutorials  to learn more about how to build\nApp Services Applications. Tutorials start with our  Template Apps \nand and then walk you through building additional features powered by App Services,\nincluding Device Sync and Role-based Permissions. App Services is a serverless application platform that makes it easy to deploy\nand scale. You can build an app with custom logic via functions and triggers,\ncustom permissions via rules, and authentication via third-party SSO. App Services's built-in  authentication system \nsimplifies user data access.\nYou can add users to your app and define  permissions  that control who can read and write data. Backend use cases for App Services include: Crunching data Reacting to data changes in MongoDB Atlas Interacting with third-party services",
            "code": [],
            "preview": "Discover how Atlas App Services can help you solve common server-client development challenges.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "data-model",
            "title": "Define a Data Model",
            "headings": [
                "Overview",
                "Document Schemas",
                "Relationships",
                "Values & Secrets",
                "Query Filters"
            ],
            "paragraphs": "Your App's data model is a description of the data that your App uses.\nYour data model ensures that your data complies with a specific structure, contains\nonly valid values, and only changes in ways that you expect. A data model consists of several components, including: Defining your App's data model is a key part of configuring and using\nDevice Sync. You don't need to define a data model to use other App\nServices. However, a data model unlocks many powerful features that can\nsave you time and let you focus on building features instead of writing\nboilerplate code. For example, once you define a data model, you can automatically generate\nsyncable Device SDK object models in your native programming language using an App\nadministration tool. To learn more, see  Generate SDK Object Models . A set of declarative schemas that describe your App's data types. A set of relationships that define logical connections between your\nschema. A set of static values that you can use to define global constants,\nenvironment values, and secret values. A set of rule-based query filters that you can use to dynamically\nlimit MongoDB operations to a subset of all data for a given request. A document schema is a JSON object that defines the structure of a\nparticular type of data that your App uses. Document schemas describe\nwhich fields a document has, the type of values those fields contain,\nand conditions that must be met for a change of value to be valid. You define schemas at the collection level. This means that you\nassociate each schema with a particular MongoDB collection that holds\nthe documents of that type. Schemas use a superset of the JSON schema\nstandard that includes support for MongoDB's built-in BSON types, which\nallows you to fully describe your data in MongoDB. Your App enforces schemas at runtime whenever data is written to\nMongoDB. This includes inserts, updates, and deletes from an API\nservice, Function, or Device Sync. To learn more, see  Schemas . A relationship is a connection between two document schemas that lets\nyou logically associate one document with zero or more other documents. In a relationship, a \"source\" document stores a reference to related\ndocuments, for example a list of related document  _id  values. When\nyou use Device Sync, App Services automatically resolves the references to the\nrelated documents so you can query them directly. To learn more, see  Relationships . A value is a named constant that you can access by name in various ways\nthroughout your App. For example, you can use a value to define the base\nURL of an external API service and then reference the value instead of\nduplicating the base URL across many  Function \ndefinitions. You can also define an environment value, which can change depending on\nyour App's  environment  tag. For example, you\nmight use a different API base URL in your development and production\nenvironments. For sensitive information like API keys, you can define a named Secret\nvalue. Secret values are never directly exposed after you define them.\nInstead, you reference them by name in configuration files and rule\nexpressions. To learn more, see  Values & Secrets . A query filter constrains read and write operations on a data source to\na subset of all of the data it contains. Query filters are standard\nMongoDB query and projection objects that are automatically concatenated\nto operations before they're sent to the data source. Filters are dynamic and can apply different conditions based on which\nuser ran the operations or other execution information. Adding a filter\nto a collection allows you to contextually limit access and can improve\nquery performance for large data sets. To learn more, see  Filter Incoming Queries .",
            "code": [],
            "preview": "Your App's data model is a description of the data that your App uses.\nYour data model ensures that your data complies with a specific structure, contains\nonly valid values, and only changes in ways that you expect.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "activity",
            "title": "Monitor App Activity",
            "headings": [
                "App Logs",
                "Error Logs",
                "Log Filters",
                "Log Lines",
                "Log Retention",
                "App Metrics",
                "Atlas Alerts"
            ],
            "paragraphs": "App Services keeps a log of application events, records metrics that\nsummarize your App's usage and performance, and publishes notifications\nto your Atlas project's activity feed. App Services logs all incoming requests and application events. These\nevents include API requests, Device Sync operations, Trigger execution,\nUser Authentication, and other activities. App Services saves logs for\n10 days, after which they are deleted. To learn how to view, filter, and analyze your application logs, see\n View Application Logs . A log entry describes a single application event of a given type. Each\nentry contains information about the event and how the system responded.\nFor example, a log may include the IP address that issued a request as\nwell as a summary of the data access permissions that were evaluated and\nassigned to serve the request. Apps log the following event types: Authentication , including user\ncreation, login, and deletion. Change Stream , including any time a user\nopens or closes a stream of change events. Device Sync , including all events related to\ndata synchronization between client devices and MongoDB Atlas. Endpoint , including any time a request is made\nto a Data API endpoint. Function , including both Atlas Functions\nas well as individual queries to linked MongoDB Atlas data sources\nmade using the Atlas Device SDK. Service , including HTTPS endpoints and\nservice action calls issued from the Atlas Device SDK. Schema , including any events related to\nchanges to an application's schema. Trigger , including Database Triggers,\nAuthentication Triggers, and Scheduled Triggers. All App Services log entries have one of two possible statuses: For example, App Services would log an error for any of the following\nevents: OK , which represents a normal event that succeeded without an error. Error , which represents an event that did not run successfully for\nany reason. You attempt to access data stored in Atlas for which there is no\napplicable rule. You throw or fail to handle an error or promise rejection in an Atlas\nFunction. You call  context.services.get()  for a service which does not exist. For performance reasons, App Services limits individual queries to a\nmaximum of 100 log entries per page. You can filter entries by type,\nstatus, timestamp, user, and request ID to return only logs that are\nrelevant to your query. Functions can log information using JavaScript's  console.log() \nmethod. App Services stringifies each console log and stores each string\nas a single line. App Services truncates lines to 512 bytes in length.\nFor ASCII character sets, this translates to 512 characters; depending\non the character set you use, you may see truncation at lower character\ncounts. App Services saves only the first 25 log lines for a given log entry. App Services retains logs for 10 days, after which they\nare deleted. If you require logs older than 10 days, you can\nautomatically  forward logs  to another service. You\ncan also download a dump of currently available logs from the UI or use\nthe  Admin API Logging endpoints  to\nfetch logs before they expire. App Services constantly measures your App's usage and records aggregate\nmetrics over time. You can access and use the metrics to assess your\nApp's performance and see trends in how your App is used. For example,\nyou can see how much time your App spent performing computations or find\nthe 95th percentile response time for Data API requests. To learn more about which metrics are available and how to access them,\nsee  App Services Metrics . Your App publishes alert events to your Atlas project's activity feed.\nApp Services alerts for administrative events, such as when someone\ndeploys changes to an App or when something like a Trigger or Device\nSync process fails and cannot restart automatically. To learn more about\nyour App's alerts, see  Activity Feed & Atlas App Services Alerts .",
            "code": [],
            "preview": "Monitor your App activity through logs of application events, metrics, and notifications.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "domain-migration",
            "title": "Domain Migration",
            "headings": [
                "Relevant SDK Versions",
                "Begin Migration Now",
                "Admin API",
                "Client Applications",
                "Domain Access Lists",
                "Regions-Specific Base URLs"
            ],
            "paragraphs": "The App Services base URL is being updated from  https://realm.mongodb.com/ \nto a new URL:  https://services.cloud.mongodb.com . This change applies to the App Services UI, Admin API requests, and Client\nAPI requests through the Atlas Device SDKs. Starting in the following versions, the SDKs will use the new base URL by\ndefault: C++ SDK: version TBD Flutter SDK: version TBD Java SDK: version TBD Kotlin SDK: version TBD .NET SDK: version TBD Node.js SDK: version TBD React Native SDK: version TBD Swift SDK: version TBD The new URL ( https://services.cloud.mongodb.com ) is available now and you\ncan begin using it by specifying a base URL in your app configuration. You should\nalso update your domain access lists to allow the new URL. The old URL ( https://realm.mongodb.com ) will continue to work, but will be\ndeprecated in the future. We recommend migrating to the new URL as soon as\npossible. To avoid service disruptions, update all Admin API requests to use the new URL\n( https://services.cloud.mongodb.com ). You can do this now. While the old URL will continue to work for some time, it will eventually be\ndeprecated. Begin using the new URL by specifying it as the base URL in your App\nConfiguration. Refer to the SDK docs for details about specifying a base URL: C++ SDK Flutter SDK Java SDK Kotlin SDK .NET SDK Node.js SDK React Native SDK Swift SDK If you use a domain access list, add the new domain to this list to avoid\nservice disruptions: If your app uses a local region instead of the global endpoint: Update the local region domain in your App Configuration's base URL to avoid\nservice disruptions:",
            "code": [
                {
                    "lang": "text",
                    "value": "*.services.cloud.mongodb.com"
                },
                {
                    "lang": "text",
                    "value": " https://us-east-1.aws.realm.mongodb.com"
                },
                {
                    "lang": "text",
                    "value": "https://us-east-1.aws.services.cloud.mongodb.com"
                }
            ],
            "preview": "The App Services base URL is being updated from https://realm.mongodb.com/\nto a new URL: https://services.cloud.mongodb.com.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync",
            "title": "Atlas Device Sync",
            "headings": [
                "Key Features",
                "Get Started",
                "Explore the Device Sync Documentation",
                "Maintain Data Integrity with Schemas",
                "Configure & Enable Device Sync",
                "Protect User Data with Permissions",
                "Handle Errors",
                "Go to Production",
                "End-to-End Security"
            ],
            "paragraphs": "You're developing a mobile app. Your users want their data saved in the cloud\nand accessible from their other devices. Network access on a mobile device\ncan be intermittent, so you write data locally on the device first. A\nbackground process then synchronizes the data to the cloud and resolves any\nconflicting writes. Atlas Device Sync provides all of the above, so you can build better apps faster. Watch our  Intro to Atlas Device Sync \nLearning Byte to learn more. Atlas Device Sync is a bridge between client apps using the  Realm SDKs \nand a MongoDB instance running in Atlas. Realm is a lightweight database\noptimized for mobile development. Device Sync handles conflicts for you, so you don't have to write\ncomplex custom code to resolve conflicting writes from multiple clients.\nA user-based permissions system lets you control who can access which data. Realm Database and Device Sync seamlessly handle intermittent\nconnectivity so users can continue using your app regardless of their\ncurrent network status. The best way to start using Device Sync is with a template starter app.\nIn the Atlas App Services UI, under the  App Services  tab, you can select a\ntemplate. Atlas configures a backend instance and gives you the\nfrontend code for a selection of platforms. The mobile app tutorials guide you through building a feature on top of a\ntemplate starter app with Atlas Device Sync. Define your application data model with standard schemas that you can use\nto validate data and generate language-specific classes. Read the\n Sync Data Model Overview  to get oriented. Whether you based your app on one of our template apps or started from\nscratch, at some point you'll want to configure Sync specially for your\nneeds. Read more:  Configure and Enable Atlas Device Sync . Device Sync provides a user-based permissions system that lets you\ncontrol who can access which data. To learn how to set up permissions for\na variety of real use cases, check out the\n Device Sync Permissions Guide . Develop a robust app that can handle schema changes and data recovery\nscenarios with the  Client Resets  documentation. Troubleshoot other\nerrors and edge cases with the  Sync Errors  reference. Test your app under a simulated load with the\n Sync Production Load Testing  guide, then get your app ready to ship\nwith the  Device Sync Production Checklist . To understand how costs scale\nwith usage beyond the free tier, refer to the  Billing  page. Encrypt sensitive data in Realm Database files with the encryption APIs. Built-in user authentication providers include anonymous, email/password,\nAPI key, Custom Function, Custom JWT, Facebook, Google, and Apple. Device Sync encrypts all network traffic using Transport Layer Security\n(TLS). Document and field-level permissions determine which data may be\nsynced to the device. MongoDB Atlas has features that meet essential compliance standards: Encrypted storage volumes Network isolation Role-based access management ISO, SOC, PCI, HIPAA, HITRUST, VPAT, GDPR, CSA",
            "code": [],
            "preview": "Define your application data model with standard schemas that you can use\nto validate data and generate language-specific classes. Read the\nSync Data Model Overview to get oriented.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "",
            "title": "What are the Atlas Application Services?",
            "headings": [
                "Build Data-Driven Apps And Services",
                "Install the Atlas Device SDK",
                "Enable and Configure Sync",
                "Read and Write Synced Data",
                "Enable the Data API CRUD & Aggregation Endpoints",
                "Define Custom Endpoints",
                "Secure Your API",
                "Send a Data API Request",
                "Create and Configure Your Trigger",
                "Write the Trigger's Handler Logic",
                "Fire the Trigger Automatically",
                "Power Your Workload With Secure Backend Services",
                "Join The Community And Learn From Experts"
            ],
            "paragraphs": "Atlas Application Services are fully-managed backend services and APIs\nthat help you build apps, integrate services, and connect to your Atlas\ndata faster. Get started now by building on our  free tier . Services like Device Sync, APIs, and Triggers make it easy to build and\nrun virtually any app or backend service on top of Atlas. Device Sync uses the SDK's native language objects to define a\ndata model and work with data. Everything you need to add\nsync to your app is included in the Atlas Device SDK. To install the SDK, see the  SDK documentation  for your language and platform. In your Atlas App, enable Device Sync and define read and\nwrite permissions to control the data that users can sync.\nOnce Sync is turned on, you can start syncing from any app\nwith the SDKs. To enable Sync, see  Enable Atlas Device Sync . Data that you subscribe to with the SDK is automatically\nkept in sync with your Atlas cluster and other clients.\nApps keep working offline and deterministically sync changes\nwhenever a network connection is available. To learn how to read, write, and subscribe to data, see the\n SDK documentation  for your language and\nplatform. The MongoDB Atlas Data API lets you read and write data in\nAtlas using HTTPS requests. Once enabled, you can use a\nbuilt-in endpoint to work with data. See  Data API Endpoints . You can tailor your API with custom endpoints for your\napplication. Each endpoint handles incoming HTTPS requests\nfor a specific route and returns configurable HTTPS\nresponses. See  Custom Endpoints . Generate an API key that you can use to authenticate\nrequests. Alternatively, you can use other authentication\nprovider credentials associated with your app. For details, see  Data API Authentication . Send requests from any server-side environment that supports\nHTTPS. Set up a trigger to respond to events like database\noperations or user logins as they occur. Or, configure a\ntrigger to run on a periodic schedule. For each trigger type\nyou can define custom conditions to control exactly when the\ntrigger should fire. To learn more, see  Triggers . When a trigger fires, it passes the event data to a handler\nthat can perform any action you want. You can run a\nserverless JavaScript function or pass the event to AWS\nEventBridge. Once deployed, the trigger constantly listens for events\nthat it is configured to fire on. When an event causes the\ntrigger to fire, it automatically runs the handler logic you\ndefined. Run your application's backend logic and connect to external\nservices with functions that automatically scale to meet demand. Securely access data from anywhere with dynamic role-based\npermissions. Define your application data model with standard schemas that you\ncan use to validate data and generate language-specific classes. Let users register and log in to your app with their preferred\nauthentication method. Define and access global values and private secrets from your\napp's functions and configuration files. Deploy your app in any of the three major cloud providers with\nregions around the globe. Meet other MongoDB developers and find tutorials and examples that\nuse your favorite tools. Submit and vote on feature requests for MongoDB products. Get help from MongoDB's official team of support specialists.",
            "code": [
                {
                    "lang": "bash",
                    "value": "curl -X POST \\\n     https://data.mongodb-api.com/app/<YOUR APP ID>/endpoint/data/v1/action/insertOne \\\n     -H 'api-key: <YOUR API KEY>' \\\n     -H 'Content-Type: application/json' \\\n     -d '{\n       \"dataSource\": \"mongodb-atlas\",\n       \"database\": \"learn-data-api\",\n       \"collection\": \"hello\",\n       \"document\": {\n         \"text\": \"Hello from the Data API!\",\n       }\n     }'"
                }
            ],
            "preview": "Services like Device Sync, APIs, and Triggers make it easy to build and\nrun virtually any app or backend service on top of Atlas.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "security",
            "title": "Secure Your App",
            "headings": [
                "Overview",
                "Application Users",
                "Data Access Permissions",
                "App Services Applications with Sync",
                "MongoDB Atlas Data Sources",
                "Values and Secrets",
                "Summary"
            ],
            "paragraphs": "Atlas App Services provides a variety of security features to protect your data\nand prevent unauthorized access to your application. This includes things\nlike: Built-in user management Data access permissions Network security features The ability to store and substitute values and secrets You can secure App Services Apps with  built-in user management . With the built-in user management of App Services,\nonly authorized users can access your App. You can\n delete  or  disable  users, and\n revoke user sessions . Users can log in with: You can enable one or more  authentication providers  in the App Services backend, and then\nimplement them in your client code. You can also link user accounts with\nclient SDKs. Existing provider credentials, such as  Facebook ,  Google , or\n Apple ID New credentials with  email/password ,\n custom JWT , or  custom function\nauthentication Anonymous authentication , if you don't\nneed to store user data Use App Services  data access rules  to grant read and write\naccess to data. Apps that use Atlas Device Sync define data access permissions\nduring the process of enabling Device Sync. Apps that do not use Device Sync can\nlink an MongoDB Atlas data source, and define permissions to perform CRUD\noperations on that data source. MongoDB data access rules prevent operations where users do not have\nappropriate permissions. Users who do not meet your data access rules\ncannot view or modify data. Atlas Device Sync allows you to define data access rules that determine which\nusers can read or write which data. To learn how to configure these rules, refer\nto  Role-based Permissions . When you access  MongoDB Atlas \nthrough App Services, you can define roles that enable users to read and\nmodify data. App Services uses a strict rules system that prevents\nall operations unless they are explicitly enabled. When you  define a role , you create a\nset of CRUD permissions that App Services evaluates individually for each\ndocument associated with a query. You can set roles to have document-level\nor field-level access, and you can give roles read or read and write access.\nApp Services blocks requests from roles that do not have permission to\nsearch or read data. When you access MongoDB Atlas through an App with Atlas Device Sync enabled, the\npermissions you define for Device Sync apply, instead of the\n role-based permissions  you define\nwhen you link an MongoDB Atlas data source. App Services enables you to define  values and secrets \nthat you can access or link to from your application. This enables you to\nremove deployment-specific configuration data and sensitive information from\nyour app's business logic. Instead, you refer to it by name and\nApp Services substitutes the value when executing your request. Built-in user management handles authentication and ensures only logged-in\nusers can access your App. Data access permissions enable you to specify read and write permissions for\nAtlas Device Sync, linked MongoDB Atlas data sources, and developers building your apps. Network security features enable you to guard against unauthorized access\nfrom unknown IP addresses or URLs. Store values and secrets and refer to them by name to remove sensitive\ninformation from your business logic.",
            "code": [],
            "preview": "Atlas App Services provides a variety of security features to protect your data\nand prevent unauthorized access to your application. This includes things\nlike:",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "mongodb",
            "title": "Connect to MongoDB Data Sources",
            "headings": [
                "Overview",
                "Read, Write, and Aggregate Data",
                "Secure and Validate Data",
                "Automatically Sync Data",
                "React to Changing Data",
                "Link a Data Source",
                "Navigate to the Data Source Management Screen",
                "Link a New Data Source",
                "Save the Linked Data Source",
                "Pull the Latest Version of Your App",
                "Create a Data Source Configuration Directory",
                "Add a Data Source Configuration File",
                "Deploy the Data Source Configuration",
                "Data Source Limitations",
                "Atlas Data Federation",
                "Serverless Instances"
            ],
            "paragraphs": "A  data source  represents a  MongoDB Atlas \ninstance in the same project as your app. You use data sources to store\nand retrieve your application's data. Most apps connect to a single data source, but you can configure\nmultiple data sources if your data is spread across instances. You\ndefine a unique name for each data source linked to your application and\nuse the name to refer to the data source throughout your app. Requests to data sources are routed through Atlas App Services. Because of this,\nApp Services automatically opens and closes database connections. This means you\ndon't need to worry about calling  db.close()  when using a data source. All internal communication between App Services and Atlas is encrypted with\nx509 certificates. You can read and write data in a data source from a server-side function\nor connect from a client application. You write queries using standard\nMongoDB query syntax. To learn how to work with a data source in an Atlas Function, refer to the\nfollowing guides: To learn how to work with a data source from a Realm SDK,\nrefer to  Query Atlas from Client Apps . Read Data from MongoDB Atlas Write Data in MongoDB Atlas Aggregate Data in MongoDB Atlas Linked data sources do not support all MongoDB CRUD and Aggregation\noperations. Some operations are not available when you query MongoDB\nas a specific  user  due to overhead\nfrom schema validation and data access rules. You can bypass some\nlimitations by querying MongoDB as the  system user  instead. For more information on which operations are supported, see\n CRUD & Aggregation APIs . Data sources allow you to define  access rules  and\n document schemas  for the data they contain. Rules\ndynamically authorize users to read and write subsets of your data and\nschemas control the shape and contents of each document. Read and write operations on a linked data source are secure by default.\nA user cannot read or write any data unless a rule explicitly allows\nthem to. Any data inserted or modified by an authorized user must\nconform to the corresponding schema. If you do not define rules for a collection, queries on the collection\nwill fail. This means that you can safely expose a properly configured\ndata source interface in client applications without risking data\ncorruption or leaks. Atlas Device Sync applications store data and sync data changes to a linked\ncluster, called the  synced cluster . The sync protocol ensures that\nyour data is eventually consistent across all sync clients and the\nsynced cluster. To learn more about Atlas Device Sync and how it interacts with your app's data\nsources, see  Atlas Device Sync . In order to use Atlas Device Sync, your cluster must run MongoDB version\n4.4 or newer. You can create database triggers that run functions automatically in\nresponse to changing data. Triggers use MongoDB change streams to\nobserve the data source and execute a function whenever a change event\nmatches the trigger configuration. A trigger function can run arbitrary\ncode and can access a change event for detailed information about the\nchange that caused it to run. To learn more about how triggers work and how to define your own, see\n Database Triggers . Triggers are only available for data sources that support change\nstreams. You cannot define triggers on a Federated database instance or\nserverless Atlas instance. A configured data source in your app is  linked  to the underlying\ninstance in Atlas. You can link multiple instances to your app\nand even create multiple data sources that link to the same underlying\ninstance. You can configure a new linked data source in the App Services UI or by defining\nand pushing a configuration file with the  App Services CLI  or GitHub deployment: In the App Services UI, click  Linked Data Sources  under  Manage \nin the left navigation menu. Click  Link a Data Source  and provide the following\nconfiguration information on the  Data Source Configuration  screen: Field Description Data Source A MongoDB Atlas cluster or Federated database instance associated with the same project as\nyour App. App Services Service Name A name for the App Services service that connects to the data source.\nYou will use this name when referring to the data source in other\nparts of your application, such as when you instantiate a\nMongoDB service client. Consider a MongoDB cluster data source with the  App Services\nService Name   myAtlasCluster . To create a service client in a\n function  you would use the following code: Required for Atlas clusters. Not enabled for Federated database instances.\nA boolean indicating whether App Services should allow\nclients to  connect to this cluster with a connection string over\nthe wire protocol . Read Preference Required for Atlas clusters. Not available for Federated database instances. Specifies the\n read preference  of the cluster. The default\nread preference (primary) should be sufficient for most use cases. Once you've selected and configured a MongoDB cluster or Federated database instance,\nclick  Save . App Services immediately begins the\nprocess of linking to the data source, which could take up to five\nminutes. To link a MongoDB Atlas cluster or Federated database instance with the App Services CLI, you need a\nlocal copy of your application's configuration files. To pull a local copy of the latest version of your app, run the following: You can also download a copy of your application's configuration files from\nthe  Deploy > Export App  screen in the App Services UI. Create a new subdirectory with the name that you'll use for the data source in\n /data_sources . Add a file named  config.json  to the data source subdirectory. The file can\nconfigure either a MongoDB Atlas cluster or a Federated database instance. The configuration file should have the following general form: For detailed information on the contents of a cluster configuration\nfile, see  Linked MongoDB Cluster Configuration . For more information, see  Federated database instance Configuration . Once you've defined and saved a  config.json  file for the data\nsource, you can push the config to your remote app. App Services\nimmediately begins the process of linking to the data source, which\ncould take up to five minutes. A linked data source can represent one of several instance MongoDB Atlas\ninstance types. Depending on the type of the underlying instance, not\nall functionality is supported. Once you've linked a data source, you cannot change the underlying\ninstance type. Instead, you can link a new data source with another\ninstance type. You can link a  Federated database instance  to your app\nas a MongoDB data source. However, there are some caveats to keep in\nmind when working with Atlas Data Federation: Federated data sources  do not support write operations . You can only access a Federated data source from a  system function . You cannot connect to a Federated data source via the  wire protocol . You cannot define  roles and permissions  for a Federated data source. You cannot set a  read preference  for a Federated data source. You cannot create a  database trigger  on a Federated data source. You cannot use a Federated data source as your app's  Device Sync  cluster. You can link a  serverless instance  to\nyour app as a MongoDB data source. However, serverless instances do not\ncurrently support change streams, so the following features are limited: You cannot create a  database trigger  on a serverless instance. You cannot use a serverless instance as your app's  Device Sync  cluster. You cannot watch collections for changes data sources that are serverless MongoDB Atlas instances.",
            "code": [
                {
                    "lang": "javascript\n// example Atlas Function",
                    "value": "exports = async function() {\n   const mongodb = context.services.get(\"mongodb-atlas\");\n   return mongodb\n      .db(\"myDatabase\")\n      .collection(\"myCollection\")\n      .findOne()\n}"
                },
                {
                    "lang": "javascript",
                    "value": "const myAtlasCluster = context.services.get(\"myAtlasCluster\");"
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "shell",
                    "value": "mkdir -p data_sources/<Data Source Name>"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Service Name>\",\n  \"type\": \"mongodb-atlas\",\n  \"config\": {\n    \"clusterName\": \"<Atlas Cluster Name>\",\n    \"readPreference\": \"<Read Preference>\",\n    \"wireProtocolEnabled\": <Boolean>,\n    \"sync\": <Sync Configuration>\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Service Name>\",\n  \"type\": \"datalake\",\n  \"config\": {\n     \"dataLakeName\": \"<Federated database instance name>\"\n   }\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=\"<Your App ID>\""
                }
            ],
            "preview": "Learn how to connect your Atlas App Services App to a MongoDB Atlas data source.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "users",
            "title": "Authenticate & Manage Users",
            "headings": [
                "Introduction",
                "Concepts",
                "Authentication Providers",
                "User Accounts",
                "Authentication Provider Identities",
                "Active User",
                "System User",
                "User Sessions",
                "Summary"
            ],
            "paragraphs": "Atlas App Services manages authentication for your application's end users.\nApp Services: Through user accounts, you can store and access metadata and custom\ndata for each user. Users log in through  authentication providers . Each provider represents a specific method\nof authentication. App Services includes built-in providers for common use cases, such as Facebook\nand Google. Custom providers allow you to integrate any external\nauthentication system. The following diagram shows how your client app, Atlas App Services, and\nauthentication provides interact to authenticate users: Uses role-based  data access rules  to determine\nread & write permissions. Associates every request with an authenticated user Evaluates permissions for every object included the request. In App Services, an  authentication provider  is a modular service.\nThese services provide identity verification, and maintain information\nabout app users. Users authenticate themselves by providing a set of credentials to an\nauthentication provider. With valid credentials, the provider returns a\nunique identity associated with the user. App Services logs them in as\nthe  active user . App Services includes built-in authentication providers for common use cases.\nThis includes: You can configure custom providers to integrate external authentication\nsystems. Anonymous users Email/password combinations API keys OAuth 2.0 through  Facebook ,  Google , and  Apple ID Custom JWT : provider accepts JSON\nweb tokens signed by the external system. Custom Function : provider allows\nyou to define custom login logic in an Atlas Function. Every application must have at least one authentication provider\nconfigured and enabled. Without at least one provider, no client\napplication can connect. To learn how to configure and enable\nauthentication providers, see:  Authentication Providers . A  user account  represents a single, distinct user of\nyour application. App Services creates the user when an authentication provider\nvalidates a unique identity. You can source user metadata, such as email\nor birthday, from authentication providers. You can associate each user\nwith  custom data . Apple  requires that applications distributed through the App Store  must give any user who creates\nan account the option to delete the account. Whether that app uses an\nauthentication method where you must manually register a user, such as\nemail/password authentication, or one that automatically creates a\nuser, such as Sign-In with Apple, an app distributed through the App Store\nmust implement  user account deletion . App Services stores login metadata for a user in an\n authentication provider identity . App Services uses this metadata to\nauthenticate the user. Upon first login with an authentication provider, App Services creates an\nidentity object. Each object contains a unique ID, and\n provider-specific metadata  about the user. On later\nlogins, App Services refreshes the existing identity data. A single user account can have more than one identity. Realm SDKs\nenable you to link identities to existing user accounts. This allows users\nto log in to a single account with more than one provider. For more\ninformation, see the documentation on linking identities for your\nclient SDK. In the Realm SDKs, you can log in more than one user, but only one account\ncan be  active  at any given time. The  active user  is a user account\nassociated with an application request. App Services executes requests from client applications as the active user.\nApp Services replaces dynamic references to the user - e.g.  %%user \nin a JSON expression - with the active user. You can use a specific active user, or the system user, to execute\nFunctions. The  system user  is an internal user that has advanced privileges.\nThe system user bypasses all rules. You can execute  functions  as a system user instead of the user making a request.\nTriggers run in the context of the system user. The system user is useful for administrative tasks. This includes: Tasks that need to circumvent rules and queries Tasks that need  unrestricted access  to MongoDB CRUD\nand aggregation operations Rules do not apply to the system user. Functions run as the system\nuser can become a security liability. Ensure that you do not expose\nthese functions to unauthorized users. For example, use  function context  to check if\nthe active user can call the system function. Define a condition to\ndetermine whether the user has the appropriate permissions, e.g.: A  user session  represents a period of time when an authenticated\nuser can interact with your app. User sessions begin when a user logs in\nthrough an SDK or authenticates over HTTPS. A session ends after\n30 minutes unless an SDK or API request refreshes\nthe session. To learn how to create, work with, and revoke user sessions, see\n Manage User Sessions . App Services supports authentication and user accounts through a variety of\n authentication providers . You can\nassociate users with more than one authentication provider. App Services supports having more than one user logged in at the same\ntime. There is only one  active user  at a time. The  system user  is a special user that bypasses\nall rules. Realm SDKs manage the access and refresh tokens that comprise a\n user session .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const activeUser = context.user\n  const adminUserId = context.values.get(\"adminUserId\");\n  if(activeUser.id == adminUserId) {\n    // The user can only execute code here if they're an admin.\n  } else {\n    throw Error(\"This user is not allowed to execute the system function\")\n  }\n}"
                }
            ],
            "preview": "Atlas App Services manages authentication for your application's end users.\nApp Services:",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "help",
            "title": "Get Help",
            "headings": [
                "Overview",
                "Professional Support",
                "Community Forums",
                "Stack Overflow",
                "Feature Requests"
            ],
            "paragraphs": "Atlas App Services provides various resources for getting help with your App. MongoDB offers professional support for App Services and Atlas. Paid\nsupport includes help with training, upgrades, and general technical\nsupport for issues in your database or app. We strongly recommend\nMongoDB's professional support for production apps and mission-critical\nuse cases. To learn more about professional support, see  Atlas Support &\nSubscriptions . To contact support, visit the  MongoDB Help Center . The official  MongoDB Community Forums  are a great\nplace to meet other developers, ask and answer questions, and stay\nup-to-date with the latest App Services features and releases. You can also\ninteract with MongoDB employees, like our community team, engineers, and\nproduct managers, who are active forum contributors. MongoDB monitors & answers questions with the  realm  tag on Stack\nOverflow, a forum to ask programming questions and get help with\nspecific errors.  Ask an App Services- or Realm-related question  or read through\nthe  existing questions . To request a feature, check the status of your feedback, or see top feature\nrequests from other users, check out the  MongoDB Feedback Engine for App Services .",
            "code": [],
            "preview": "Atlas App Services provides various resources for getting help with your App.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "data-api",
            "title": "Atlas Data API",
            "headings": [
                "Endpoints",
                "How the Data API Works",
                "When to Use the Data API"
            ],
            "paragraphs": "The Data API is a managed service that lets you securely work with data\nstored in Atlas using standard HTTPS requests. The Data API is not a\ndirect connection to your database. Instead, the API is a fully-managed\nmiddleware service that sits between your cluster and the clients that\nsend requests. You can use the Data API to connect to MongoDB Atlas from any platform\nthat supports HTTPS, including: You don't need to install any database drivers or opinionated libraries\nto work with the Data API. Instead, you send standard HTTPS requests\nlike the following: Web browsers Web servers CI/CD pipelines Serverless & Edge compute environments Mobile applications Internet-Of-Things devices The Data API supports two types of endpoints: Data API Endpoints  are automatically generated endpoints that each\nrepresent a MongoDB operation. You can use the endpoints to create,\nread, update, delete, and aggregate documents in a MongoDB data\nsource. To learn more, including how to set up and call endpoints, see\n Data API Endpoints . Custom Endpoints  are app-specific API routes handled by Atlas\nFunctions that you write. You can use custom endpoints to define\noperations that fit your use case specifically. For example, you could\ncreate an endpoint that runs a pre-defined aggregation or that\nintegrates with an external webhook service. To learn more, including how to create and call endpoints, see\n Custom HTTPS Endpoints . Data API requests may resemble traditional database operations, like\n find  or  insertOne , but the Data API is not a direct connection\nto your database. Instead, the Data API adds additional authentication,\nauthorization, and correctness checks to ensure that your data is only\naccessed or modified in the ways you allow. This allows you to safely\naccess data in Atlas from potentially vulnerable clients like web apps. For each incoming request, the Data API: Authenticates the calling user.  This might involve validating an\naccess token, logging in with header credentials, or directly\nassigning a specific runtime user based on your configuration. Authorizes the request.  This ensures that the user sent a\nwell-formed request and has permission to perform the requested\noperation based on your endpoint authorization scheme. Runs the requested operation.  This might involve reading or\nwriting data in Atlas with a generated endpoint or invoke a custom\nfunction that you wrote. For requests that read or write data in Atlas, the Data API also\nenforces the access control rules and document schemas defined in\nyour App. This means that users can only access data they're allowed\nto read and write. Requests fail if they include an invalid write\noperation. Returns an HTTPS response to the caller.  The response includes\nthe result of a generated endpoint operation or any data that you\nreturn from a custom endpoint. In the request, you can choose to receive the\nresponse in either  JSON or EJSON format . For server applications, and especially for high-load and latency\nsensitive use-cases, we recommend connecting directly to Atlas with a\nMongoDB driver. Operations called through a Data API endpoint take\nlonger to complete than the corresponding MongoDB operations called\nthrough a driver. Additionally, the drivers provide more flexibility and\ncontrol over how your operations are executed. To learn more, visit the\n MongoDB Drivers  documentation. We recommend using the Data API when: You want to run MongoDB operations from a web application or other\nclient that you can't trust. You can't or don't want to manage a MongoDB driver in your server-side\nenvironment. For example, some edge compute environments don't support\ndatabase drivers or connection pooling. You want to develop a new feature and prefer a flexible solution for\nworking on the client side first before later creating and refining\nthe API layer. You want to integrate Atlas data access into a federated API gateway. You want to connect to App Services from an environment not currently\nsupported by a Realm SDK and don't want to use a driver to connect\nover the  wire protocol .",
            "code": [
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/data/v1/action/insertOne\" \\\n  -X POST \\\n  -H \"Content-Type: application/ejson\" \\\n  -H \"Accept: application/json\" \\\n  -H \"apiKey: TpqAKQgvhZE4r6AOzpVydJ9a3tB1BLMrgDzLlBLbihKNDzSJWTAHMVbsMoIOpnM6\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"learn-data-api\",\n    \"collection\": \"hello\",\n    \"document\": {\n      \"text\": \"Hello, world!\"\n    }\n  }'\n"
                },
                {
                    "lang": "json",
                    "value": "{ \"insertedId\": \"63dc56ac74ddb86ed3eb8474\" }"
                }
            ],
            "preview": "The Data API is a managed service that lets you securely work with data\nstored in Atlas using standard HTTPS requests. The Data API is not a\ndirect connection to your database. Instead, the API is a fully-managed\nmiddleware service that sits between your cluster and the clients that\nsend requests.",
            "tags": "json web token",
            "facets": {
                "programming_language": [
                    "shell",
                    "json"
                ],
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "triggers",
            "title": "Atlas Triggers",
            "headings": [
                "Trigger Types",
                "Limitations",
                "Atlas Function Constraints Apply",
                "Event Processing Throughput",
                "Number of Triggers Cannot Exceed Available Change Streams",
                "Diagnose Duplicate Events"
            ],
            "paragraphs": "Atlas Triggers execute application and database logic. Triggers\ncan respond to events or use pre-defined schedules. Triggers listen for events of a configured type. Each Trigger links to a\nspecific  Atlas Function .\nWhen a Trigger observes an event that matches your\nconfiguration, it  \"fires\" . The Trigger passes this event object as the\nargument to its linked Function. A Trigger might fire on: App Services keeps track of the latest execution time for each\nTrigger and guarantees that each event is processed at least once. A specific  operation type  in a given Collection. An authentication event, such as user creation or deletion. A scheduled time. App Services supports three types of triggers: Database triggers \nrespond to document insert, changes, or deletion. You can configure\nDatabase Triggers for each linked MongoDB collection. Authentication triggers \nrespond to user creation, login, or deletion. Scheduled triggers \nexecute functions according to a pre-defined schedule. Triggers invoke Atlas Functions. This means they have the same\nconstraints as all Atlas Functions. Learn more about Atlas Function constraints. Triggers process events when capacity becomes available. A Trigger's\ncapacity is determined by its event ordering configuration: Trigger capacity is not a direct measure of throughput or a guaranteed\nexecution rate. Instead, it is a threshold for the maximum number of\nevents that a Trigger can process at one time. In practice, the rate at\nwhich a Trigger can process events depends on the Trigger function's run\ntime logic and the number of events that it receives in a given\ntimeframe. To increase the throughput of a Trigger, you can try to: Ordered triggers process events from the change stream one at a time\nin sequence. The next event begins processing only after the previous\nevent finishes processing. Unordered triggers can process multiple events concurrently, up to\n10,000 at once by default. If your Trigger data source is an M10+\nAtlas cluster, you can configure individual unordered triggers to\nexceed the 10,000 concurrent event threshold. To learn more, see\n Maximum Throughput Triggers . Optimize the Trigger function's run time behavior. For example, you\nmight reduce the number of network calls that you make. Reduce the size of each event object with the Trigger's\n projection filter . For the best\nperformance, limit the size of each change event to 2KB or less. Use a match filter to reduce the number of events that the Trigger\nprocesses. For example, you might want to do something only if a\nspecific field changed. Instead of matching every update event and\nchecking if the field changed in your Function code, you can use the\nTrigger's match filter to fire only if the field is included in the\nevent's  updateDescription.updatedFields  object. App Services limits the total number of Database Triggers. The size of your\nAtlas cluster drives this limit. Each Atlas cluster tier has a maximum number of supported change\nstreams. A Database Trigger requires its own change stream. Other App Services\nalso use change streams, such as Atlas Device Sync. Database Triggers\nmay not exceed the number of available change streams. Learn more about the number of supported change streams for Atlas tiers. During normal Trigger operation, Triggers do not send duplicate events.\nHowever, when some failure or error conditions occur, Triggers may deliver\nduplicate events. You may see a duplicate Trigger event when: If you notice duplicate Trigger events, check the  App Logs  for suspended\nTriggers or server failures. A server responsible for processing and tracking events experiences a\nfailure. This failure prevents the server from recording its progress in a\ndurable or long-term storage system, making it \"forget\" it has processed\nsome of the latest events. Using unordered processing where events 1 through 10 are sent simultaneously.\nIf event 9 fails and leads to Trigger suspension, events like event 10 might\nget processed again when the system resumes from event 9. This can lead to\nduplicates, as the system doesn't strictly follow the sequence of events and\nmay reprocess already-handled events.",
            "code": [],
            "preview": "Use Atlas Triggers to execute application and database logic in response to events or schedules.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "graphql/cli",
            "title": "Run GraphQL Operations from a CLI",
            "headings": [
                "Overview",
                "Run a Query",
                "Run a Mutation"
            ],
            "paragraphs": "You can access your App's  Atlas GraphQL API \nthrough a terminal or command line interface. GraphQL operates over HTTP, so the\nCLI can be a standard HTTP client, like  curl , or a specialized GraphQL CLI,\nlike  graphqurl . To send GraphQL requests to your app, you'll need the following: Your App ID. A valid user access token. For details on how to get an access token, see\n Authenticate GraphQL Requests .",
            "code": [
                {
                    "lang": "shell",
                    "value": "gq https://services.cloud.mongodb.com/api/client/v2.0/app/<Your App ID>/graphql \\\n    -H 'Authorization: Bearer <Valid Access Token>' \\\n    -q 'query AllMoviesFromYear($year: Int!) { movies(query: { year: $year }) { title year runtime } }' \\\n    -v 'year=2000'"
                },
                {
                    "lang": "shell",
                    "value": "curl https://services.cloud.mongodb.com/api/client/v2.0/app/<Your App ID>/graphql \\\n  -X POST \\\n  -H 'Authorization: Bearer <Valid Access Token>' \\\n  -d '{ \"query\": \"query AllMoviesFromYear($year: Int!) { movies(query: { year: $year }) { title year runtime } }\",\n        \"variables\": { \"year\": 2000 } }'"
                },
                {
                    "lang": "shell",
                    "value": "gq https://services.cloud.mongodb.com/api/client/v2.0/app/<Your App ID>/graphql \\\n    -H 'Authorization: Bearer <Valid Access Token>' \\\n    -q 'mutation UpdateMovieTitle($oldTitle: String!, $newTitle: String!) { updateOneMovie(query: { title: $oldTitle } set: { title: $newTitle }) { title year } }'\n    -v 'oldTitle=The Matrix Reloaded'\n    -v 'newTitle=The Matrix 2'"
                },
                {
                    "lang": "shell",
                    "value": "curl https://services.cloud.mongodb.com/api/client/v2.0/app/<Your App ID>/graphql \\\n  -X POST \\\n  -H 'Authorization: Bearer <Valid Access Token>' \\\n  -d '{ \"query\": \"mutation UpdateMovieTitle($oldTitle: String!, $newTitle: String!) { updateOneMovie(query: { title: $oldTitle } set: { title: $newTitle }) { title year } }\",\n        \"variables\": { \"oldTitle\": \"The Matrix Reloaded\", \"newTitle\": \"The Matrix 2\" } }'"
                }
            ],
            "preview": "You can access your App's Atlas GraphQL API\nthrough a terminal or command line interface. GraphQL operates over HTTP, so the\nCLI can be a standard HTTP client, like curl, or a specialized GraphQL CLI,\nlike graphqurl.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "graphql/migrate-hasura",
            "title": "Migrate GraphQL to Hasura",
            "headings": [
                "Before You Begin",
                "Migrate to Hasura",
                "Create a New Project in Hasura",
                "Authorize Hasura for MongoDB",
                "Migrate GraphQL Schema",
                "Test Your GraphQL Queries",
                "Authorization and Authentication",
                "Set Up Custom Resolvers",
                "Update Client Applications",
                "Shut down MongoDB Atlas App Services Endpoints"
            ],
            "paragraphs": "Hasura empowers developers to rapidly build and deploy GraphQL and  REST APIs\non MongoDB  and many other data\nsources. By radically cutting down API development times, Hasura enables rapid\naccess to data, reduces friction across teams and services, and enables\nenterprises to shorten time to market on data-powered products and features. Always refer to the official documentation of both MongoDB Atlas and  Hasura \nfor the most up-to-date and accurate information. Specific steps may vary\ndepending on the details of your project and the technologies\nused. If you haven't already, Create an account on Hasura's website  cloud.hasura.io . Migrating your GraphQL API endpoints from MongoDB Atlas App Services to Hasura\ninvolves a multi-step process that encompasses setting up your environment in\nHasura, configuring database connections, migrating schemas, and implementing\nauthorization and authentication mechanisms. Below is an expanded guide\ndetailing each step, with a focus on authorization and role-based access control\n(RBAC) within Hasura. For more information, check out the  Hasura docs . To migrate to Hasura: Create a new project in Hasura Authorize Hasura for MongoDB Migrate your GraphQL schema Test your GraphQL queries Handle authorization and authentication Set up custom resolvers Update client applications Shut down MongoDB Atlas App Services After logging in to Hasura, navigate to Projects and create a new project\nby clicking on  New Project . Select a pricing plan, cloud\nprovider, and region that aligns with your MongoDB Atlas setup for optimal\nperformance. After creating your project, take note of the Hasura Cloud IP on this page\nas you'll use this in your MongoDB Atlas setup to authorize Hasura for\nMongoDB. Click on  Launch Console  to open up the Hasura Console. Click on the  DATA  tab at the top, choose MongoDB from the list\nof various databases that Hasura supports, and click  Connect\nExisting Database . Hasura can connect to a new or existing MongoDB Atlas database and generate the\nGraphQL API for you. Go to  cloud.mongodb.com  and navigate to the Network Access page on the\nAtlas dashboard. Click the  ADD IP ADDRESS  button and enter the Hasura Cloud IP\nthat you obtained from the Hasura Cloud dashboard earlier. Describe this\nentry as Hasura. Now, Hasura Cloud can communicate with your MongoDB Atlas instance. On the Database page, find the Atlas cluster that is connected to\nyour App Services app and click on  Connect . Select the Drivers\noption and copy the connection string. Go back to the Hasura Cloud dashboard. On the Connect Existing Database\nfor MongoDB page, enter the name of the MongoDB database you want to\nconnect to and the connection string you copied from the previous step. Click  Connect Database .\nYou are finished setting up the connection between Hasura and MongoDB,\neach hosted on their respective Cloud instances. Migrate your MongoDB Atlas GraphQL schema to Hasura's GraphQL schema. This\ninvolves defining your types, queries, mutations, and subscriptions in the\nHasura Console or the Hasura CLI. In the Hasura console click the Data tab and select your MongoDB database\nfrom the left hand pane. You'll see all the collections from your MongoDB database. To generate a schema for a collection, click on the  Track \nbutton next to the desired collection. You have three options to infer the schema of a collection: Infer the schema from a sample document you copy and paste from your\ncollection. Use Database Schema  allows you to use your database's\nvalidation schema if one is present in your database (a GraphQL schema\nwill be automatically generated for you). Use Existing Logical Model  allows you to select a previously\ncreated logical model. Click the  Validate  button to validate the JSON document. In\nthe next step, you will see the models derived from this document.\nFinally, click the  Track Collection  button. After tracking a collection and getting a schema, you can navigate to the API\nExplorer page on the Hasura Console to test out some GraphQL queries. Hasura also uses the GraphiQL interface, which is similar to how you test\nqueries in Atlas App Services. Hasura does not directly handle authentication. Instead, it relies on session\nvariables provided by an external authentication service. These session\nvariables include user, role, and organization information crucial for\ndetermining data access rights. For details, refer to the  Hasura authentication\ndocs Data access permissions, including roles and rule expressions, can be converted\ninto Hasura role-based permission rules. All Authentication methods that Atlas provides are compatible with Hasura's\nWebhook and JWT auth methods. If you are using Email/Pass, Anonymous, or API Key\nauthentication, use Hasura Webhook. If you are using a Custom JWT, integrate\ndirectly with Hasura's JWT auth method. Hasura recommends using an external IdP for managing authentication processes\nfor enhanced security and flexibility. You can integrate Hasura with any\nauthentication provider of your choice, such as Auth0, Firebase Auth, AWS\nCognito, or even a custom solution, to verify the user and set the necessary\nsession variables. For configuring JWT or webhook authentication in Hasura,\nrefer to the documentation at: JWT Authentication Webhook Authentication If your existing GraphQL API endpoints include custom resolvers or business\nlogic, you'll need to implement these in Hasura. Hasura supports custom business\nlogic using remote schemas, event triggers, and actions. Hasura Actions can be used to create custom resolvers. Custom queries and\nmutations including their payload type and input type can be defined within\nHasura, and Hasura can integrate with the resolver function over HTTP. The resolver function must be deployed somewhere and Hasura can generate the\nJavaScript code for the function. Update any client applications that interact with your GraphQL API endpoints to\npoint to the new Hasura endpoint URLs. Any existing Apollo client will work with\nHasura. Update the GraphQL operations used by applications to match Hasura-style GraphQL\noperations. Once you have verified that your GraphQL API endpoints are fully migrated\nand operational on Hasura, you can delete your MongoDB Atlas App\nServices app to avoid unnecessary costs. As a reminder, Atlas GraphQL\nendpoints will no longer be supported beginning March 12, 2025.",
            "code": [],
            "preview": "Learn how to migrate your GraphQL host from Atlas App Services to Hasura.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "graphql/migrate-apollo",
            "title": "Migrate GraphQL to Apollo",
            "headings": [
                "Create an Apollo Server",
                "Update Client Applications",
                "Shut Down Atlas App Services Endpoints",
                "Next Steps",
                "Additional Resources"
            ],
            "paragraphs": "Apollo  provides a  developer platform  and open-source tools and SDKs,\nincluding Apollo Server, to unify your data and services. Apollo Server is a\nspec-compliant, production-ready server library that can use data from any\nsource, including MongoDB Atlas. Migrating from MongoDB Atlas App Services to Apollo Server involves the\nfollowing steps: Always refer to the official documentation of both MongoDB Atlas and\n Apollo Server  for the\nmost up-to-date and accurate information. Specific steps may vary\ndepending on the details of your project and the technologies used. Apollo Server is an open-source GraphQL server that's compatible with any\nGraphQL client, including  Apollo Client . If you are using an Express server, follow along with this tutorial on how\nto  add Apollo Server \nto an existing MERN stack project. If you are starting from scratch, follow along with the  Apollo Server\nquickstart tutorial .\nOn Step 4, follow the docs for  fetching data from MongoDB . Be sure to update authorization and authentication. Refer to the\n Authentication and Authorization \ndocumentation for Apollo Server for details. Update any client applications that interact with your GraphQL API\nendpoints to point to the new Apollo endpoint URLs. Once you have verified that your GraphQL API endpoints are fully migrated\nand operational on Apollo Server, you can delete your MongoDB Atlas App\nServices app to avoid unnecessary costs. As a reminder, Atlas GraphQL\nendpoints will no longer be supported beginning March 12, 2025. Check out the  Apollo Federation docs \nto learn how to build a federated architecture that combines multiple GraphQL\nAPIs to create a unified supergraph. Teaching the MERN stack to speak GraphQL Apollo Documentation",
            "code": [],
            "preview": "Learn how to migrate your GraphQL host from Atlas App Services to Apollo Server.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "functions",
            "title": "Atlas Functions",
            "headings": [
                "Overview",
                "Functions are Serverless",
                "Functions have Context",
                "When To Use Functions",
                "How to Write a Function",
                "User and System Functions",
                "Define a Function",
                "Create a New Function",
                "Name the New Function",
                "Configure User Authentication",
                "Configure Function Execution Logs",
                "Specify an Authorization Expression",
                "Configure the Function's Privacy Level",
                "Write the Function Code",
                "Save the Function",
                "Pull Your App's Latest Configuration Files",
                "Write the Function Source Code",
                "Configure the Function",
                "Set User Authentication",
                "Set Execution Logging",
                "Specify an Authorization Expression",
                "Configure the Function's Privacy Level",
                "Deploy the Function",
                "Call a Function",
                "Call from a Function",
                "Call from App Services CLI",
                "Call from Rule Expressions",
                "Call from Realm SDKs",
                "Constraints"
            ],
            "paragraphs": "GraphQL is deprecated.  Learn More . An Atlas Function is a piece of server-side JavaScript\ncode that you write to define your app's behavior. You can call your\napp's functions directly from a client app or define services that\nintegrate and call functions automatically. Functions can call other functions and include a built-in client for\nworking with data in MongoDB Atlas clusters. They also include helpful\nglobal utilities, support common Node.js built-in modules, and can\nimport and use external packages from the npm registry. When a function is called, your app routes the request to a managed app\nserver that evaluates your code and returns the result. This model makes\nfunctions  serverless , which means that you don't have to deploy and\nmanage a server to run the code. Instead, you write the function source\ncode and your app handles the execution environment. A function runs in a context that reflects its execution environment.\nThe context includes the user that called the function, how they called\nit, and the state of your app when they called it. You can use context\nto run user-specific code and work with other parts of your app. To learn more about how to work with function context, see  Context . Functions can run arbitrary JavaScript code that you define, which means\nyou can use them for almost anything. Common use cases include\nlow-latency, short-running tasks like data movement, transformations,\nand validation. You can also use them to connect to external services\nand abstract away implementation details from your client applications. In addition to functions that you invoke directly, you also write\nfunctions for various services like HTTPS Endpoints, Triggers, and\nGraphQL custom resolvers. These services automatically call functions to\nhandle specific events. For example, whenever a database trigger\nobserves a change event it calls its associated function with the change\nevent as an argument. In the trigger function, you can then access\ninformation from the change event and respond appropriately. HTTPS Endpoints Triggers GraphQL Custom Resolvers Email/Password Registration The code for a function is essentially a named JavaScript source file,\nwhich means you can define multiple JavaScript functions in a single\nfunction file. The file must export a single JavaScript function from to\nserve as the entrypoint for incoming calls. When you call a function by\nname, you're actually calling the JavaScript function assigned to\n exports  in the function's source file. For example, here's a simple function that accepts a  name  argument,\nadds a log message, and returns a greeting for the provided name: You can use modern JavaScript syntax and import packages to define more\ncomplex functions: Functions automatically serialize returned values to  Extended\nJSON . This is useful to preserve\ntype information but may not be what your application expects. For example, the values in the object returned from the following\nfunction are converted into structured EJSON values: To return a value as standard JSON, call  JSON.stringify()  on the\nvalue and then return the stringified result: A function can run in two contexts depending on how they're configured\nand called: A  user function  runs in the context of a specific  user  of your application. Typically this is the logged in\nuser that called the function. User functions are subject to\n rules  and  schema validation . A  system function  runs as the  system user \ninstead of a specific application user. System functions have full\naccess to MongoDB CRUD and Aggregation APIs and bypass all rules and\nschema validation. References to  context.user  always resolve to\nthe authenticated user that called a function if there was one, even\nif the function runs as a system function. To determine if a function\nis running as a system function, call  context.runningAsSystem() . If a function executes without being called by an authenticated user,\nsuch as in a trigger or webhook, then dynamic references resolve to\nthe  system user  which has no  id  or other\nassociated data. You can create and manage functions in your application from the App Services UI\nor by importing the function configuration and source code with\nApp Services CLI or GitHub deployment. To define a new server-side function from the App Services UI: Click  Functions  in the left navigation menu. Click  New Function  in the top right of the  Functions  page. Enter a unique, identifying name for the function in the\n Name  field. This name must be distinct from all other\nfunctions in the application. You can define functions inside of nested folders. Function names\nare slash-separated paths, so a function named  utils/add  maps\nto  functions/utils/add.js  in the app's configuration files. Functions in App Services always execute in the context of a specific\napplication user or as a  system user  that\nbypasses rules. To configure the function's execution user, specify\nthe type of authentication that App Services should use. Authentication Type Description Application Authentication This type of authentication configures a function to run in\nthe context of the existing application user that was logged in\nwhen the client application called the function. If the\nfunction was called from another function then it inherits the\nexecution user from that function. System This type of authentication configures a function to run as a\n system user  that has full access to\nMongoDB CRUD and Aggregation APIs and is not affected by any\nrules, roles, or permissions. User ID This type of authentication configures a function to always run\nas a specific application user. Script This type of authentication configures a function to run as a\nspecific application user determined based on the result of a\ncustom  function  that you define. The\nfunction must return a specific user's  id  string or can\nspecify a system user by returning  { \"runAsSystem\": true } . By default, App Services includes the arguments that a function received in\nthe  log entry  for each execution of the\nfunction. If you want to prevent App Services from logging the arguments,\ndisable  Log Function Arguments . You can dynamically authorize requests based on the contents of each request\nby defining a  Can Evaluate   expression .\nApp Services evaluates the expression whenever the function is called. If\nyou do not specify an expression then App Services automatically\nauthorizes all authenticated incoming requests. The expression can expand standard  expression variables ,\nincluding the  %%request  and  %%user \nexpansions. By default, you can call a function from client applications as well\nas other functions in the same application. You can prevent client\napplications from seeing or calling a function by setting\n Private  to  true . You can still call a private function from  expression  and\nother functions, including incoming webhooks and triggers. Once you've created and configured the new function, it's time to\nwrite the JavaScript code that runs when you call the function.\nYou can write the code directly in the App Services UI using the function\neditor. From the function's  Settings  page: You can use most modern (ES6+) JavaScript features in functions,\nincluding async/await, destructuring, and template literals. Click the  Function Editor  tab. Add javascript code to the function. At minimum, the code must\nassign a function to  exports , as in the following example: Once you've written the function code, click  Save  from\neither the  Function Editor  or  Settings  tab. After you save the function, you can begin using it immediately. Atlas Functions run standard ES6+ JavaScript functions that you export from\nindividual files. Create a  .js  file with the same name as the function in\nthe  functions  directory or one of its subdirectories. Once you've created the function's  .js  file, write the function source code, e.g.: You can define functions inside of nested folders in the\n functions  directory. Use slashes in a function name to indicate\nits directory path. You can use most modern (ES6+) JavaScript features in functions, including\nasync/await, destructuring, and template literals. To see if App Services supports\na specific feature, see  JavaScript Support . In the  functions  directory of your application, open the  config.json \nfile and add a  configuration object  for your new\nfunction to the array. The object must have the following form: Functions in App Services always execute in the context of a specific\napplication user or as a  system user  (which\nbypasses rules). To configure the function's execution user, specify\nthe type of authentication that App Services should use: System To execute a function as a System user, use the following configuration: User To execute a function as a specific user, use the following configuration: Script The third way to execute a function is to specify another function that\nreturns a user id. Your function will execute as this user. To do this,\nuse the following configuration: To include any values that the function receives as arguments in its\n log entry , set  disable_arg_logs  to\n false . You can dynamically authorize requests based on the contents of each request\nby defining a  Can Evaluate   expression .\nApp Services evaluates the expression whenever the function is called. If you\ndo not specify an expression then App Services automatically authorizes all\nauthenticated incoming requests. The expression can expand standard  expression variables ,\nincluding the  %%request  and  %%user \nexpansions. The following expression only authorizes incoming requests if the\nsender's IP address is not included in the specified list of addresses. By default, you can call a function from client applications as well\nas other functions in the same application. You can prevent client\napplications from seeing or calling a function by setting  private \nto  true . You can call a private function from a  rule expression \nor another function, including HTTPS endpoints and triggers. Push the function configuration and source code to deploy it to your app. Once\nyou have pushed the function, you can begin using it immediately. You can call a function from other functions, from a connected client\napplication, or with App Services CLI. The examples in this section demonstrate calling a simple function named\n sum  that takes two arguments, adds them, and returns the result: You can call a function from another function through the\n context.functions  interface, which is\navailable as a global variable in any function. This include HTTPS\nendpoints, triggers, and GraphQL custom resolvers. The called function\nruns in the same context as the function that called it. You can call a function through App Services CLI with the  function run  command. The command returns the function\nresult as EJSON as well as any log or error messages. By default, functions run in the  system context . To call a function in the context of a specific\nuser, include their User ID in the  --user  argument. You can call a function from a  rule expression  by\nusing the  %function  operator. The operator evaluates to\nthe return value of the function. If the function throws an error, the\nexpression evaluates to  false . You can call a function from client applications that are connected with\na Realm SDK or over the wire protocol. For code examples that demonstrate\nhow to call a function from a client application, see the documentation\nfor the Realm SDKs: Make sure to sanitize client data to protect against code injection\nwhen using Functions. C++ SDK Flutter SDK Java SDK Kotlin SDK .NET SDK Node SDK React Native SDK Swift SDK Web SDK Functions are capped at 300 seconds of runtime\nper request, after which a function will time out and fail. Functions may use up to 256MB of memory at any time. Functions are limited to 1000 async operations. Functions support most commonly used ES6+ features and Node.js\nbuilt-in modules. However, some features that are uncommon or unsuited\nto serverless workloads are not supported. For more information, see\n JavaScript Support . A function may open a maximum of 25 sockets using the  net  built-in module. Incoming requests are limited to a maximum size of 18 MB. This limit\napplies to the total size of all arguments passed to the function as\nwell as any request headers or payload if the function is called\nthrough an HTTPS endpoint.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "exports = function(name) {\n  return `Hello, ${name ?? \"stranger\"}!`\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function Hello(name) {\n  console.log(`Said hello to ${name}`);\n  return `Hello, ${name}!`;\n};\n"
                },
                {
                    "lang": "javascript",
                    "value": "// You can use ES6 arrow functions\nconst uppercase = (str) => {\n  return str.toUpperCase();\n};\n\n// You can use async functions and await Promises\nexports = async function GetWeather() {\n  // You can get information about the user called the function\n  const city = context.user.custom_data.city;\n\n  // You can import Node.js built-ins and npm packages\n  const { URL } = require(\"url\");\n  const weatherUrl = new URL(\"https://example.com\");\n  weatherUrl.pathname = \"/weather\";\n  weatherUrl.search = `?location=\"${city}\"`;\n\n  // You can send HTTPS requests to external services\n  const weatherResponse = await context.http.get({\n    url: url.toString(),\n    headers: {\n      Accept: [\"application/json\"],\n    },\n  });\n  const { current, forecasts } = JSON.parse(weatherResponse.body.text());\n\n  return [\n    `Right now ${uppercase(city)} is ${current.temperature}\u00b0F and ${current.weather}.`,\n    `Here's the forecast for the next 7 days:`,\n    forecasts\n      .map((f) => `${f.day}: ${f.temperature}\u00b0F and ${f.weather}`)\n      .join(\"\\n  \"),\n  ].join(\"\\n\");\n};\n"
                },
                {
                    "lang": null,
                    "value": "Right now NEW YORK CITY is 72\u00b0F and sunny.\nHere's the forecast for the next 7 days:\n  Tuesday: 71\u00b0F and sunny\n  Wednesday: 72\u00b0F and sunny\n  Thursday: 73\u00b0F and partly cloudy\n  Friday: 71\u00b0F and rainy\n  Saturday: 77\u00b0F and sunny\n  Sunday: 76\u00b0F and sunny\n  Monday: 74\u00b0F and sunny\n"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  return {\n    pi: 3.14159,\n    today: new Date(),\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"pi\": {\n    \"$numberDouble\": \"3.14159\"\n  },\n  \"today\": {\n    \"$date\": {\n      \"$numberLong\": \"1652297239913\"\n    }\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  return JSON.stringify({\n    pi: 3.14159,\n    today: new Date(),\n  })\n}"
                },
                {
                    "lang": "json",
                    "value": "\"{\\\"pi\\\":3.14159,\\\"today\\\":\\\"2022-05-11T19:27:32.207Z\\\"}\""
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  return \"Hello, world!\";\n};"
                },
                {
                    "lang": "shell",
                    "value": "appservices pull --remote=<App ID>"
                },
                {
                    "lang": "bash",
                    "value": "touch functions/myFunction.js"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function hello(...args) {\n  // Write your function logic here! You can...\n  // Import dependencies\n  const assert = require(\"assert\")\n  assert(typeof args[0] === \"string\")\n  // Use ES6+ syntax\n  const sayHello = (name = \"world\") => {\n    console.log(`Hello, ${name}.`)\n  }\n  // Return values back to clients or other functions\n  return sayHello(args[0])\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Function Name>\",\n  \"private\": <Boolean>,\n  \"can_evaluate\": { <JSON Expression> },\n  \"disable_arg_logs\": <Boolean>,\n  \"run_as_system\": <Boolean>,\n  \"run_as_user_id\": \"<App Services User ID>\",\n  \"run_as_user_id_script_source\": \"<Function Source Code>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"run_as_system\": true,\n  \"run_as_user_id\": \"\",\n  \"run_as_user_id_script_source\": \"\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"run_as_system\": false,\n  \"run_as_user_id\": \"<App Services User Id>\",\n  \"run_as_user_id_script_source\": \"\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"run_as_system\": false,\n  \"run_as_user_id\": \"\",\n  \"run_as_user_id_script_source\": \"<Function Source Code>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n    \"%%request.remoteIPAddress\": {\n        \"$nin\": [\n            \"248.88.57.58\",\n            \"19.241.23.116\",\n            \"147.64.232.1\"\n        ]\n    }\n}"
                },
                {
                    "lang": "shell",
                    "value": "appservices push"
                },
                {
                    "lang": "javascript",
                    "value": "// sum: adds two numbers\nexports = function sum(a, b) {\n  return a + b;\n};"
                },
                {
                    "lang": "javascript",
                    "value": "// difference: subtracts b from a using the sum function\nexports = function difference(a, b) {\n  return context.functions.execute(\"sum\", a, -1 * b);\n};"
                },
                {
                    "lang": "sh",
                    "value": "appservices function run \\\n  --function=sum \\\n  --args=1 --args=2"
                },
                {
                    "lang": "sh",
                    "value": "appservices function run \\\n  --function=sum \\\n  --args=1 --args=2 \\\n  --user=61a50d82532cbd0de95c7c89"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"numGamesPlayed\": {\n    \"%function\": {\n      \"name\": \"sum\",\n      \"arguments\": [\n        \"%%root.numWins\",\n        \"%%root.numLosses\"\n      ]\n    }\n  }\n}"
                }
            ],
            "preview": "Define Atlas Functions and call them manually or automatically to execute server-side JavaScript code.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "billing",
            "title": "Billing",
            "headings": [
                "Overview",
                "Monthly Free Tier",
                "Free tier thresholds",
                "Invoicing and Payment",
                "Usage Types",
                "App Services Requests",
                "App Services Compute",
                "Atlas Device Sync",
                "Data Transfer",
                "Examples",
                "Mobile Application",
                "Web Application",
                "Backend Application"
            ],
            "paragraphs": "GraphQL is deprecated.  Learn More . Atlas App Services Apps are billed according to how much they're used on a\nmonthly basis. There are four independently-measured usage types that determine\nhow much you pay each month: App Services Requests , which measures the number of\nrequests that App Services sends and receives from client\napplications and external services. Requests include function calls,\ntrigger executions, and sync updates, but exclude user authentication\nand blocked or invalid requests. App Services Compute , which measures the runtime and memory\nusage of all requests excluding sync and authentication. Atlas Device Sync , which measures the total amount of time in\nwhich a client application is actively syncing. Data Transfer , which measures the amount of\ndata that App Services sends to external services and client applications. You can track an application's usage for the current month from the\napplication dashboard in the App Services UI. All App Services Apps in a MongoDB Atlas project share a single monthly free\ntier. All usage below the free tier thresholds in a given month is not billed.\nAs soon as a project exceeds any monthly free tier threshold, App Services starts\nbilling for additional usage of any kind for that project. App Services is free to use below the following monthly free tier thresholds: 1,000,000  requests  or 500 hours of\n compute  or 10,000 hours of  sync  runtime (whichever occurs first) 10GB of  data transfer App Services is billed as part of MongoDB Atlas. All payment methods that\nwork with Atlas can be used to pay for App Services, including\nprepaid Atlas Credits. Any costs associated with a linked Atlas data\nsource, such as database operations and backup, are billed separately\nfrom App Services. See  MongoDB Atlas Billing  for more information. App Services counts the number of requests that an application\nreceives and handles. Requests are billed at a set rate for each\nrequest. There are several types of requests: All incoming requests count toward your monthly usage except for the\nfollowing: Price:  $2.00 / 1,000,000 requests ($0.000002 / request) Formula:  (Function Executions + Trigger Executions + GraphQL/Webhook/HTTPS Endpoint Requests + Sync Updates) * $0.000002 Free Tier Threshold:  1,000,000  requests  or 500\nhours of  compute  or 10,000 hours of  sync  runtime (whichever occurs first) Function Executions , such as when a user calls a function from a client\napp, when a function calls an external service, or when a custom error\nhandler is invoked by a failed EventBridge trigger. If an executing function\ncalls another function, App Services considers both executions as part of\nthe same request. Trigger Executions , such as when a database trigger matches a change\nevent, a scheduled trigger is configured to fire, or an authentication\ntrigger responds to a user login. The request resulting from a trigger\nexecution also includes all operations invoked by the trigger, including\nfunctions and EventBridge forwarding. GraphQL Requests , such as when a client application issues a\nGraphQL query operation or calls a custom resolver. HTTPS Requests , such as when an external service sends a notification\nevent or interaction payload to an incoming webhook or HTTPS endpoint. Sync Operations , such as when a sync client uploads a  changeset , when App Services  resolves a conflict  in an uploaded changeset, or when\nApp Services sends changesets to a connected sync client. For more information on sync operations, see  Atlas Device Sync Protocol . Requests to an authentication provider Requests blocked by an application's  IP Access List Requests to an invalid HTTP endpoint Consider answering the following questions to help estimate the number of\nrequests that your application will receive each month. Triggers Functions/GraphQL Atlas Device Sync Requests How many times does this trigger execute each month? How many requests do you receive each user session? How many monthly active users does your app have? How many sessions does an average user open each month? How many write operations do users issue each session? How many updates do users receive each session? How many devices or active users does your app have? App Services counts the total amount of time and memory that each non-sync\napplication request takes to process. Compute is billed at a set rate for each\nmillisecond of runtime. For large requests, this rate is multiplied by a factor\nthat reflects the amount of memory the request used. The memory multiplier begins at 1 (i.e. it does not affect your billing rate)\nand increases by 1 for every 32MB of memory that a given request uses. Price:  $10.00 / 500 runtime hours (~$0.000000005 / ms) Formula:  (# Requests) * (Runtime (ms)) * (Memory (MB) / 32MB) * $0.000000005 / ms Free Tier Threshold:  1,000,000  requests  or 500\nhours of  compute  or 10,000 hours of  sync  runtime (whichever occurs first) Consider answering the following questions to help estimate the amount of\ncompute time that your application will use each month. Runtime Memory Usage How many sessions does a user open each month? How many requests do you receive each session? How long does each request take? What does each request do? How many requests do you make to MongoDB Atlas or another service? Does a request handle heavy data processing? How much in-memory data does your application process for each request? How many MongoDB documents does a request affect? App Services counts the total amount of time in which a client application\nuser has an active connection to the sync server even if they are not\ntransferring data at the time. Sync is billed at a set rate for each\nmillisecond of sync runtime per user. Price:  $0.08 / 1,000,000 runtime minutes ($0.00000008 / min) Formula:  (# Active Users) * (Sync time (min / user)) * ($0.00000008 / min) Free Tier Threshold:  1,000,000  requests  or 500\nhours of  compute  or 10,000 hours of  sync  runtime (whichever occurs first) Device Sync pauses automatically after 30 days\nof inactivity. Consider answering the following questions to help estimate the amount of\nsync time that your application will use each month. Sync Sessions How many monthly active users does your app have? How much time is an average user online each month? App Services measures the total amount of data that your application sends\nout to client applications and external services. Data transfer is billed at a\nset rate for each gigabyte of data egress. Price:  $0.12 per GB Formula:  (Data sent to clients/services (GB)) * ($0.12 / GB) Free Tier Threshold:  10GB Data transfer from a linked MongoDB Atlas data source to App Services is billed as\nan Atlas Data Egress charge. To learn more about Atlas\negress rates, read the  Data Transfer \nbilling documentation. Consider answering the following questions to help estimate the amount of\ndata that your application will transfer each month. Data Transfer How many updates/requests does an average user issues each month? How much data is sent to users per update/request? How often is data sent to external services? How much data might be sent in each external service call? A group messaging application uses App Services to manage permissions and sync\nmessages between users in the same group. The app has the following usage: The following calculations approximate the cost of running this application for\none month: There are around 2,000 active group chats in any given month. The average group chat has 4 users that each send around 900 messages each\nmonth. Messages are usually small but may contain up to 2KB of data. The average user has the app open for about 9 hours each month. Usage Type Calculation App Services Requests (2000 chats) * (4 users / chat) * (900 messages / user) = 7,200,000 messages (7,200,000 messages) * ((1 write + 3 reads) / message) = 28,800,000 requests (28,800,000 requests) - (1,000,000 free tier requests) = 27,800,000 requests (27,800,000 requests) * ($0.000002 / request) =  $55.60 Atlas Device Sync (2000 chats) * (4 users / chat) * (9 hrs / user) = 72,000 active hrs (72,000 active hrs) - (2,500 free tier hours) = 69,500 active hrs (69,500 active hrs) * (60 mins / hr) * ($0.00000008 / min) =  $0.33 Data Transfer (7,200,000 messages) * (3 reads / message) = 21,600,000 reads (21,600,000 reads) * (0.000002 GB / read) = 43.2 GB (43.2 GB) - (10 free tier GB) = 33.2 GB (33.2 GB) * ($0.12 / GB) =  $3.98 TOTAL $55.60 (requests) + $0.33 (sync) + $3.98 (data transfer) =  $59.91 / month The way that free tier usage affects your bill depends on your consumption\neach month. For this example, we assume that all requests and activity are\nspread evenly throughout the month. On every day of a 30 day month this application would handle 960,000\nrequests, 2,400 sync hours, and 1.44 GB of data transfer. The app would hit\nthe requests/compute/sync threshold on the second day of the month and the\ndata transfer threshold on the seventh day. The free tier would cover the following usage in each area: App Services Requests:  1,000,000 requests Atlas Device Sync:  2,500 hours Data Transfer:  10 GB A hardware store uses a web application that allows customers to browse and\norder items online. The app uses Atlas Functions and the GraphQL API to fetch\nitems based on a user's search, get detailed information for specific items, and\nsubmit orders. The app has the following usage: The following calculations approximate the cost of running this application for\none month: There are around 100,000 visitors each month An average visitor performs 2 searches, looks at 10 items, and submits an\norder 20% of the time. Search results, item data, and order requests contain 4KB or less Usage Type Calculation App Services Requests (2 searches + 10 items + 0.20 orders) / visitor = 12.2 requests / visitor (12.2 requests / visitor) * (100,000 visitors) = 1,220,000 requests (1,220,000 requests) - (1,000,000 free tier requests) = (220,000 requests) (220,000 requests) * ($0.000002 / request) =  $0.44 App Services Compute (100,000 visitors) * ((2 searches + 10 items + 0.20 orders) / visitor) = 1,220,000 requests (1,220,000 requests) * (100 ms / request) * 1 = 122,000,000 ms 100 ms is a conservative runtime estimate for lightweight functions 4KB is well under 32MB, so the memory multiplier is 1 (122,000,000 ms) - (28,152,000 free tier ms) = 93,848,000 ms (93,848,000 ms) * ($0.000000005 / ms) =  $0.47 Data Transfer (1,220,000 requests) * (0.000002 GB / request) = 4.88 GB (4.88 GB) - (10 free tier GB) = 0GB (0GB) * ($0.12 / GB) =  FREE TOTAL $0.44 (requests) + $0.47 (compute) + $0 (data transfer) =  $0.91 / month The way that free tier usage affects your bill depends on your consumption\neach month. For this example, we assume that all requests and activity are\nspread evenly throughout the month. On every day of a 30 day month this application would handle 41,290 requests,\n1.3 compute hours, and 16.8 GB of data transfer. The app would hit the\nrequests/compute threshold on the 24th day of the month and the data transfer\nthreshold on the first day. The free tier would cover the following usage in each area: App Services Requests:  1,000,000 requests App Services Compute:  7.82 hours Data Transfer:  10 GB A regional pizza chain uses App Services to handle orders for all of its\nrestaurants. The app has the following usage: The chain has 25 stores and each store gets about 12,000 orders each month For each order, a database trigger sends the customer an email receipt.\nAnother trigger sends text message updates to the customer when their order is\nreceived, when it's sent out for delivery, and when it's about to be\ndelivered. A scheduled trigger runs every 5 minutes to monitor delivery times. If a\ndelivery is taking too long, it sends a text message update to the user. About\n10% of all deliveries take too long. Orders vary in size but are no larger than 4KB Usage Type Calculation App Services Requests (25 stores) * (12,000 orders / store) = 300,000 orders (300,000 orders) * ((1 email + 3.10 texts) / order) = 1,230,000 requests (12 triggers / hr) * (720 hrs / month) = 8,640 requests (1,230,000 + 8,640 requests) - (1,000,000 free tier requests) = 238,640 requests (238,640 requests) * ($0.000002 / request) =  $0.48 App Services Compute (1,230,000 requests) * (300 ms / request) * 1 = 369,000,000 ms 300 ms is a runtime estimate for the trigger function. To get a\nbetter estimate, create a POC function and test how long it takes to\nconnect to external services, etc. 4KB is well under 32MB, so the memory multiplier is 1 (369,000,000 ms) - (300,276,000 free tier ms) = 68,724,000 ms (68,724,000 ms) * ($0.000000005 / ms) =  $0.34 Data Transfer (1,230,000 requests) * (0.000004 GB / request) = 4.92 GB (4.92 GB) - (10 free tier GB) = 0GB (0GB) * ($0.12 / GB) =  FREE TOTAL $0.48 (requests) + $0.34 (compute) + $0 (data transfer) =  $0.82 / month The way that free tier usage affects your bill depends on your consumption\neach month. For this example, we assume that all requests and activity are\nspread evenly throughout the month. On every day of a 30 day month this application would handle 41,000 requests,\n3.42 compute hours, and 0.164 GB of data transfer. The app would hit the\nrequests/compute threshold on the 25th day of the month and would not exceed\nthe data transfer free tier threshold. The free tier would cover the following usage in each area: App Services Requests:  1,000,000 requests App Services Compute:  83.41 hours Data Transfer:  4.92 GB",
            "code": [],
            "preview": "Review the billing structure for App Services Apps in MongoDB Atlas.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "graphql/expose-data",
            "title": "Expose Data in a Collection",
            "headings": [
                "Overview",
                "Procedure",
                "1. Configure Roles for the Collection",
                "2. Define a Schema for Documents in the Collection",
                "3. Define Relationships to Other Collections",
                "4. Name the Data Type",
                "Next Steps"
            ],
            "paragraphs": "You can expose data from a MongoDB collection to client applications through the\nAtlas GraphQL API. Atlas App Services automatically generates GraphQL types and resolvers based on\nthe  collection schema  and enforces  collection rules  for all GraphQL operations. App Services enforces  collection rules  for all incoming\nGraphQL requests, so you need to define at least one  collection\nrole  with the permissions that\nyour application requires. All GraphQL requests include an authentication token that identifies the\nlogged in App Services user that sent the request. App Services evaluates a role for\nevery document included in a GraphQL operation and only returns fields\nand documents that the user has permission to see. If App Services omits a\nfield, the field has a  null  value in the returned document. GraphQL requires that all data conforms to a well-defined type, so you\nmust  define a schema  for\ndocuments in the collection. App Services automatically generates\n GraphQL types and resolvers  for documents in the collection\nbased on the collection schema and regenerates new types whenever the\nschema changes. App Services can generate a collection schema for you based on a sample of\nexisting documents in the collection. If you don't have existing\ndata, you can insert a new document that has a mock implementation of\nthe fields you want to include in your schema and then generate a\nschema based on the mock. You can define  relationships  that connect each\ndocument in the collection to one or more documents in a foreign\ncollection. To learn how to define a relationship, see\n Define a Relationship . Relationships allow you to fluently reference and query related\ndocuments in GraphQL read and write operations. For example, you can\nquery for a person and include the full document for each of their\nchildren from the same  people  collection: App Services names the GraphQL types that it generates based on the data type\nthat documents in the collection conform to. You can configure the name\nof the GraphQL types by setting the  title  field in a schema to the\nname of the data type that the schema defines. There are three situations where you can set the  title  field: You can define the type name for each document in a collection by\nsetting  title  at the root level of the schema. If you don't\nspecify a title, App Services uses the name of the collection instead. You can define the type name for an embedded object by setting\n title  in the embedded object schema. You can define the type name for a field that has a defined\nrelationship by setting  title  in the field schema. App Services uses the\n title  instead of the defined field name when it resolves\nrelationships in GraphQL. App Services generates two  GraphQL queries  for each collection: A  singular  query that finds a specific document in the collection. The\nquery uses the same name as the schema's  title . If the schema's\n title  is a plural noun, App Services attempts to use its singular\nform as determined by the  Rails ActiveSupport inflection rules . A  plural  query that finds a subset of all documents in the collection.\nIf possible, the query uses the plural form of the singular query name. If\nApp Services is unable to pluralize the name or if the pluralized name\nis the same as the singular name, the plural query uses the same name as\nthe singular query with an additional  \"s\"  appended to the end. The following schema is configured for the  laboratory.mice \ncollection: App Services generates two queries,  mouse  (singular) and  mice \n(plural), based on the schema: Once you have defined a schema for the collection, App Services automatically exposes\nthe documents in the collection through the GraphQL API. You can now connect\nfrom a client application and execute queries and mutations. Apollo Client (React) - Web SDK Run GraphQL Operations from a CLI",
            "code": [
                {
                    "lang": "graphql",
                    "value": "query {\n  person(query: { name: \"Molly Weasley\" }) {\n    _id\n    name\n    age\n    picture\n    children {\n      _id\n      name\n      age\n      picture\n    }\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"movie\",\n  \"properties\": {\n    \"_id\": { \"bsonType\": \"objectId\" },\n    \"title\": { \"bsonType\": \"string\" },\n    \"year\": { \"bsonType\": \"int\" },\n    \"director\": { \"bsonType\": \"int\" }\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"Mouse\",\n  \"bsonType\": \"object\",\n  \"properties\": {\n    \"_id\": { \"bsonType\": \"objectId\" },\n    \"name\": { \"bsonType\": \"string\" },\n    \"age\": { \"bsonType\": \"int\" }\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "query Mice {\n  mouse(query: { _id: \"5ebe6819197003ddb1f74475\" }) {\n    name\n    age\n  }\n  mice {\n    name\n    age\n  }\n}"
                }
            ],
            "preview": "You can expose data from a MongoDB collection to client applications through the\nAtlas GraphQL API. Atlas App Services automatically generates GraphQL types and resolvers based on\nthe collection schema and enforces collection rules for all GraphQL operations.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "graphql/authenticate",
            "title": "Authenticate GraphQL Requests",
            "headings": [
                "Overview",
                "Bearer Authentication",
                "Credential Headers",
                "Email/Password",
                "API Key",
                "Custom JWT"
            ],
            "paragraphs": "The GraphQL API requires that incoming requests include authentication\ninformation for the user making the request. This lets the API enforce\nrules and validate document schemas for each operation. Requests must include authentication data in specific request headers.\nApp Services uses the following process to authenticate a given request: Check for an  Authorization  header. If it's present, the request\nmust use  Bearer Authentication \nwith a valid user access token. If the token is invalid, the request\nfails. If the  Authorization  header is not present, check for\n Credential Headers . The headers\nmust contain valid Email/Password, API Key, or Custom JWT credentials\nfor an App user. You must  enable an authentication provider  before users can authenticate with\nit. The GraphQL API supports Bearer Authentication, which lets you\nauthenticate a request by including a valid user access token in the\n Authorization  header. To learn how to get and manage an access\ntoken, see  Manage User Sessions . The Authorization header uses the following format: For example, the following request uses Bearer Authentication: In general, bearer authentication with an access token has higher\nthroughput and is more secure than credential headers. Use an access\ntoken instead of credential headers when possible. The token lets you\nrun multiple requests without re-authenticating the user. It also lets\nyou send requests from a web browser that enforces  CORS . If you're authenticating from a browser or another user-facing client application,\navoid using an API key to log in. Instead, use another authentication\nprovider that takes user-provided credentials. Never store API keys or other\nsensitive credentials locally. Bearer authentication is useful for: sending requests from a web browser. sending multiple requests without storing user credentials or prompting the user on each request. sending requests from an app that also uses a  Realm SDK  to authenticate users. You can authenticate a GraphQL request by including the user's login\ncredentials in the request headers. The exact headers to include depend\non the authentication provider. Credential headers are useful for: requests sent from a server-side application requests sent from a command-line tool manual or test requests sent from a GraphQL client like Postman You cannot use credential headers to authenticate requests sent from\na web browser due to  Cross-Origin Resource Sharing  restrictions. Instead, to\nauthenticate GraphQL requests from a browser, use  Bearer\nAuthentication . To authenticate a GraphQL request as an  email/password  user, include the user's credentials in\nthe request's  email  and  password  headers. To authenticate a GraphQL request with an  API Key , include the API key in the request's\n apiKey  header. If you're authenticating from a browser or another user-facing client application,\navoid using an API key to log in. Instead, use another authentication\nprovider that takes user-provided credentials. Never store API keys or other\nsensitive credentials locally. To authenticate a GraphQL request as a  Custom JWT  user, include the JWT string in the\nrequest's  jwtTokenString  header.",
            "code": [
                {
                    "lang": "text",
                    "value": "Authorization: Bearer <AccessToken>"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST 'https://services.cloud.mongodb.com/api/client/v2.0/app/<AppID>/graphql' \\\n   --header 'Authorization: Bearer <AccessToken>' \\\n   --header 'Content-Type: application/json' \\\n   --data-raw '{\n     \"query\": \"query AllMovies {\\n  movies {\\n    title\\n    year\\n  }\\n}\"\n   }'"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST 'https://services.cloud.mongodb.com/api/client/v2.0/app/<AppID>/graphql' \\\n   --header 'email: <EmailAddress>' \\\n   --header 'password: <Password>' \\\n   --header 'Content-Type: application/json' \\\n   --data-raw '{\n     \"query\": \"query AllMovies {\\n  movies {\\n    title\\n    year\\n  }\\n}\"\n   }'"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST 'https://services.cloud.mongodb.com/api/client/v2.0/app/<AppID>/graphql' \\\n   --header 'apiKey: <APIKey>' \\\n   --header 'Content-Type: application/json' \\\n   --data-raw '{\n     \"query\": \"query AllMovies {\\n  movies {\\n    title\\n    year\\n  }\\n}\"\n   }'"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST 'https://services.cloud.mongodb.com/api/client/v2.0/app/<AppID>/graphql' \\\n   --header 'jwtTokenString: <JWT>' \\\n   --header 'Content-Type: application/json' \\\n   --data-raw '{\n     \"query\": \"query AllMovies {\\n  movies {\\n    title\\n    year\\n  }\\n}\"\n   }'"
                }
            ],
            "preview": "The GraphQL API requires that incoming requests include authentication\ninformation for the user making the request. This lets the API enforce\nrules and validate document schemas for each operation.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/convert-webhooks-to-endpoints",
            "title": "Convert Webhooks to HTTPS Endpoints",
            "headings": [
                "Overview",
                "Automatically Convert Existing HTTP Webhooks",
                "Manually Convert GitHub and Twilio Webhooks",
                "GitHub Webhooks",
                "Twilio Webhooks",
                "Update Callback URLs in Client Apps & Services",
                "Differences between Webhooks and Their Converted Endpoints"
            ],
            "paragraphs": "The Atlas App Services third-party service webhooks are deprecated in favor of\ncustom HTTPS endpoints. HTTPS endpoints are functionally almost\nidentical to webhooks but are now a first-class service. This guide walks through converting your app's existing webhooks and\nupdating external apps that call them. For new webhooks or callback\nURLs, you should define an HTTPS endpoint. App Services can automatically convert any existing HTTP service\nwebhooks in your app into HTTPS endpoints. This conversion is one-way\nwhich means that you cannot convert HTTPS endpoints back into webhooks.\nThe conversion process does not delete the original webhooks so incoming\nwebhook requests will continue to execute after the conversion. To run the webhook to HTTPS endpoint conversion process for all webhooks\nin your application: Click  HTTPS Endpoints  in the left navigation menu of the App Services UI. Click the  Convert  button. Confirm that you want to run the conversion. The converted HTTPS endpoints are independent from their source\nwebhooks. If you choose to edit a webhook after running the conversion,\nyou can run the conversion again with the \"Convert & Override\" option to\npropogate those changes to your new endpoints. App Services cannot automatically convert GitHub and Twilio\nwebhooks to HTTPS endpoints because they use deprecated client\nlibraries. However, you can manually migrate webhooks from these\nservices to new HTTPS endpoints. To migrate a GitHub webhook to an HTTPS endpoint: Create a new HTTPS endpoint Copy the existing GitHub webhook code into the new endpoint's function Update your code to parse the incoming request body instead of using\nthe pre-parsed GitHub payload. To migrate a Twilio webhook to an HTTPS endpoint: Create a new HTTPS endpoint Copy the existing Twilio webhook code into the new endpoint's function Update your code to parse the incoming request body instead of using\nthe pre-parsed Twilio payload. If your webhook uses the built-in\nTwilio service client, update your code to use the  twilio  Node library instead. HTTPS endpoints use a different URL scheme than service webhooks. You\nshould update any apps or services that call your webhooks to instead\ncall the converted HTTPS endpoint URL. Existing webhooks URLs will\ncontinue to accept requests and execute their associated functions after\nyou've run the conversion. This means that you can gradually migrate to\nthe new URLs without breaking existing workflows. To migrate to the converted HTTPS endpoint URLs: Identify all client apps and services that call service webhooks For each app or service, modify the source code or configuration to\nuse the new URLs. Monitor your app's logs for  service webhook \nrecords to look for any active webhook callers that you missed. Once all clients are updated with the new URLs, delete the deprecated\nwebhooks from your app. To migrate a service's integration from an App Services webhook to the\nconverted HTTPS endpoint, modify the service's external configuration\nfor outgoing webhook requests to point to the converted endpoint URL. If you configured your converted HTTPS endpoint to use  Respond with Result \nand the webhook function it's based on returns a value but does not invoke  response.setBody() ,\nthe generated endpoint includes the function's return value as its response body.\nThe webhook, in contrast, only returns a body specified by  response.setBody() . The webhook for this function only responds with status code  200  without a body. For the endpoint, if you enable  Respond with Result , the endpoint responds\nwith status code  200   and  the body  \"Hello world\" .\nIf you don't enable  Respond with Result , the endpoint only responds\nwith status code  200 .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "exports = async function(payload, response) {\n  // Webhooks only provide the parsed request body\n  const { commits, pusher, repository } = payload;\n\n  // ... your code here\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function(payload, response) {\n  // Endpoints pass an entire request payload, not just a parsed body\n  const { commits, pusher, repository } = JSON.parse(payload.body.text());\n\n  // ... your code here\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function(payload, response) {\n  // Webhooks only provide the parsed request body\n  const { To, From, Body } = payload;\n\n  // Webhooks could use a built-in Twilio client\n  const twilio = context.services.get(\"twilio\")\n  twilio.send({\n    To: From,\n    From: context.values.get(\"TwilioPhoneNumber\"),\n    Body: `Message received!`\n })\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function(payload, response) {\n  // Endpoints pass an entire request payload, not just a parsed body\n  const { To, From, Body } = JSON.parse(payload.body.text())\n\n  // The endpoint should use `twilio` from npm instead of the built-in Twilio client\n  const twilio = require('twilio')(\n    // Your Account SID and Auth Token from https://www.twilio.com/console\n    // Specify the same Twilio credentials as the service configuration\n    context.values.get(\"TwilioAccountSID\"),\n    context.values.get(\"TwilioAuthToken\"),\n  )\n  await twilio.messages.create({\n    To: From,\n    From: context.values.get(\"TwilioPhoneNumber\"),\n    Body: `Message received!`\n  })\n}"
                },
                {
                    "lang": "text",
                    "value": "https://webhooks.mongodb-realm.com/api/client/v2.0/app/myapp-abcde/service/myHttpService/incoming_webhook/handleIncomingEvent"
                },
                {
                    "lang": "text",
                    "value": "https://data.mongodb-api.com/app/myapp-abcde/endpoint/myHttpService/handleIncomingEvent"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function({ query, headers, body}, response){\n   response.setStatusCode(200);\n   return \"Hello world\";\n};"
                }
            ],
            "preview": "The Atlas App Services third-party service webhooks are deprecated in favor of\ncustom HTTPS endpoints. HTTPS endpoints are functionally almost\nidentical to webhooks but are now a first-class service.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/call-a-service-action",
            "title": "Call a Service Action [Deprecated]",
            "headings": [
                "Overview",
                "Call from a Function"
            ],
            "paragraphs": "Third party services and push notifications in App Services have been deprecated\nin favor of creating HTTP endpoints that use  external dependencies  in functions. Webhooks have been renamed to  HTTPS Endpoints  with no change in behavior. You should\n migrate  existing Webhooks. Existing services will continue to work until  November 1, 2024 . Because third party services and push notifications are now deprecated, they have\nbeen removed by default from the App Services UI. If you need to manage an existing third party\nservice or push notification, you can add the configurations back to the UI by doing\nthe following: In the left navigation, under the  Manage  section, click\n App Settings . Enable the toggle switch next to\n Temporarily Re-Enable 3rd Party Services , and then save your\nchanges. You can call actions associated with a  service  from\nAtlas Functions, including incoming webhooks and triggers, or directly\nfrom a connected client application. Each service action is available as\na method on its respective service client. The examples in this section demonstrate calling the  post()  action from the\nHTTP Service. Each service action follows a similar pattern but has\ndistinct parameters. For details on the parameters and usage of a\nspecific action, refer to that action's reference page. You must  configure a service rule  that enables an action before\nyou can call it. To call a service action from a  function : Instantiate a service client from  function context  using the  context.services  interface. Call the method associated with the action.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const http = context.services.get(\"myHttpService\");\nhttp.post({\n    \"url\": \"https://www.example.com\",\n    \"headers\": { \"Content-Type\": [\"application/json\"] },\n    \"body\": { \"msg\": \"Hello from a service action!\" }\n  })\n  .then(() => \"Successfully sent the post request!\")"
                }
            ],
            "preview": "You can call actions associated with a service from\nAtlas Functions, including incoming webhooks and triggers, or directly\nfrom a connected client application. Each service action is available as\na method on its respective service client.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/aws",
            "title": "AWS Service [Deprecated]",
            "headings": [
                "Overview",
                "Configuration Parameters",
                "Service Actions",
                "AWS Service Rules",
                "Rule Conditions",
                "Example",
                "S3 PutObject",
                "S3 GetObject",
                "Usage",
                "Call an AWS Service from an Atlas Function",
                "S3 Service",
                "S3 PutObject",
                "S3 GetObject",
                "S3 PresignURL",
                "Kinesis Service",
                "Lambda Service",
                "SES Service",
                "Supported AWS Services"
            ],
            "paragraphs": "Third party services and push notifications in App Services have been deprecated\nin favor of creating HTTP endpoints that use  external dependencies  in functions. Webhooks have been renamed to  HTTPS Endpoints  with no change in behavior. You should\n migrate  existing Webhooks. Existing services will continue to work until  November 1, 2024 . Because third party services and push notifications are now deprecated, they have\nbeen removed by default from the App Services UI. If you need to manage an existing third party\nservice or push notification, you can add the configurations back to the UI by doing\nthe following: In the left navigation, under the  Manage  section, click\n App Settings . Enable the toggle switch next to\n Temporarily Re-Enable 3rd Party Services , and then save your\nchanges. Amazon Web Services (AWS)  provides an\nextensive collection of cloud-based services. Atlas App Services provides a\ngeneric AWS service that enables you to connect to many of these\nservices. You will need to provide values for the following parameters when you\n create an AWS service interface : Parameter Description A unique name for the service. The access key ID for an AWS IAM user. The user should have\nprogrammatic access and appropriate permissions for all AWS\nservices that you plan to interact with. The name of a  Secret  that stores a secret\naccess key for the IAM user whose ID is specified in\n Access Key ID . You can find this value next to the\nAccess key ID when you create a new IAM user or create a new\naccess key for an existing IAM user. Managing Access Keys for Your AWS Account Create IAM Users Each AWS service has different actions that you can perform on that\nservice. App Services uses the action names specified in the  AWS SDK\nfor Go  for each service. For each supported AWS service, App Services supports any action that: For example, the  S3  service includes a  PutObject \naction. App Services supports this action because it takes a single\ninput type of  PutObjectInput , and returns\neither a  PutObjectOutput  or an error. App Services uses the same names (and casing) for the AWS services\nand actions as the AWS Go SDK. Takes a single input parameter. Returns one of two objects: an output object, or an error. You must specify rules to enable the AWS services and actions. Each rule\napplies to a single service API, plus one or all actions on that service.\nAs with other service rules in App Services, a rule must evaluate\nto  true  to enable the action. For example, the following rule enables all actions on the Kinesis\nservice: This rule can also be expressed as the following  rule expression . Note that in the array of  actions , the asterisk ( * )\nafter the service name indicates that all actions of that service are enabled: The default value of the  When \nfield contains only empty brackets, which means the rule will always\nevaluate to  true , and therefore all calls to the action are valid. For each service action, the rule you create may include any of the\nproperties of the input object for that action as a condition for\nvalidating the rule. The  %%args  expansion provides access to these\nproperties. The  S3  service includes the  PutObject \naction, which takes an input object of type of  PutObjectInput . You can reference any of the properties on the\n PutObjectInput  object in a rule's  When  expression with the\n %%args  expansion. Using the  Bucket  property of the   PutObjectInput  object, you can\ncreate a rule that enables the  PutObject  action on the S3 service,\nbut restricts the action to a list of approved buckets.  In this example,\nwe use a user-defined constant called  myS3Buckets  for the list of approved\nbucket names: This can also be expressed as the following JSON: The  S3  service includes the  GetObject \naction, which takes an input object of type of  GetObjectInput . You can reference any of the properties on the\n GetObjectInput  object in a rule's  When  expression with the\n %%args  expansion. In the following example, we create a rule that enables the  GetObject \naction on a specific bucket called  realmReadWritableBucket : This can also be expressed as the following JSON: You can call an AWS service from an Atlas Function and from the\nSDKs. The following sections show each of these processes. The following examples show how to call various AWS services from within\nan Atlas Function. In each example, it is assumed the named\nservice has already been created. Refer to the  S3 API Reference  for implementation details. App Services supports a maximum file size of 4 Megabytes when working with\nAWS S3 objects. Refer to the  Kinesis API Reference  for implementation details. Refer to the  Lambda API Reference  for implementation details. Refer to the  SES API Reference  for implementation details. Your App can connect to the following AWS services: Athena Batch CloudWatch Comprehend EC2 Firehose Glacier IOT Kinesis Lambda Lex Runtime Service Machine Learning Mobile Analytics Polly RDS Redshift Rekognition S3 SES Step Functions (SFN) SNS SQS Textract",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Service Name>\",\n  \"type\": \"aws\",\n  \"config\": {\n    \"accessKeyId\": <Access Key ID>,\n    \"region\": \"us-east-1\"\n  },\n  \"secret_config\": {\n    \"secretAccessKey\": \"<Secret Name>\"\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n   \"name\": \"kinesis\",\n   \"actions\": [\n      \"kinesis:*\"\n   ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n   \"name\": \"s3\",\n   \"actions\": [\n      \"s3:PutObject\"\n   ],\n   \"when\": {\n      \"%%args.Bucket\": {\n            \"$in\": \"%%values.myS3Buckets\"\n      }\n   }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n   \"name\": \"s3\",\n   \"actions\": [\n      \"s3:GetObject\"\n   ],\n   \"when\": {\n      \"%%args.Bucket\": {\n            \"$in\": \"realmReadWritableBucket\"\n      }\n   }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const s3 = context.services.get('MyAwsService').s3(\"us-east-1\");\n  const result = await s3.PutObject({\n    \"Bucket\": \"my-bucket\",\n    \"Key\": \"example\",\n    \"Body\": \"hello there\"\n  });\n  console.log(EJSON.stringify(result));\n  return result;\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function(arg) {\n   const s3 = context.services.get('MyAwsService').s3(\"us-east-1\");\n   const result = await s3.GetObject({\n      \"Bucket\": \"realmReadWritableBucket\",\n      \"Key\": \"coffee.jpeg\"\n   });\n\n   console.log(EJSON.stringify(result));\n   return result;\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function(){\n  const s3 = context.services.get(\"MyAwsService\").s3(\"us-east-1\");\n  const presignedUrl = s3.PresignURL({\n   \"Bucket\": \"my-s3-bucket-name\",\n   \"Key\": \"/some/path\",\n   // HTTP method that is valid for this signed URL. Can use PUT for uploads, or GET for downloads.\n   \"Method\": \"GET\",\n   // Duration of the lifetime of the signed url, in milliseconds\n   \"ExpirationMS\": 30000,\n  })\n  return presignedUrl\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function(event) {\n  const kinesis = context.services.get('MyAwsService').kinesis(\"us-east-1\");\n  const result = await kinesis.PutRecord({\n    Data: JSON.stringify(event.fullDocument),\n    StreamName: \"realmStream\",\n    PartitionKey: \"1\"\n  });\n  console.log(EJSON.stringify(result));\n  return result;\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const lambda = context.services.get('MyAwsService').lambda(\"us-east-1\");\n\n  const result = await lambda.Invoke({\n    FunctionName: \"myLambdaFunction\",\n    Payload: context.user.id\n  });\n\n  console.log(result.Payload.text());\n  return EJSON.parse(result.Payload.text());\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function(){\n  const ses = context.services.get('MyAwsService').ses(\"us-east-1\");\n  const result = await ses.SendEmail({\n    Source: \"sender@example.com\",\n    Destination: { ToAddresses: [\"docs@mongodb.com\"] },\n    Message: {\n      Body: {\n        Html: {\n          Charset: \"UTF-8\",\n          Data: `This is a message from user ${context.user.id}`\n        }\n      },\n      Subject: {\n        Charset: \"UTF-8\",\n        Data: \"Test Email Please Ignore\"\n      }\n    }\n  });\n  console.log(EJSON.stringify(result));\n  return result;\n};"
                }
            ],
            "preview": "Amazon Web Services (AWS) provides an\nextensive collection of cloud-based services. Atlas App Services provides a\ngeneric AWS service that enables you to connect to many of these\nservices.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/replace-with-npm-modules",
            "title": "Replace Services with npm Modules",
            "headings": [
                "Overview",
                "HTTP Service",
                "node-fetch",
                "axios",
                "Twilio Service",
                "AWS Service",
                "S3",
                "Kinesis",
                "Lambda",
                "SES"
            ],
            "paragraphs": "You should migrate deprecated third-party service integrations to use\nthe corresponding official libraries available from  npm . The\nsections later on this page contain details and examples for each\nservice. To migrate from a deprecated service: Add a library  for the service from\n npm  to your app. Import the library  in your\nfunctions. Replace any calls to the built-in service actions with\ncalls to the corresponding methods in the imported library. If the service has webhooks,  convert them to HTTPS endpoints . Replace HTTP requests sent through an  HTTP Service \nclient with calls to an HTTP library like  node-fetch  or  axios . Atlas App Services does not support v3 of  node-fetch . Use v2 instead. Replace calls through a  Twilio Service  client\nwith calls to the official  Twilio Node Helper Library . To authenticate Twilio requests, store your Account SID and Auth Token\nas  values . You can then access them within\nfunctions and pass them to the SDK. Replace calls through an  AWS Service  client with\ncalls to the official  AWS JavaScript SDK . To authenticate AWS requests, store your Access Key ID and Secret Access\nKey as  values . You can then access them\nwithin functions and pass them to the SDK. App Services does not yet support version 3 of the AWS SDK. Use the version\n2 SDK to replace the deprecated AWS Service in your functions.",
            "code": [
                {
                    "lang": "sh",
                    "value": "npm i node-fetch@^2"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const http = context.services.get(\"myHttp\");\n  const response = await http.get({\n    url: \"https://www.example.com\",\n    headers: { \"Content-Type\": [ \"application/json\" ] }\n  })\n  return response.body.text()\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const fetch = require(\"node-fetch\"); // require calls must be in exports function\n  const response = await fetch(\"https://www.example.com\", {\n    method: \"GET\",\n    headers: { \"Content-Type\": \"application/json\" }\n  })\n  return response.text()\n}"
                },
                {
                    "lang": "sh",
                    "value": "npm i axios"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const http = context.services.get(\"myHttp\");\n  const response = await http.get({\n    url: \"https://www.example.com\",\n    headers: { \"Content-Type\": [ \"application/json\" ] }\n  })\n  return response.body.text()\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const axios = require(\"axios\"); // require calls must be in exports function\n  const response = await axios.get(\"https://www.example.com\", {\n    headers: { \"Content-Type\": \"application/json\" }\n  })\n  return response.data\n}"
                },
                {
                    "lang": "sh",
                    "value": "npm i twilio"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const twilio = context.services.get(\"myTwilio\");\n  twilio.send({\n    to: \"+15558675309\",\n    from: \"+15551234567\",\n    body: \"Hello from App Services!\",\n  });\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  // Note: require calls must be in the exported function\n  const twilio = require('twilio')(\n    // Your Account SID and Auth Token from https://www.twilio.com/console\n    // Specify the same Twilio credentials as the service configuration\n    context.values.get(\"TwilioAccountSID\"),\n    context.values.get(\"TwilioAuthToken\"),\n  )\n  await twilio.messages.create({\n    to: \"+15558675309\",\n    from: \"+15551234567\",\n    body: \"Hello from App Services!\",\n  })\n}"
                },
                {
                    "lang": "sh",
                    "value": "npm i aws-sdk"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const s3 = context.services.get(\"myAWS\").s3(\"us-east-1\");\n\n  const putResult = await s3.PutObject({\n    Bucket: \"bucketName\",\n    Key: \"keyName\",\n    Body: EJSON.stringify({ hello: \"world\" }),\n  });\n\n  const getResult = await s3.GetObject({\n    Bucket: \"bucketName\",\n    Key: \"keyName\",\n  });\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const S3 = require('aws-sdk/clients/s3'); // require calls must be in exports function\n  const s3 = new S3({\n    accessKeyId: context.values.get(\"awsAccessKeyId\"),\n    secretAccessKey: context.values.get(\"awsSecretAccessKey\"),\n    region: \"us-east-1\",\n  });\n  // https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#putObject-property\n  const putResult = await s3.putObject({\n    Bucket: \"bucketName\",\n    Key: \"keyName\",\n    Body: EJSON.stringify({ hello: \"world\" }),\n  }).promise();\n  // https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#getObject-property\n  const getResult = await s3.getObject({\n    Bucket: \"bucketName\",\n    Key: \"keyName\",\n  }).promise();\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const kinesis = context.services.get(\"myAWS\").kinesis(\"us-east-1\");\n\n  const putResult = await kinesis.PutRecord({\n    Data: EJSON.stringify({ hello: \"world\" }),\n    StreamName: \"myStream\",\n    PartitionKey: \"myPartition\",\n  });\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const Kinesis = require('aws-sdk/clients/kinesis'); // require calls must be in exports function\n  const kinesis = new Kinesis({\n    accessKeyId: context.values.get(\"awsAccessKeyId\"),\n    secretAccessKey: context.values.get(\"awsSecretAccessKey\"),\n    region: \"us-east-1\",\n  });\n\n  // https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Kinesis.html#putRecord-property\n  const putResult = await kinesis.putRecord({\n    Data: EJSON.stringify({ hello: \"world\" }),\n    StreamName: \"myStream\",\n    PartitionKey: \"myPartition\",\n  }).promise();\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const lambda = context.services.get('MyAwsService').lambda(\"us-east-1\");\n\n  const invokeResult = await lambda.Invoke({\n    FunctionName: \"myLambdaFunction\",\n    Payload: EJSON.stringify({ hello: \"world\" }),\n  });\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const Lambda = require('aws-sdk/clients/lambda'); // require calls must be in exports function\n  const lambda = new Lambda({\n    accessKeyId: context.values.get(\"awsAccessKeyId\"),\n    secretAccessKey: context.values.get(\"awsSecretAccessKey\"),\n    region: \"us-east-1\",\n  });\n\n  // https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Lambda.html#invoke-property\n  const invokeResult = await lambda.invoke({\n    FunctionName: \"myLambdaFunction\",\n    Payload: EJSON.stringify({ hello: \"world\" }),\n  }).promise();\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const ses = context.services.get('MyAwsService').ses(\"us-east-1\");\n\n  const sendResult = await ses.SendEmail({\n    Source: \"sender@example.com\",\n    Destination: { ToAddresses: [\"receiver@example.com\"] },\n    Message: {\n      Body: {\n        Html: {\n          Charset: \"UTF-8\",\n          Data: `This is a message from user ${context.user.id} sent through AWS SES`\n        }\n      },\n      Subject: {\n        Charset: \"UTF-8\",\n        Data: \"Test Email Please Ignore\"\n      },\n    },\n  });\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const SES = require('aws-sdk/clients/ses'); // require calls must be in exports function\n  const ses = new SES({\n    accessKeyId: context.values.get(\"awsAccessKeyId\"),\n    secretAccessKey: context.values.get(\"awsSecretAccessKey\"),\n    region: \"us-east-1\",\n  });\n\n  // https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/SES.html#sendEmail-property\n  const sendResult = await ses.sendEmail({\n    Source: \"sender@example.com\",\n    Destination: { ToAddresses: [\"receiver@example.com\"] },\n    Message: {\n      Body: {\n        Html: {\n          Charset: \"UTF-8\",\n          Data: `This is a message from user ${context.user.id} sent through AWS SES`\n        }\n      },\n      Subject: {\n        Charset: \"UTF-8\",\n        Data: \"Test Email Please Ignore\"\n      },\n    },\n  }).promise();\n}"
                }
            ],
            "preview": "You should migrate deprecated third-party service integrations to use\nthe corresponding official libraries available from npm. The\nsections later on this page contain details and examples for each\nservice.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "graphql/custom-resolvers",
            "title": "Define a Custom Resolver",
            "headings": [
                "Overview",
                "Procedure",
                "Create a New Custom Resolver",
                "Define the Resolver Field Name",
                "Define the Parent Type",
                "Define the Input Type",
                "Define the Payload Type",
                "Define the Resolver Function",
                "Save and Deploy the Resolver",
                "Custom Resolver Examples",
                "Scenario & Schemas",
                "Custom Query Resolver",
                "Configuration",
                "Example Usage",
                "Custom Mutation",
                "Configuration",
                "Example Usage",
                "Computed Properties",
                "Configuration",
                "Example Usage"
            ],
            "paragraphs": "You can define custom resolvers that extend the GraphQL API for your app's use\ncases. Custom resolvers allow you to define new root-level operations that are\nmore complex or specific than the generated  query  and  mutation \nresolvers. You can also add new computed fields to  generated document\ntypes  that dynamically evaluate a result whenever an\noperation reads a document of the extended type. In the App Services UI, click  GraphQL  in the navigation sidebar and then\nselect the  Custom Resolvers  tab. Click the  Add a Custom Resolver  button to open the configuration\nscreen for a new custom resolver. Specify App Servicese name for the resolver in the  GraphQL Field\nName  input. App Services exposes the custom resolver in its parent type using this\nname, so the name should describe what the resolver does in a way that is\nuseful to developers who work with the GraphQL API. App Services exposes every custom resolver as a field on a parent type. The parent\ntype can be a root-level  query ,\n mutation , or a  generated document\ntype . In the  Parent Type  dropdown, select one of the following options: Option Description Query The resolver is a root-level  query  operation: A custom resolver for a query named  myCustomQuery  has the\nfollowing generated schema: Mutation The resolver is a root-level  mutation  operation: A custom resolver for a mutation named  myCustomMutation  has the\nfollowing generated schema: Document Type The resolver is a computed property on the specified document type.\nAny query or mutation that returns the document type can also ask for\ncomputed properties defined by custom resolvers on the type. A custom resolver that defines a computed property on the  Task \ntype named  myCustomTaskProperty  has the following generated\nschema: A custom resolver can accept input parameters from the incoming query or\nmutation. You can use an existing  generated input type  or define a new custom input type specifically for the\nresolver. If you specify an input type, App Services exposes the  input  parameter in the\ncustom resolver's generated GraphQL schema definition as an optional parameter\nthat accepts the specified input type. If you don't specify an input type, the\ncustom resolver does not accept any arguments. In the  Input Type  dropdown, select one of the following options: Option Description None The resolver does not accept any input. A custom resolver named  myCustomQuery  that does not accept an\ninput has the following generated schema: Scalar The resolver uses an existing  scalar type \nfrom the generated GraphQL schema. In the second dropdown input, select either a single scalar or an array\nof multiple scalars of the same type. A custom resolver named  myCustomQuery  that uses the\n Scalar Type  option to specify an input type of\n DateTiem  has the following generated schema: Existing Type The resolver uses an existing  input type \nfrom the generated GraphQL schema. In the second dropdown input, select either a single input object or an\narray of multiple input objects of the same type. A custom resolver named  myCustomQuery  that uses the\n Existing Type  option to specify an input type of\n TaskInsertInput  has the following generated schema: Custom Type App Services generates a new input type specifically for the resolver based on\na  schema  that you define. The schema must be an\n object  that contains at least one property and a  title  field\nthat defines a unique name for the generated input type. A custom resolver named  myCustomQuery  that uses the\n Custom Type  option with an input type named\n MyCustomQueryInput  has the following generated schema: All GraphQL resolvers must return a payload that conforms to a specific type\nin the schema. For a custom resolver, you can use an existing  generated\ndocument type , define a new custom payload type\nspecifically for the resolver, or use a default payload. App Services includes the\nspecified payload type in the custom resolver's generated GraphQL schema\ndefinition. In the  Payload Type  dropdown, select one of the following options: Option Description DefaultPayload The resolver returns the automatically generated  DefaultPayload \ntype which has the following signature: The  status  field will always resolve to  \"complete\"  regardless\nof the resolver function's return value. A custom resolver named  myCustomQuery  that uses the\n DefaultPayload  option has the following generated\nschema: Scalar The resolver uses an existing  scalar type \nfrom the generated GraphQL schema. In the second dropdown input, select either a single scalar or an array\nof multiple scalars of the same type. A custom resolver named  myCustomQuery  that uses the\n Scalar Type  option to specify a payload type of\n DateTime  has the following generated schema: Existing Type The resolver returns an existing  document type  from the generated GraphQL schema. In the second dropdown input, select either a single document type or an\narray of multiple documents of the same type. A custom resolver named  myCustomQuery  that uses the\n Existing Type  option to specify an input type of\n TaskInsertInput  has the following generated schema: A custom resolver named  myCustomQuery  that uses the\n Existing Type  option to specify a payload type of\n [Task]  has the following generated schema: Custom Type App Services generates a new payload type specifically for the resolver based\non a  schema  that you define. The schema must be an\n object  that contains at least one property and a  title  field\nthat defines a unique name for the generated input type. A custom resolver named  myCustomQuery  that uses the\n Custom Type  option with a payload type named\n MyCustomQueryPayload  has the following generated schema: When a user calls a custom resolver App Services executes the resolver\nfunction and returns the result, which must conform to the resolver's\n Payload Type . App Services passes the function any input data from the operation, if applicable. If\nthe resolver is a computed property on a document type, App Services\npasses the function the specific document that the resolver was called on. A custom resolver function has one of two possible signatures, depending on\nwhether or not it accepts an input: To define the resolver function, click the  Function  dropdown\nand either select an existing function or create a new one. Once you have configured the resolver, click  Save  and\ndeploy your application. Once deployed, you can call the\ncustom resolver through the GraphQL API. Consider a hypothetical dashboard that a sales team uses to show various\nstatistics and other performance metrics for a given time period. The dashboard\nuses the custom resolvers in this section to handle some of its specific use\ncases. The resolvers all reference  Sale  documents, which have the following schema: The sales team's hypothetical dashboard uses a custom query resolver that\nreturns aggregated sales data for a specific month. App Services generates schema definitions for the resolver's custom input and payload\ntypes and adds the resolver to its parent type, the root-level  Query : The resolver uses the following configuration: Option Description Parent Type Query GraphQL Field Name averageSaleForMonth Input Type Custom Type:  AverageSaleForMonthInput Payload Type Custom Type:  AverageSaleForMonthPayload Function To call this custom query, you could use the following operation and variables: The sales team's hypothetical dashboard uses a custom mutation resolver that\nadds a string note to a specific  Sale  document, identified by its  _id . App Services generates schema definitions for the resolver's custom input type and adds\nthe resolver to its parent type, the root-level  Mutation : The resolver uses the following configuration: Option Description Parent Type Mutation GraphQL Field Name addNoteToSale Input Type Custom Type:  AddNoteToSaleInput Payload Type Existing Type:  Sale Function To call this custom query, you could use the following operation and variables: The sales team's hypothetical dashboard uses a custom resolver that adds a new\ncomputed property to each  Sale  document. When an operation requests the\ncomputed field for a given  Sale , the resolver queries an external system and\nreturns support cases filed by the associated customer. App Services generates schema definitions for the resolver's custom payload type and\nadds the resolver to its parent type,  Sale : The resolver uses the following configuration: Option Description Parent Type Sale GraphQL Field Name customerSupportCases Input Type None Payload Type Custom Type:  [CustomerSupportCase] Function To use this custom computed property, you could run the following operation:",
            "code": [
                {
                    "lang": "graphql",
                    "value": "type Query {\n  myCustomQuery: DefaultPayload\n  ...\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type Mutation {\n  myCustomMutation: DefaultPayload\n  ...\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type Task {\n  myCustomTaskProperty: DefaultPayload\n  ...\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type Query {\n  myCustomQuery: DefaultPayload\n  ...\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type Query {\n  myCustomQuery(input: DateTime): DefaultPayload\n  ...\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type Query {\n  myCustomQuery(input: TaskInsertInput): DefaultPayload\n  ...\n}"
                },
                {
                    "lang": "graphql",
                    "value": "input MyCustomQueryInput {\n  someArgument: String;\n}\ntype Query {\n  myCustomQuery(input: MyCustomQueryInput): DefaultPayload\n  ...\n}"
                },
                {
                    "lang": "graphql",
                    "value": "{\n  status: \"complete\"\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type DefaultPayload {\n  status: String!\n}\n"
                },
                {
                    "lang": "graphql",
                    "value": "type Query {\n  myCustomQuery: DefaultPayload\n  ...\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type Query {\n  myCustomQuery: DateTime\n  ...\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type Query {\n  myCustomQuery: [Task]\n  ...\n}"
                },
                {
                    "lang": "graphql",
                    "value": "input MyCustomQueryPayload {\n  someValue: String;\n}\ntype Query {\n  myCustomQuery: MyCustomQueryPayload\n  ...\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function myCustomResolver(input, source) {\n  // The `input` parameter that contains any input data provided to the resolver.\n  // The type and shape of this object matches the resolver's input type.\n  const { someArgument } = input;\n\n  // If the resolver is a computed property, `source` is the parent document.\n  // Otherwise `source` is undefined.\n  const { _id, name } = source;\n\n  // The return value must conform to the resolver's configured payload type\n  return {\n    \"someValue\": \"abc123\",\n  };\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function myCustomResolver(source) {\n  // If the resolver is a computed property, `source` is the parent document.\n  // Otherwise `source` is undefined.\n  const { _id, name } = parent;\n\n  // The return value must conform to the resolver's configured payload type\n  return {\n    \"someValue\": \"abc123\",\n  };\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type Sale {\n  _id: ObjectId!\n  customer_id: String!\n  year: String!\n  month: String!\n  saleTotal: Float!\n  notes: [String]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"Sale\",\n  \"bsonType\": \"object\",\n  \"required\": [\"_id\", \"customer_id\", \"year\", \"month\", \"saleTotal\"],\n  \"properties\": {\n    \"_id\": { \"bsonType\": \"objectId\" },\n    \"customer_id\": { \"bsonType\": \"string\" },\n    \"year\": { \"bsonType\": \"string\" },\n    \"month\": { \"bsonType\": \"string\" },\n    \"saleTotal\": { \"bsonType\": \"decimal\" },\n    \"notes\": {\n      \"bsonType\": \"array\",\n      \"items\": { \"bsonType\": \"string\" }\n    }\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type Query {\n  averageSaleForMonth(input: AverageSaleForMonthInput): AverageSaleForMonthPayload\n}\n\ninput AverageSalesForMonthInput {\n  month: String!;\n  year: String!;\n}\n\ntype AverageSaleForMonthPayload {\n  month: String!;\n  year: String!;\n  averageSale: Float!;\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"bsonType\": \"object\",\n  \"title\": \"AverageSaleForMonthInput\",\n  \"required\": [\"month\", \"year\"],\n  \"properties\": {\n    \"month\": {\n      \"bsonType\": \"string\"\n    },\n    \"year\": {\n      \"bsonType\": \"string\"\n    }\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "input AverageSalesForMonthInput {\n  month: String!;\n  year: String!;\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"bsonType\": \"object\",\n  \"title\": \"AverageSaleForMonthPayload\",\n  \"required\": [\"month\", \"year\", \"averageSale\"],\n  \"properties\": {\n    \"month\": {\n      \"bsonType\": \"string\"\n    },\n    \"year\": {\n      \"bsonType\": \"string\"\n    },\n    \"averageSale\": {\n      \"bsonType\": \"decimal\"\n    }\n  }\n}"
                },
                {
                    "lang": "text",
                    "value": "type AverageSaleForMonthPayload {\n  month: String!;\n  year: String!;\n  averageSale: Float!;\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function averageSaleForMonth({ month, year }) {\n  const cluster = context.services.get(\"mongodb-atlas\");\n  const sales = cluster.db(\"corp\").collection(\"sales\");\n  const averageSalePayload = await sales\n    .aggregate([\n      { $match: { month: month, year: year } },\n      {\n        $group: {\n          _id: { month: \"$month\", year: \"$year\" },\n          averageSale: { $avg: \"$saleTotal\" },\n        }\n      },\n      {\n        $project: {\n          month: \"$_id.month\",\n          year: \"$_id.year\",\n          averageSale: 1\n        }\n      }\n    ])\n    .next();\n  return averageSalePayload;\n};"
                },
                {
                    "lang": "graphql",
                    "value": "query GetAverageSaleForMonth($averageSaleInput: AverageSaleForMonthInput!) {\n  averageSaleForMonth(input: $averageSaleInput) {\n    month\n    year\n    averageSale\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"variables\": {\n    \"averageSaleInput\": { month: \"March\", year: \"2020\" }\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type Mutation {\n  addNoteToSale(input: AddNoteToSaleInput): Sale\n}\n\ninput AddNoteToSaleInput {\n  sale_id: ObjectId!;\n  note: String!;\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"bsonType\": \"object\",\n  \"title\": \"AddNoteToSaleInput\",\n  \"required\": [\"sale_id\", \"note\"],\n  \"properties\": {\n    \"sale_id\": {\n      \"bsonType\": \"objectId\"\n    },\n    \"note\": {\n      \"bsonType\": \"string\"\n    }\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "input AddNoteToSaleInput {\n  sale_id: ObjectId!;\n  note: String!;\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type Sale {\n  _id: ObjectId!\n  customer_id: String!\n  year: String!\n  month: String!\n  saleTotal: Float!\n  notes: [String]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"Sale\",\n  \"bsonType\": \"object\",\n  \"required\": [\"_id\", \"customer_id\", \"year\", \"month\", \"saleTotal\"],\n  \"properties\": {\n    \"_id\": { \"bsonType\": \"objectId\" },\n    \"customer_id\": { \"bsonType\": \"string\" },\n    \"year\": { \"bsonType\": \"string\" },\n    \"month\": { \"bsonType\": \"string\" },\n    \"saleTotal\": { \"bsonType\": \"decimal\" },\n    \"notes\": {\n      \"bsonType\": \"array\",\n      \"items\": { \"bsonType\": \"string\" }\n    }\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function addNoteToSale({ sale_id, note }) {\n  const cluster = context.services.get(\"mongodb-atlas\");\n  const sales = cluster.db(\"corp\").collection(\"sales\");\n\n  const sale = await sales.findOneAndUpdate(\n    { _id: sale_id },\n    { $push: { notes: note } },\n    { returnNewDocument: true }\n  );\n  return sale;\n}"
                },
                {
                    "lang": "graphql",
                    "value": "mutation AddNoteToSale($addNoteToSaleInput: AddNoteToSaleInput) {\n  addNoteToSale(input: $addNoteToSaleInput) {\n    _id\n    customer_id\n    month\n    year\n    saleTotal\n    notes\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"variables\": {\n    \"addNoteToSaleInput\": {\n      \"sale_id\": \"5f3c2779796615b661fcdc25\",\n      \"note\": \"This was such a great sale!\"\n    }\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type Sale {\n  _id: ObjectId!\n  customer_id: String!\n  year: String!\n  month: String!\n  saleTotal: Float!\n  notes: [String]\n  customerSupportCases: [CustomerSupportCase]\n}\n\ntype CustomerSupportCase {\n  caseId: String!\n  description: String!\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"bsonType\": \"array\",\n  \"items\": {\n    \"title\": \"CustomerSupportCase\",\n    \"bsonType\": \"object\",\n    \"required\": [\"caseId\", \"description\"],\n    \"properties\": {\n      \"caseId\": { \"bsonType\": \"string\" },\n      \"description\": { \"bsonType\": \"string\" }\n    }\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type CustomerSupportCase {\n  caseId: String!\n  description: String!\n}\n\ntype Sale {\n  _id: ObjectId!\n  customer_id: String!\n  year: String!\n  month: String!\n  saleTotal: Float!\n  notes: [String]\n  customerSupportCases: [CustomerSupportCase]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function customerSupportCases(sale) {\n  // Return a list of objects from some external system\n  const cases = await fetchCustomerSupportCases({\n    customerId: sale.customer_id\n  });\n  return cases;\n};"
                },
                {
                    "lang": "graphql",
                    "value": "query GetSalesWithSupportCases {\n  sales {\n    _id\n    customer_id\n    year\n    month\n    saleTotal\n    notes\n    customerSupportCases {\n      caseId\n      description\n    }\n  }\n}"
                }
            ],
            "preview": "You can define custom resolvers that extend the GraphQL API for your app's use\ncases. Custom resolvers allow you to define new root-level operations that are\nmore complex or specific than the generated query and mutation\nresolvers. You can also add new computed fields to generated document\ntypes that dynamically evaluate a result whenever an\noperation reads a document of the extended type.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "graphql/types-and-resolvers",
            "title": "GraphQL Types, Resolvers, and Operators",
            "headings": [
                "Overview",
                "Scalar Types",
                "Document Types",
                "Field Mapping",
                "BSON Type Mapping",
                "Input Types",
                "QueryInput",
                "Comparison Operator Fields",
                "Logical Operator Fields",
                "Element Operator Fields",
                "InsertInput",
                "UpdateInput",
                "RelationInput",
                "SortByInput",
                "Query Resolvers",
                "Find a Single Document",
                "Find Multiple Documents",
                "Mutation Resolvers",
                "Insert a Single Document",
                "Insert Multiple Documents",
                "Update a Single Document",
                "Update Multiple Documents",
                "Upsert a Single Document",
                "Replace a Single Document",
                "Delete a Single Document",
                "Delete Multiple Documents",
                "Paginate Data"
            ],
            "paragraphs": "Atlas App Services automatically generates a GraphQL schema for any collection that has a\ndefined  schema . For each collection, App Services generates the\nfollowing: A  document type  that represents a single\ndocument in the collection A set of  queries  and  mutations  that allow you to access and manipulate\ndocuments in the collection. A set of  input types  that allow you to filter\nqueries, modify specific fields, and sort results. This page includes examples that demonstrate generated values based\non the following schema for a  movies  collection: App Services supports all of the standard  GraphQL scalar types  and also generates the  ObjectId  scalar. The following scalar types are supported: ObjectId : An  ObjectId  value serialized as a string Boolean :  true  or  false String : A UTF\u20108 character sequence Int : A signed 32\u2010bit integer Long : A signed 64\u2010bit integer Float : A signed double-precision floating-point value DateTime : An  RFC 3339  UTC DateTime (e.g. \"2020-09-01T15:38:14.918Z\") App Services generates a single  GraphQL type  for the\ndocuments in a collection based on the collection schema. The type uses the name\nset in the  title  field of the schema or the collection name if no  title \nis specified. App Services attempts to map fields in your collection schema directly to\nfields in your GraphQL types. The  definition of valid names  described in the GraphQL spec\ndoes not support all possible valid document field names, so App Services\napplies the following transformation rules to determine field names in generated\nGraphQL types: strip unsupported characters strip leading numbers convert to camel case omit fields that begin with a double underscore (e.g.  __myField ) The GraphQL type system is similar but not identical to the BSON types that you\ncan use in a  schema . App Services automatically attempts to map\nbetween the BSON types in your schema and supported GraphQL types. If a field\ntype does not have a GraphQL equivalent, App Services does not include the field in the\ngenerated GraphQL document type. The following table lists BSON types that you can use in a schema and the\nGraphQL types that they map to: JSON/BSON Type GraphQL Type objectId ObjectId int Int long Int double Float decimal Float date DateTime timestamp DateTime JSON supports two types that represent \"no value\":  undefined  and\n null . The GraphQL spec supports  null  but not  undefined ,\nso your app converts  undefined  values in the following way: If a document field is explicitly set to  undefined  then the\ncorresponding GraphQL type is an empty object, i.e.  {} . If the field name is not defined for the document at all, or if the\nvalue is explicitly set to  null , then the corresponding GraphQL\ntype is  null . GraphQL uses  input types  to represent\nparameters that you pass to queries and mutations. This is a standard approach\nused by all GraphQL APIs to define unambiguous, type-safe user inputs. A  QueryInput  object defines a set of one or more conditions that a document\nmust meet in order to be included in a query. The object may include fields from\nthe document type as well as any of the  operator fields  that App Services\nautomatically generates based on each field's type. Document Fields:  If a  QueryInput  field has the same name as a field in\nthe document type, then App Services matches a document if the value specified in\nthe input field and the value of the field in the document are the same. The following query includes a  QueryInput  with two fields,  rated \nand  year . Both of these field names are defined in the  Movie  document\ntype, so App Services performs an equality match for both. The query returns the title of all movies released in the year 2000 that\nare rated R. Operator Fields:  If a  QueryInput  field is a valid operator field for the\nqueried type, then App Services matches a document if the operator evaluates to\n true . The following query includes a  QueryInput  with two fields,  rated_in \nand  year_gt . Both of these are operator fields, so App Services evaluates\neach operator to determine if a document matches. The query returns the title of all movies released after the year 2000 that\nare rated either G or PG-13. A comparison operator field allows you to define a condition that is more\ncomplex than exact equality, such as a range query. App Services generates a set of\ncomparison operator fields for every field in the document type based on the\nfield type. Each comparison operator typically applies to only a subset of all\nfield types, so App Services only generates operator fields for valid combinations. A comparison operator field evaluates to  true  for a given document if the\nvalue of the field in the document satisfies the operator condition relative to\nthe specified value. Comparison operator fields have the following form: Operator Supported Field Types Operator Value Type Description gt <Field Type> Finds documents where the field is greater than the specified\nvalue. This query finds all movies that were released after the year 2000: gte <Field Type> Finds documents where the field is greater than or equal to the\nspecified value. This query finds all movies that were released in or after the year 2000: lt <Field Type> Finds documents where the field is less than the specified value. This query finds all movies that were released before the year 2000: lte <Field Type> Finds documents where the field is less than or equal to the\nspecified value. This query finds all movies that were released in or before the year 2000: ne <Field Type> Finds documents where the field is not equal to the specified value. This query finds all movies that were released in any year other than 2000: in [<Field Type>] Finds documents where the field is equal to any value in the specified\narray. If the field is an  Array , this finds all documents where\nany value in the field array is also included in the specified array. This query finds all movies that feature either or both of Emma Stone\nand Ryan Gosling: nin [<Field Type>] Finds documents where the field is not equal to any value in the\nspecified array. If the field is an  Array , this finds all documents\nwhere any value in the field array is also in the specified\narray. This query finds all movies that are not rated either G or PG-13: A logical operator field allows you to define logical combinations of\nindependent  QueryInput  objects. App Services generates root-level logical operator\nfields for all  QueryInput  types. A logical operator field evaluates to  true  for a given document if the\nevaluated result of all specified  QueryInput  objects satisfy the operator\ncondition. Logical operator fields have the following form: Operator Operator Value Type Description AND [QueryInput!] Finds documents that match  all  of the provided  QueryInput  objects. This query finds all movies that are rated PG-13  and  have a runtime\nof less than 120 minutes: OR [QueryInput!] Finds documents that match  any  of the provided  QueryInput  objects. This query finds all movies that are rated either G or PG-13: An element operator field allows you to define a boolean condition that\ndescribes a field in the document. App Services generates a set of element operator\nfields for every field in the document type. An element operator field evaluates to  true  for a given document if the\nresult of evaluating the operator condition on the field in the document matches\nthe specified boolean value. Element operator fields have the following form: Operator Supported Types Operator Value Type Description exists Available for all types Boolean Finds documents where the field is not  null . This query finds all movies that do not have a value defined for the\n year  field: An  InsertInput  object defines a document to insert into a collection. The\ndocument must conform to the GraphQL document type and include all required\nfields. The following mutation includes an  InsertInput  with several fields that\nare all defined in the  Movie  document type. The  Movie  type requires\nall documents to have a  title  field, so the  InsertInput  must include\none. The mutation inserts a new movie named \"My Fake Film\". An  UpdateInput  object defines a new value for one or more fields in a\ndocument. The updated document includes the new field values. Any fields that\nyou do not specify remain unchanged. The updated values must conform to the\nGraphQL document type. The following mutation includes an  UpdateInput  that sets the  title \nfield to \"My Super Real Film\". A  RelationInput  defines a new set of related documents for a relationship\nfield in the mutated document. You can reference documents that already exist in\nthe related collection with the  link  field or insert new documents into the\nrelated collection with the  create  field. You cannot use both  link  and  create  at the same time. If both\nare specified, the  create  operation takes precedence and the\n link  is ignored. The following mutation includes an  UpdateInput  that modifies the\n reviews  field. The field contains an array of  _id  values for\ndocuments in a separate  reviews  collection for to the field has a defined\nrelationship. The mutation sets the relationship to point to one newly created document and\ntwo existing documents in the  reviews  collection. A  SortByInput  enum defines a sort order for documents returned by a query.\nYou can sort in ascending and descending order by any root-level field that does\nnot have a type of  object  or  array . The GraphQL API does not support\nnested sorts. App Services generates two sort enum values for each field. Each value is a\nfully-capitalized identifier that combines the field name and the sort\ndirection, either  ASC  or  DESC . The following query returns movies sorted by the year they were released with\nthe most recent movies listed first. App Services generates two  GraphQL queries \nfor each collection: A  singular  query that finds a specific document in the collection. A  plural  query that finds all documents in the collection. You can\nfilter a plural query to include only the subset of documents in a\ncollection that match a  QueryInput . The single document query field uses the same name as the data type\nthat the collection contains. It returns a single document of the\nqueried type and accepts the following parameters: Parameter Type Description query QueryInput Optional. An object that defines a filter for documents in the\ncollection. The object may specify one or more fields from the\ndata type and must include a value for each field. The query\nmatches all documents that include the specified field values. If you do not specify a  query  parameter then the query\noperation matches all documents. The multiple document query field uses the same name as the data type\nthat the collection contains but has an additional  \"s\"  appended to\nthe type name. It returns an array of documents of the queried type and\naccepts the following parameters: Parameter GraphQL Type Description query QueryInput Optional. An object that defines a filter for documents in the\ncollection. The object may specify one or more fields from the\ndata type and must include a value for each field. The query\nmatches all documents that include the specified field values. If you do not specify a  query  argument then the query\noperation matches all documents. limit Int Optional. Default  100 . The maximum number of documents to\ninclude in the query result set. If the query matches more than\nthe set limit then it only returns a subset of matched documents. sortBy SortByInput Optional. A value that defines a sort order for documents in the\nquery result set. You can sort in ascending and descending order\nby any root-level field that does not have a type of  object \nor  array . The  sortBy  value is a fully-capitalized identifier that\ncombines the field name and the sort direction. For example: If you do not specify a  sortBy  argument then the query\noperation does not guarantee the order of documents in the result\nset. to sort by title from A to Z you would use  TITLE_ASC to sort by rating from highest to lowest you would use  RATING_DESC App Services generates a set of mutations for the documents in each\ncollection. These allow you insert, modify, and delete one or more\ndocuments. The single document insert mutation field uses the name\n insertOne<Type>  where  <Type>  is the singular name of the data\ntype that the collection contains. It returns the inserted document and\naccepts the following parameters: Parameter Type Description data InsertInput! Required. A document to insert into the collection. If the\ncollection schema marks a field as required then this document\nmust include a valid value for that field. App Services automatically\nconverts GraphQL types in the  InsertInput  object into their\nrespective BSON type. The multiple document insert mutation field uses the name\n insertMany<Type>s  where  <Type>  is the singular name of the data\ntype that the collection contains. It returns the inserted document and\naccepts the following parameters: Parameter Type Description data [ InsertInput! ]! Required. An array of documents to insert into the collection.\nThe array must contain at least one document. If the collection\nschema marks a field as required then each document must include\na valid value for that field. App Services automatically converts\nGraphQL types in the  InsertInput  object into their respective\nBSON type as defined in the collection schema. The single document update mutation field uses the name\n updateOne<Type>  where  <Type>  is the singular name of the data\ntype that the collection contains. It returns the updated document and\naccepts the following parameters: Parameter Type Description query QueryInput Optional. An object that configures which documents in the\ncollection to update. The object may specify one or more fields\nfrom the data type and must include a value for each field. The\nquery matches all documents that include the specified field\nvalues. If you do not specify a  query  argument then the mutation\nupdates the first document in the result set, which is likely but\nnot guaranteed to be the most recently inserted document. set UpdateInput! Required. An object that defines a new value for one or more\nfields in the document. The updated document will include the new\nfield values. Any fields that you do not specify remain\nunchanged. App Services automatically converts GraphQL types in the\n UpdateInput  object into their respective BSON type. The multiple document update mutation field uses the name\n updateMany<Type>s  where  <Type>  is the singular name of the data\ntype that the collection contains. It returns an  UpdateManyPayload \ndocument that describes the number of fields that were matched and\nmodified and accepts the following parameters: Parameter Type Description query QueryInput Optional. An object that configures which documents in the\ncollection to update. The object may specify one or more fields\nfrom the data type and must include a value for each field. The\nquery matches all documents that include the specified field\nvalues. If you do not specify a  query  argument then the mutation\nupdates the first document in the result set, which is likely but\nnot guaranteed to be the most recently inserted document. set UpdateInput! Required. An object that defines a new value for one or more\nfields in the document. The updated document will include the new\nfield values. Any fields that you do not specify remain\nunchanged. App Services automatically converts GraphQL types in the\n UpdateInput  object into their respective BSON type. The single document upsert mutation field uses the name\n upsertOne<Type>  where  <Type>  is the singular name of the data\ntype that the collection contains. This resolver updates a document that\nmatches the query parameter and inserts a new document if none match the\nquery. It returns the upserted document and accepts the following\nparameters: Parameter Type Description query QueryInput Optional. An object that configures which document to update. The\nobject may specify one or more fields from the data type and must\ninclude a value for each field. The query matches all documents\nthat include the specified field values. If you do not specify a  query  argument or no documents match,\nthen the mutation inserts the document specified in the  data \nparameter. data InsertInput! Required. The document to insert if the  query  does not match\nany existing documents. If the  query  does match a document\nreplaces the queried document. If the collection schema marks a\nfield as required then this document must include a valid value\nfor that field. App Services automatically converts GraphQL types in\nthe  InsertInput  object into their respective BSON type. The single document replacement mutation field uses the name\n replaceOne<Type>  where  <Type>  is the singular name of the data\ntype that the collection contains. It returns the replaced document and\naccepts the following parameters: Parameter Type Description query QueryInput Optional. An object that configures which documents in the\ncollection to replace. The object may specify one or more fields\nfrom the data type and must include a value for each field. The\nquery matches all documents that include the specified field\nvalues. If you do not specify a  query  argument then the mutation\nreplaces the first document in the result set, which is likely\nbut not guaranteed to be the most recently inserted document. data InsertInput! Required. The document that replaces the queried document. If the\ncollection schema marks a field as required then this document\nmust include a valid value for that field. App Services automatically\nconverts GraphQL types in the  InsertInput  object into their\nrespective BSON type. The single document delete mutation field uses the name\n deleteOne<Type>  where  <Type>  is the singular name of the data\ntype that the collection contains. It returns the deleted document and\naccepts the following parameters: Parameter Type Description query QueryInput Required. An object that configures which document in the\ncollection to delete. The object may specify one or more fields\nfrom the data type and must include a value for each field. The\nquery matches all documents that include the specified field\nvalues. If the  query  matches multiple documents, the mutation deletes\nthe first document in the result set, which is likely but not\nguaranteed to be the most recently inserted document. The multiple document delete mutation field uses the name\n deleteMany<Type>s  where  <Type>  is the singular name of the data\ntype that the collection contains. It returns a  DeleteManyPayload \ndocument that describes the number of documents that were deleted and\naccepts the following parameters: Parameter Type Description query QueryInput Optional. An object that configures which documents in the\ncollection to delete. The object may specify one or more fields\nfrom the data type and must include a value for each field. The\nquery matches all documents that include the specified field\nvalues. If you do not specify a  query  argument then the mutation\ndeletes all documents in the collection. You can paginate data in your queries with the types provided by the GraphQL API's\ngenerated schema. The Atlas GraphQL API  does not  have an  offset  operator, like the\n GraphQL documentation recommends for pagination . Instead you can use the generated schema's  find multiple documents query resolvers \nwith  query ,  limit , and  sortBy  operators to paginate data. To paginate data in  ascending  order: To paginate data in  descending  order: For a example of this pagination pattern implemented in a client application,\nrefer to  Paginate Data  in the Realm Web SDK documentation. This approach to pagination is similar to range queries for MongoDB drivers,\nas described in the  MongoDB Server documentation .",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"Movie\",\n  \"required\": [\"title\"],\n  \"properties\": {\n    \"_id\": { \"bsonType\": \"objectId\" },\n    \"title\": { \"bsonType\": \"string\" },\n    \"year\": { \"bsonType\": \"int\" },\n    \"rated\": { \"bsonType\": \"string\" },\n    \"runtime\": { \"bsonType\": \"int\" },\n    \"director\": { \"bsonType\": \"string\" },\n    \"reviews\": {\n      \"bsonType\": \"array\",\n      \"items\": { \"bsonType\": \"objectId\" }\n    },\n    \"cast\": {\n      \"bsonType\": \"array\",\n      \"items\": { \"bsonType\": \"string\" }\n    }\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type Movie {\n  _id: ObjectId\n  title: String!\n  year: Int\n  rated: String\n  runtime: Int\n  director: String\n  cast: [String]\n}"
                },
                {
                    "lang": "graphql",
                    "value": "movies(query: { rated: \"R\", year: 2000 }) {\n  title\n}"
                },
                {
                    "lang": "graphql",
                    "value": "movies(query: { rated_in: [\"G\", \"PG-13\"], year_gt: 2000 }) {\n  title\n}"
                },
                {
                    "lang": "text",
                    "value": "<Field Name>_<Operator>: <Operator Value>"
                },
                {
                    "lang": "graphql",
                    "value": "movies(query: { year_gt: 2000 }) {\n  title\n  year\n}"
                },
                {
                    "lang": "graphql",
                    "value": "movies(query: { year_gte: 2000 }) {\n  title\n  year\n}"
                },
                {
                    "lang": "graphql",
                    "value": "movies(query: { year_lt: 2000 }) {\n  title\n  year\n}"
                },
                {
                    "lang": "graphql",
                    "value": "movies(query: { year_lte: 2000 }) {\n  title\n  year\n}"
                },
                {
                    "lang": "graphql",
                    "value": "movies(query: { year_ne: 2000 }) {\n  title\n  year\n}"
                },
                {
                    "lang": "graphql",
                    "value": "movies(query: { cast_in: [\"Emma Stone\", \"Ryan Gosling\"] }) {\n  title\n  year\n}"
                },
                {
                    "lang": "graphql",
                    "value": "movies(query: { rated_nin: [\"G\", \"PG-13\"] }) {\n  title\n  year\n}"
                },
                {
                    "lang": "text",
                    "value": "<Operator>: [<QueryInput>, ...]"
                },
                {
                    "lang": "graphql",
                    "value": "query {\n  movies(query: { AND: [{ rated: \"PG-13\" }, { runtime_lt: 120 }] }) {\n    title\n    year\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "query {\n  movies(query: { OR: [{ rated: \"G\" }, { rated: \"PG-13\" }] }) {\n    title\n    year\n  }\n}"
                },
                {
                    "lang": "text",
                    "value": "<Field Name>_<Operator>: <Operator Value>"
                },
                {
                    "lang": "graphql",
                    "value": "query {\n  movies(query: { year_exists: false }) {\n    _id\n    title\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "insertOneMovie(input: {\n  title: \"My Fake Film\",\n  rated: \"UNRATED\",\n  year: 2020\n}) {\n  title\n}"
                },
                {
                    "lang": "graphql",
                    "value": "updateOneMovie(\n  query: { title: \"My Fake Film\" }\n  set: { title: \"My Super Real Film\" }\n) {\n  title\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type RelationInput {\n  link: [ObjectId]\n  create: [InsertInput]\n}"
                },
                {
                    "lang": "graphql",
                    "value": "updateOneMovie(\n  query: { title: \"My Fake Film\" }\n  set: {\n    reviews: {\n      link: [\"\", \"\"]\n      create: []\n    }\n  }\n) {\n  title\n}"
                },
                {
                    "lang": "graphql",
                    "value": "movies(sortBy: YEAR_DESC) {\n  title\n}"
                },
                {
                    "lang": "graphql",
                    "value": "query {\n  movie(query: { title: \"The Matrix\" }) {\n    title\n    year\n    runtime\n    director\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "query {\n  movies(\n    query: { year: 2000 }\n    limit: 100\n    sortBy: TITLE_ASC\n  ) {\n    title\n    year\n    runtime\n    director\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "mutation {\n  insertOneMovie(data: {\n    title: \"Little Women\"\n    director: \"Greta Gerwig\"\n    year: 2019\n    runtime: 135\n  }) {\n    _id\n    title\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "mutation {\n  insertManyMovies(data: [\n    {\n      title: \"Little Women\"\n      director: \"Greta Gerwig\"\n      year: 2019\n      runtime: 135\n    },\n    {\n      title: \"1917\"\n      director: \"Sam Mendes\"\n      year: 2019\n      runtime: 119\n    }\n  ]) {\n    _id\n    title\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "mutation {\n  updateOneMovie(\n    query: { title: \"The Room\" }\n    set: { runtime: 99 }\n  ) {\n    _id\n    title\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "mutation {\n  updateManyMovies(\n    query: { director: \"Tommy Wiseau\" }\n    set: { director: \"Tom Wiseau\" }\n  ) {\n    matchedCount\n    modifiedCount\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "mutation {\n  upsertOneMovie(\n    query: { title: \"Blacksmith Scene\" }\n    data: {\n      title: \"Sandcastles in the Sand\",\n      director: \"Robin Scherbatsky\"\n      runtime: 90\n      year: 2002\n    }\n  ) {\n    _id\n    title\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "mutation {\n  replaceOneMovie(\n    query: { title: \"Blacksmith Scene\" }\n    data: {\n      title: \"Sandcastles in the Sand\",\n      director: \"Robin Scherbatsky\"\n      runtime: 90\n      year: 2002\n    }\n  ) {\n    _id\n    title\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "mutation {\n  deleteOneMovie(query: { title: \"The Room\" }) {\n    _id\n    title\n    year\n    runtime\n    director\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "mutation {\n  deleteManyMovies(query: { director: \"Tommy Wiseau\" }) {\n    deletedCount\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "query PaginateAscending(\n  # Do not include `previousTitle` for the first query\n  # in a pagination sequence.\n  $previousTitle: String,\n  $limit: Int!,\n) {\n  movies(\n    query: { title_gt: $previousTitle  }\n    limit: $limit\n    sortBy: TITLE_ASC\n  ) {\n    title\n    year\n    runtime\n    director\n  }\n}"
                },
                {
                    "lang": "graphql",
                    "value": "query PaginateAscending(\n  # Do not include `nextTitle` for the first query\n  # in a pagination sequence.\n  $nextTitle: String,\n  $limit: Int!,\n) {\n  movies(\n    query: { title_lt: $nextTitle  }\n    limit: $limit\n    sortBy: TITLE_DESC\n  ) {\n    title\n    year\n    runtime\n    director\n  }\n}"
                }
            ],
            "preview": "Atlas App Services automatically generates a GraphQL schema for any collection that has a\ndefined schema. For each collection, App Services generates the\nfollowing:",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/webhook-requests-and-responses",
            "title": "Webhook Requests & Responses [Deprecated]",
            "headings": [
                "Overview",
                "Request Validation Methods",
                "Payload Signature Verification",
                "Secret as a Query Parameter",
                "Webhook Response Object"
            ],
            "paragraphs": "Third party services and push notifications in App Services have been deprecated\nin favor of creating HTTP endpoints that use  external dependencies  in functions. Webhooks have been renamed to  HTTPS Endpoints  with no change in behavior. You should\n migrate  existing Webhooks. Existing services will continue to work until  November 1, 2024 . Because third party services and push notifications are now deprecated, they have\nbeen removed by default from the App Services UI. If you need to manage an existing third party\nservice or push notification, you can add the configurations back to the UI by doing\nthe following: In the left navigation, under the  Manage  section, click\n App Settings . Enable the toggle switch next to\n Temporarily Re-Enable 3rd Party Services , and then save your\nchanges. Depending on the service,  incoming webhooks \noffer several ways to validate requests and customize the response that\nAtlas App Services sends back to the external service. To validate that a webhook request is coming from a trusted source, some\nexternal services require that incoming requests incorporate a secret\nstring in one of several prescribed manners. Other services, like the\n HTTP service , allow you to optionally require\nrequest validation. There are two type of  Request Validation  for webhooks:\n Payload Signature Verification \nand  Secret as a Query Parameter . HTTP/1.1 or greater is required when making requests. For maximum security, programmatically generate the  secret  string\nusing a secure package such as the  Python secrets module . Make sure that\nyou do not publish the secret or include it in your version control\nsystem. The  Verify Payload Signature  request validation option\nrequires that incoming requests include a hexadecimal-encoded\n HMAC (Hash-based Message Authentication Code)  SHA-256 hash\ngenerated from the request body and  secret  string in the\n X-Hook-Signature  header. Consider the following webhook request body and secret: The following  Atlas Function  generates the hash\nfor this  body  and  secret : The hash value must be assigned to the  X-Hook-Signature  HTTP\nrequest header on every request: To test that the request was properly signed, we could run the\nfollowing  curl  command: The  Require Secret as Query Param  request validation option\nrequires that incoming requests include the specified  secret  string\nas a  query parameter \nappended to the end of the URL. Consider a webhook configured to use a  secret  value of\n 12345 . All requests must be made to the webhook URL appended with\nthe secret as a query parameter: To test that requests to this URL are properly verified, we could run\nthe following  curl  command: App Services automatically passes a  response  object that represents the\nwebhook's HTTP response as the second argument to webhook Function.\nThe following table lists the available methods for modifying the\n response  object: Method Arguments Description setStatusCode(code) code   integer Set the HTTP response  status code . setBody(body) body   string  or  BSON.Binary Set the HTTP response  body . If  body  is a string, it will be encoded to  BSON.Binary \nbefore being returned. setHeader(name, value) Set the HTTP response  header \nspecified by  name  to the value passed in the  value \nargument. This overrides any other values that may have already\nbeen assigned to that  header. addHeader(name, value) Set the HTTP response  header \nspecified by  name  to the value passed in the  value \nargument. Unlike  setHeader , this does not override other\nvalues that have already been assigned to the header.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const body = { \"message\":\"MESSAGE\" }\nconst secret = 12345"
                },
                {
                    "lang": "javascript",
                    "value": "// Generate an HMAC request signature\nexports = function(secret, body) {\n  // secret = the secret validation string\n  // body = the webhook request body\n  return utils.crypto.hmac(EJSON.stringify(body), secret, \"sha256\", \"hex\");\n}\n// Returns: \"828ee180512eaf8a6229eda7eea72323f68e9c0f0093b11a578b0544c5777862\""
                },
                {
                    "lang": "none",
                    "value": "X-Hook-Signature:sha256=<hex-encoded-hash>"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-Hook-Signature:sha256=828ee180512eaf8a6229eda7eea72323f68e9c0f0093b11a578b0544c5777862\" \\\n  -d '{\"message\":\"MESSAGE\"}' \\\n  <webhook URL>"
                },
                {
                    "lang": "none",
                    "value": "<webhook URL>?secret=12345"
                },
                {
                    "lang": "none",
                    "value": "curl -H \"Content-Type: application/json\" \\\n     -d '{ \"message\": \"HELLO\" }' \\\n     -X POST '<webhook URL>?secret=12345'"
                },
                {
                    "lang": "javascript",
                    "value": "response.setStatusCode(201);"
                },
                {
                    "lang": "javascript",
                    "value": "response.setBody(\n  \"{'message': 'Hello, World!'}\"\n);"
                },
                {
                    "lang": "javascript",
                    "value": "response.setHeader(\n  \"Content-Type\",\n  \"application/json\"\n);"
                },
                {
                    "lang": "javascript",
                    "value": "response.addHeader(\n  \"Cache-Control\",\n  \"max-age=600\"\n);\n\nresponse.addHeader(\n  \"Cache-Control\",\n  \"min-fresh=60\"\n)"
                }
            ],
            "preview": "Depending on the service, incoming webhooks\noffer several ways to validate requests and customize the response that\nAtlas App Services sends back to the external service.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/twilio",
            "title": "Twilio Service [Deprecated]",
            "headings": [
                "Overview",
                "Configuration Parameters",
                "Service Actions",
                "Incoming Webhooks",
                "Configuration",
                "Request Payload",
                "Example Webhook Function",
                "Configure Twilio",
                "Create a Messaging Service",
                "Add a Webhook to a Twilio Project"
            ],
            "paragraphs": "Third party services and push notifications in App Services have been deprecated\nin favor of creating HTTP endpoints that use  external dependencies  in functions. Webhooks have been renamed to  HTTPS Endpoints  with no change in behavior. You should\n migrate  existing Webhooks. Existing services will continue to work until  November 1, 2024 . Because third party services and push notifications are now deprecated, they have\nbeen removed by default from the App Services UI. If you need to manage an existing third party\nservice or push notification, you can add the configurations back to the UI by doing\nthe following: In the left navigation, under the  Manage  section, click\n App Settings . Enable the toggle switch next to\n Temporarily Re-Enable 3rd Party Services , and then save your\nchanges. Twilio provides messaging, voice, and chat services for web and mobile\napps. The Atlas App Services Twilio service supports integrating Twilio's\n Programmable SMS  service into your application. To send an outbound text message, use the  send()  action. To handle and optionally respond to incoming text messages,\n create an incoming webhook  and\n add it to a Twilio messaging service . See the  incoming webhooks  section on this page for an example. To use Twilio with App Services, you must have a  Twilio Phone\nNumber  registered to a messaging service associated with your Twilio\naccount. You can create a new number from the  Numbers \npage of the Twilio dashboard, or by following Twilio's\n Programmable SMS Quickstart  guide. You will need to provide values for the following parameters when you\n create a Twilio service interface : Parameter Description The name of this Twilio service interface. This must be unique\nfrom all other service interfaces in your application. A unique identifier for your Twilio account. You can find this\nvalue on your Twilio account dashboard. The name of a  Secret  that stores a Twilio\nauthorization token, which proves that you are the owner of a\nTwilio account. You can find this value on your Twilio account\ndashboard. The Twilio service in App Services provides the following actions which are\navailable in  functions  and in the SDKs: For instructions on using a service action, see  Call a Service\nAction . Action Description twilio.send() Sends a text message to a specified phone number. Incoming webhooks for the Twilio service enable your App\nto handle incoming text messages. Once you've created an incoming\nwebhook, you can add it to a Twilio messaging service to handle incoming\nmessages for that service. Twilio Service webhoooks are deprecated in favor of custom HTTPS\nendpoints. To learn how to migrate your existing Twilio webhooks to\nendpoints, see  Convert Webhooks to HTTPS Endpoints . You will need to provide values for the following parameters when you\n create a Twilio incoming webhook : You will need to provide a configuration file of the following form when\nyou  create a Twilio incoming webhook : Configuration Value Description Required. The name of the webhook. Each incoming webhook in a Twilio service interface must\nhave a unique name. Required. If  true , App Services sends the return value of the\nwebhook function to Twilio in the response body. Twilio will automatically send a text message containing the\nwebhook response's body to the phone number that sent the\ninitial message. Optional. The id of the  App Services user  that\nexecutes the webhook function when the webhook is called. There are three ways to configure the execution user: System : The execution user is the  system user , which has full access to MongoDB CRUD and Aggregation APIs and\nbypasses all rules and schema validation. User Id : You select a specific application user to execute the function. Script : You define a  function  that\nreturns the  id  of the execution user. You can specify the user id directly in  run_as_user_id  or\nprovide a stringified  Atlas Function  that accepts the webhook payload and returns the user\nid in  run_as_user_id_script_source . If you do not specify a\nspecific user id or a function that resolves to a user id,\nApp Services executes the webhook function as the  system\nuser  that has full access to MongoDB CRUD and\nAggregation APIs and bypasses all rules and schema validation. App Services automatically passes a  payload  document as the first argument\nto incoming webhook functions. In a Twilio Service incoming webhook the\n payload  object represents an incoming SMS message and has the\nfollowing form: Field Description From A string that contains the  E.164-formatted  phone number that sent the incoming\ntext message. To A string that contains the  E.164-formatted  phone number associated with your\nTwilio messaging service that the incoming text message was sent\nto. Body A string that contains the content of the incoming text message. A text message sent from the phone number  (555)867-5309  to the\nTwilio phone number  (805)716-6646  with the message  \"Hello! How\nare you?\"  would be represented by the following  payload \ndocument: The following webhook function stores text messages sent to a Twilio\nphone number in a MongoDB collection and sends a text message response\nto the phone number that sent the text. Log in to  Twilio . Click  Programmable SMS  in the left navigation menu of your Twilio project. Click  SMS > Messaging Services . Click  Create new Messaging Service . Enter a  Friendly Name  and  Use Case Click  Create Your App is now integrated with Twilio's SMS messaging\nservice. Send a message to your Twilio phone number to invoke the\nincoming webhook for your App. Click  Programmable SMS  in the left navigation menu of your Twilio project. Click  SMS > Messaging Services . Click the messaging service that you want to use. On the messaging service configuration page, check the\n PROCESS INBOUND MESSAGES  box. Enter the incoming webhook URL in the  Request URL  box. Click  Save .",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Service Name>\",\n  \"type\": \"twilio\",\n  \"config\": {\n    \"sid\": <Twilio Account SID>\n  },\n  \"secret_config\": {\n    \"auth_token\": \"<Secret Name>\"\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"name\": <string>,\n  \"respond_result\": <boolean>,\n  \"run_as_user_id\": <string>,\n  \"run_as_user_id_script_source\": <string>,\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"From\": \"<Sender's Phone Number>\",\n   \"To\":   \"<Receiver's Phone Number>\",\n   \"Body\": \"<SMS Body>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"From\": \"+15558675309\",\n  \"To\": \"+18057166646\",\n  \"Body\": \"Hello! How are you?\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function(payload, response) {\n  // const { To, From, Body } = payload;\n  const mongodb = context.services.get(\"mongodb-atlas\");\n  const texts = mongodb.db(\"demo\").collection(\"texts\");\n\n  try {\n    // Save the text message body, to number, and from number\n    const { insertedId } = await texts.insertOne(payload);\n    // Send the user a confirmation text message\n    response.setBody(`Saved your text message with _id: ${insertedId}`);\n  } catch (error) {\n    // Send the user an error notification text message\n    response.setBody(`Failed to save your text message: ${error}`);\n  }\n}"
                }
            ],
            "preview": "Atlas App Services, Atlas App Services Documentation Manual, MongoDB Backend as a Service, Integrate Atlas App Services with Twilio, Twilio Integration with Atlas App Services",
            "tags": "Atlas App Services, MongoDB, App Services, MongoDB Backend as a Service, Integrate, Twilio",
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/github",
            "title": "GitHub Service [Deprecated]",
            "headings": [
                "Overview",
                "Configuration Parameters",
                "Service Actions",
                "Incoming Webhooks",
                "Configuration",
                "Request Payload",
                "Example Webhook Function",
                "Configure GitHub",
                "Add a Webhook to a GitHub Repository"
            ],
            "paragraphs": "Third party services and push notifications in App Services have been deprecated\nin favor of creating HTTP endpoints that use  external dependencies  in functions. Webhooks have been renamed to  HTTPS Endpoints  with no change in behavior. You should\n migrate  existing Webhooks. Existing services will continue to work until  November 1, 2024 . Because third party services and push notifications are now deprecated, they have\nbeen removed by default from the App Services UI. If you need to manage an existing third party\nservice or push notification, you can add the configurations back to the UI by doing\nthe following: In the left navigation, under the  Manage  section, click\n App Settings . Enable the toggle switch next to\n Temporarily Re-Enable 3rd Party Services , and then save your\nchanges. GitHub is a web-based development platform for hosting and reviewing\nGit repositories. The Atlas App Services GitHub service allows your application to react to events in\na GitHub repository, such as new pull requests or issues. You will need to provide values for the following parameters when you\n create a GitHub service interface : Parameter Description The name of this GitHub service interface. This must be unique\nfrom all other service interfaces in your application. GitHub Services do not provide any service actions. Use an\n incoming webhook  to respond to events\nin your GitHub repo. GitHub can invoke one or more webhooks whenever a particular event\noccurs in a repository. If you'd like to learn more about GitHub's\nwebhook functionality, including detailed reference information about\nGitHub event types, see GitHub's  Webhook \ndocumentation. GitHub Service webhoooks are deprecated in favor of custom HTTPS\nendpoints. To learn how to migrate your existing GitHub webhooks to\nendpoints, see  Convert Webhooks to HTTPS Endpoints . You will need to provide values for the following parameters\nwhen you  configure a GitHub incoming webhook : You will need to provide a configuration file of the following\nform when you  configure a GitHub incoming webhook : Configuration Value Description Required. The name of the webhook. Each incoming webhook in a\nGitHub service interface must have a unique name. Required. If  true , App Services sends the return value of the\nwebhook function to GitHub in the response body. Optional. The id of the  App Services user  that\nexecutes the webhook function when the webhook is called. There are three ways to configure the execution user: System : The execution user is the  system user , which has full access to MongoDB CRUD and Aggregation APIs and\nbypasses all rules and schema validation. User Id : You select a specific application user to execute the function. Script : You define a  function  that\nreturns the  id  of the execution user. You can specify the user id directly in  run_as_user_id  or\nprovide a stringified  Atlas Function  that accepts the webhook payload and returns the user\nid in  run_as_user_id_script_source . If you do not specify a\nspecific user id or a function that resolves to a user id,\nApp Services executes the webhook function as the  system\nuser  that has full access to MongoDB CRUD and\nAggregation APIs and bypasses all rules and schema validation. The  GitHub Secret  string that GitHub includes with\nincoming requests to prove that the requests are valid. You must\nspecify this value in the settings of your GitHub repo when you\nprovide a webhook URL. App Services automatically passes a  payload  document as the first argument\nto incoming webhook functions. In a GitHub Service incoming webhook the\n payload  object represents the GitHub event that caused GitHub to\ncall the webhook. The exact content of GitHub  payload  documents varies depending on\nthe event type that it represents. For a detailed description of a\nspecific event type's payload document, refer to GitHub's\n Event Types & Payloads \ndocumentation. The following webhook function inserts incoming data into a MongoDB\ncollection. The  payload  document is passed by the GitHub service and contains\ninformation from the request. Log into  GitHub . Navigate to the repository that you want to subscribe to. Click the  Settings  tab of the repository and select\n Webhooks  from the left hand menu. Click  Add Webhook . Add the webhook URL to the  Payload URL  field. Set the content type to  application/json . Enter the  GitHub Secret . This should match the value you\nprovided in the webhook configuration. Choose the type of events that you want to subscribe to. Click  Add webhook .",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Service Name>\",\n  \"type\": \"github\",\n  \"config\": {}\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"name\": <string>,\n  \"respond_result\": <boolean>,\n  \"run_as_user_id\": <string>,\n  \"run_as_user_id_script_source\": <string>,\n  \"options\": {\n    \"secret\": <string>\n  },\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function(payload) {\n    const mongodb = context.services.get(\"mongodb-atlas\");\n    const requestlogs = mongodb.database(\"test\").collection(\"requestlogs\");\n\n    return requestlogs\n      .insertOne({\n        \"commits\": payload.commits,\n        \"pushed_by\": payload.pusher,\n        \"repo\": payload.repository.html_url\n      })\n      .then(({ insertedId }) => `Inserted document with _id: ${insertedId}`)\n}"
                }
            ],
            "preview": "GitHub is a web-based development platform for hosting and reviewing\nGit repositories.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/configure/services",
            "title": "Configure Third-Party Services [Deprecated]",
            "headings": [
                "Overview",
                "Procedure",
                "Create a New Service",
                "Enter Configuration Values",
                "Authenticate a MongoDB Atlas User",
                "Pull the Latest Version of Your App",
                "Create a Service Configuration Directory",
                "Add a Service Configuration File",
                "Import the Service"
            ],
            "paragraphs": "Third party services and push notifications in App Services have been deprecated\nin favor of creating HTTP endpoints that use  external dependencies  in functions. Webhooks have been renamed to  HTTPS Endpoints  with no change in behavior. You should\n migrate  existing Webhooks. Existing services will continue to work until  November 1, 2024 . Because third party services and push notifications are now deprecated, they have\nbeen removed by default from the App Services UI. If you need to manage an existing third party\nservice or push notification, you can add the configurations back to the UI by doing\nthe following: In the left navigation, under the  Manage  section, click\n App Settings . Enable the toggle switch next to\n Temporarily Re-Enable 3rd Party Services , and then save your\nchanges. You can create and configure interfaces for all  external service  from the App Services UI or by  importing  a service configuration\ndirectory. Select the tab below that corresponds to the method you want\nto use. Click  Services  in the left navigation menu. Click  Add a Service . Select the type of service you wish to create from the list. Enter a  Service Name  that uniquely identifies the\nservice. Enter additional configuration values specific to the type of\nservice you are creating. Some Service configuration values are\nprivate and require you to create and reference a  Secret  that contains the configuration value. Refer to the individual service reference pages for details on the\nconfiguration parameters for each type of service. Click  Add Service  to save the service. Use your MongoDB Atlas Admin API Key to log in to the CLI: Get a local copy of your App's configuration files. To get the latest\nversion of your App, run the following command: You can also export a copy of your application's configuration files\nfrom the UI or with the Admin API. To learn how, see  Export an App . Create a new subdirectory in the  /services  folder of the\napplication directory that you exported. The name of the subdirectory\nshould match the configured name of the service. Add a file named  config.json  to the new service directory.\nThe configuration file should have the following form: Configuration Value Description Service Type Required. The type of the configured service. Service Name Required. The name of the configured service. Each service interface in your application must have a\nunique name. Configuration Required. A document that contains configuration values\nspecific to the type of the service you are configuring. To\nfind the configuration values for a specific service, refer to\nthat service's reference page. Once you have added the appropriate configuration files to the service\nsubdirectory, you can import the service into your application. Navigate to the root of the application directory and run the\nfollowing command:",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices login --api-key=\"<my api key>\" --private-api-key=\"<my private api key>\""
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "shell",
                    "value": "mkdir -p services/myService"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"type\": <boolean>,\n  \"name\": <string>,\n  \"config\": <document>\n}"
                },
                {
                    "lang": "shell",
                    "value": "realm-cli import"
                }
            ],
            "preview": "You can create and configure interfaces for all external service from the App Services UI or by importing a service configuration\ndirectory. Select the tab below that corresponds to the method you want\nto use.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/http",
            "title": "HTTP Service [Deprecated]",
            "headings": [
                "Overview",
                "Configuration Parameters",
                "Service Actions",
                "Incoming Webhooks",
                "Configuration",
                "Request Payload",
                "Example Webhook Function"
            ],
            "paragraphs": "Third party services and push notifications in App Services have been deprecated\nin favor of creating HTTP endpoints that use  external dependencies  in functions. Webhooks have been renamed to  HTTPS Endpoints  with no change in behavior. You should\n migrate  existing Webhooks. Existing services will continue to work until  November 1, 2024 . Because third party services and push notifications are now deprecated, they have\nbeen removed by default from the App Services UI. If you need to manage an existing third party\nservice or push notification, you can add the configurations back to the UI by doing\nthe following: In the left navigation, under the  Manage  section, click\n App Settings . Enable the toggle switch next to\n Temporarily Re-Enable 3rd Party Services , and then save your\nchanges. The Atlas App Services HTTP Service is a generic interface that enables you to\ncommunicate with any service that is available over HTTP, such as those\nthat provide a REST API. This is useful when you need to use a service\nthat does not have a custom service built-in to App Services. To send an outbound HTTP request, call one of the  HTTP actions . To handle incoming requests from an external service,  configure an\nincoming webhook  and provide it to\nthe service, if possible. See the  incoming webhooks  section on this page for an example. You will need to provide values for the following parameters when you\n create an HTTP service interface : Parameter Description The name of this HTTP service interface. This must be unique from\nall other service interfaces in your application. The HTTP service in App Services provides the following actions that you can\ncall in  functions  and in the SDKs. Each action maps\nto a standard  HTTP request method . For instructions on using an HTTP service action, see  Call a\nService Action . You must enable a service action in a  service rule  before you can call it. Action Description http.get() Send an  HTTP GET  request. http.post() Send an  HTTP POST  request. http.put() Send an  HTTP PUT  request. http.patch() Send an  HTTP PATCH  request. http.delete() Send an  HTTP DELETE  request. http.head() Send an  HTTP HEAD  request. HTTP Service webhoooks are deprecated in favor of custom HTTPS\nendpoints. You can automatically migrate your existing webhooks to\nendpoints in one-click. To learn how, see  Convert Webhooks to\nHTTPS Endpoints . You will need to provide values for the following parameters\nwhen you  configure an HTTP incoming webhook : You will need to provide a  configuration file  of the following form\nwhen you  configure an HTTP incoming webhook : Configuration Value Description Required. The name of the webhook. Each incoming webhook in an HTTP service interface must\nhave a unique name. Required. If  true , App Services sends a response to the client that\ncalled the webhook. The response body will be the return value\nof the webhook function. Optional. The id of the  App Services user  that\nexecutes the webhook function when the webhook is called. There are three ways to configure the execution user: System : The execution user is the  system user , which has full access to MongoDB CRUD and Aggregation APIs and\nbypasses all rules and schema validation. User Id : You select a specific application user to execute the function. Script : You define a  function  that\nreturns the  id  of the execution user. You can specify the user id directly in  run_as_user_id  or\nprovide a stringified  Atlas Function  that accepts the webhook payload and returns the user\nid in  run_as_user_id_script_source . If you do not specify a\nspecific user id or a function that resolves to a user id,\nApp Services executes the webhook function as the  system\nuser  that has full access to MongoDB CRUD and\nAggregation APIs and bypasses all rules and schema validation. The  HTTP method  that incoming webhook\nrequests should use. You can configure a webhook to accept any\nmethod or specify a specific method. The following methods are\nsupported: GET POST PUT PATCH DELETE HEAD ANY The  request validation  method\nincoming requests should use. The following validation types are\nsupported: Verify Payload Signature Require Secret as Query Param No validation. VERIFY_PAYLOAD :  Verify Payload Signature SECRET_AS_QUERY_PARAM :  Require Secret as Query Param NO_VALIDATION : No validation. If  Request Validation  is enabled, this is the\nvalidation secret. App Services automatically passes a  payload  document as the first argument\nto incoming webhook functions. In an HTTP Service incoming webhook the\n payload  object represents an incoming HTTP request and has the\nfollowing form: Field Description query A document where each field corresponds to a  query\nparameter  that the external service included in\nthe webhook URL. A request sent to a webhook URL with the query parameters\n someParameter=42&anotherParameter=hello  would have the\nfollowing  query  document: headers A document where each field corresponds to an  HTTP header  that the external service included in the\nwebhook URL. A request sent to a webhook URL with a  Content-Type:\napplication/json  header would have the following  headers \ndocument: body A  BSON.Binary  object encoded from the\nrequest body. You can access the request body by serializing the\nbinary object to a string and then parsing the string to EJSON: The following webhook function inserts incoming data into a MongoDB\ncollection and returns the  insertedId  in the response  body .",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Service Name>\",\n  \"type\": \"http\",\n  \"config\": {}\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Webhook Name>\",\n  \"can_evaluate\": { <JSON Expression> },\n  \"run_as_authed_user\": <Boolean>,\n  \"run_as_user_id\": \"<App Services User ID>\",\n  \"run_as_user_id_script_source\": \"<Function Source Code>\",\n  \"respond_result\": <Boolean>,\n  \"fetch_custom_user_data\": <Boolean>,\n  \"create_user_on_auth\": <Boolean>,\n  \"options\": {\n    \"httpMethod\": \"<HTTP Method>\",\n    \"validationMethod\": \"<Webhook Validation Method>\",\n    \"secret\": \"<Webhook Secret>\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"query\": <query parameters>,\n   \"headers\": <request headers>,\n   \"body\": <request body (BSON)>\n}"
                },
                {
                    "lang": "json",
                    "value": "\"query\": {\n  \"someParameter\": 42,\n  \"anotherParameter\": \"hello\"\n}"
                },
                {
                    "lang": "json",
                    "value": "\"headers\": {\n  \"Content-Type\": [\"application/json\"]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "const body = EJSON.parse(payload.body.text())"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function(payload, response) {\n  const mongodb = context.services.get(\"mongodb-atlas\");\n  const requestLogs = mongodb.db(\"test\").collection(\"requestlogs\");\n  requestLogs.insertOne({\n    body: EJSON.parse(payload.body.text()),\n    query: payload.query\n  }).then(result => {\n    response.setStatusCode(201);\n    response.setBody(result.insertedId);\n  })\n};"
                }
            ],
            "preview": "The Atlas App Services HTTP Service is a generic interface that enables you to\ncommunicate with any service that is available over HTTP, such as those\nthat provide a REST API. This is useful when you need to use a service\nthat does not have a custom service built-in to App Services.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/http-actions/request-authentication",
            "title": "Request Authentication",
            "headings": [
                "Basic Authentication",
                "Digest Authentication"
            ],
            "paragraphs": "You can authenticate an outbound HTTP request using one of the standard\n HTTP authentication schemes . Atlas App Services\nsupports the following authentication schemes: HTTP  basic authentication \nrequires that incoming requests include a valid username and password\nfor the requested service. You can specify the user credentials in the\n username  and  password  fields of the request document, directly\nin a  url  string, or in an  Authorization  HTTP header. The following examples demonstrate three equivalent ways to\nauthenticate an HTTP service request using basic authentication. The\nexamples all use the username  MyUser  and the password\n Mypassw0rd . You would pass one of these objects as an argument to\nthe given HTTP method. HTTP  digest authentication \nrequires that incoming requests include an authorization key based on a\nrandom  nonce  value returned from the\nserver. App Services can automatically construct the key and authorize requests\ngiven a valid username and password. To configure a request to use digest authentication, set the\n digestAuth  field to  true  and specify the user credentials in the\n username  and  password  fields of the request document or directly\nin a  url  string. The following examples demonstrate two equivalent ways to\nauthenticate an HTTP service request using digest authentication. The\nexamples all use the username  MyUser  and the password\n Mypassw0rd .",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"scheme\": \"https\",\n  \"username\": \"MyUser\",\n  \"password\": \"Mypassw0rd\",\n  \"domain\": \"www.example.com\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": \"https://MyUser:Mypassw0rd@www.example.com\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"url\": \"https://www.example.com\",\n  \"headers\": {\n    \"Authorization\": [\n      `Basic ${BSON.Binary.fromText(\"MyUser:Mypassw0rd\").toBase64()}`\n    ]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"scheme\": \"https\",\n  \"username\": \"MyUser\",\n  \"password\": \"Mypassw0rd\",\n  \"domain\": \"www.example.com\",\n  \"digestAuth\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": \"https://MyUser:Mypassw0rd@www.example.com\",\n  \"digestAuth\": true\n}"
                }
            ],
            "preview": "You can authenticate an outbound HTTP request using one of the standard\nHTTP authentication schemes. Atlas App Services\nsupports the following authentication schemes:",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/http-actions/http.head",
            "title": "http.head()",
            "headings": [
                "Definition",
                "Usage",
                "Example",
                "Parameters",
                "Alternative URL Parameters",
                "Return Value",
                "Request Authentication",
                "Basic Authentication",
                "Digest Authentication",
                "Rule Templates",
                "Users Can Only Send Requests to a Specific Host",
                "Requests URLs Must Include a Specific Query Parameter",
                "Requests Must Be to a Specific Path"
            ],
            "paragraphs": "Sends an  HTTP HEAD  request to the\nspecified URL using the Atlas App Services  HTTP Service . This example assumes you have  configured an HTTP service called\n\"myHttp\" . If you do not need this\nHTTP action validated with  Service Rules ,\nyou can use the  Send an HTTP Request ( context.http )  module. The  http.head()  action accepts one argument of the\nfollowing form: Field Description Request URL Required. The target URL for the HTTP request. Alternatively,\nyou can specify the components of the URL as root-level fields.\nSee  Alternative URL Arguments . Request Headers Optional. A document where each field name corresponds to a type of\nHTTP header and each field value is an array of one or more\nstring values for that header. Request Cookies Optional. A document where each field name corresponds to a\ncookie name and each field value is that cookie's string value. Digest Authentication Optional. If  true , App Services authenticates the request using\n digest authentication . You must specify a\n username  and  password  (either as fields in the request\ndocument or as part of the URL) to use digest authentication.\nFor more details, see  Request Authentication . Request Authentication URL Optional. A URL that grants authorization cookies for the HTTP\nrequest. Follow Redirects Optional. If  true , the request will follow any HTTP\nredirects it receives for the target URL. If you need to specify the individual components of the request's\ntarget URL, omit the  url  field and specify the components as\nroot-level fields. The following URL component fields are available: Name Contents scheme The URL scheme. host The hostname of the target resource. path The path of the target resource. query A document where each field maps to a parameter in the URL\nquery string. The value of each field is an array of strings\nthat contains all arguments for the parameter. fragment The URL fragment. This portion of the URL includes everything\nafter the hash ( # ) symbol. username The username with which to authenticate the request. Typically,\nusers utilize this argument with the  password  argument. password The password with which to authenticate the request. The\npassword should correspond to the user specified in the\n username  argument. The  http.head()  action returns a promise that resolves to a document\nwith the following form: Field Type Description status string The HTTP Request status message. statusCode integer The HTTP Request status code. contentLength integer The number of bytes returned in the response  body . headers document A document where each field name corresponds to a type of\nHTTP header and each field value is an array of one or more\nstring values for that header. cookies document A document where each field name corresponds to a\ncookie name, and each field value is that cookie's string value. body binary The binary-encoded body of the HTTP response. You can authenticate an outbound HTTP request using one of the standard\n HTTP authentication schemes . Atlas App Services\nsupports the following authentication schemes: HTTP  basic authentication \nrequires that incoming requests include a valid username and password\nfor the requested service. You can specify the user credentials in the\n username  and  password  fields of the request document, directly\nin a  url  string, or in an  Authorization  HTTP header. The following examples demonstrate three equivalent ways to\nauthenticate an HTTP service request using basic authentication. The\nexamples all use the username  MyUser  and the password\n Mypassw0rd . You would pass one of these objects as an argument to\nthe given HTTP method. HTTP  digest authentication \nrequires that incoming requests include an authorization key based on a\nrandom  nonce  value returned from the\nserver. App Services can automatically construct the key and authorize requests\ngiven a valid username and password. To configure a request to use digest authentication, set the\n digestAuth  field to  true  and specify the user credentials in the\n username  and  password  fields of the request document or directly\nin a  url  string. The following examples demonstrate two equivalent ways to\nauthenticate an HTTP service request using digest authentication. The\nexamples all use the username  MyUser  and the password\n Mypassw0rd .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const http = context.services.get(\"myHttp\");\n  return http\n    .head({ url: \"https://www.example.com/users\" })\n    .then(response => {\n      // The response body is encoded as raw BSON.Binary. Parse it to JSON.\n      const ejson_body = EJSON.parse(response.body.text());\n      return ejson_body;\n    })\n};"
                },
                {
                    "lang": "javascript",
                    "value": "{\n    \"url\": <string>,\n    \"headers\": <document>,\n    \"cookies\": <string>,\n    \"authUrl\": <string>,\n    \"followRedirects\": <boolean>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"Content-Type\": [ \"application/json\" ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"favoriteTeam\": \"Chicago Cubs\"\n}"
                },
                {
                    "lang": "none",
                    "value": "<scheme>://<host>/<path>?<query>#<fragment>"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"scheme\": <string>,\n   \"host\": <string>,\n   \"path\": <string>,\n   \"query\": <document>,\n   \"fragment\": <string>,\n   \"username\": <string>,\n   \"password\": <string>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/\n{ scheme: \"https\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/\n{ host: \"www.example.com\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/api/v1/users\n{ path: \"/api/v1/users\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/?id=8675309&color=red&color=blue\n{\n  query: {\n    \"id\": [\"8675309\"],\n    \"color\": [\"red\", \"blue\"]\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/?id=8675309#someFragment\n{ fragment: \"someFragment\" }"
                },
                {
                    "lang": "javascript",
                    "value": "{\n   \"status\": <string>,\n   \"statusCode\": <integer>,\n   \"contentLength\": <integer>,\n   \"headers\": <document>,\n   \"cookies\": <array>,\n   \"body\": <binary>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"Content-Type\": [ \"application/json\" ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"favoriteTeam\": \"Chicago Cubs\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"scheme\": \"https\",\n  \"username\": \"MyUser\",\n  \"password\": \"Mypassw0rd\",\n  \"domain\": \"www.example.com\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": \"https://MyUser:Mypassw0rd@www.example.com\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"url\": \"https://www.example.com\",\n  \"headers\": {\n    \"Authorization\": [\n      `Basic ${BSON.Binary.fromText(\"MyUser:Mypassw0rd\").toBase64()}`\n    ]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"scheme\": \"https\",\n  \"username\": \"MyUser\",\n  \"password\": \"Mypassw0rd\",\n  \"domain\": \"www.example.com\",\n  \"digestAuth\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": \"https://MyUser:Mypassw0rd@www.example.com\",\n  \"digestAuth\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.url.host\": \"example.com\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.url.query.someParameter\": \"importantValue\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.url.scheme: \"https\",\n  \"%%args.url.host\" : \"www.example.com\",\n  \"%%args.url.path\" : \"/api/v1.0/messages\"\n}"
                }
            ],
            "preview": "Sends an HTTP HEAD request to the\nspecified URL using the Atlas App Services HTTP Service.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/configure/service-rules",
            "title": "Configure Service Rules [Deprecated]",
            "headings": [
                "Overview",
                "Procedure",
                "Create a New Rule",
                "Configure the Rule",
                "Save the Configured Rule",
                "Pull Your App's Latest Configuration Files",
                "Create a Service Rule Directory",
                "Create and Configure a New Service Rule",
                "Deploy the Rule"
            ],
            "paragraphs": "Third party services and push notifications in App Services have been deprecated\nin favor of creating HTTP endpoints that use  external dependencies  in functions. Webhooks have been renamed to  HTTPS Endpoints  with no change in behavior. You should\n migrate  existing Webhooks. Existing services will continue to work until  November 1, 2024 . Because third party services and push notifications are now deprecated, they have\nbeen removed by default from the App Services UI. If you need to manage an existing third party\nservice or push notification, you can add the configurations back to the UI by doing\nthe following: In the left navigation, under the  Manage  section, click\n App Settings . Enable the toggle switch next to\n Temporarily Re-Enable 3rd Party Services , and then save your\nchanges. In order to call a service action, you must first define a  service\nrule  that enables and configures the capabilities of\nthe action. You can define service rules from the Atlas App Services UI or by\n importing  a service\nconfiguration directory that contains a rule configuration file. Select\nthe tab below that corresponds to the method you want to use. You can create and configure a service rule from its associated\n service  page in the App Services UI. To define a new service rule: Click  Services  in the left navigation menu. Click on the service that you want to specify a rule for. Click on the  Rules  tab of the service. Click  New Rule . Specify a name for the rule in the textbox that appears. Click  Add Rule  to confirm the creation of the new rule. To configure the new service rule you just created: Click on the newly created rule in the rules list on the left-hand\nside of the page. In the list of  Actions , select all of the actions that\nyou want the rule to apply to. For the  When  box, specify a  rule expression  that evaluates to  true  when you want the action to be\npermitted. If you specify  {} , the expression will always evaluate to\n true  and App Services will always allow the associated actions to\nbe called. Once you've finished configuring the rule, click  Save .\nOnce saved, the rule takes effect immediately. If it doesn't already exist, create a new  rules  subdirectory in\nthe  /services  folder of the exported directory. Add a rule configuration JSON file to the  rules  directory. The\nconfiguraiton file should have the same name as the rule and have the\nfollowing form: Configuration Value Description Required. A name for the rule. The name should be unique\nwithin the service. Required. An array of strings where each item is the name of\na service action that the rule applies to. An HTTP service rule that only applies to the\n http.post()  service\naction would have the following  action  value: Required. A  rule expression  that determines if a\nservice action should actually execute after being called. App Services\nwill only execute the action if the expression resolves to  true . If\nyou do not specify an expression,  Can Evaluate  defaults to\n true . Push the rule configuration to deploy it to your app. Once you have deployed,\nApp Services immediately begins to enforce the rule.",
            "code": [
                {
                    "lang": "shell",
                    "value": "realm-cli pull --remote=<App ID>"
                },
                {
                    "lang": "shell",
                    "value": "mkdir -p services/<service name>/rules"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": <string>,\n  \"actions\": [<action>, ...],\n  \"when\": <JSON expression>\n}"
                },
                {
                    "lang": "json",
                    "value": "\"actions\": [\n  \"post\"\n]"
                },
                {
                    "lang": "shell",
                    "value": "realm-cli push"
                }
            ],
            "preview": "In order to call a service action, you must first define a service\nrule that enables and configures the capabilities of\nthe action. You can define service rules from the Atlas App Services UI or by\nimporting a service\nconfiguration directory that contains a rule configuration file. Select\nthe tab below that corresponds to the method you want to use.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/http-actions/http.post",
            "title": "http.post()",
            "headings": [
                "Definition",
                "Usage",
                "Example",
                "Parameters",
                "Alternative URL Parameters",
                "Return Value",
                "Request Authentication",
                "Basic Authentication",
                "Digest Authentication",
                "Rule Templates",
                "Users Can Only Send Requests to a Specific Host",
                "Requests URLs Must Include a Specific Query Parameter",
                "Request Bodies Must Include a Specific Field",
                "Request Bodies Must Include Fields with Specific Values"
            ],
            "paragraphs": "Sends an  HTTP POST  request to the\nspecified URL using the Atlas App Services  HTTP Service . This example assumes you have  configured an HTTP service called\n\"myHttp\" . If you do not need this\nHTTP action validated with  Service Rules ,\nyou can use the  Send an HTTP Request ( context.http )  module. The  http.post()  action accepts one argument of the\nfollowing form: Field Description Request URL Required. The target URL for the HTTP request. Alternatively,\nyou can specify the components of the URL as root-level fields.\nSee  Alternative URL Arguments . Request Body Required. The object or stringified body of the HTTP request.\nIf the request payload has a content type of\n multipart/form-data , use the  form  parameter instead of\n body . If the request body is an object, the\n encodeBodyAsJSON  parameter must be  true . Encode Body as JSON If  true , the body is automatically encoded as an EJSON string using  EJSON.stringify() . Only use when the  body \nparameter is an object. Form Request Body A document where each field maps to a field in a\n multipart/form-data  request. You cannot use this parameter at\nthe same time as the  body  parameter. Requests that use the  form  parameter should also\ninclude a  Content-Type: multipart/form-data  header. Request Headers Optional. A document where each field name corresponds to a type of\nHTTP header and each field value is an array of one or more\nstring values for that header. Request Cookies Optional. A document where each field name corresponds to a\ncookie name and each field value is that cookie's string value. Digest Authentication Optional. If  true , App Services authenticates the request using\n digest authentication . You must specify a\n username  and  password  (either as fields in the request\ndocument or as part of the URL) to use digest authentication.\nFor more details, see  Request Authentication . Request Authentication URL Optional. A URL that returns an Authorization cookie for the\nHTTP request. Follow Redirects Optional. If  true , the request will follow any HTTP\nredirects it receives for the target URL. If you need to specify the individual components of the request's\ntarget URL, omit the  url  field and specify the components as\nroot-level fields. The following URL component fields are available: Name Contents scheme The URL scheme. host The hostname of the target resource. path The path of the target resource. query A document where each field maps to a parameter in the URL\nquery string. The value of each field is an array of strings\nthat contains all arguments for the parameter. fragment The URL fragment. This portion of the URL includes everything\nafter the hash ( # ) symbol. username The username with which to authenticate the request. Typically,\nusers utilize this argument with the  password  argument. password The password with which to authenticate the request. The\npassword should correspond to the user specified in the\n username  argument. The  http.post()  action returns a promise that resolves to a document\nwith the following form: Field Type Description status string The HTTP Request status message. statusCode integer The HTTP Request status code. contentLength integer The number of bytes returned in the response  body . headers document A document where each field name corresponds to a type of\nHTTP header and each field value is an array of one or more\nstring values for that header. cookies document A document where each field name corresponds to a\ncookie name, and each field value is that cookie's string value. body binary The binary-encoded body of the HTTP response. You can authenticate an outbound HTTP request using one of the standard\n HTTP authentication schemes . Atlas App Services\nsupports the following authentication schemes: HTTP  basic authentication \nrequires that incoming requests include a valid username and password\nfor the requested service. You can specify the user credentials in the\n username  and  password  fields of the request document, directly\nin a  url  string, or in an  Authorization  HTTP header. The following examples demonstrate three equivalent ways to\nauthenticate an HTTP service request using basic authentication. The\nexamples all use the username  MyUser  and the password\n Mypassw0rd . You would pass one of these objects as an argument to\nthe given HTTP method. HTTP  digest authentication \nrequires that incoming requests include an authorization key based on a\nrandom  nonce  value returned from the\nserver. App Services can automatically construct the key and authorize requests\ngiven a valid username and password. To configure a request to use digest authentication, set the\n digestAuth  field to  true  and specify the user credentials in the\n username  and  password  fields of the request document or directly\nin a  url  string. The following examples demonstrate two equivalent ways to\nauthenticate an HTTP service request using digest authentication. The\nexamples all use the username  MyUser  and the password\n Mypassw0rd .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const http = context.services.get(\"myHttp\");\n  return http.post({\n      url: \"https://www.example.com/messages\",\n      body: { msg: \"This is in the body of a POST request!\" },\n      encodeBodyAsJSON: true\n    })\n    .then(response => {\n      // The response body is encoded as raw BSON.Binary. Parse it to JSON.\n      const ejson_body = EJSON.parse(response.body.text());\n      return ejson_body;\n    })\n};"
                },
                {
                    "lang": "javascript",
                    "value": "{\n    \"url\": <string>,\n    \"headers\": <document>,\n    \"body\": <object> or <string>,\n    \"encodeBodyAsJSON\": <boolean>,\n    \"form\": <document>,\n    \"cookies\": <string>,\n    \"authUrl\": <string>,\n    \"followRedirects\": <boolean>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "\"form\": {\n  \"to-address\": \"name@example.com\",\n  \"from-address\": \"other-name@example.com\",\n  \"subject\": \"test subject please ignore\",\n  \"message-body\": \"hello from the message body\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"Content-Type\": [ \"application/json\" ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"favoriteTeam\": \"Chicago Cubs\"\n}"
                },
                {
                    "lang": "none",
                    "value": "<scheme>://<host>/<path>?<query>#<fragment>"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"scheme\": <string>,\n   \"host\": <string>,\n   \"path\": <string>,\n   \"query\": <document>,\n   \"fragment\": <string>,\n   \"username\": <string>,\n   \"password\": <string>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/\n{ scheme: \"https\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/\n{ host: \"www.example.com\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/api/v1/users\n{ path: \"/api/v1/users\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/?id=8675309&color=red&color=blue\n{\n  query: {\n    \"id\": [\"8675309\"],\n    \"color\": [\"red\", \"blue\"]\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/?id=8675309#someFragment\n{ fragment: \"someFragment\" }"
                },
                {
                    "lang": "javascript",
                    "value": "{\n   \"status\": <string>,\n   \"statusCode\": <integer>,\n   \"contentLength\": <integer>,\n   \"headers\": <document>,\n   \"cookies\": <array>,\n   \"body\": <binary>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"Content-Type\": [ \"application/json\" ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"favoriteTeam\": \"Chicago Cubs\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"scheme\": \"https\",\n  \"username\": \"MyUser\",\n  \"password\": \"Mypassw0rd\",\n  \"domain\": \"www.example.com\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": \"https://MyUser:Mypassw0rd@www.example.com\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"url\": \"https://www.example.com\",\n  \"headers\": {\n    \"Authorization\": [\n      `Basic ${BSON.Binary.fromText(\"MyUser:Mypassw0rd\").toBase64()}`\n    ]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"scheme\": \"https\",\n  \"username\": \"MyUser\",\n  \"password\": \"Mypassw0rd\",\n  \"domain\": \"www.example.com\",\n  \"digestAuth\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": \"https://MyUser:Mypassw0rd@www.example.com\",\n  \"digestAuth\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.url.host\": \"example.com\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.url.query.someParameter\": \"importantValue\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"body.name\": { \"%exists\": 1 }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"body.city\": \"New York City\"\n}"
                }
            ],
            "preview": "Sends an HTTP POST request to the\nspecified URL using the Atlas App Services HTTP Service.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/http-actions/http.get",
            "title": "http.get()",
            "headings": [
                "Definition",
                "Usage",
                "Example",
                "Parameters",
                "Alternative URL Parameters",
                "Return Value",
                "Request Authentication",
                "Basic Authentication",
                "Digest Authentication",
                "Rule Templates",
                "Users Can Only Send Requests to a Specific Host",
                "Requests URLs Must Include a Specific Query Parameter",
                "Requests Must Be To A Specific Path"
            ],
            "paragraphs": "Sends an  HTTP GET  request to the specified\nURL using the Atlas App Services  HTTP Service . This example assumes you have  configured an HTTP service called\n\"myHttp\" . If you do not need this\nHTTP action validated with  Service Rules ,\nyou can use the  Send an HTTP Request ( context.http )  module. The  http.get()  action accepts one argument of the\nfollowing form: Field Description Request URL Required. The target URL for the HTTP request. Alternatively,\nyou can specify the components of the URL as root-level fields.\nSee  Alternative URL Arguments . Request Headers Optional. A document where each field name corresponds to a type of\nHTTP header and each field value is an array of one or more\nstring values for that header. Request Cookies Optional. A document where each field name corresponds to a\ncookie name and each field value is that cookie's string value. Digest Authentication Optional. If  true , App Services authenticates the request using\n digest authentication . You must specify a\n username  and  password  (either as fields in the request\ndocument or as part of the URL) to use digest authentication.\nFor more details, see  Request Authentication . Request Authentication URL Optional. A URL that returns an Authorization cookie for the\nHTTP request. Follow Redirects Optional. If  true , the request will follow any HTTP\nredirects it receives for the target URL. If you need to specify the individual components of the request's\ntarget URL, omit the  url  field and specify the components as\nroot-level fields. The following URL component fields are available: Name Contents scheme The URL scheme. host The hostname of the target resource. path The path of the target resource. query A document where each field maps to a parameter in the URL\nquery string. The value of each field is an array of strings\nthat contains all arguments for the parameter. fragment The URL fragment. This portion of the URL includes everything\nafter the hash ( # ) symbol. username The username with which to authenticate the request. Typically,\nusers utilize this argument with the  password  argument. password The password with which to authenticate the request. The\npassword should correspond to the user specified in the\n username  argument. The  http.get()  action returns a promise that resolves to a document\nwith the following form: Field Type Description status string The HTTP Request status message. statusCode integer The HTTP Request status code. contentLength integer The number of bytes returned in the response  body . headers document A document where each field name corresponds to a type of\nHTTP header and each field value is an array of one or more\nstring values for that header. cookies document A document where each field name corresponds to a\ncookie name, and each field value is that cookie's string value. body binary The binary-encoded body of the HTTP response. You can authenticate an outbound HTTP request using one of the standard\n HTTP authentication schemes . Atlas App Services\nsupports the following authentication schemes: HTTP  basic authentication \nrequires that incoming requests include a valid username and password\nfor the requested service. You can specify the user credentials in the\n username  and  password  fields of the request document, directly\nin a  url  string, or in an  Authorization  HTTP header. The following examples demonstrate three equivalent ways to\nauthenticate an HTTP service request using basic authentication. The\nexamples all use the username  MyUser  and the password\n Mypassw0rd . You would pass one of these objects as an argument to\nthe given HTTP method. HTTP  digest authentication \nrequires that incoming requests include an authorization key based on a\nrandom  nonce  value returned from the\nserver. App Services can automatically construct the key and authorize requests\ngiven a valid username and password. To configure a request to use digest authentication, set the\n digestAuth  field to  true  and specify the user credentials in the\n username  and  password  fields of the request document or directly\nin a  url  string. The following examples demonstrate two equivalent ways to\nauthenticate an HTTP service request using digest authentication. The\nexamples all use the username  MyUser  and the password\n Mypassw0rd .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const http = context.services.get(\"myHttp\");\n  return http\n    .get({ url: \"https://www.example.com/users\" })\n    .then(response => {\n      // The response body is encoded as raw BSON.Binary. Parse it to JSON.\n      const ejson_body = EJSON.parse(response.body.text());\n      return ejson_body;\n    })\n};"
                },
                {
                    "lang": "javascript",
                    "value": "{\n    \"url\": <string>,\n    \"headers\": <document>,\n    \"cookies\": <string>,\n    \"digestAuth\": <boolean>,\n    \"authUrl\": <string>,\n    \"followRedirects\": <boolean>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"Content-Type\": [ \"application/json\" ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"favoriteTeam\": \"Chicago Cubs\"\n}"
                },
                {
                    "lang": "none",
                    "value": "<scheme>://<host>/<path>?<query>#<fragment>"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"scheme\": <string>,\n   \"host\": <string>,\n   \"path\": <string>,\n   \"query\": <document>,\n   \"fragment\": <string>,\n   \"username\": <string>,\n   \"password\": <string>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/\n{ scheme: \"https\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/\n{ host: \"www.example.com\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/api/v1/users\n{ path: \"/api/v1/users\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/?id=8675309&color=red&color=blue\n{\n  query: {\n    \"id\": [\"8675309\"],\n    \"color\": [\"red\", \"blue\"]\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/?id=8675309#someFragment\n{ fragment: \"someFragment\" }"
                },
                {
                    "lang": "javascript",
                    "value": "{\n   \"status\": <string>,\n   \"statusCode\": <integer>,\n   \"contentLength\": <integer>,\n   \"headers\": <document>,\n   \"cookies\": <array>,\n   \"body\": <binary>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"Content-Type\": [ \"application/json\" ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"favoriteTeam\": \"Chicago Cubs\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"scheme\": \"https\",\n  \"username\": \"MyUser\",\n  \"password\": \"Mypassw0rd\",\n  \"domain\": \"www.example.com\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": \"https://MyUser:Mypassw0rd@www.example.com\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"url\": \"https://www.example.com\",\n  \"headers\": {\n    \"Authorization\": [\n      `Basic ${BSON.Binary.fromText(\"MyUser:Mypassw0rd\").toBase64()}`\n    ]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"scheme\": \"https\",\n  \"username\": \"MyUser\",\n  \"password\": \"Mypassw0rd\",\n  \"domain\": \"www.example.com\",\n  \"digestAuth\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": \"https://MyUser:Mypassw0rd@www.example.com\",\n  \"digestAuth\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.url.host\": \"example.com\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.url.query.someParameter\": \"importantValue\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.url.scheme: \"https\",\n  \"%%args.url.host\" : \"www.example.com\",\n  \"%%args.url.path\" : \"/api/v1.0/messages\"\n}"
                }
            ],
            "preview": "Sends an HTTP GET request to the specified\nURL using the Atlas App Services HTTP Service.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/http-actions/http.delete",
            "title": "http.delete()",
            "headings": [
                "Definition",
                "Usage",
                "Example",
                "Parameters",
                "Alternative URL Parameters",
                "Return Value",
                "Request Authentication",
                "Basic Authentication",
                "Digest Authentication",
                "Rule Templates",
                "Users Can Only Send Requests to a Specific Host",
                "Requests URLs Must Include a Specific Query Parameter",
                "Requests Must Be To A Specific Path"
            ],
            "paragraphs": "Sends an  HTTP DELETE  request to the\nspecified URL using the Atlas App Services  HTTP Service . This example assumes you have  configured an HTTP service called\n\"myHttp\" . If you do not need this\nHTTP action validated with  Service Rules ,\nyou can use the  Send an HTTP Request ( context.http )  module. The  http.delete()  action accepts one argument of the\nfollowing form: Field Description Request URL Required. The target URL for the HTTP request. Alternatively,\nyou can specify the components of the URL as root-level fields.\nSee  Alternative URL Arguments . Request Headers Optional. A document where each field name corresponds to a type of\nHTTP header and each field value is an array of one or more\nstring values for that header. Digest Authentication Optional. If  true , App Services authenticates the request using\n digest authentication . You must specify a\n username  and  password  (either as fields in the request\ndocument or as part of the URL) to use digest authentication.\nFor more details, see  Request Authentication . Request Authentication URL Optional. A URL that returns an Authorization cookie for the\nHTTP request. If you need to specify the individual components of the request's\ntarget URL, omit the  url  field and specify the components as\nroot-level fields. The following URL component fields are available: Name Contents scheme The URL scheme. host The hostname of the target resource. path The path of the target resource. query A document where each field maps to a parameter in the URL\nquery string. The value of each field is an array of strings\nthat contains all arguments for the parameter. fragment The URL fragment. This portion of the URL includes everything\nafter the hash ( # ) symbol. username The username with which to authenticate the request. Typically,\nusers utilize this argument with the  password  argument. password The password with which to authenticate the request. The\npassword should correspond to the user specified in the\n username  argument. The  http.delete()  action returns a promise that resolves to a\ndocument with the following form: Field Type Description status string The HTTP Request status message. statusCode integer The HTTP Request status code. contentLength integer The number of bytes returned in the response  body . headers document A document where each field name corresponds to a type of\nHTTP header and each field value is an array of one or more\nstring values for that header. cookies document A document where each field name corresponds to a\ncookie name, and each field value is that cookie's string value. body binary The binary-encoded body of the HTTP response. You can authenticate an outbound HTTP request using one of the standard\n HTTP authentication schemes . Atlas App Services\nsupports the following authentication schemes: HTTP  basic authentication \nrequires that incoming requests include a valid username and password\nfor the requested service. You can specify the user credentials in the\n username  and  password  fields of the request document, directly\nin a  url  string, or in an  Authorization  HTTP header. The following examples demonstrate three equivalent ways to\nauthenticate an HTTP service request using basic authentication. The\nexamples all use the username  MyUser  and the password\n Mypassw0rd . You would pass one of these objects as an argument to\nthe given HTTP method. HTTP  digest authentication \nrequires that incoming requests include an authorization key based on a\nrandom  nonce  value returned from the\nserver. App Services can automatically construct the key and authorize requests\ngiven a valid username and password. To configure a request to use digest authentication, set the\n digestAuth  field to  true  and specify the user credentials in the\n username  and  password  fields of the request document or directly\nin a  url  string. The following examples demonstrate two equivalent ways to\nauthenticate an HTTP service request using digest authentication. The\nexamples all use the username  MyUser  and the password\n Mypassw0rd .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const http = context.services.get(\"myHttp\");\n  return http\n    .delete({ url: \"https://www.example.com/user/8675309\" })\n    .then(response => {\n      // The response body is encoded as raw BSON.Binary. Parse it to JSON.\n      const ejson_body = EJSON.parse(response.body.text());\n      return ejson_body;\n    })\n};"
                },
                {
                    "lang": "javascript",
                    "value": "{\n    \"url\": <string>,\n    \"headers\": <document>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"Content-Type\": [ \"application/json\" ]\n}"
                },
                {
                    "lang": "none",
                    "value": "<scheme>://<host>/<path>?<query>#<fragment>"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"scheme\": <string>,\n   \"host\": <string>,\n   \"path\": <string>,\n   \"query\": <document>,\n   \"fragment\": <string>,\n   \"username\": <string>,\n   \"password\": <string>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/\n{ scheme: \"https\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/\n{ host: \"www.example.com\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/api/v1/users\n{ path: \"/api/v1/users\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/?id=8675309&color=red&color=blue\n{\n  query: {\n    \"id\": [\"8675309\"],\n    \"color\": [\"red\", \"blue\"]\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/?id=8675309#someFragment\n{ fragment: \"someFragment\" }"
                },
                {
                    "lang": "javascript",
                    "value": "{\n   \"status\": <string>,\n   \"statusCode\": <integer>,\n   \"contentLength\": <integer>,\n   \"headers\": <document>,\n   \"cookies\": <array>,\n   \"body\": <binary>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"Content-Type\": [ \"application/json\" ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"favoriteTeam\": \"Chicago Cubs\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"scheme\": \"https\",\n  \"username\": \"MyUser\",\n  \"password\": \"Mypassw0rd\",\n  \"domain\": \"www.example.com\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": \"https://MyUser:Mypassw0rd@www.example.com\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"url\": \"https://www.example.com\",\n  \"headers\": {\n    \"Authorization\": [\n      `Basic ${BSON.Binary.fromText(\"MyUser:Mypassw0rd\").toBase64()}`\n    ]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"scheme\": \"https\",\n  \"username\": \"MyUser\",\n  \"password\": \"Mypassw0rd\",\n  \"domain\": \"www.example.com\",\n  \"digestAuth\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": \"https://MyUser:Mypassw0rd@www.example.com\",\n  \"digestAuth\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.url.host\": \"example.com\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.url.query.someParameter\": \"importantValue\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.url.scheme: \"https\",\n  \"%%args.url.host\" : \"www.example.com\",\n  \"%%args.url.path\" : \"/api/v1.0/messages\"\n}"
                }
            ],
            "preview": "Sends an HTTP DELETE request to the\nspecified URL using the Atlas App Services HTTP Service.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/http-actions/http.patch",
            "title": "http.patch()",
            "headings": [
                "Definition",
                "Usage",
                "Example",
                "Parameters",
                "Alternative URL Parameters",
                "Return Value",
                "Request Authentication",
                "Basic Authentication",
                "Digest Authentication",
                "Rule Templates",
                "Users Can Only Send Requests to a Specific Host",
                "Requests URLs Must Include a Specific Query Parameter",
                "Request Bodies Must Include a Specific Field",
                "Request Bodies Must Include Fields with Specific Values"
            ],
            "paragraphs": "Sends an  HTTP PATCH  request to the\nspecified URL using the Atlas App Services  HTTP Service . This example assumes you have  configured an HTTP service called\n\"myHttp\" . If you do not need this\nHTTP action validated with  Service Rules ,\nyou can use the  Send an HTTP Request ( context.http )  module. The  http.patch()  action accepts one argument of the\nfollowing form: Field Description Request URL Required. The target URL for the HTTP request. Alternatively,\nyou can specify the components of the URL as root-level fields.\nSee  Alternative URL Arguments . Request Body Required. The stringified body of the HTTP request. Request Headers Optional. A document where each field name corresponds to a type of\nHTTP header and each field value is an array of one or more\nstring values for that header. Request Cookies Optional. A document where each field name corresponds to a\ncookie name and each field value is that cookie's string value. Digest Authentication Optional. If  true , App Services authenticates the request using\n digest authentication . You must specify a\n username  and  password  (either as fields in the request\ndocument or as part of the URL) to use digest authentication.\nFor more details, see  Request Authentication . Request Authentication URL Optional. A URL that grants authorization cookies for the HTTP\nrequest. Follow Redirects Optional. If  true , the request will follow any HTTP\nredirects it receives for the target URL. If you need to specify the individual components of the request's\ntarget URL, omit the  url  field and specify the components as\nroot-level fields. The following URL component fields are available: Name Contents scheme The URL scheme. host The hostname of the target resource. path The path of the target resource. query A document where each field maps to a parameter in the URL\nquery string. The value of each field is an array of strings\nthat contains all arguments for the parameter. fragment The URL fragment. This portion of the URL includes everything\nafter the hash ( # ) symbol. username The username with which to authenticate the request. Typically,\nusers utilize this argument with the  password  argument. password The password with which to authenticate the request. The\npassword should correspond to the user specified in the\n username  argument. The  http.patch()  action returns a promise that resolves to a\ndocument with the following form: Field Type Description status string The HTTP Request status message. statusCode integer The HTTP Request status code. contentLength integer The number of bytes returned in the response  body . headers document A document where each field name corresponds to a type of\nHTTP header and each field value is an array of one or more\nstring values for that header. cookies document A document where each field name corresponds to a\ncookie name, and each field value is that cookie's string value. body binary The binary-encoded body of the HTTP response. You can authenticate an outbound HTTP request using one of the standard\n HTTP authentication schemes . Atlas App Services\nsupports the following authentication schemes: HTTP  basic authentication \nrequires that incoming requests include a valid username and password\nfor the requested service. You can specify the user credentials in the\n username  and  password  fields of the request document, directly\nin a  url  string, or in an  Authorization  HTTP header. The following examples demonstrate three equivalent ways to\nauthenticate an HTTP service request using basic authentication. The\nexamples all use the username  MyUser  and the password\n Mypassw0rd . You would pass one of these objects as an argument to\nthe given HTTP method. HTTP  digest authentication \nrequires that incoming requests include an authorization key based on a\nrandom  nonce  value returned from the\nserver. App Services can automatically construct the key and authorize requests\ngiven a valid username and password. To configure a request to use digest authentication, set the\n digestAuth  field to  true  and specify the user credentials in the\n username  and  password  fields of the request document or directly\nin a  url  string. The following examples demonstrate two equivalent ways to\nauthenticate an HTTP service request using digest authentication. The\nexamples all use the username  MyUser  and the password\n Mypassw0rd .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const http = context.services.get(\"myHttp\");\n  return http\n    .patch({ url: \"https://www.example.com/diff.txt\" })\n    .then(response => {\n      // The response body is encoded as raw BSON.Binary. Parse it to JSON.\n      const ejson_body = EJSON.parse(response.body.text());\n      return ejson_body;\n    })\n};"
                },
                {
                    "lang": "javascript",
                    "value": "{\n    \"url\": <string>,\n    \"headers\": <document>,\n    \"cookies\": <string>,\n    \"authUrl\": <string>,\n    \"followRedirects\": <boolean>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"Content-Type\": [ \"application/json\" ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"favoriteTeam\": \"Chicago Cubs\"\n}"
                },
                {
                    "lang": "none",
                    "value": "<scheme>://<host>/<path>?<query>#<fragment>"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"scheme\": <string>,\n   \"host\": <string>,\n   \"path\": <string>,\n   \"query\": <document>,\n   \"fragment\": <string>,\n   \"username\": <string>,\n   \"password\": <string>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/\n{ scheme: \"https\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/\n{ host: \"www.example.com\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/api/v1/users\n{ path: \"/api/v1/users\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/?id=8675309&color=red&color=blue\n{\n  query: {\n    \"id\": [\"8675309\"],\n    \"color\": [\"red\", \"blue\"]\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/?id=8675309#someFragment\n{ fragment: \"someFragment\" }"
                },
                {
                    "lang": "javascript",
                    "value": "{\n   \"status\": <string>,\n   \"statusCode\": <integer>,\n   \"contentLength\": <integer>,\n   \"headers\": <document>,\n   \"cookies\": <array>,\n   \"body\": <binary>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"Content-Type\": [ \"application/json\" ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"favoriteTeam\": \"Chicago Cubs\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"scheme\": \"https\",\n  \"username\": \"MyUser\",\n  \"password\": \"Mypassw0rd\",\n  \"domain\": \"www.example.com\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": \"https://MyUser:Mypassw0rd@www.example.com\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"url\": \"https://www.example.com\",\n  \"headers\": {\n    \"Authorization\": [\n      `Basic ${BSON.Binary.fromText(\"MyUser:Mypassw0rd\").toBase64()}`\n    ]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"scheme\": \"https\",\n  \"username\": \"MyUser\",\n  \"password\": \"Mypassw0rd\",\n  \"domain\": \"www.example.com\",\n  \"digestAuth\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": \"https://MyUser:Mypassw0rd@www.example.com\",\n  \"digestAuth\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.url.host\": \"example.com\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.url.query.someParameter\": \"importantValue\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"body.name\": { \"%exists\": 1 }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"body.city\": \"New York City\"\n}"
                }
            ],
            "preview": "Sends an HTTP PATCH request to the\nspecified URL using the Atlas App Services HTTP Service.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/http-actions/http.put",
            "title": "http.put()",
            "headings": [
                "Definition",
                "Usage",
                "Example",
                "Parameters",
                "Alternative URL Parameters",
                "Return Value",
                "Request Authentication",
                "Basic Authentication",
                "Digest Authentication",
                "Rule Templates",
                "Users Can Only Send Requests to a Specific Host",
                "Requests URLs Must Include a Specific Query Parameter",
                "Request Bodies Must Include a Specific Field",
                "Request Bodies Must Include Fields with Specific Values"
            ],
            "paragraphs": "Sends an  HTTP PUT  request to the specified\nURL using the Atlas App Services  HTTP Service . This example assumes you have  configured an HTTP service called\n\"myHttp\" . If you do not need this\nHTTP action validated with  Service Rules ,\nyou can use the  Send an HTTP Request ( context.http )  module. The  http.put()  action accepts one argument of the\nfollowing form: Field Description Request URL Required. The target URL for the HTTP request. Alternatively,\nyou can specify the components of the URL as root-level fields.\nSee  Alternative URL Arguments . Request Body Required. The stringified body of the HTTP request.\nIf the request payload has a content type of\n multipart/form-data , use the  form  parameter instead of\n body . Form Request Body A document where each field maps to a field in a\n multipart/form-data  request. You cannot use this parameter at\nthe same time as the  body  parameter. Requests that use the  form  parameter should also\ninclude a  Content-Type: multipart/form-data  header. Request Headers Optional. A document where each field name corresponds to a type of\nHTTP header and each field value is an array of one or more\nstring values for that header. Request Cookies Optional. A document where each field name corresponds to a\ncookie name and each field value is that cookie's string value. Digest Authentication Optional. If  true , App Services authenticates the request using\n digest authentication . You must specify a\n username  and  password  (either as fields in the request\ndocument or as part of the URL) to use digest authentication.\nFor more details, see  Request Authentication . Request Authentication URL Optional. A URL that returns an Authorization cookie for the\nHTTP request. Follow Redirects Optional. If  true , the request will follow any HTTP\nredirects it receives for the target URL. If you need to specify the individual components of the request's\ntarget URL, omit the  url  field and specify the components as\nroot-level fields. The following URL component fields are available: Name Contents scheme The URL scheme. host The hostname of the target resource. path The path of the target resource. query A document where each field maps to a parameter in the URL\nquery string. The value of each field is an array of strings\nthat contains all arguments for the parameter. fragment The URL fragment. This portion of the URL includes everything\nafter the hash ( # ) symbol. username The username with which to authenticate the request. Typically,\nusers utilize this argument with the  password  argument. password The password with which to authenticate the request. The\npassword should correspond to the user specified in the\n username  argument. The  http.put()  action returns a promise that resolves to a\ndocument with the following form: Field Type Description status string The HTTP Request status message. statusCode integer The HTTP Request status code. contentLength integer The number of bytes returned in the response  body . headers document A document where each field name corresponds to a type of\nHTTP header and each field value is an array of one or more\nstring values for that header. cookies document A document where each field name corresponds to a\ncookie name, and each field value is that cookie's string value. body binary The binary-encoded body of the HTTP response. You can authenticate an outbound HTTP request using one of the standard\n HTTP authentication schemes . Atlas App Services\nsupports the following authentication schemes: HTTP  basic authentication \nrequires that incoming requests include a valid username and password\nfor the requested service. You can specify the user credentials in the\n username  and  password  fields of the request document, directly\nin a  url  string, or in an  Authorization  HTTP header. The following examples demonstrate three equivalent ways to\nauthenticate an HTTP service request using basic authentication. The\nexamples all use the username  MyUser  and the password\n Mypassw0rd . You would pass one of these objects as an argument to\nthe given HTTP method. HTTP  digest authentication \nrequires that incoming requests include an authorization key based on a\nrandom  nonce  value returned from the\nserver. App Services can automatically construct the key and authorize requests\ngiven a valid username and password. To configure a request to use digest authentication, set the\n digestAuth  field to  true  and specify the user credentials in the\n username  and  password  fields of the request document or directly\nin a  url  string. The following examples demonstrate two equivalent ways to\nauthenticate an HTTP service request using digest authentication. The\nexamples all use the username  MyUser  and the password\n Mypassw0rd .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const http = context.services.get(\"myHttp\");\n  return http.put({\n    url: \"https://www.example.com/messages\",\n    body: { msg: \"This is in the body of a PUT request!\" },\n    encodeBodyAsJSON: true\n  })\n  .then(response => {\n    // The response body is encoded as raw BSON.Binary. Parse it to JSON.\n    const ejson_body = EJSON.parse(response.body.text());\n    return ejson_body;\n  })\n};"
                },
                {
                    "lang": "javascript",
                    "value": "{\n    \"url\": <string>,\n    \"headers\": <document>,\n    \"body\": <string>,\n    \"form\": <document>,\n    \"cookies\": <string>,\n    \"authUrl\": <string>,\n    \"followRedirects\": <boolean>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "\"form\": {\n  \"to-address\": \"name@example.com\",\n  \"from-address\": \"other-name@example.com\",\n  \"subject\": \"test subject please ignore\",\n  \"message-body\": \"hello from the message body\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"Content-Type\": [ \"application/json\" ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"favoriteTeam\": \"Chicago Cubs\"\n}"
                },
                {
                    "lang": "none",
                    "value": "<scheme>://<host>/<path>?<query>#<fragment>"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"scheme\": <string>,\n   \"host\": <string>,\n   \"path\": <string>,\n   \"query\": <document>,\n   \"fragment\": <string>,\n   \"username\": <string>,\n   \"password\": <string>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/\n{ scheme: \"https\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/\n{ host: \"www.example.com\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/api/v1/users\n{ path: \"/api/v1/users\" }"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/?id=8675309&color=red&color=blue\n{\n  query: {\n    \"id\": [\"8675309\"],\n    \"color\": [\"red\", \"blue\"]\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// https://www.example.com/?id=8675309#someFragment\n{ fragment: \"someFragment\" }"
                },
                {
                    "lang": "javascript",
                    "value": "{\n   \"status\": <string>,\n   \"statusCode\": <integer>,\n   \"contentLength\": <integer>,\n   \"headers\": <document>,\n   \"cookies\": <array>,\n   \"body\": <binary>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"Content-Type\": [ \"application/json\" ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"favoriteTeam\": \"Chicago Cubs\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"scheme\": \"https\",\n  \"username\": \"MyUser\",\n  \"password\": \"Mypassw0rd\",\n  \"domain\": \"www.example.com\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": \"https://MyUser:Mypassw0rd@www.example.com\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"url\": \"https://www.example.com\",\n  \"headers\": {\n    \"Authorization\": [\n      `Basic ${BSON.Binary.fromText(\"MyUser:Mypassw0rd\").toBase64()}`\n    ]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"scheme\": \"https\",\n  \"username\": \"MyUser\",\n  \"password\": \"Mypassw0rd\",\n  \"domain\": \"www.example.com\",\n  \"digestAuth\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": \"https://MyUser:Mypassw0rd@www.example.com\",\n  \"digestAuth\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.url.host\": \"example.com\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.url.query.someParameter\": \"importantValue\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"body.name\": { \"%exists\": 1 }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"body.city\": \"New York City\"\n}"
                }
            ],
            "preview": "Sends an HTTP PUT request to the specified\nURL using the Atlas App Services HTTP Service.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/snippets/github",
            "title": "GitHub Snippets [Deprecated]",
            "headings": [
                "Overview",
                "Log All Commits in MongoDB",
                "Automatically Comment On New Pull Requests"
            ],
            "paragraphs": "Third party services and push notifications in App Services have been deprecated\nin favor of creating HTTP endpoints that use  external dependencies  in functions. Webhooks have been renamed to  HTTPS Endpoints  with no change in behavior. You should\n migrate  existing Webhooks. Existing services will continue to work until  November 1, 2024 . Because third party services and push notifications are now deprecated, they have\nbeen removed by default from the App Services UI. If you need to manage an existing third party\nservice or push notification, you can add the configurations back to the UI by doing\nthe following: In the left navigation, under the  Manage  section, click\n App Settings . Enable the toggle switch next to\n Temporarily Re-Enable 3rd Party Services , and then save your\nchanges. The code snippets on this page cover demonstrate how you can respond to\nevents in a GitHub repository through the  GitHub Service . All of the snippets require a GitHub Service\ninterface with rules that allow the service actions used in the snippet. If your app does not have a GitHub Service interface,  create one  before using these snippets. This  GitHub incoming webhook  function\nrecords all commits pushed to a repo in MongoDB based on a\n PushEvent  payload\nfrom GitHub. This  GitHub incoming webhook  function\nadds a comment to new pull requests that thanks users for submitting.\nThe webhook accepts a  PullRequestEvent  payload from GitHub and\nuses an  HTTP service  client to\n create a comment \nthrough the GitHub API.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "exports = async function(pushEvent) {\n  // Parse the list of commits from the PushEvent payload.\n  // Also grab the user that pushed the commits and the repo information.\n  const { commits, pusher, repository } = pushEvent;\n\n  // Create a new array of log documents, one for each commit\n  const commitLogs = commits.map(commit => {\n    return {\n      commit: commit,\n      pushed_by: pusher,\n      repo: {\n        name: repository.name,\n        url: repository.url\n      }\n    }\n  })\n\n  // Get a client for the `GitHubRepo.logs` collection in MongoDB\n  const mongodb = context.services.get(\"mongodb-atlas\");\n  const logs = mongodb.db(\"GitHubRepo\").collection(\"commit-logs\");\n\n  // Insert the log documents in MongoDB\n  try {\n    const insertResult = await logs.insertMany(commitLogs)\n    console.log(insertResult.insertedIds);\n  } catch(err) {\n    console.error(err)\n  }\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function(pullRequest) {\n  // Get information about the PR from the PullRequestEvent\n  const { action, repository, pull_request: pr } = pullRequest;\n\n  // Only run if this is a new PR\n  if (action !== \"opened\") { return }\n\n  // Construct the GitHub API URL for this PR's Comments\n  const pr_comments_url = {\n    scheme: \"https\",\n    host: \"api.github.com\",\n    path: `/repos/${repository.owner.login}/${repository.name}/issues/${pr.number.$numberInt}/comments`,\n  };\n  // Specify GitHub API Basic Authentication Fields and Headers\n  const github_basic_auth = {\n    username: context.values.get(\"github-credentials\").username,\n    password: context.values.get(\"github-credentials\").password,\n  };\n  const headers = {\n    // OPTIONAL: Include this header if your security settings require a 2fa code\n    \"X-GitHub-OTP\": [\"<2fa Code>\"]\n  };\n  // Specify the comment text\n  const body = EJSON.stringify({\n    body: `Thank you for submitting a pull request, ${pr.user.login}!`\n  });\n\n  try {\n    // Get an HTTP service client. The service rules should allow you\n    // to send POST requests to `https://api.github.com`.\n    const http = context.services.get(\"<HTTP Service Name>\");\n\n    // Send the Request to GitHub\n    const request = { ...github_basic_auth, ...pr_comments_url, headers, body };\n    const result = await http.post(request);\n\n    // Check for a Successful Result\n    if (result.statusCode == 201) {\n      return \"Successfully commented on the pull request!\";\n    } else {\n      throw new Error(`Received a bad result status from GitHub: ${result.body.text()}`);\n    }\n  } catch (err) {\n    console.error(\"Something went wrong while posting the comment.\", err);\n  }\n};"
                }
            ],
            "preview": "The code snippets on this page cover demonstrate how you can respond to\nevents in a GitHub repository through the GitHub Service. All of the snippets require a GitHub Service\ninterface with rules that allow the service actions used in the snippet.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/twilio-actions/send",
            "title": "twilio.send()",
            "headings": [
                "Definition",
                "Usage",
                "Example",
                "Parameters",
                "Return Value",
                "Rule Templates",
                "Users Can Send Only Messages from a Specific Phone Number",
                "Users Can Only Send Messages to a Limited Set of Phone Numbers",
                "Users Can Only Send Messages to Themselves"
            ],
            "paragraphs": "Sends an SMS text message with Twilio. To send or receive messages via the  Twilio API for WhatsApp ,\nprepend the  to  or  from  numbers with  whatsapp: . Parameter Type Description args document A document of the following form: args.to string The recipient's phone number in  E.164 Format . args.from string A phone number associated with your Twilio account in\n E.164 Format . args.body string The message to send. The  twilio.send()  action does not return a value. This template calls an example function named\n isCurrentUsersPhoneNumber  that does the following: Accepts the phone number provided in the  to  argument Queries MongoDB for a user document that matches the current user's id Compares the provided phone number to the number listed in the user document Returns the boolean result of the comparison",
            "code": [
                {
                    "lang": "javascript",
                    "value": "to: \"whatsapp:+15558675309\",\nfrom: \"whatsapp:+15551234567\","
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n    const twilio = context.services.get(\"myTwilio\");\n    twilio.send({\n        to: \"+15558675309\",\n        from: \"+15551234567\",\n        body: \"Hello from App Services!\"\n    });\n};"
                },
                {
                    "lang": "javascript",
                    "value": "{\n    \"to\": <string>,   // recipient phone #\n    \"from\": <string>, // sender phone #\n    \"body\": <string>  // message\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.from\": \"+15551234\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.to\": {\n    \"$in\": [\n      \"+15551234\",\n      \"+18675309\"\n    ]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%true\": {\n    \"%function\": {\n      \"name\": \"isCurrentUsersPhoneNumber\",\n      \"arguments\": [\n        \"%%args.to\"\n      ]\n    }\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function(toPhone) {\n  const mdb = context.services.get('mongodb-atlas');\n  const users = mdb.db('demo').collection('users');\n  const user = users.findOne({ _id: context.user.id });\n  return user.phoneNumber === toPhone;\n}"
                }
            ],
            "preview": "Sends an SMS text message with Twilio.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/snippets/s3",
            "title": "AWS S3 Snippets [Deprecated]",
            "headings": [
                "Overview",
                "Upload an Image to S3",
                "Function Parameters",
                "Get an Image From S3",
                "Function Parameters"
            ],
            "paragraphs": "Third party services and push notifications in App Services have been deprecated\nin favor of creating HTTP endpoints that use  external dependencies  in functions. Webhooks have been renamed to  HTTPS Endpoints  with no change in behavior. You should\n migrate  existing Webhooks. Existing services will continue to work until  November 1, 2024 . Because third party services and push notifications are now deprecated, they have\nbeen removed by default from the App Services UI. If you need to manage an existing third party\nservice or push notification, you can add the configurations back to the UI by doing\nthe following: In the left navigation, under the  Manage  section, click\n App Settings . Enable the toggle switch next to\n Temporarily Re-Enable 3rd Party Services , and then save your\nchanges. The code snippets on this page demonstrate how to work with Amazon\nSimple Storage Service through the  AWS Service .\nAll of the snippets require an AWS Service interface with a\nconfiguration of AWS Service rules that allow the service actions\nused in the snippet. You can run these snippets yourself by copying this\ncode into an  Atlas Function . If your app does not have an AWS Service interface,  create one  before using these snippets. This  Atlas Function  uploads a Base64 encoded image\nto AWS S3 using the  PutObject  action. Parameter Type Description base64EncodedImage string A Base64 encoded image. You can convert an image\n File  to Base64 with the  readAsDataURL  method from the FileReader\nWeb API. bucket string The name of the S3 bucket that will hold the image. fileName string The name of the image file, including its file extension. fileType string The  MIME Type  of the image. This  Atlas Function  retrieves an object from AWS S3\nusing the  GetObject  action. Parameter Type Description bucket string The name of the S3 bucket that will hold the image. fileName string The name of the image file, including its file extension.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "exports = function(base64EncodedImage, bucket, fileName, fileType) {\n  // Convert the base64 encoded image string to a BSON Binary object\n  const binaryImageData = BSON.Binary.fromBase64(base64EncodedImage, 0);\n  // Instantiate an S3 service client\n  const s3Service = context.services.get('myS3Service').s3('us-east-1');\n  // Put the object to S3\n  return s3Service.PutObject({\n    'Bucket': bucket,\n    'Key': fileName,\n    'ContentType': fileType,\n    'Body': binaryImageData\n  })\n  .then(putObjectOutput => {\n    console.log(putObjectOutput);\n    // putObjectOutput: {\n    //   ETag: <string>, // The object's S3 entity tag\n    // }\n    return putObjectOutput\n  })\n  .catch(console.error);\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function(bucket, fileName) {\n  // Instantiate an S3 service client\n  const s3Service = context.services.get('myS3Service').s3('us-east-1');\n  // Get the object from S3\n  return s3Service.GetObject({\n    'Bucket': bucket,\n    'Key': fileName,\n  })\n  .then(getObjectOutput => {\n    console.log(getObjectOutput);\n    // {\n    //   ETag: <string>, // The object's S3 entity tag\n    //   Body: <binary>, // The object data\n    //   ContentType: <string>, // The object's MIME type\n    // }\n    const base64EncodedImage = getObjectOutput.Body\n    return base64EncodedImage\n  })\n  .catch(console.error);\n};"
                }
            ],
            "preview": "The code snippets on this page demonstrate how to work with Amazon\nSimple Storage Service through the AWS Service.\nAll of the snippets require an AWS Service interface with a\nconfiguration of AWS Service rules that allow the service actions\nused in the snippet. You can run these snippets yourself by copying this\ncode into an Atlas Function.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "services/configure/service-webhooks",
            "title": "Configure Service Webhooks [Deprecated]",
            "headings": [
                "Overview",
                "Procedure",
                "Set Up a New Webhook",
                "Name the New Webhook",
                "Configure User Authentication",
                "Select the Webhook's HTTP Method",
                "Configure the Webhook Response",
                "Specify an Authorization Expression",
                "Specify the Request Validation Method",
                "Write the Webhook Function",
                "Save the Webhook",
                "Pull the Latest Version of Your App",
                "Add a Webhook Configuration Directory",
                "Add a Webhook Configuration File",
                "Name the New Webhook",
                "Configure User Authentication",
                "Specify the Webhook's HTTP Method",
                "Configure the Webhook Response",
                "Specify an Authorization Expression",
                "Specify the Request Validation Method",
                "Add the Webhook Function Source Code",
                "Deploy the Incoming Webhook Configuration"
            ],
            "paragraphs": "Third party services and push notifications in App Services have been deprecated\nin favor of creating HTTP endpoints that use  external dependencies  in functions. Webhooks have been renamed to  HTTPS Endpoints  with no change in behavior. You should\n migrate  existing Webhooks. Existing services will continue to work until  November 1, 2024 . Because third party services and push notifications are now deprecated, they have\nbeen removed by default from the App Services UI. If you need to manage an existing third party\nservice or push notification, you can add the configurations back to the UI by doing\nthe following: In the left navigation, under the  Manage  section, click\n App Settings . Enable the toggle switch next to\n Temporarily Re-Enable 3rd Party Services , and then save your\nchanges. Some external services allow you to create  incoming webhooks  that external clients can call over HTTP. You can create\nwebhooks for these services from the App Services UI or with App Services CLI. Select the tab\nbelow that corresponds to the method you want to use. Incoming webhooks are scoped to individual  services . You can create and manage a webhook from its associated\nservice page in the App Services UI. To create an incoming webhook: Click  Services  in the left navigation menu. Click the Service for which you want to add an incoming webhook. Select the  Incoming Webhooks  tab for the service. Click  Add Incoming Webhook . App Services will redirect you to\nthe  Settings  screen for the new webhook. Enter a unique, identifying name for the webhook in the\n Webhook Name  field. This name must be distinct from any\nother webhooks that you've created for the service. Atlas Functions, including webhooks, always execute in\nthe context of a specific application user or as the\n system user , which bypasses rules. To\nconfigure the webhook's execution user, specify the type\nof authentication that App Services should use for the webhook. Authentication Type Description Application Authentication This type of authentication configures a webhook to run in the\ncontext of an existing application user specified by each\nincoming request. Incoming requests must include the user's\n authentication provider \ncredentials in either the request body or the request headers. The following examples demonstrate the field names and values\nfor each supported authentication provider: If a request includes credentials in both the request\nheaders and the request body, then App Services throws an error\nand does not execute the function. You can configure a webhook that uses application authentication to\nperform additional user-related work for each request: If you enable  Fetch Custom User Data ,\nApp Services queries the requesting user's  custom user\ndata  and, if it exists, exposes the data as an\nobject on the  context.user.custom_data  property. If you enable  Create User Upon Authentication ,\nApp Services automatically creates a new user based on the\nprovided user credentials if they don't match an already existing\nuser. The authentication provider that corresponds to the\ncredentials must be enabled at the time of the request to create a\nnew user. System This type of authentication configures a webhook to run as the\n system user , which has full\naccess to MongoDB CRUD and Aggregation APIs and is\nnot affected by any rules, roles, or permissions. User ID This type of authentication configures a webhook to always run\nas a specific application user. Script This type of authentication configures a webhook to run as a\nspecific application user determined by the result of a\ncustom  function  that you define. The\nfunction must return a specific user's  id  string or can\nspecify a system user by returning  { \"runAsSystem\": true } . You can require that incoming requests use a specific  HTTP\nmethod  or you can accept all HTTP methods and\nhandle each one individually in the webhook function by inspecting the\n httpMethod  property on the  context.request  object, as in the following example function: You can send a configurable  HTTP Response  to external services that call the\nwebhook. If you enable  Respond With Result , the webhook will\nrespond to incoming requests with a basic  HTTP 200  response that includes the webhook function\nreturn value as its  body  field. You can configure a custom HTTP\nresponse from within the webhook function using the  response \nobject that App Services automatically passes as the second argument. You can dynamically authorize requests based on the contents of each request\nby defining a  Can Evaluate   expression . App Services\nevaluates the expression for every incoming request that the webhook receives.\nIf you do not specify an expression then App Services automatically authorizes all\nauthenticated incoming requests. The expression can expand standard  expression variables ,\nincluding the  %%request  expansion. To validate that a webhook request was sent from a trusted source,\nsome external services require that incoming requests incorporate a\nsecret string in one of several prescribed manners. Other services,\nlike the  HTTP service , allow you to optionally\nrequire request validation. If your webhook requires request validation: Select the  request validation method . Enter a  Secret  string to use in the request validation\nprocess. Once you've configured the webhook, all that's left is to write the\nfunction that executes when someone calls the webhook. App Services\nautomatically passes two objects as the webhook function's arguments: You can use the following webhook function as a base for your own webhook: Argument Description payload An EJSON representation of the incoming request payload. The\ncontents of the payload document will vary depending on the\nservice and event that caused a webhook to fire. For a\ndescription of the  payload  object for a specific service,\nsee that service's reference page. response An  HTTP response object  that\nconfigures the response to the client that called the\nwebhook. The object has methods that allow you to set the\nresponse's headers, body, and status code. Calling any of these\nmethods overrides the default response behavior. If you want to debug a webhook function response from the function\neditor, you must manually provide the HTTP response object when you\nrun the function. You must save changes to your webhook before they take effect. To do\nso, click  Save  from either the  Settings  screen\nor the  Function Editor . This procedure uses version 2 of the Realm CLI [deprecated].\nIf you have an older version of  realm-cli , upgrade to the latest\nversion or use the  --help  flag for a list of commands supported\nin your version. To define an incoming webhook with  realm-cli , you need a local copy of your\napplication's configuration files. To pull a local copy of the latest version of your app, run the following: You can also download a copy of your application's configuration files from\nthe  Deploy > Export App  screen in the App Services UI. Create a new subdirectory with the same name as the webhook in\n /http_endpoints/<service>/incoming_webhooks/ : Add an  incoming webhook configuration file  named  config.json \nto the new webhook directory. The configuration file should have the following form: Enter a name for the webhook in the configuration file's  name \nfield. This name must be distinct from any other webhooks that you've\ncreated for the service. Specify the type of authentication that App Services should use for the\nwebhook. App Services supports the following webhook authentication methods: Authentication Method Description Application Authentication This type of authentication configures a webhook to run in the\ncontext of an existing application user specified by each\nincoming request. Incoming requests must include the user's\n authentication provider \ncredentials in either the request body or the request headers. To configure a webhook to use application authentication, set\nthe value of  run_as_authed_user  to  true : The following examples demonstrate the field names and\nvalues that incoming requests should include as body or\nheader fields for each supported authentication provider: If a request includes credentials in both the request\nheaders and the request body, then App Services throws an error\nand does not execute the function. System This type of authentication configures a webhook to run as the\n system user , which has full\naccess to MongoDB CRUD and Aggregation APIs and is\nnot affected by any rules, roles, or permissions. To configure a webhook to run as a system user, do not set any\nother authentication fields: User ID This type of authentication configures a webhook to always run\nas a specific application user. To configure a webhook to always run as a specific user, set\n run_as_user_id  to the user's id: Script This type of authentication configures a webhook to run as a\nspecific application user determined based on the result of a\ncustom  function  that you define. The\nfunction must return a specific user's  id  string or can\nspecify a system user by returning  { \"runAsSystem\":\ntrue} . To configure a webhook to run as a user determined by a\nfunction, set  run_as_user_id_script_source  to the\nstringified function code: You can require that incoming requests use a specific  HTTP\nmethod  or you can accept all HTTP methods and\nhandle each one individually in the webhook function by inspecting the\n httpMethod  property on the  context.request  object, as in the following example function: To specify a webhook method, set the  options.httpMethod  field to\nthe name of the method using all capital letters or  \"ANY\" . You can send a configurable  HTTP Response  to external services that call the\nwebhook. To configure the webhook to send a response to incoming\nrequests, set  respond_result  to  true . If you enable  Respond With Result , the webhook will\nrespond to incoming requests with a basic  HTTP 200  response that includes the webhook function\nreturn value as its  body  field. You can configure a custom HTTP\nresponse from within the webhook function using the  response \nobject that App Services automatically passes as the second argument. You can dynamically authorize requests based on the contents of each request\nby defining a  Can Evaluate   expression . App Services\nevaluates the expression for every incoming request that the webhook receives.\nThe expression can expand standard  expression variables ,\nincluding the  %%request  expansion. To define an authorization expression, set the value of the\n can_evaluate  field to the expression. If you do not specify an\nexpression then App Services automatically authorizes all authenticated\nincoming requests. The following expression only authorizes incoming requests if the\nsender's IP address is not included in the list of addresses. To validate that a webhook request was sent from a trusted source,\nsome external services require that incoming requests incorporate a\nsecret string in one of several prescribed manners. Other services,\nlike the  HTTP service , allow you to optionally\nrequire request validation. You can configure a webhook's request authorization method in the\n options  document of the webhook configuration. App Services supports the\nfollowing  request validation methods : Method Description No Additional Authorization Incoming webhook requests do not require additional\nauthorization. Verify Payload Signature Incoming webhook requests must include a hashed signature of\nthe request body and a secret value. For details, refer to\n Payload Signature Verification . Require Secret Incoming webhook requests must include a secret string value as\nthe  secret  query parameter in the webhook URL. For details,\nrefer to  Secret as a Query Parameter . Add a file named  source.js  to the new webhook directory. The file\nshould contain a valid function that will execute when the webhook is\ncalled. App Services automatically passes two objects as the webhook function's\narguments: You can use the following webhook function as a base for your own webhook: Argument Description payload An EJSON representation of the incoming request payload. The\ncontents of the payload document will vary depending on the\nservice and event that caused a webhook to fire. For a\ndescription of the  payload  object for a specific service,\nsee that service's reference page. response An  HTTP response object  that\nconfigures the response to the client that called the\nwebhook. The object has methods that allow you to set the\nresponse's headers, body, and status code. Calling any of these\nmethods overrides the default response behavior. Once you've set the read preference for the cluster in  config.json , you\ncan push the config to your remote app. App Services CLI immediately deploys the\nupdate on push.",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"email\": \"<User's Email Address>\",\n  \"password\": \"<User's Password>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"api-key\": \"<User's API Key>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"jwtTokenString\": \"<User's JWT Token>\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function(payload, response) {\n  switch(context.request.httpMethod) {\n    case \"GET\": { /* Handle GET requests */ }\n    case \"POST\": { /* Handle POST requests */ }\n    case \"PUT\": { /* Handle PUT requests */ }\n    case \"DELETE\": { /* Handle DELETE requests */ }\n    case \"PATCH\": { /* Handle PATCH requests */ }\n    default: {}\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function (payload, response) {\n  // Convert the webhook body from BSON to an EJSON object\n  const body = EJSON.parse(payload.body.text());\n\n  // Execute application logic, such as working with MongoDB\n  if (body.someField) {\n    const mdb = context.services.get(\"mongodb-atlas\");\n    const requests = mdb.db(\"demo\").collection(\"requests\");\n    const { insertedId } = await requests.insertOne({\n      someField: body.someField,\n    });\n    // Respond with an affirmative result\n    response.setStatusCode(200);\n    response.setBody(`Successfully saved \"someField\" with _id: ${insertedId}.`);\n  } else {\n    // Respond with a malformed request error\n    response.setStatusCode(400);\n    response.setBody(`Could not find \"someField\" in the webhook request body.`);\n  }\n  // This return value does nothing because we already modified the response object.\n  // If you do not modify the response object and you enable *Respond with Result*,\n  // App Services will include this return value as the response body.\n  return { msg: \"finished!\" };\n};\n"
                },
                {
                    "lang": "javascript",
                    "value": "exports(\n  { body: \"This document is the webhook payload\" },\n  new HTTPResponse()\n)"
                },
                {
                    "lang": "bash",
                    "value": "realm-cli pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "shell",
                    "value": "mkdir -p http_endpoints/<service>/incoming_webhooks/<webhook name>"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Webhook Name>\",\n  \"can_evaluate\": { <JSON Expression> },\n  \"run_as_authed_user\": <Boolean>,\n  \"run_as_user_id\": \"<App Services User ID>\",\n  \"run_as_user_id_script_source\": \"<Function Source Code>\",\n  \"respond_result\": <Boolean>,\n  \"fetch_custom_user_data\": <Boolean>,\n  \"create_user_on_auth\": <Boolean>,\n  \"options\": {\n    \"httpMethod\": \"<HTTP Method>\",\n    \"validationMethod\": \"<Webhook Validation Method>\",\n    \"secret\": \"<Webhook Secret>\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Unique Webhook Name>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"run_as_authed_user\": true,\n  \"run_as_user_id\": \"\",\n  \"run_as_user_id_script_source\": \"\",\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"email\": \"<User's Email Address>\",\n  \"password\": \"<User's Password>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"api-key\": \"<User's API Key>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"jwtTokenString\": \"<User's JWT Token>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"run_as_authed_user\": false,\n  \"run_as_user_id\": \"\",\n  \"run_as_user_id_script_source\": \"\",\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"run_as_authed_user\": false,\n  \"run_as_user_id\": \"<App Services User ID>\",\n  \"run_as_user_id_script_source\": \"\",\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"run_as_authed_user\": false,\n  \"run_as_user_id\": \"\",\n  \"run_as_user_id_script_source\": \"<Stringified Function>\",\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"options\": {\n    \"httpMethod\": \"POST\"\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function(payload, response) {\n  switch(context.request.httpMethod) {\n    case \"GET\": { /* Handle GET requests */ }\n    case \"POST\": { /* Handle POST requests */ }\n    case \"PUT\": { /* Handle PUT requests */ }\n    case \"DELETE\": { /* Handle DELETE requests */ }\n    case \"PATCH\": { /* Handle PATCH requests */ }\n    default: {}\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n    \"%%request.remoteIPAddress\": {\n        \"$nin\": [\n            \"248.88.57.58\",\n            \"19.241.23.116\",\n            \"147.64.232.1\"\n        ]\n    }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"validationMethod\": \"NO_VALIDATION\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"validationMethod\": \"VERIFY_PAYLOAD\",\n  \"secret\": \"<Secret Value>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"validationMethod\": \"SECRET_AS_QUERY_PARAM\",\n  \"secret\": \"<Secret Value>\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function (payload, response) {\n  // Convert the webhook body from BSON to an EJSON object\n  const body = EJSON.parse(payload.body.text());\n\n  // Execute application logic, such as working with MongoDB\n  if (body.someField) {\n    const mdb = context.services.get(\"mongodb-atlas\");\n    const requests = mdb.db(\"demo\").collection(\"requests\");\n    const { insertedId } = await requests.insertOne({\n      someField: body.someField,\n    });\n    // Respond with an affirmative result\n    response.setStatusCode(200);\n    response.setBody(`Successfully saved \"someField\" with _id: ${insertedId}.`);\n  } else {\n    // Respond with a malformed request error\n    response.setStatusCode(400);\n    response.setBody(`Could not find \"someField\" in the webhook request body.`);\n  }\n  // This return value does nothing because we already modified the response object.\n  // If you do not modify the response object and you enable *Respond with Result*,\n  // App Services will include this return value as the response body.\n  return { msg: \"finished!\" };\n};\n"
                },
                {
                    "lang": "bash",
                    "value": "realm-cli push --remote=\"<Your App ID>\""
                }
            ],
            "preview": "Some external services allow you to create incoming webhooks that external clients can call over HTTP. You can create\nwebhooks for these services from the App Services UI or with App Services CLI. Select the tab\nbelow that corresponds to the method you want to use.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "logs/trigger",
            "title": "Trigger Logs",
            "headings": [
                "Overview",
                "Log Format",
                "Fields",
                "Error Fields"
            ],
            "paragraphs": "Trigger logs are created whenever Atlas App Services executes a Function via a\n Trigger . Trigger log entries have the following form: Field Description Compute Used The computational load of the operation. Logs A list of  console.log  outputs. App Services saves the first 512 bytes of the first 25  console.log()  calls. See Function. See Trigger. Links to the Trigger that launched this event as well as the Function that was run by this event. Log entries created by unsuccessful operations may feature additional\nfields for debugging purposes. These include the following: Field Description Error A brief description of an error.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "Logs:\n[\n   <log line>,\n   <log line>,\n   ...\n]\n\n See Function. See Trigger.\n\n Compute Used: <number> bytes\u2022ms"
                }
            ],
            "preview": "Trigger logs are created whenever Atlas App Services executes a Function via a\nTrigger.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "logs/error-handler",
            "title": "Trigger Error Handler Logs",
            "headings": [
                "Overview",
                "Log Format",
                "Fields",
                "Error Fields"
            ],
            "paragraphs": "Trigger error handler logs are created whenever Atlas executes a custom error\nhandler via an  AWS EventBridge Trigger \nthat failed and could not be successfully retried. Currently, custom error handling is only supported for database-type\nEventBridge triggers. Trigger error handler log entries have the following form: Field Description Compute Used The computational load of the operation. Logs A list of  console.log  outputs. App Services saves the first 512 bytes of the first 25  console.log()  calls. See Function. See Trigger. See Original Logs. Links to the custom error handler Function that was run by this event, the EventBridge Trigger whose failure launched this event, and the error logs created by the original Trigger failure event. Log entries created by unsuccessful operations may feature additional\nfields for debugging purposes. These include the following: Field Description Error A brief description of an error.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "Logs:\n[\n   <log line>,\n   <log line>,\n   ...\n]\n\n See Function. See Trigger. See Original Logs.\n\n Compute Used: <number> bytes\u2022ms"
                }
            ],
            "preview": "Trigger error handler logs are created whenever Atlas executes a custom error handler via an AWS EventBridge database Trigger that failed and could not be successfully retried.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "logs/cli",
            "title": "View Logs with App Services CLI",
            "headings": [
                "View Recent Logs",
                "Tail Logs in Real Time",
                "View Error Logs",
                "Filter Logs by Type",
                "View Logs for a Date Range"
            ],
            "paragraphs": "You can list the 100 most recent log entries for your application: You can use the  --tail  flag to open a stream that displays application logs\nas they come in. You can use the  --errors  flag to view only error logs. If you don't specify\nthe flag, the command returns both error logs and regular logs. You can use the  --type  flag to view logs of one or more specific types. If\nyou don't specify a type, the command returns logs of all types. The following types are valid: auth function push service trigger graphql sync schema You can use the  --start  and  --end  flags to view logs from a range of\ndates. The flags accept ISODate strings and you can use them separately or\ntogether.",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices logs list"
                },
                {
                    "lang": "shell",
                    "value": "appservices logs list --tail"
                },
                {
                    "lang": "shell",
                    "value": "appservices logs list --errors"
                },
                {
                    "lang": "shell",
                    "value": "appservices logs list --type=function --type=trigger"
                },
                {
                    "lang": "shell",
                    "value": "appservices logs list --start=\"2021-01-01T00:00:00.000+0000\" --end=\"2021-02-01T00:00:00.000+0000\""
                }
            ],
            "preview": "You can list the 100 most recent log entries for your application:",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "logs/schema",
            "title": "Schema Logs",
            "headings": [
                "Overview",
                "Fields"
            ],
            "paragraphs": "Events related to your App's Atlas Schema create schema logs.\nThis includes both breaking (destructive) and non-breaking (additive)\nschema updates to tables and fields, as well as the collection scans used\nto automatically generate schemas based on the form of existing documents. Field Description Type The kind of event that effected the schema. Logs Important information about the schema update. This can include: Whether the schema change originated in a client while\nDevelopment Mode was enabled or from the Atlas App Services backend. The name of the table the schema change effected. The name of the field added or removed from the table. Whether or not the field is/was an embedded list type. Whether or not the field is/was required. The type of the field added or removed from the table. If a new table was created, the primary key field name and type. SDK The SDK used to send the request. Potential values include any SDK.\nFor a browser, this is  <browser> <SDK version> . This request came from the Realm Web SDK version 10.0.0\nrunning on Mozilla Firefox: This request came from the Realm Java SDK version 10.0.0 running\non Android Marshmallow: Platform Version The version of App Services that processed the change. Rule Performance Metrics A summary of the  roles ,  schemas , and  filters  applied to a\nreturn value. Provides the number of documents that a role applies to as\nwell as the number of fields in those documents that App Services evaluated and\npotentially withheld.",
            "code": [
                {
                    "lang": "none",
                    "value": "SDK: firefox v10.0.0"
                },
                {
                    "lang": "none",
                    "value": "SDK: android v10.0.0"
                }
            ],
            "preview": "Events related to your App's Atlas Schema create schema logs.\nThis includes both breaking (destructive) and non-breaking (additive)\nschema updates to tables and fields, as well as the collection scans used\nto automatically generate schemas based on the form of existing documents.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "logs/authentication",
            "title": "Authentication Logs",
            "headings": [
                "Overview",
                "Log Format",
                "Fields",
                "Error Fields"
            ],
            "paragraphs": "Atlas App Services creates authentication logs whenever a user is created,\ndeleted, or logs in. Authentication log entries have the following form: Remote IP Address\nSDK\nPlatform Version Field Description Remote IP Address The IP Address that sent the request to App Services. (e.g.  52.21.89.200 ) Platform Version The version of the platform that sent the request. SDK The SDK used to send the request. Potential values include any SDK.\nFor a browser, this is  <browser> <SDK version> . This request came from the Realm Web SDK version 4.0.0 running\non Mozilla Firefox: This request came from the Realm Java SDK version 4.6.0 running\non Android Marshmallow: Log entries created by unsuccessful operations may feature additional\nfields for debugging purposes. These include the following: Field Description Error A brief description of an error.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "Remote IP Address: <ip address>\n\nSDK: <sdk name>\n\nPlatform Version: <version number>"
                },
                {
                    "lang": "none",
                    "value": "SDK: firefox v4.0.0"
                },
                {
                    "lang": "none",
                    "value": "SDK: android v4.6.0"
                }
            ],
            "preview": "Atlas App Services creates authentication logs whenever a user is created,\ndeleted, or logs in.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "logs/changestream",
            "title": "Change Stream Logs",
            "headings": [
                "Overview",
                "Log Format",
                "Fields"
            ],
            "paragraphs": "Change Stream log events are created whenever a user opens or closes a\nchange stream on a linked data source using the  watch()  API. Change Stream log entries have the following form: Field Description Remote IP Address The IP address that sent the request to Atlas App Services (e.g.  52.21.89.200 ). Platform Version The version of the platform that sent the request. SDK The SDK used to send the request. Potential values include any SDK.\nFor a browser, this is  <browser> <SDK version> . This request came from the Realm Web SDK version 4.0.0\nrunning on Mozilla Firefox: This request came from the Realm Java SDK version 4.6.0 running\non Android Marshmallow: Function Call Location The data center in which the function was executed.",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n   \"arguments\": [\n      {\n      \"collection\": <collection>,\n      \"database\": <database>,\n      \"filter\": {\n         \"documentKey._id\": {\n            \"$in\": [\n               {\n               \"$oid\": <uid>\n               }\n            ]\n         }\n      },\n      \"useCompactEvents\": <bool>\n      }\n   ],\n   \"name\": \"watch\",\n   \"service\": \"mongodb-atlas\"\n}\n\nFunction Call Location: <location>\nRemote IP Address: <ip address>\nSDK: <sdk>\nPlatform Version: <version>>"
                },
                {
                    "lang": "none",
                    "value": "SDK: firefox v4.0.0"
                },
                {
                    "lang": "none",
                    "value": "SDK: android v4.6.0"
                }
            ],
            "preview": "Change Stream log events are created whenever a user opens or closes a\nchange stream on a linked data source using the watch() API.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "logs/endpoint",
            "title": "Endpoint Logs",
            "headings": [
                "Overview",
                "Log Format",
                "Fields",
                "Error Fields"
            ],
            "paragraphs": "Endpoint logs are created whenever a user calls an\n HTTPS Endpoint . Endpoint log entries have the following form: Field Description Logs Information about the Endpoint Call. Includes the name of the function,\nas well as the arguments passed to it. Also includes a list of  console.log \noutputs, when relevant. Function Call Location The data center in which the function was executed. Compute Used The computational load of the operation. Remote IP Address The IP Address that sent the request to the endpoint. (e.g. 52.21.89.200) Platform Version The version of the platform that sent the request. Endpoint Query Arguments The parameters passed to the endpoint function. Endpoint Headers A list of headers and the information passed to them, to be used\nby the endpoint and its function. Log entries created by unsuccessful operations may feature additional\nfields for debugging purposes. These include the following: Field Description Error A brief description of an error. Remote IP Address The IP Address that sent the request to the endpoint. (e.g. 52.21.89.200)",
            "code": [
                {
                    "lang": "json",
                    "value": "Logs:\n[\n   <log line>,\n   <log line>,\n   ...\n]\n\n{\n   \"arguments\": [\n      <arg1>,\n      <arg2>\n   ],\n   \"name\": <function name>,\n}\n\nFunction Call Location: <location>\nEndpoint Query Arguments: <arguments>\nEndpoint Headers: <headers>\nCompute Used: <number> bytes\u2022ms\nRemote IP Address: <ip address>"
                }
            ],
            "preview": "Endpoint logs are created whenever a user calls an\nHTTPS Endpoint.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "logs/sync",
            "title": "Device Sync Logs",
            "headings": [
                "Overview",
                "Connections",
                "Sessions",
                "MongoDB Atlas Sync Events",
                "Fields"
            ],
            "paragraphs": "Atlas Device Sync creates sync logs whenever a user interacts with Sync. This\nincludes writing data to Atlas App Services from clients, reading changes from\nApp Services to clients, and starting or ending a connection. Every user communicates with App Services using a  connection . Each\nconnection uses a single websocket to push and pull information to and\nfrom a single user. Logging in a user using any authentication provider\nstarts a connection. Logging out that user ends the connection. Users download and upload changes to a synced realm using a  session .\nInstantiating a local instance of a synced realm starts a session.\nDestroying that realm object ends the session. Every sync event is associated with a user identity that tells you\nexactly which user caused the event to occur. However, you may notice\nsome sync events that aren't associated with any specific user. These\nevents include downloaded changes from all clients as well as as well as\nany change made in MongoDB Atlas. Any sync event that does not include\na user ID is part of the MongoDB Atlas data synchronization process. You\nmay see a large number of MongoDB Atlas synchronization events when you\ninitialize sync if your cluster already contains a large amount of data. Field Description Logs Debugging information about the operation. Includes the schema\nversion on client and server, the number of changes, and whether\nor not conflict resolution was required in order to incorporate\nthe operation into the server's copy of the data. Write Summary Includes information about the data that changed, such as: The type or table of data that changed. The operation used to change data: insertion, deletion, update, or replace. The primary key of the data that changed. SDK The SDK used to send the request. Potential values include any SDK.\nFor a browser, this is  <browser> <SDK version> . This request came from the Realm Web SDK version 4.0.0\nrunning on Mozilla Firefox: This request came from the Realm Java SDK version 4.6.0 running\non Android Marshmallow: Platform Version The version of the platform that sent the request.",
            "code": [
                {
                    "lang": "none",
                    "value": "SDK: firefox v4.0.0"
                },
                {
                    "lang": "none",
                    "value": "SDK: android v4.6.0"
                }
            ],
            "preview": "Atlas Device Sync creates sync logs whenever a user interacts with Sync. This\nincludes writing data to Atlas App Services from clients, reading changes from\nApp Services to clients, and starting or ending a connection.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "logs/service",
            "title": "Service Logs",
            "headings": [
                "Overview",
                "MongoDB Atlas",
                "Fields",
                "Error Fields",
                "Webhook / HTTP Service",
                "Fields",
                "Error Fields"
            ],
            "paragraphs": "Service logs are created by whenever a user interacts with\n External Services . The fields present in\nService log entries are determined by the service that emitted the log. MongoDB Atlas  is offered as a first-class service\nwithin Atlas App Services. Service log entries have the following form: Field Description Remote IP Address The IP Address that sent the request to App Services. (e.g.  52.21.89.200 ) Compute Used Computational load of the operation. Platform Version The version of the platform that sent the request. SDK The SDK used to send the request. Potential values include any SDK.\nFor a browser, this is  <browser> <SDK version> . This request came from the Realm Web SDK version 4.0.0\nrunning on Mozilla Firefox: This request came from the Realm Java SDK version 4.6.0 running\non Android Marshmallow: Function Call Location The data center in which the function was executed. Rule Performance Metrics A summary of the  roles ,\n schemas , and  filters \napplied to a return value. Provides the number of documents that a role\napplies to as well as the number of fields in those documents that App Services\nevaluated and potentially withheld. Log entries created by unsuccessful operations may feature additional\nfields for debugging purposes. These include the following: Field Description Error A brief description of an error. Stack Trace A printout of an exception stack trace. Details Extra information about an execution, including action, reason, service name, and service type. Webhook  log entries\nadhere to the following form: Field Description Remote IP Address The IP Address that sent the request to App Services. (e.g.  52.21.89.200 ) Compute Used The  computational load  of the operation measured\nin  byte-ms . Function Call Location The data center in which the function was executed. Logs A list of  console.log  outputs. App Services saves the first 512 bytes of the first 25  console.log()  calls. Log entries created by unsuccessful operations may feature additional\nfields for debugging purposes. These include the following: Field Description Error A brief description of an error. Stack Trace A printout of an exception stack trace. Details Extra information about an execution, including action, reason, service name, and service type.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "{\n \"arguments\": [\n   {\n     \"collection\": <collection>,\n     \"database\": <database>,\n     \"query\": <query document>,\n     \"limit\": {\n       \"$numberInt\": <number>\n     }\n   }\n ],\n \"name\": <action>,\n \"service\": \"mongodb-atlas\"\n}\n\nFunction Call Location: <location>\n\nCompute Used: <number> bytes\u2022ms\n\nRemote IP Address: <ip address>\n\nSDK: <sdk>>\n\nPlatform Version: <version>\n\nRule Performance Metrics:\n{\n \"database.collection\": {\n   \"roles\": {\n     \"owner\": {\n       \"matching_documents\": 5,\n       \"evaluated_fields\": 0,\n       \"discarded_fields\": 0\n     }\n   },\n   \"no_matching_role\": 0\n   }\n}"
                },
                {
                    "lang": "none",
                    "value": "SDK: firefox v4.0.0"
                },
                {
                    "lang": "none",
                    "value": "SDK: android v4.6.0"
                },
                {
                    "lang": "javascript",
                    "value": "Logs:\n[\n   <log line>,\n   <log line>,\n   ...\n]\n\nFunction Call Location: <location>\n\nCompute Used: <number> bytes\u2022ms\n\nRemote IP Address: <ip address>"
                }
            ],
            "preview": "Service logs are created by whenever a user interacts with\nExternal Services. The fields present in\nService log entries are determined by the service that emitted the log.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "activity/alerts",
            "title": "Activity Feed & Atlas App Services Alerts",
            "headings": [
                "Introduction",
                "Project Activity Feed",
                "Filter Activities",
                "App Services Alerts",
                "Default Alerts",
                "Custom App Services Alerts"
            ],
            "paragraphs": "MongoDB Atlas provides an Activity Feed that displays events for each\n project . Atlas App Services\nactivities are included in this feed. To view and/or download only\nApp Services events, you can specify a filter, as described below. There are two ways to access the Project Activity Feed: through the\nAtlas UI, and through the Atlas API. The following Atlas\ndocumentation pages describe each approach: View Activity Feed in the UI Get All Project Events through the API App Services activities are a subset of\n Atlas alerts . You can filter the Project Activity\nFeed results to show only App Services events, or a subset of\nApp Services events. To do this, click the\n Filter by event(s)  menu and check  App Services . In the\nright-hand side of the menu, you will see\nall App Services events selected. Deselect any that you do not want to see.\nThe feed list automatically updates with each change you make. The  Category  for\nall App Services activities is \"App Services\"; within that category, there are\nseveral  Events : The Activity Feed does not support filtering events by App Services\napplication, but each event (and its corresponding alert) provides context\nand routing information for the appropriate application. Event Type Condition Description Device Sync Sync Failure A Sync process has failed and cannot be restarted Trigger Trigger Failure A Trigger has failed and cannot be restarted Deployment A Custom URL is confirmed for static hosting Deployment Successful Deploy A user (or Code Deployment) deployment has succeeded Deployment Deployment Failure A user (or Code Deployment) deployment has failed Limit Request Rate Limit An overall request rate limit was hit There are default alerts that map to three of the activities listed above. In\naddition, you can create custom alerts for the other App Services\nactivities. App Services default alerts trigger a notification and email to the\nProject Owners. The default App Services alerts occur when: A Trigger has failed and cannot be restarted A Trigger was automatically resumed A Sync process has failed and cannot be restarted An overall request rate limit has been hit A Log Forwarder has failed and cannot be restarted For any activity that is not mapped to a default alert, you can set a custom\nalert at the Project level. You create App Services alerts in the same way you  create other\nAtlas alerts . When creating a new alert, in the\n\" Alert if \" section, select  App Services  from the\n Target .",
            "code": [],
            "preview": "Learn about the Project Activity Feed and Alerts available for App Services.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "logs/function",
            "title": "Function Logs",
            "headings": [
                "Overview",
                "Log Format",
                "Fields",
                "Error Fields"
            ],
            "paragraphs": "Function logs are created whenever a user calls an Atlas Function using a\nRealm SDK. While you can execute Functions with a\n Trigger , only functions directly called\nfrom an SDK generate logs of the function type. Function log entries have the following form: Field Description Remote IP Address The IP Address that sent the request to Atlas App Services. (e.g.  52.21.89.200 ) Compute Used The computational load of the operation. Platform Version The version of the platform that sent the request. SDK The SDK used to send the request. Potential values include any SDK.\nFor a browser, this is  <browser> <SDK version> . This request came from the Realm Web SDK version 4.0.0\nrunning on Mozilla Firefox: This request came from the Realm Java SDK version 4.6.0 running\non Android Marshmallow: Function Call Location The data center in which the function was executed. Logs A list of  console.log  outputs. App Services saves the first 512 bytes of the first 25  console.log()  calls. Log entries created by unsuccessful operations may feature additional\nfields for debugging purposes. These include the following: Field Description Error A brief description of an error. Stack Trace A printout of an exception stack trace. Details Extra information about an execution, including action, reason, service name, and service type.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "Logs:\n[\n   <log line>,\n   <log line>,\n   ...\n]\n\n{\n   \"arguments\": [\n      <arg1>,\n      <arg2>\n   ],\n   \"name\": <function name>,\n   \"service\": \"\"\n}\n\nFunction Call Location: <location>\n\nCompute Used: <number> bytes\u2022ms\n\nRemote IP Address: <ip address>\n\nSDK: <sdk>\n\nPlatform Version: <version>"
                },
                {
                    "lang": "none",
                    "value": "SDK: firefox v4.0.0"
                },
                {
                    "lang": "none",
                    "value": "SDK: android v4.6.0"
                }
            ],
            "preview": "Function logs are created whenever a user calls an Atlas Function using a\nRealm SDK. While you can execute Functions with a\nTrigger, only functions directly called\nfrom an SDK generate logs of the function type.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "activity/view-logs",
            "title": "View Application Logs",
            "headings": [
                "App Services UI",
                "Filter Logs",
                "Limit the Number of Log Entries",
                "Download Logs",
                "App Services CLI",
                "View Recent Logs",
                "Tail Logs in Real Time",
                "View Error Logs",
                "Filter Logs by Type",
                "View Logs for a Date Range",
                "App Services API",
                "Get Recent Logs",
                "Get Logs for a Date Range",
                "Get Paginated Logs"
            ],
            "paragraphs": "You can view, filter, search, and download an application's logs in the\nApp Services UI. To see your App's logs, click  Logs  in the\nleft navigation menu. Drop-down menus enable filtering by predefined types of log entry, the status\nof entries. You can also specify a date range, filter by user ID, and show\nonly entries associated with a specific request ID. In the text field with the placeholder text  Max # of Logs , enter\nthe maximum number of results you want to display on the page. If there are\nmore records beyond your limit, you will see a  Load More  button\nat the bottom of the list of log entries. Click the download button, to the right of the  Apply  button,\nto download the log entries that meet your filter criteria (or all log\nentries if you haven't specified any filters). When you click this\nbutton, a dialog appears confirming the filters that will be applied and\nprompting you to (optionally) limit the number of results. The resulting\ncollection of log entries is in a .zip file that contains a single JSON\nfile. You can access an application's logs in your terminal or a shell script\nwith the App Services CLI. To return the 100 most recent log entries for your application, run  appservices\nlogs list . You can use the  --tail  flag to open a stream that displays application logs\nas they come in. You can use the  --errors  flag to view only error logs. If you don't specify\nthe flag, the command returns both error logs and regular logs. You can use the  --type  flag to view logs of one or more specific types. If\nyou don't specify a type, the command returns logs of all types. The following types are valid: auth function push service trigger graphql sync schema trigger_error_handler log_forwarder endpoint You can use the  --start  and  --end  flags to view logs from a range of\ndates. The flags accept ISODate strings and you can use them separately or\ntogether. You can access an application's logs over HTTPS by calling the\n Admin API Logs endpoints . To use the Admin API you'll need your Project ID, App ID, and\nauthentication credentials. To learn how to find these, see\n Project & Application IDs  and  Get\nAuthentication Tokens . The examples in this section use the following helper functions in a\n Function : To return the 100 most recent log entries for your application, call the\nLogging endpoint with no additional parameters: To return log entries for a specific date range, call the Logging\nendpoint with either or both of the  start_date  and  end_date \nfields: If the date range that you specify includes more than 100 log\nentries, you will need to run multiple queries to access all of the\nentries. To learn how, see  Get Paginated Logs . App Services returns a maximum of 100 log entries for each request. If a\nquery matches more than 100 log entries, the API returns the\nfirst \"page\" of 100 results and include additional parameters in the\nresponse that you can provide to get the next page(s) of up to 100\nentries. A paginated response resembles the following document, where\n nextEndDate  and  nextSkip  are optional:",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices logs list"
                },
                {
                    "lang": "shell",
                    "value": "appservices logs list --tail"
                },
                {
                    "lang": "shell",
                    "value": "appservices logs list --errors"
                },
                {
                    "lang": "shell",
                    "value": "appservices logs list --type=function --type=trigger"
                },
                {
                    "lang": "shell",
                    "value": "appservices logs list --start=\"2021-01-01T00:00:00.000+0000\" --end=\"2021-02-01T00:00:00.000+0000\""
                },
                {
                    "lang": "javascript",
                    "value": "async function authenticate(publicApiKey, privateApiKey) {\n  const result = await context.http.post({\n    url: `${ADMIN_API_BASE_URL}/auth/providers/mongodb-cloud/login`,\n    headers: {\n      \"Content-Type\": [\"application/json\"],\n      \"Accept\": [\"application/json\"],\n    },\n    body: {\n      \"username\": publicApiKey,\n      \"apiKey\": privateApiKey,\n    },\n    encodeBodyAsJSON: true\n  })\n  return EJSON.parse(result.body.text());\n}\n\nfunction formatQueryString(queryParams) {\n  const params = Object.entries(queryParams);\n  return params.length > 0\n    ? \"?\" + params.map(([a, b]) => `${a}=${b}`).join(\"&\")\n    : \"\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "const ADMIN_API_BASE_URL = \"https://services.cloud.mongodb.com/api/admin/v3.0\";\nexports = async function() {\n  // Get values that you need for requests\n  const projectId = \"<Atlas Project ID>\";\n  const appId = \"<App ID>\";\n  const publicApiKey = \"<Atlas Public API Key>\";\n  const privateApiKey = \"<Atlas Private API Key>\";\n\n  // Authenticate with the Atlas API Key\n  const { access_token } = await authenticate(publicApiKey, privateApiKey);\n\n  // Get logs for your App\n  const logsEndpoint = `${ADMIN_API_BASE_URL}/groups/${projectId}/apps/${appId}/logs`;\n  const  request = {\n    \"url\": logsEndpoint,\n    \"headers\": {\n      \"Authorization\": [`Bearer ${access_token}`]\n    }\n  };\n  const result = await context.http.get(request);\n  const logs = EJSON.parse(result.body.text());\n  return logs;\n}"
                },
                {
                    "lang": "javascript",
                    "value": "const ADMIN_API_BASE_URL = \"https://services.cloud.mongodb.com/api/admin/v3.0\";\nexports = async function() {\n  // Get values that you need for requests\n  const projectId = \"<Atlas Project ID>\";\n  const appId = \"<App ID>\";\n  const publicApiKey = \"<Atlas Public API Key>\";\n  const privateApiKey = \"<Atlas Private API Key>\";\n\n  // Authenticate with the Atlas API Key\n  const { access_token } = await authenticate(publicApiKey, privateApiKey);\n\n  // Get logs for your App\n  const logsEndpoint = `${ADMIN_API_BASE_URL}/groups/${projectId}/apps/${appId}/logs`;\n  const  request = {\n    \"url\": logsEndpoint + formatQueryString({\n      start_date: \"2019-07-01\",\n      end_date: \"2019-07-31\",\n    }),\n    \"headers\": {\n      \"Authorization\": [`Bearer ${access_token}`]\n    }\n  };\n  const result = await context.http.get(request);\n  const logs = EJSON.parse(result.body.text());\n  return logs;\n}"
                },
                {
                    "lang": "javascript",
                    "value": "const ADMIN_API_BASE_URL = \"https://services.cloud.mongodb.com/api/admin/v3.0\";\nexports = async function() {\n  // Get values that you need for requests\n  const projectId = \"<Atlas Project ID>\";\n  const appId = \"<App ID>\";\n  const publicApiKey = \"<Atlas Public API Key>\";\n  const privateApiKey = \"<Atlas Private API Key>\";\n\n  // Authenticate with the Atlas API Key\n  const { access_token } = await authenticate(publicApiKey, privateApiKey);\n\n  // Get logs for your App\n  const pager = new LogPager(projectId, appId, access_token);\n  const firstPage = await pager.getNextPage();\n  const secondPage = await pager.getNextPage(firstPage);\n  const thirdPage = await pager.getNextPage(secondPage);\n  const allLogs = await pager.getAllLogs();\n}\n\nclass LogPager {\n  constructor(projectId, appId, access_token, queryParams={}) {\n    this.logsEndpoint = `${ADMIN_API_BASE_URL}/groups/${projectId}/apps/${appId}/logs`;\n    this.queryParams = queryParams;\n    this.authHeaders = { Authorization: [`Bearer ${access_token}`] }\n  }\n\n  async getNextPage(prevPage) {\n    const { nextEndDate, nextSkip } = prevPage || {};\n    if(prevPage && !nextEndDate) {\n      throw new Error(\"Paginated API does not have any more pages.\")\n    }\n    const request = {\n      \"headers\": this.authHeaders,\n      \"url\": this.logsEndpoint + formatQueryString({\n        ...this.queryParams,\n        end_date: nextEndDate,\n        skip: nextSkip,\n      }),\n    }\n    const result = await context.http.get(request);\n    const nextPage = EJSON.parse(result.body.text());\n    return nextPage\n  }\n\n  async getAllLogs() {\n    // Note: If your query parameters match too many logs this might time out\n    let logs = []\n    let hasNext = true;\n    let prevPage = null\n    while(hasNext) {\n      const page = await getNextPage(prevPage);\n      logs = logs.concat(page.logs);\n      hasNext = page.nextEndDate\n      prevPage = page\n    }\n    return logs;\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  logs: [<Log Entry>, ...],\n  nextEndDate: \"<End date of the next page>\",\n  nextSkip: <Offset of the next page>,\n}"
                }
            ],
            "preview": "View, filter, search, and download an application's logs in the Atlas App Services UI, or access logs using the App Services CLI or App Services API.",
            "tags": null,
            "facets": {
                "programming_language": [
                    "shell",
                    "json"
                ],
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "activity/forward-logs",
            "title": "Forward Logs to a Service",
            "headings": [
                "Overview",
                "Why Should I Configure Log Forwarding?",
                "How is Log Forwarding Billed?",
                "Set Up a Log Forwarder",
                "1. Create a Log Forwarder",
                "2. Choose Which Logs to Forward",
                "3. Configure Log Batching",
                "5. Define an Action",
                "Store Logs in a MongoDB Collection",
                "Forward Logs with a Custom Function",
                "6. Save and Deploy your Changes",
                "Restart a Suspended Log Forwarder"
            ],
            "paragraphs": "You can configure a log forwarder to automatically store your\napplication's server-side logs in a MongoDB collection or send them to\nan external service. Atlas App Services can forward logs individually as they're\ncreated or batch logs together to reduce overhead. A log forwarder consists of the following components: An  action  that controls how and where App Services forwards logs. A  filter  that controls which logs App Services forwards. A  policy  that controls whether App Services batches logs or forwards them individually. Consider setting up a log forwarder if you need to do any of the\nfollowing: Store logs for longer than App Services's retention period of 10 days. Integrate logs into an external logging service Access logs in Atlas Search, Online Archive, and Charts Each log forward action invocation (on either an individual log or a\nbatch) is billed as one  App Services request . To create a new log forwarder, navigate to the  Logs \npage and select the  Forwarding  tab. Then, click the\n Create a Log Forwarder  button. On the next screen, specify a unique name for the log forwarder. To create a new log forwarder, add a new configuration file to the\n log_forwarders  directory of your app. The file name should\nmatch the value in the configuration's  name  field. App Services can forward all of your app's logs or send only a subset to the\ntarget collection or service. You control this subset for each log\nforwarder by defining filters for the log type (e.g. functions, sync,\netc.) and status (i.e. success or error) that invoke the forwarder's\naction. Choose one or more types of log to forward in the  Log\nType  dropdown: Choose one or more statuses to forward in the  Log\nStatus  dropdown: Specify one or more types and one or more statuses for the\nforwarder to match and forward: App Services supports forwarding the following log types: App Services supports forwarding the following log statuses: auth endpoint function graphql push schema service sync trigger trigger_error_handler error success App Services only forwards a given log if both its type  and  status are\nspecified in the filter. For example, consider a forwarder that filters for  sync  logs with\nan  error  status. The filter  would  forward the following log: The filter  would not  forward the following logs: App Services can combine multiple logs in to a single batched request to reduce\noverhead. The way that App Services groups logs into batches is controlled by a\nbatching policy. App Services supports the following batching policies: No Batching:  App Services forwards logs individually as their\ncorresponding requests occur. Batching:  The forwarder groups documents into a batch as they\nhappen. Each batch may include up to 100 log entries. When a batch is\nfull, App Services forwards the entire batch in a single request. App Services\nforwards logs at least once a minute regardless of the number of logs\nin the current batch. To configure batching, select either the  No batch  or\n Batching  policy. To configure batching, specify the policy type, either  single \nor  batch , in the  policy  field: A log forwarder can automatically store logs in a linked MongoDB\ncollection or call a custom function that sends the logs to an external\nservice. To store logs in a collection, select the  To\nCollection  action and enter the names of the linked cluster,\ndatabase, and collection that should hold the forwarded logs. To store logs in a collection, specify an  action  of type\n collection  that includes the names of the linked cluster,\ndatabase, and collection that should hold the forwarded logs. To forward logs to an external service,  write a function  that accepts an array of log objects and calls the\nservice through an API, SDK, or library. The function should have the same signature as the following example: Once you've written the log forwarding function, you can assign it to a\nlog forwarder by name. Depending on your batching policy and log frequency, App Services may call a\nlog forwarding function with an array of up to 100 log objects. To assign a function to a log forwarder, select the  To\nFunction  action and then select the function from the dropdown\ninput. To assign a function to a log forwarder, specify an  action  of\ntype  function  that includes the name of the log forwarding\nfunction. Once you've configured the log forwarder, click  Save .\nIf you have deployment drafts enabled, make sure to deploy your\nchanges. Once you've configured the log forwarder, save the configuration\nfile and then push your updated app configuration: A log forwarder may suspend in response to an event that prevents it\nfrom continuing, such as a network disruption or a change to the\nunderlying cluster that stores the logs. Once suspended, a forwarder\ncannot be invoked and does not forward any logs. You can restart a suspended log forwarder from the  Logs >\nForwarding  screen of the App Services UI. If a log forwarder is suspended, App Services sends the project\nowner an email alerting them of the issue.",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<name>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<name>\",\n  \"log_types\": [ \"<type>\", ... ],\n  \"log_statuses\": [ \"<status>\", ... ]\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"type\": \"sync\", \"status\": \"error\", ... }"
                },
                {
                    "lang": "json",
                    "value": "{ \"type\": \"sync\", \"status\": \"success\", ... }\n{ \"type\": \"schema\", \"status\": \"error\", ... }"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<name>\",\n  \"log_types\": [ \"<type>\", ... ],\n  \"log_statuses\": [ \"<status>\", ... ],\n  \"policy\": { \"type\": \"<single|batch>\" }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<name>\",\n  \"log_types\": [ \"<type>\", ... ],\n  \"log_statuses\": [ \"<status>\", ... ],\n  \"policy\": { \"type\": \"<single|batch>\" },\n  \"action\": {\n    \"type\": \"collection\",\n    \"data_source\": \"<data source name>\",\n    \"database\": \"<database name>\",\n    \"collection\": \"<collection name>\"\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function(logs) {\n  // `logs` is an array of 1-100 log objects\n  // Use an API or library to send the logs to another service.\n  await context.http.post({\n    url: \"https://api.example.com/logs\",\n    body: logs,\n    encodeBodyAsJSON: true\n  });\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<name>\",\n  \"log_types\": [ \"<type>\", ... ],\n  \"log_statuses\": [ \"<status>\", ... ],\n  \"policy\": { \"type\": \"<single|batch>\" },\n  \"action\": {\n    \"type\": \"function\",\n    \"name\": \"<function name>\"\n  }\n}"
                },
                {
                    "lang": "sh",
                    "value": "appservices push"
                }
            ],
            "preview": "Configure log forwarders to store Atlas App Services application logs in a MongoDB collection or send them to an external service.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "values-and-secrets/access-a-value",
            "title": "Access a Value",
            "headings": [
                "Overview",
                "Usage",
                "Reference a Value in a JSON Expression",
                "Reference a Value in a Function",
                "Summary"
            ],
            "paragraphs": "You can access an  Atlas App Services Value  from a  rule\nexpression  or an  Atlas Function . You can access a Value's stored data from a JSON expression\nusing the  %%values  expansion. The following JSON expression evaluates to  true  when the active\nuser's id is included in the plain text array Value\n adminUsers : You can access a Value's stored data from a Function\nusing the  context.values  module. The following Function returns  true  when the active\nuser's id is included in the plain text array Value\n adminUsers : Trying to access an environment variable? Check out the function context\ndocumentation for more info on  context.environment.values . You can access an App Services Value from a  rule expression  or from an  Atlas Function .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "\"%%values.<Value Name>\""
                },
                {
                    "lang": "javascript",
                    "value": "{ \"%%user.id\": { \"$in\": \"%%values.adminUsers\" } }"
                },
                {
                    "lang": "javascript",
                    "value": "context.values.get(\"<Value Name>\")"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n    const adminUsers = context.values.get(\"adminUsers\");\n    const isAdminUser = adminUsers.indexOf(context.user.id) > 0;\n    return isAdminUser;\n}"
                }
            ],
            "preview": "You can access an Atlas App Services Value from a rule\nexpression or an Atlas Function.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "hosting/enable-hosting",
            "title": "Enable Hosting",
            "headings": [
                "Overview",
                "Procedure",
                "Navigate to the Hosting Configuration Page",
                "Enable Hosting"
            ],
            "paragraphs": "You need to enable static hosting for your application before you can\nupload and access content. You can enable static hosting from the App Services UI. To enable static hosting, you must have a paid-tier (i.e.  M2  or\nhigher) Atlas cluster linked to your app as a data source. For more\ninformation on Atlas cluster tiers, see  Create a Cluster . Enable hosting from the UI with the following procedure: To open the hosting configuration page, click  Hosting  in\nthe left navigation menu of the App Services UI. On the  Hosting  configuration page, click  Enable\nHosting . App Services will begin provisioning hosting for your application. It may take a few minutes for App Services to finish provisioning hosting\nfor your application once you've enabled it. You can  upload\ncontent to App Services  immediately,\nbut you will need to wait for provisioning to complete before\nApp Services serves your files.",
            "code": [],
            "preview": "You need to enable static hosting for your application before you can\nupload and access content. You can enable static hosting from the App Services UI.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "hosting/migrate-vercel",
            "title": "Migrate Static Hosting to Vercel",
            "headings": [
                "Before You Begin",
                "Deploy with Git",
                "Deploy with the Vercel CLI",
                "Install Vercel CLI",
                "Login to Vercel",
                "Deploy to Vercel",
                "Update DNS records (optional)",
                "Shut Down Atlas App Services Hosting",
                "Deployment Protection on Vercel",
                "Learn More"
            ],
            "paragraphs": "Vercel is well-suited for hosting and web site or application, including static\nsites, single-page applications (SPAs), dynamic server-rendered applications,\nand more. It is compatible with your App Services application. Migrating a web application from MongoDB Atlas hosting to Vercel involves a few\nkey steps. Below is a general guide to help you through the process. Always refer to the official documentation of both MongoDB Atlas and Vercel\nfor the most up-to-date and accurate information. Specific steps may vary\ndepending on the details of your project and the technologies used. As a precaution, ensure that you have a backup of your application and data\nbefore making any significant changes. Create a Vercel account. To learn how to create deployments on Vercel, visit\nthe  official Vercel docs . We\nwill be showing you how to deploy using Git and the Vercel CLI below. If your application isn't already in a version control system, consider\nsetting it up. Vercel seamlessly integrates with popular version control\nplatforms. Vercel documentation:  Deploy with Git Push your code to your git repository (GitHub, GitLab, BitBucket). Import your project into Vercel. Optionally configure your project before it's deployed. Select the Deploy button to initiative a deployment. Your application is deployed! (e.g. create-react-template.vercel.app) Vercel documentation:  Deploy from CLI When you create a deployment, Vercel automatically adds a new and unique\ngenerated URL. You can visit this URL to preview your changes in a live\nenvironment. After deploying, your new site will automatically be assigned a .vercel.app\nsuffixed domain. You can then add a Custom Domain of your choice, either from a\nthird-party or purchased through Vercel. Install the Vercel CLI on your local machine. This allows you to deploy\nand manage your projects using the command line. Vercel documentation:  https://vercel.com/docs/cli#installing-vercel-cli Log in to your Vercel account using the CLI Deploy your application to Vercel using the following command: Follow the prompts to complete the deployment process. If your domain is currently pointed to MongoDB Atlas and you want to use\nthe same domain with Vercel, update your DNS records to point to the\nVercel domain. Once you have verified that your application deploys successfully to\nVercel, delete your hosted files from your Atlas App Services app.\nAs a reminder, hosting domains on Atlas App Services will no longer run\nstarting on March 12, 2025. Vercel documentation:  Deployment Protection Vercel offers the following Deployment Protection features: Vercel Authentication :\nRestricts access to your deployments to only Vercelusers with suitable access\nrights. Vercel Authentication is available on all plans Password Protection :\nRestricts access to your deployments to only users with the correct password.\nPassword Protection is available on the Enterprise plan, or as a paid add-on\nfor Pro plans Trusted IPs :\nRestricts access to your deployments to only users with the correct IP\naddress. Trusted IPs is available on the Enterprise plan. Feature Vercel App Services Plan Availability (on Vercel) Restrict access to platform users \u2705 \u2705 All plans Password protection \u2705 \ud83d\udeab For purchase on Pro, included in Enterprise Trusted IPs \u2705 \u2705 Enterprise plan Handling redirects Configuring CDN caching",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm i -g vercel"
                },
                {
                    "lang": "shell",
                    "value": "vercel login"
                },
                {
                    "lang": "shell",
                    "value": "vercel deploy"
                }
            ],
            "preview": "Learn how to migrate your static assets from Atlas App Services to Vercel.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "values-and-secrets/define-a-value",
            "title": "Define a Value",
            "headings": [
                "Overview",
                "Procedure",
                "Create a New Value",
                "Name the Value",
                "Define the Value",
                "Save the Value",
                "Pull Your App's Latest Configuration Files",
                "Add a Value Configuration File",
                "Import Your Application Directory",
                "Deploy the Value",
                "Summary"
            ],
            "paragraphs": "You can define a new  Value  from the App Services UI or by\nimporting an application directory that contains one or more\nValue configuration files. Select the tab below that\ncorresponds to the method that you want to use. You can define a new  Value \nfor your application in the App Services UI. To navigate to the\nValue configuration screen, click  Values &\nSecrets  in the left navigation menu. Ensure that the\n Values  tab is selected and then click\n Create New Value . Enter a unique  Value Name . This name is how you refer to\nthe value in functions and rules. Value names must not exceed 64 characters and may only contain\nASCII letters, numbers, underscores, and hyphens. The first\ncharacter must be a letter or number. Specify the  Value Type . You can define two different types\nof Value:  plain text  and  secret . A  plain text  value is a string, array, or object that you define\nmanually using standard JSON syntax. To define a  plain text  value, select the  Plain Text \nradio button and then enter the value in the input box. A  secret  value exposes a  Secret  for use in\nFunctions and rules. To reference an existing Secret, select the\n Secret  radio button and then select the name of\nthe Secret that the value should reference from the\n Secret Name  dropdown. Alternatively, you can create and reference a new Secret by\nentering the new Secret's name in the  Secret Name \ndropdown and then clicking  Create . Enter the new\n Secret Value  in the input box that appears. After you have named and defined the new Value, click\n Save . Once saved, you can immediately\n access the Value  in Functions and\nrules. Each Value is defined in its own JSON file in the  values \nsubdirectory of your exported application. For example, a Value named\n myValue  would be defined in the file  /values/myValue.json . Add a configuration file for the new Value to the  values  directory: The configuration file should have the following general form: Field Description name A unique name for the value. This name is how you refer to\nthe value in functions and rules. from_secret Default:  false . If  true , the Value exposes a\n Secret  instead of a plain-text JSON value. value The stored data that App Services exposes when the Value is referenced. If  from_secret  is  false ,  value  can be a standard\nJSON string, array, or object. If  from_secret  is  true ,  value  is a string that\ncontains the name of the Secret the value exposes. Ensure that the Value configuration file is saved and then navigate to\nthe root of your exported application directory. Log in to MongoDB\nAtlas with the App Services CLI: Once you're logged in, import the directory: Once the import completes, you can immediately begin to\n access the Value  in rules and\nfunctions. Push the value configuration to deploy it to your app. You can use the App Services UI or App Services CLI to create and define\nApp Services Values.",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices pull --remote=<App ID>"
                },
                {
                    "lang": "shell",
                    "value": "touch values/<Value Name>.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Value Name>\",\n  \"from_secret\": <boolean>,\n  \"value\": <Stored JSON Value|Secret Name>\n}"
                },
                {
                    "lang": "shell",
                    "value": "appservices login --api-key=\"<my api key>\" --private-api-key=\"<my private api key>\""
                },
                {
                    "lang": "shell",
                    "value": "appservices import"
                },
                {
                    "lang": "shell",
                    "value": "appservices push"
                }
            ],
            "preview": "You can define a new Value from the App Services UI or by\nimporting an application directory that contains one or more\nValue configuration files. Select the tab below that\ncorresponds to the method that you want to use.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "values-and-secrets/define-and-manage-secrets",
            "title": "Define and Manage Secrets",
            "headings": [
                "Define a Secret",
                "Create a New Secret",
                "Define the Secret Value",
                "Save and Deploy",
                "View Secrets",
                "Update a Secret",
                "Access a Secret",
                "Delete a Secret"
            ],
            "paragraphs": "Click  Values  in the left navigation menu and then click  Create\nNew Value . Enter a name for the value and then select  Secret  for\nthe value type. Secret names must not exceed 64 characters and may only contain\nASCII letters, numbers, underscores, and hyphens. The first\ncharacter must be a letter or number. Enter the secret value in the  Add Content  input. Secret values may not exceed 500 characters. Once you've defined the secret, click  Save . If\nyour application deployment drafts enabled, click\n Review & Deploy  to deploy the changes. To define a new secret, call  appservices secrets create . The CLI will\nprompt you for your App ID as well as a name and value for the secret. You can also specify the arguments when you call the program: To see a list of the names and IDs of all secrets in an app, click\n Values  in the left navigation menu. The table on the  Values \nscreen shows a list of all values, including secrets, and indicates each\nvalue's type in its row. To list the names and IDs of all secrets in an app, call  appservices\nsecrets list . The CLI will prompt you for your App ID. You can also specify the App ID when you call the program: To update a secret: Click  Values  in the left navigation menu. Find the value that you want to update in the table, open its\n Actions  menu, and select  Edit Secret . Select the  Add Content  input and enter the new value. Click  Save . To update the value of a secret, call  appservices secrets update . The\nCLI will prompt you for your App ID. You can also specify the App ID when you call the program: You cannot directly read the value of a Secret after defining it.\nThere are two ways to access a Secret that already exists in your app: Link to the Secret by name in  Authentication Provider \nand  Service configurations . Expose the Secret with a  Secret Value . You can\naccess the Secret Value in function's with  context.values  or in a rule expression with\n %%values . To delete a secret: Click  Values  in the left navigation menu. Find the value that you want to delete in the table, open its\n Actions  menu, and select  Delete Secret . Confirm that you want to delete the secret. To delete a secret, call  appservices secrets delete . The CLI will prompt\nyou for your App ID and list users in that app for you to select. You can also specify the arguments when you call the program: You can delete multiple secrets with a single command by specifying\ntheir  name  or  id  values as a comma-separated list.",
            "code": [
                {
                    "lang": "bash",
                    "value": "appservices secrets create"
                },
                {
                    "lang": "bash",
                    "value": "appservices secrets create --app=<Your App ID> \\\n  --name=\"<Secret Name>\" \\\n  --value=\"<Secret Value>\""
                },
                {
                    "lang": "bash",
                    "value": "appservices secrets list"
                },
                {
                    "lang": "bash",
                    "value": "appservices secrets list --app=<Your App ID>"
                },
                {
                    "lang": "bash",
                    "value": "appservices secrets update"
                },
                {
                    "lang": "bash",
                    "value": "appservices secrets update --app=<Your App ID> \\\n  --secret=\"<Secret ID or Current Name>\" \\\n  --name=\"<Updated Secret Name>\" \\\n  --value=\"<Updated Value>\""
                },
                {
                    "lang": "bash",
                    "value": "appservices secrets delete"
                },
                {
                    "lang": "bash",
                    "value": "appservices secrets delete --app=<Your App ID> --secret=<Secret ID>"
                },
                {
                    "lang": "bash",
                    "value": "appservices secrets delete --app=<Your App ID> --secret=some-api-key,609af850b78eca4a8db4303f"
                }
            ],
            "preview": "You cannot directly read the value of a Secret after defining it.\nThere are two ways to access a Secret that already exists in your app:",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "hosting/flush-the-cdn-cache",
            "title": "Flush the CDN Cache",
            "headings": [
                "Overview",
                "Procedure",
                "Navigate to the Hosted File Tree",
                "Select the Purge Cache Action",
                "Select the Attribute Type and Value",
                "Pull the Latest Version of Your App",
                "Deploy with the Reset CDN Cache Flag"
            ],
            "paragraphs": "Atlas App Services serves hosted files through a  Content Delivery Network\n(CDN)  in order to minimize the latency\nbetween receiving a request for a resource and returning the requested\nresource. When a client requests a resource that's hosted in App Services, the CDN server\nthat processes the request checks for a cached copy of the file. If the\nserver finds a valid cached copy of the resource, it returns it to the\nclient. Otherwise, it forwards the request to App Services and caches the\nreturned file before returning it to the client. The CDN caching process decreases the latency for end users when they\nrequest a resource but may cause users to receive an out-of-date\nversion of a resource if the file has changed since the CDN server\ncached it. You can  flush  the CDN cache in order to make it drop all\ncached files and start serving the latest version of each file. The CDN automatically refreshes cached files periodically. You can\nconfigure the caching behavior for an individual file by adding a a\n Cache-Control  attribute to the file. Click  Hosting  in the left navigation menu. You can purge the cache for all hosted files from the\n Files  tab. Click the  Actions  button above the list of files. Click  Purge Cache . In the  Purge Cache  modal, click  Confirm . The new metadata attribute will not have a type or value when you\nfirst add it. Select the attribute type from new attribute's dropdown menu. Enter a value for the attribute in the input box. Click  Save . To purge the CDN cache with the  App Services CLI , you need a local copy of your\napplication's configuration files. To pull a local copy of the latest version of your app, run the following: You can also download a copy of your application's configuration files from\nthe  Deploy > Import/Export App  screen in the App Services UI. To purge the CDN cache, re-deploy your application configuration with the\n --reset-cdn-cache  flag:",
            "code": [
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=\"<Your App ID>\" --reset-cdn-cache"
                }
            ],
            "preview": "Atlas App Services serves hosted files through a Content Delivery Network\n(CDN) in order to minimize the latency\nbetween receiving a request for a resource and returning the requested\nresource.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "hosting/configure-file-metadata",
            "title": "Configure File Metadata",
            "headings": [
                "Overview",
                "Procedure",
                "Navigate to the File in the File Tree",
                "Add a New Attribute",
                "Select the Attribute Type and Value",
                "Pull Your App's Latest Configuration Files",
                "Add Attributes to the Metadata Configuration File",
                "Import the File Metadata Configuration"
            ],
            "paragraphs": "You can configure a file's  metadata attributes  to describe aspects of the file,\nsuch as its content type, language, or encoding. You can also use\nmetadata attributes to configure the CDN's caching behavior and specify\nhow clients should handle files when they access them. If you do not specify a  Content-Type  metadata attribute for a hosted\nfile, Atlas App Services will attempt to automatically add a  Content-Type \nattribute to it based on the file extension. For example, App Services would automatically add the attribute\n Content-Type: application/html  to the file  myPage.html . To add a metadata attribute to a specific file you need to find the\nfile in the hosted file tree. To find the file: Click  Hosting  in the left navigation menu. Select the  Files  tab. Find the entry for the file in the file tree. Once you have found the file, you need to add a new metadata\nattribute to it. To add a new metadata attribute: Click the file's  Actions (...)  button. Click  Edit Attributes... In the  Set Attributes  modal, click  Add\nAttribute . The new metadata attribute will not have a type or value when you\nfirst add it. Select the attribute type from new attribute's left-hand dropdown Enter a value for the attribute in the right-hand input box. Click  Save . The metadata for all hosted files associated with your application is\nconfigured by an array of metadata attribute definition objects that\nyou define in a file named  metadata.json  in your application's  hosting \nsubdirectory. Atlas App Services will automatically infer and assign a  Content-Type  to every\nfile you upload. If you you want to override the default\n Content-Type  or specify additional  metadata attributes  for a file, add an entry to\n metadata.json  for the file and\nattributes that you want to configure. Each file's entry should be a\ndocument with the following form: Field Description path Required. The  resource path  of the\nfile. attrs Required. An array of documents where each document represents a\nsingle metadata attribute. Attribute documents have the following\nform: Field Description name The name of the metadata attribute. This should be one of\nthe  file metadata attributes  that App Services\nsupports. value The value of the metadata attribute. If you include an entry in  metadata.json  for a file that does\nnot exist, you will not be able to import your changes. Ensure that\nonly hosted files are represented in  metadata.json . Once you have added all the metadata attributes you want to configure\nto  metadata.json , you can import the application directory to\nactually update the associated file metadata. Navigate to the root of the application directory and run the\nfollowing command:",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices pull --remote=<App ID>"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    \"path\": \"<File Resource Path>\",\n    \"attrs\": [\n       ...,\n       <Attribute Definition>\n    ],\n  },\n  ...\n]"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Attribute Type>\",\n  \"value\": \"<Attribute Value>\"\n}"
                },
                {
                    "lang": "shell",
                    "value": "appservices push --include-hosting"
                }
            ],
            "preview": "You can configure a file's metadata attributes to describe aspects of the file,\nsuch as its content type, language, or encoding. You can also use\nmetadata attributes to configure the CDN's caching behavior and specify\nhow clients should handle files when they access them.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "hosting/upload-content-to-app-services",
            "title": "Upload Content to Atlas App Services",
            "headings": [
                "Overview",
                "Procedure",
                "Navigate to the Hosting Configuration Page",
                "Upload Files to Atlas App Services",
                "Pull Your App's Latest Configuration Files",
                "Add a Hosting Directory",
                "Add a Metadata Configuration File",
                "Add Files to the Hosting Directory",
                "Upload the Files"
            ],
            "paragraphs": "You can upload content to App Services from the  Hosting  screen of\nthe App Services UI or by  importing  an application directory that\nincludes the files. Select the tab below that corresponds to the method\nyou want to use. To open the hosting configuration page, click  Hosting  in\nthe left navigation menu of the App Services UI. You can upload files to App Services individually or select multiple files\nor file directories to upload simultaneously. All files that you\nupload must be available on your local filesystem. There are two ways to upload files through the App Services UI. You can\nuse either of the following methods: Once you have dropped in or selected your files, App Services will begin\nuploading them immediately. The rate of upload time will vary depending\non network connection speed and the number and size of the files\nselected. Drag and drop files from your local filesystem on to the\n Hosting  screen. Click  Upload Files  and select the files you wish to\nupload from a file picker. Before you start using the App Services CLI with Static Hosting, you must\n enable hosting in the App Services UI . App Services looks for files to upload in the  /hosting/files \nsubdirectory of your application directory. If this directory doesn't\nalready exist, create it: To deploy hosted files through the  App Services CLI  you must include a\n metadata.json  file in the  /hosting  directory. If the\nconfiguration file does not exist, create it: The configuration does not need to specify metadata for any files but\nmust be present. For more information, see  Configure File\nMetadata . Field Description path Required. The  resource path  of the\nfile. attrs Required. An array of documents where each document represents a\nsingle metadata attribute. Attribute documents have the following\nform: Field Description name The name of the metadata attribute. This should be one of\nthe  file metadata attributes  that App Services\nsupports. value The value of the metadata attribute. If you include an entry in  metadata.json  for a file that does\nnot exist, you will not be able to import your changes. The\nmetadata.json must only have hosted files represented. To upload or edit files, add them to the  /hosting/files \nsubdirectory of your application directory. When you import your\napplication, App Services will map this subdirectory to the root of your\napplication's hosted file tree. You can nest additional directories inside of the\n /hosting/files  subdirectory. Nested directories are listed\nbefore a file's name in its resource path. To upload files from the  /hosting/files  subdirectory, push your\nconfiguration directory with the  --include-hosting  flag: The upload speed will vary depending on your network connection and the\nnumber and size of the files that you upload.",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices pull --remote=<App ID>"
                },
                {
                    "lang": "shell",
                    "value": "mkdir -p hosting/files"
                },
                {
                    "lang": "shell",
                    "value": "touch hosting/metadata.json"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    \"path\": \"<File Resource Path>\",\n    \"attrs\": [\n       ...,\n       <Attribute Definition>\n    ],\n  },\n  ...\n]"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Attribute Type>\",\n  \"value\": \"<Attribute Value>\"\n}"
                },
                {
                    "lang": "shell",
                    "value": "appservices push --include-hosting"
                }
            ],
            "preview": "You can upload content to App Services from the Hosting screen of\nthe App Services UI or by importing an application directory that\nincludes the files. Select the tab below that corresponds to the method\nyou want to use.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "hosting/migrate-netlify",
            "title": "Migrate Static Hosting to Netlify",
            "headings": [
                "Before You Begin",
                "Netlify UI",
                "Import From a Git Repository",
                "Deploy Local Files",
                "Netlify CLI",
                "Set up the Netlify CLI",
                "Log in to Netlify",
                "Deploy manually with the CLI",
                "Setup continuous deployment",
                "Shut Down Atlas App Services Hosting",
                "Next Steps",
                "404 Page and Redirects",
                "Update DNS Records",
                "Learn More About How Netlify Manages CDN Caching",
                "Invite Your Team"
            ],
            "paragraphs": "Netlify is well-suited for hosting static websites and single-page applications\n(SPAs), and therefore will be compatible with your existing App Services\napplication. Netlify provides different methods to deploy your static websites depending on\nyour needs. Connecting Netlify's CI/CD to your Git repository is ideal, but you\ncan also deploy using a CLI, API, or even drag and drop. Below is an overview of\nmethods that may be most helpful for developers currently using MongoDB Atlas\nhosting. See Netlify's full documentation on how to  add a new site  to explore all options in\ndetail. Always refer to the official documentation of both MongoDB Atlas and Netlify\nfor the most up-to-date and accurate information. Specific steps may vary\ndepending on the details of your project and the technologies used. As a precaution, ensure that you have a backup of your application and data\nbefore making any significant changes. A git repository with your website code. Netlify's CI/CD integrates seamlessly\nwith many popular  Git providers .\nEven if you are not utilizing Netlify's CI/CD, it is highly recommended to use\na version control system. A Netlify account. Creating a new Netlify account is free and the signup flow\nshould optionally walk you through the following process. The free tier should\ncover your hosting requirements when migrating from MongoDB Atlas hosting. If\nyou need more services and features check out  Netlify's Pro and Enterprise\nplans . You can use the Netlify UI to create and manage projects. You can import an existing project from a Git repository and deploy it on\nNetlify. When you import your existing project repository, Netlify's continuous\ndeployment will automatically update your site each time you push changes. To\nset up using the Netlify CLI, refer to the Netlify CLI section on this page. To create a new site from a Git repo using the Netlify UI: Go to your Netlify team's 'Sites' page, open the 'Add new site' menu, and\nselect 'Import an existing project'. Select the Git provider where your project is hosted. Select your project's existing repository. Adjust site and build settings. If you have an existing project on your local machine that isn't linked to a Git\nrepository, you can manually deploy your site by using Netlify's deploy\ndropzone. Go to your team's 'Sites' page, open the 'Add new site' menu, and select\n'Deploy manually'. Drag and drop your site's output folder to the deploy\ndropzone to deploy your site. Netlify's command line interface (CLI) lets you configure  continuous deployment \nstraight from the command line. You can use Netlify CLI to  run a local\ndevelopment server  that you\ncan share with others,  run a local build and plugins , and  deploy\nyour site manually . The following instructions are abbreviated to get you started migratingfrom\nMongoDB Atlas hosting. For advanced configuration see Netlify's full\ndocumentation for getting started with the Netlify CLI. Install the Netlify CLI on your local machine. This allows you to deploy\nand manage your projects using the command line. Netlify CLI uses an access token to authenticate with Netlify. To\nauthenticate and obtain an access token using the command line, enter the\nfollowing command from any directory: This will open a browser window, asking you to log in with Netlify and\ngrant access to Netlify CLI. To deploy your application manually, without continuous deployment, run\nthe following command from your project directory: The first time you run the command, Netlify CLI will prompt you to\nselect an existing site or create a new one, linking the site for all\nfuture deploys. For repositories stored on GitHub.com, you can use Netlify CLI to connect\nyour repository by running the following command from your local\nrepository: Netlify CLI will need access to create a deploy key and a webhook on the\nrepository. When you run the command above, you'll be prompted to log in\nto your GitHub account, which will create an account-level access token.\nThe access token will be stored in the Netlify CLI config.json. Your login\npassword will never be stored. Once you have verified that your application deploys successfully to\nNetlify, delete your hosted files from your Atlas App Services app.\nAs a reminder, hosting domains on Atlas App Services will no longer run\nstarting on March 12, 2025. If you need additional support for Netlify services, reach out to  their support\nteam . You can set up a custom 404 page for all paths that don't resolve to a static\nfile. This doesn't require any redirect rules. If you add a 404.html page to\nyour site, it will be picked up and displayed automatically for any failed\npaths. Netlify's redirect rules accept a number of options to customize how the paths\nare matched and redirected using the _redirects file syntax ( see examples ). If your domain is currently pointed to MongoDB Atlas and you want to use the\nsame domain with Netlify, update your DNS records to point to Netlify. See\nNetlify's documentation on  configuring external DNS . Static asset responses on Netlify are cached on Netlify's global edge nodes and\nautomatically invalidated whenever a deploy changes the content. Static asset\nresponses can only change with new deploys. For more advanced control of caching\nsee Netlify's documentation on  global caching infrastructure . If you need to add someone to the team, you can set up role-based access\ncontrols that allow the Admin to take control and give access to individuals on\nthe team. See Netlify's documentation on  team management .",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install netlify-cli -g"
                },
                {
                    "lang": "shell",
                    "value": "netlify login"
                },
                {
                    "lang": "shell",
                    "value": "netlify deploy"
                },
                {
                    "lang": "shell",
                    "value": "netlify init"
                }
            ],
            "preview": "Learn how to migrate your static assets from Atlas App Services to Netlify.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "hosting/use-a-custom-domain-name",
            "title": "Use a Custom Domain Name",
            "headings": [
                "Overview",
                "Procedure",
                "Acquire the Custom Domain Name",
                "Specify the Custom Domain in Atlas App Services",
                "Add a Validation CNAME Record",
                "Add a Redirect CNAME Record",
                "Acquire the Custom Domain Name",
                "Pull the Latest Version of Your App",
                "Specify the Custom Domain",
                "Import the Application Directory",
                "Deploy the Updated Hosting Configuration",
                "Add a Validation CNAME Record",
                "Add a Redirect CNAME Record"
            ],
            "paragraphs": "You can use your own custom domain name for your hosted content. By\ndefault, content that you upload is available at a domain with the\nfollowing form: You can configure a custom domain name for your application's hosted\ncontent from the App Services UI or by  importing  an application configuration\ndirectory that specifies the domain in its  config.json  configuration\nfile. Select the tab below that corresponds to the method you want to\nuse. You must own the custom domain name that you want to serve content\nfrom. If you don't already own the domain that you want to use, you\nwill need to purchase it from a domain name registrar. You need to provide your custom domain name to App Services before it will\nserve content from the domain. To specify the domain: Click  Hosting  in the left navigation menu. Click the  Settings  tab. Under  Custom Domain , set the toggle to\n Enabled . Enter your custom domain in the input box under the\n Enabled  toggle. This value should be the root domain\nwithout any subdomains. For example, you should enter\n example.com  instead of  www.example.com . Click  Save . If an error is encountered during custom domain name validation,\nApp Services sends the project owner an email alerting them of the\nissue. To verify that you own the domain you must add a new  CNAME\nrecord  in your domain's DNS configuration. The\n Custom Domain  section will include an information box that\nlists the host name and target value to use in the record. Once you have added the validation record it may take some time for the\nDNS record to propagate. App Services will periodically check the domain's\nDNS records for the validation  CNAME  and will mark the domain as\nverified if it finds the record. In the  Name  field of the App Services UI, App Services provides\nthe full address for the CNAME record,  <Subdomain>.<Your Base Domain> .\nHowever, many hosting providers just request the  <Subdomain>  part\nto put in their UI. If you run into issues, check your hosting provider's\ndocumentation on how to add CNAME records. App Services may not be able to find the validation record if your DNS\nprovider proxies requests for the domain. If App Services cannot validate\nyour domain, ensure that you have disabled any HTTP proxies for the\nvalidation CNAME record on your DNS provider. If an error is encountered during custom domain name validation,\nApp Services sends the project owner an email alerting them of the\nissue. Once App Services has verified your domain, all that's left is to add a\n CNAME  DNS record for your domain that points to the default App Services\ndomain. Use the following host name and target value: Value Description Host Name A subdomain such as  www . Target The default domain for your application. This value is listed\nas the  App Services Domain  in the  Settings  tab\nof the  Hosting  page. The default domain has the\nfollowing form: You must own the custom domain name that you want to serve content\nfrom. If you don't already own the domain that you want to use, you\nwill need to purchase it from a domain name registrar. To configure a custom domain name with the  App Services CLI , you need a local copy of\nyour application's configuration files. To pull a local copy of the latest version of your app, run the following: You can also download a copy of your application's configuration files from\nthe  Deploy > Import/Export App  screen in the App Services UI. In  hosting/config.json , set  custom_domain  to your custom domain name\nthen save the file. The value of  custom_domain  should be the root domain without\nany subdomains. For example, you should enter  example.com \ninstead of  www.example.com . Once you have specified your custom domain name, you can import the\napplication directory. Navigate to the root of the application directory and run the\nfollowing command: Once you've updated and saved  hosting/config.json  you can push the updated\nconfig to your remote app. App Services CLI immediately deploys the configuration on\npush and App Services immediately starts trying to verify your domain name. To verify that you own the domain you must add a new  CNAME\nrecord  in your domain's DNS configuration. You can\nfind the host name and target value to use in the record\n Custom Domain  section of the  Hosting >\nSettings  tab in the App Services UI. Once you have added the validation record it may take some time for the\nDNS record to propagate. App Services will periodically check the domain's\nDNS records for the validation  CNAME  and will mark the domain as\nverified if it finds the record. In the  Name  field of the App Services UI, App Services provides\nthe full address for the CNAME record,  <Subdomain>.<Your Base Domain> .\nHowever, many hosting providers just request the  <Subdomain>  part\nto put in their UI. If you run into issues, check your hosting provider's\ndocumentation on how to add CNAME records. App Services may not be able to find the validation record if your DNS\nprovider proxies requests for the domain. If App Services cannot validate\nyour domain, ensure that you have disabled any HTTP proxies for the\nvalidation CNAME record on your DNS provider. If an error is encountered during custom domain name validation,\nApp Services sends the project owner an email alerting them of the\nissue. Once App Services has verified your domain, all that's left to do is to add one or\nmore  CNAME  DNS records for your domain that point to the default\nApp Services domain. Use the following host name and target value: Value Description Host Name A subdomain such as  www . Target The default domain for your application. This value is listed\nas the  App Services Domain  in the  Settings  tab\nof the  Hosting  page. The default domain has the\nfollowing form: If you have trouble verifying your domain: clear any unnecessary  CAA 's\nfrom your site's records add the following CAA records: Domain Record Type Flags Tag Value www.example.com CAA 0 issue \"amazon.com\" www.example.com CAA 0 issue \"amazontrust.com\" www.example.com CAA 0 issue \"awstrust.com\"",
            "code": [
                {
                    "lang": "none",
                    "value": "<Your App ID>.mongodbstitch.com"
                },
                {
                    "lang": "none",
                    "value": "<Your App ID>.mongodbstitch.com"
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "json",
                    "value": "{\n  \"enabled\": true,\n  \"custom_domain\": \"example.com\"\n}"
                },
                {
                    "lang": "shell",
                    "value": "appservices import --include-hosting"
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=\"<Your App ID>\" --include-hosting"
                },
                {
                    "lang": "none",
                    "value": "<Your App ID>.mongodbstitch.com"
                }
            ],
            "preview": "You can use your own custom domain name for your hosted content. By\ndefault, content that you upload is available at a domain with the\nfollowing form:",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "hosting/file-metadata-attributes",
            "title": "File Metadata Attributes",
            "headings": [
                "Overview",
                "Content-Type",
                "Examples",
                "Indicate that a File Contains HTML",
                "Content-Disposition",
                "Examples",
                "Display the File Inline",
                "Download the File",
                "Download the File with a Custom Filename",
                "Content-Encoding",
                "Examples",
                "Indicate No Encoding",
                "Indicate GZIP Encoding",
                "Indicate Multiple Encodings in Application Order",
                "Content-Language",
                "Examples",
                "Specify a Single Language",
                "Specify Multiple Languages",
                "Cache-Control",
                "Examples",
                "Refresh a Cached File Every Five Minutes",
                "Never Cache a File",
                "Website-Redirect-Location",
                "Examples",
                "Redirect Requests to a Different File"
            ],
            "paragraphs": "You can define metadata attributes for each file that you host with\nAtlas App Services. Metadata attributes map to standard  HTTP headers  and allow you to optionally configure how App Services\nserves your files as well as how clients that request resources should\nhandle them. This page provides examples and describes the purpose of\neach available attribute. The  Content-Type  file attribute\nindicates the  media type  of the file. If you do not specify a  Content-Type  attribute for a file, App Services\nwill attempt to automatically add a  Content-Type  attribute to it\nbased on the file extension. For example, App Services would automatically add the attribute\n Content-Type: application/html  to the file  myPage.html . The  Content-Disposition \nfile attribute indicates to client applications (such as your web\nbrowser) whether the file should be downloaded as an attachment or\ndisplayed inline as a web page. The  Content-Encoding  file\nattribute indicates any encodings that were applied to the file. Client\napplications can use this header to determine how to properly decode the\nfile. The  Content-Language  file\nattribute optionally specifies the language used by the file's intended\ntarget audience. This attribute does not necessarily represent the\nlanguage that file is actually written in. The  Cache-Control  file attribute\ninstructs CDN servers on how they should handle cached copies of the\nfile. The  Website-Redirect-Location  file redirects requests to the\nspecified destination.",
            "code": [
                {
                    "lang": "none",
                    "value": "Content-Type: application/html"
                },
                {
                    "lang": "none",
                    "value": "Content-Disposition: inline"
                },
                {
                    "lang": "none",
                    "value": "Content-Disposition: attachment"
                },
                {
                    "lang": "none",
                    "value": "Content-Disposition: attachment; filename=\"myFile.txt\""
                },
                {
                    "lang": "none",
                    "value": "Content-Encoding: identity"
                },
                {
                    "lang": "none",
                    "value": "Content-Encoding: gzip"
                },
                {
                    "lang": "none",
                    "value": "Content-Encoding: gzip, identity"
                },
                {
                    "lang": "none",
                    "value": "Content-Language: en-US"
                },
                {
                    "lang": "none",
                    "value": "Content-Language: en-US, en-CA, en-UK"
                },
                {
                    "lang": "none",
                    "value": "Cache-Control: max-age=300"
                },
                {
                    "lang": "none",
                    "value": "Cache-Control: no-cache"
                },
                {
                    "lang": "none",
                    "value": "Website-Redirect-Location: https://example.com/file/redirectedFile.txt"
                }
            ],
            "preview": "You can define metadata attributes for each file that you host with\nAtlas App Services. Metadata attributes map to standard HTTP headers and allow you to optionally configure how App Services\nserves your files as well as how clients that request resources should\nhandle them. This page provides examples and describes the purpose of\neach available attribute.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "hosting/migrate-blob",
            "title": "Migrate to Your Own S3 Bucket",
            "headings": [
                "Migrate to S3",
                "Create an S3 Bucket",
                "Migrate Existing Data",
                "Update Application Code",
                "Shut Down Atlas App Services Hosting",
                "Access S3 Bucket from Atlas Functions",
                "Integrate with Atlas Functions",
                "Write Functions to Access an S3 Bucket",
                "Authenticate AWS Requests",
                "Fetch an Asset and Upload to S3",
                "Upload Local Asset to S3"
            ],
            "paragraphs": "Always refer to the official documentation of both MongoDB Atlas and AWS\nfor the most up-to-date and accurate information. The specific steps may vary\ndepending on the details of your project and the technologies\nused. If you are solely using the MongoDB Atlas static hosting service as a blob store\nfor static content and are not hosting a client application, follow the steps\nbelow to migrate from using Atlas Hosting to using your own S3 bucket. Set up your own S3 bucket in your AWS account if you haven't already done\nso. Configure the bucket settings according to your requirements,\nincluding access permissions and encryption options. Migrate your existing data from MongoDB Atlas Hosting to your S3 bucket.\nDepending on the amount of data and your specific requirements, this could\ninvolve exporting data from MongoDB with the  App Services CLI  ( appservices pull --include-hosting ) and then\nuploading the data to S3 using tools like the AWS CLI or AWS SDK. Modify your application code to interact with the S3 bucket instead of\nMongoDB Atlas Hosting. This includes updating code that handles file\nuploads or downloads, if you are using the App Services CLI commands to\ndo so. Once you have verified that your files deploy successfully to your S3\nbucket, delete your hosted files from your Atlas App Services app.\nAs a reminder, hosting domains on Atlas App Services will no longer run\nstarting on March 12, 2025. You can access static content stored in your S3 bucket from\n Atlas Functions . To do so, you need to: Integrate with Atlas Functions Write Functions to access an S3 bucket Test your Atlas Functions to ensure they can successfully access and\nmanipulate objects in the S3 bucket. Validate that the functions behave as\nexpected and handle errors gracefully. Upload to S3 using the  @aws-sdk   external dependency . Atlas App Services automatically transpiles\ndependencies and also supports most default Node.js modules. To import and use an external dependency, you first need to add the dependency\nto your application. You can either  add packages by name  or  upload a directory of dependencies . App Services does not yet support version 3 of the AWS SDK. Use the version 2\nSDK when specifying the npm module Write MongoDB Atlas Functions that interact with your S3 bucket. These functions\ncan perform various operations such as uploading files, downloading files,\nlisting objects, and deleting objects from the S3 bucket. We'll cover some basic examples in this guide. You may want to explore other\nways to interact with your S3 bucket by viewing the full\n list of client commands \nfor the @aws-sdk/client-s3. To authenticate AWS requests, store your Access Key ID and Secret Access Key as\n values . You can then access them within functions\nand pass them to the SDK. Fetch static assets and upload them to S3 either by downloading static\nassets from the URL or uploading them as local files. To upload downloaded content to S3, use an HTTP library or built-in Node.js\nmodules like  http  or  https  to download the static asset from the URL.\nYou can then upload the downloaded content to S3. Here's an example of downloading an asset using the  axios  library: To upload a local asset to S3, use the following code snippet:",
            "code": [
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n   // require calls must be in exports function\n   const { S3Client, PutObjectCommand, GetObjectCommand } = require(\"@aws-sdk/client-s3\");\n\n   const s3Client = new S3Client({\n      region: \"us-east-1\", // replace with your AWS region\n      credentials: {\n         accessKeyId: context.values.get(\"awsAccessKeyId\"),\n         secretAccessKey: context.values.get(\"awsSecretAccessKey\"),\n      },\n   });\n\n   const putCommand = new PutObjectCommand({\n      Bucket: \"bucketName\",\n      Key: \"keyName\",\n      Body: EJSON.stringify({ hello: \"world\" }),\n   });\n   const putResult = await s3Client.send(putCommand);\n\n   const getCommand = new GetObjectCommand({\n      Bucket: \"bucketName\",\n      Key: \"keyName\",\n   });\n\n   const getResult = await s3Client.send(getCommand);\n}"
                },
                {
                    "lang": "javascript",
                    "value": "const axios = require('axios');\nconst stream = require('stream');\nconst { promisify } = require('util');\n\n// Promisify pipeline function to pipe streams\nconst pipeline = promisify(stream.pipeline);\n\nasync function uploadAssetToS3() {\n   try {\n      const response = await axios.get('URL_OF_YOUR_STATIC_ASSET', { responseType: 'stream' });\n\n      const uploadParams = {\n         Bucket: 'YOUR_BUCKET_NAME',\n         Key: 'YOUR_OBJECT_KEY',\n         Body: response.data\n      };\n\n      // Upload the static asset to S3\n      await s3.upload(uploadParams).promise();\n\n      console.log('Static asset uploaded successfully');\n   } catch (error) {\n      console.error('Error uploading static asset:', error);\n   }\n}\n\nuploadAssetToS3();"
                },
                {
                    "lang": "javascript",
                    "value": "const uploadParams = {\n   Bucket: 'YOUR_BUCKET_NAME',\n   // Specify the name/key for the object in the bucket (usually the file name)\n   Key: 'YOUR_OBJECT_KEY',\n   // Provide the local file to be uploaded\n   Body: 'STATIC_ASSET_CONTENT',\n};\n\n// Upload the static asset to S3\ns3.upload(uploadParams, (err, data) => {\n   if (err) {\n      console.error('Error uploading static asset:', err);\n   } else {\n      console.log('Static asset uploaded successfully:', data.Location);\n   }\n});"
                }
            ],
            "preview": "Learn how to migrate your blob assets from Atlas App Services to your own AWS S3 bucket.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "hosting/host-a-single-page-application",
            "title": "Host a Single-Page Application",
            "headings": [
                "Overview",
                "Procedure",
                "Upload Your Built Application to App Services",
                "Configure App Services to Serve Your Application",
                "Pull the Latest Version of Your App",
                "Add Your Built Application Code",
                "Configure App Services to Serve Your Application",
                "Deploy the Updated Hosting Configuration"
            ],
            "paragraphs": "Many web applications built with modern frameworks, like React, Vue, and\nAngular, are single-page applications (SPAs) that dynamically handle\nrouting and rendering client-side instead of fetching each rendered page\nfrom the server. You can use Atlas App Services to host your SPA and serve it to\nclients. To host your app, you need to specify that it's a SPA in App Services. By\ndefault, App Services handles requests for a given resource by returning the\nfile hosted at the specified resource path or a 404 if no file matches\nthe path. However, SPAs render in a single, specific HTML file so all\nrequests should return that file regardless of the requested resource\npath. This guide covers how you can configure App Services Hosting to redirect all\nresource requests to a single file to support the SPA pattern. When  single-page application hosting  is enabled, App Services always\nreturns an HTTP 200 response with the app root regardless of the\nrequested route. This means that you cannot  specify a custom\n404 page  for a SPA. Instead, you\nshould include custom code in your application to handle invalid\nroutes. Single-page applications render in a single, specific HTML file,\ntypically  /index.html . The file should include the necessary\nJavaScript code to wire up and render your application, either inline\nin a  <script>  tag or imported from an external file. You'll also\nneed to host any resources that you don't intend to access through a\nCDN. When you are ready to host your SPA, run your application's build\nscript and then  upload the build folder  to App Services. Once you've started hosting your application files, you can\nimmediately access your SPA by requesting the root HTML file directly.\nHowever, a request to any path other than the root file will return a\n404. This can break the expected behavior of a SPA that uses a\nclient-side router or otherwise relies on the URL path. To configure App Services to serve the SPA's root page for all requests: Navigate to the  Hosting  page in the App Services UI and then\nclick the  Settings  tab. Ensure that you have not specified a custom 404 page. If custom 404\nis enabled, click the trash can icon ( ) next\nto the specified 404 page. Next to  Single Page Application , click  Choose\nFile . Choose the root HTML file for your SPA and then click\n Select . Click  Save . Before you start using the App Services CLI with Static Hosting, you must\n enable hosting in the App Services UI . To configure a single-page application with the  App Services CLI , you need a local copy\nof your application's configuration files. To pull a local copy of the latest version of your app, run the following: You can also download a copy of your application's configuration files from\nthe  Deploy > Import/Export App  screen in the App Services UI. Single-page applications render in a single, specific HTML file,\ntypically  /index.html . The file should include the necessary\nJavaScript code to wire up and render your application, either inline\nin a  <script>  tag or imported from an external file. You'll also\nneed to host any resources that you don't intend to access through a\nCDN. When you are ready to host your SPA, run your application's build\nscript and then copy the output build folder into the\n /hosting/files  directory of your application directory. In  hosting/config.json , set  default_response_code  to  200  and set\n default_error_path  to the  resource path  of\nyour SPA's root HTML file. Make sure to save the file when you're done. Once you've updated and saved  hosting/config.json  you can push the updated\nconfig to your remote app. App Services CLI immediately supports your SPA on push.",
            "code": [
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "json",
                    "value": "{\n  \"enabled\": true,\n  \"default_response_code\": 200,\n  \"default_error_path\": \"/index.html\",\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=\"<Your App ID>\" --include-hosting"
                }
            ],
            "preview": "Many web applications built with modern frameworks, like React, Vue, and\nAngular, are single-page applications (SPAs) that dynamically handle\nrouting and rendering client-side instead of fetching each rendered page\nfrom the server. You can use Atlas App Services to host your SPA and serve it to\nclients.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "hosting/use-a-custom-404-page",
            "title": "Use a Custom 404 Page",
            "headings": [
                "Overview",
                "Procedure",
                "Create a Custom 404 Page File",
                "Host the 404 File in App Services",
                "Configure the 404 Resource Path in App Services",
                "Pull the Latest Version of Your App",
                "Create a Custom 404 Page HTML File",
                "Host the HTML File in App Services",
                "Specify the 404 Page in the Application Configuration",
                "Deploy the Updated Hosting Configuration"
            ],
            "paragraphs": "You can replace the default  404 page  that Atlas App Services displays when a user tries to\naccess a resource that does not exist with a custom HTML document. You\ncan specify this custom 404 page from the App Services UI or by  importing  an application configuration\ndirectory that includes the page in its  hosting  directory. Select\nthe tab below that corresponds to the method you want to use. When  single-page application hosting  is enabled, App Services always\nreturns an HTTP 200 response with the app root regardless of the\nrequested route. This means that you cannot  specify a custom\n404 page  for a SPA. Instead, you\nshould include custom code in your application to handle invalid\nroutes. You can use any hosted file as your custom 404 page. It's common to\nuse an HTML file that incorporates the following elements: A short message indicating the error, e.g. \"This page does not\nexist.\" Alternative links or options for the user to continue navigating. Once you've created the custom 404 file, you need to  upload it\nto App Services . To upload the file,\nnavigate to the  Hosting  screen of the App Services UI, click\n Upload Files , and select the custom HTML file. Once you have successfully uploaded the 404 file, all that's left is\nto configure App Services to serve that file instead of the default 404\npage. Once you have updated the  Hosting  configuration, App Services\nwill begin serving your custom file instead of the default 404 page. On the  Hosting  page, click the  Settings \ntab. Under  Custom 404 Page , click  Choose File . Select your custom 404 file from the list. Click  Save . To configure a custom 404 page with the  App Services CLI , you need a local copy of your\napplication's configuration files. To pull a local copy of the latest version of your app, run the following: You can also download a copy of your application's configuration files from\nthe  Deploy > Import/Export App  screen in the App Services UI. You can use any valid HTML file for your application's 404 page. Consider\nincorporating the following elements: A short message indicating the error, e.g. \"This page does not\nexist.\" Alternative links or options for the user to continue navigating. Once you've created the custom HTML file, you need to  host it in\nApp Services . If you aren't already hosting the HTML file in App Services, add it to\nthe  hosting/files/  directory. In  hosting/config.json , set  default_error_path  to the  resource\npath  of the 404 page HTML file then save the\nconfiguration file. Once you've updated  hosting/config.json  you can push the updated config to\nyour remote app. If you also added the 404 page, make sure to use the\n --include-hosting  flag. App Services CLI immediately starts to serve your custom\n404 page on push.",
            "code": [
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "json",
                    "value": "\"hosting\": {\n  \"enabled\": true,\n  \"default_error_path\": \"/pages/custom_404.html\"\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=\"<Your App ID>\" --include-hosting"
                }
            ],
            "preview": "You can replace the default 404 page that Atlas App Services displays when a user tries to\naccess a resource that does not exist with a custom HTML document. You\ncan specify this custom 404 page from the App Services UI or by importing an application configuration\ndirectory that includes the page in its hosting directory. Select\nthe tab below that corresponds to the method you want to use.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "security/developer-access",
            "title": "Manage Developer Access",
            "headings": [
                "Allow Non-Owner Access"
            ],
            "paragraphs": "To create, update, or delete an App, you must have access to the MongoDB\nAtlas project that contains the App. App Services determines developer\naccess permissions for a given MongoDB Cloud user based on their\nassigned  project roles . The following table describes the access permissions associated with a\ngiven project role: Project Role Access Permissions Project Owner Full read-write access for all Apps associated with the project. All Other Roles Read-only access for all Apps associated with the project. For more information about adding users and teams to Atlas, see\n Atlas Users and Teams . You cannot directly modify an App without the Project Owner role in your\nAtlas project. However, this role includes broad permissions that may be\ntoo permissive for your organization. To allow non-owners to modify an App, you can proxy the changes through\na source control system or CI/CD pipeline with more fine-grained\npermissions. For example, you could enable  Automatic GitHub\nDeployment  and have developers push changes to the\nrepository instead of directly to the App.",
            "code": [],
            "preview": "To create, update, or delete an App, you must have access to the MongoDB\nAtlas project that contains the App. App Services determines developer\naccess permissions for a given MongoDB Cloud user based on their\nassigned project roles.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "edge-server/mongodb",
            "title": "Edge Server MongoDB API Support",
            "headings": [
                "CRUD APIs",
                "Read",
                "Insert",
                "Update",
                "Delete"
            ],
            "paragraphs": "Atlas Edge Server natively implements a subset of the MongoDB wire protocol,\nwhich allows you to access your Atlas data on the Edge using standard\nMongoDB drivers and tools. Clients use a specialized MongoDB URI connection string to connect and\nsend requests. For more information about how to connect to the Edge Server\nusing a MongoDB connection string, refer to  Connect to the Edge Server with a MongoDB URI . Edge Server currently supports a subset of the MongoDB APIs over the\nwire protocol. Edge Server currently supports using these MongoDB CRUD APIs over the\nwire protocol. For examples of using these commands, refer to the documentation for the\nDriver or tool you're using to execute these commands. Read operations take a query filter and find all documents in a collection\nthat match the filter. You can find a single document using the  collection.findOne()  method. You can find multiple documents using the  collection.find()  method. Insert operations take one or more documents and add them to a MongoDB\ncollection. You can insert a document or documents into a collection using the\n collection.insert()  method. You can insert a single document using the\n collection.insertOne()  method. You can insert multiple documents at the same time using the\n collection.insertMany()  method. Update operations find existing documents in a MongoDB collection and\nmodify their data. You use standard MongoDB query syntax to specify\nwhich documents to update and  update operators  to describe the changes to apply to\nmatching documents. You can update a document or documents in a collection using the\n collection.update()  method. You can update a single document using the\n collection.updateOne()  method. You can update multiple documents in a collection using the\n collection.updateMany()  method. Edge Server does not currently support  upsert . Delete operations find existing documents in a MongoDB collection and\nremove them. You use standard MongoDB query syntax to specify which\ndocuments to delete. You can delete a single document from a collection using the\n collection.deleteOne()  method. You can delete multiple items from a collection using the\n collection.deleteMany()  method.",
            "code": [],
            "preview": "Atlas Edge Server natively implements a subset of the MongoDB wire protocol,\nwhich allows you to access your Atlas data on the Edge using standard\nMongoDB drivers and tools.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "edge-server/connect",
            "title": "Connect to an Edge Server",
            "headings": [
                "Connect to the Edge Server from the Device SDK",
                "Example: SwiftUI Template App",
                "Configure the Server Connection",
                "Adjust the App Transport Security Settings (Optional)",
                "Adjust the Log Level (Optional)",
                "Run the App",
                "Connect to the Edge Server with a MongoDB URI"
            ],
            "paragraphs": "Once the Edge Server is  configured  and\n running , you can connect to it from\na client application. There are two ways you can connect to the Edge Server from a client: When you connect to Edge Server using a Device SDK, Device Sync automatically\nsynchronizes data across devices and with the Edge Server. If you connect\nto Edge Server using MongoDB Drivers or tools, you can perform CRUD operations\nwith a subset of supported MongoDB APIs. Using the Device SDKs Using MongoDB Drivers and tools To connect to the Edge Server from a Device SDK client, your app must: For information about customizing the App configuration, refer to the\ndocumentation for your preferred SDK: Set the Sync URL on the App configuration to the public-accessible DNS\naddress you set in the Edge Server  edge_config.json   baseURL \nfield. If TLS is not enabled, use HTTP over port 80. While your Edge Server deployment is in development, we recommend connecting\nwith HTTP over port 80 for simplicity. Before moving to production,\nyou can coordinate with your Product or Account Representative to configure\nHTTPS with a self-signed certificate. C++ SDK:  Connect to App Services - C++ SDK Flutter SDK:  Connect to App Services - Flutter SDK Java SDK:  Connect to an Atlas App Services backend - Java SDK Kotlin SDK:  Connect to Atlas App Services - Kotlin SDK .NET SDK:  Connect to an Atlas App Services Backend - .NET SDK Node.js SDK:  Connect to an Atlas App Services Backend - Node.js SDK React Native SDK:  Connect to an Atlas App Services App - React Native SDK Swift SDK:  Connect to an Atlas App Services Backend - Swift SDK For a quick proof of concept using a template app, these are the\nmodifications you can make to the  Swift template app  to connect to the Edge Server: In the Swift template app's  atlasConfig , set the values to: Key Value baseUrl The public-accessible DNS of your Edge Server's host.\nThis is  http:// , or if TLS is enabled,  https:// ,\nfollowed by the same value you set in your Edge Server\n edge_config.json   baseURL  field. appId The App ID of your App Services App that has Edge Server\nenabled. For more information about finding the App ID, refer\nto  Find Your App ID . If your app uses TLS, you can disregard this step. If you are developing\nan Edge Server app with TLS not enabled during development, you must\ncomplete this step. For iOS, Apple disables insecure connections by default. You must add\nan exception for your public-accessible DNS to allow the client app\nto connect to it. Select your App target, and go to the  Signing & Capabilities \ntab. Press the  + Capability  button, and search for\n App Transport Security Settings . Add this to your app. Add an exception domain for your public-accessible DNS. This should\nbe just the string domain, similar to:  ec2-13-58-70-88.us-east-2.compute.amazonaws.com . This creates an  Info.plist  file in your project. Go to this file\nto make the following adjustments. Change this  String  key to a  Dictionary . Add two more keys for: This enables your iOS client to connect to the insecure HTTP DNS. Your  Info.plist  file should look similar to: Key Type Value NSIncludesSubdomains Boolean YES NSExceptionAllowsInsecureHTTPLoads Boolean YES You can adjust the log level if you'd like to get additional details\nabout the status of communication with the Edge Server. In the  App  file, you can add an  .onAppear  block to set\nthe log level. Add a log level line in this block to get additional\ninformation: For more information about setting the log level, refer to\n Logging - Swift SDK . Now you can build and run the app. When you log in, the app connects\nto the Edge Server. When you create new Items, those sync with the\nEdge Server. The Edge Server then syncs with the Atlas App Services App. You can check the status of the Edge Server from your host.\nWith the client running, you can see  \"num_local_clients\": 1  in\nthe Edge Server status. You can also see error messages reflected\nin the sync status. After the next time the Edge Server syncs with your App Services App,\nyou can see synced changes reflected in the linked Atlas collection. If you have previously connected directly from the client to your\nApp Services App with Device Sync, you may need to clear state on the\nsimulator or device. Resetting your cache enables your client to\nconnect to the Edge Server. In a console app, delete the  mongodb-realm  directory. If you're\nusing an iOS or Android simulator or device, uninstall and reinstall\nthe app. You can connect to the Edge Server using standard MongoDB Drivers and tools.\nClients use a specialized MongoDB URI connection string to connect to\nEdge Server and send requests. The wire protocol server is on by default. You can connect to it via the\nhost machine using this MongoDB URI: The  EDGE_SERVER_HOSTNAME  is the public DNS of your Edge Server host.\nFor more information, refer to  Install and Configure the Edge Server . You must make port 27021 accessible to the client through port mapping or\nfirewall rules. Edge Server currently supports a subset of the MongoDB APIs. For information\nabout the supported APIs, refer to  Edge Server MongoDB API Support . A wire protocol connection URI for an Edge Server hosted on Amazon EC2\nmight look similar to:",
            "code": [
                {
                    "lang": "swift",
                    "value": "var body: some Scene {\n   WindowGroup {\n      ContentView(app: app)\n         .environmentObject(errorHandler)\n         .alert(Text(\"Error\"), isPresented: .constant(errorHandler.error != nil)) {\n            Button(\"OK\", role: .cancel) { errorHandler.error = nil }\n         } message: {\n            Text(errorHandler.error?.localizedDescription ?? \"\")\n         }\n         .onAppear {\n            Logger.shared.level = .trace\n         }\n   }\n}"
                },
                {
                    "lang": "console",
                    "value": "mongodb://EDGE_SERVER_HOSTNAME:27021"
                },
                {
                    "lang": "console",
                    "value": "mongodb://ec2-13-58-70-88.us-east-2.compute.amazonaws.com:27021"
                }
            ],
            "preview": "Connect to a MongoDB Edge Server from Atlas Device SDK or a MongoDB Driver.",
            "tags": "code example",
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "security/private-endpoints",
            "title": "Use a VPC Private Endpoint (Preview)",
            "headings": [
                "Private Endpoint Limitations",
                "Set Up a Private Endpoint",
                "Create an Interface Endpoint in Your VPC",
                "Add the Private Endpoint to Your App",
                "Modify Your VPC Private DNS",
                "Create an Interface Endpoint in Your VPC",
                "Add the Private Endpoint to Your App",
                "Modify Your VPC Private DNS",
                "Change Access Restrictions"
            ],
            "paragraphs": "You can use a Private Endpoint to access Atlas App Services from within\nyour Virtual Private Cloud (VPC). This is a private connection to Atlas\nApp Services that does not go over the public internet. Private Endpoints are only available for Atlas App Services Apps\ndeployed locally to a region in AWS. They connect to your AWS VPC using\nAWS PrivateLink. To learn more, see  What is AWS PrivateLink? \nin the AWS documentation. You cannot use Private Endpoints with globally deployed Apps or local\nApps deployed to Azure or GCP. You cannot use public internet connections to access your App if you\nhave a Private Endpoint enabled. All requests must come through the\nPrivate Endpoint. If you  change your deployment model \nthen your existing Private Endpoints will not continue to work. You\nwill have to create new Private Endpoints for the App's new region. You cannot simultaneously use Private Endpoints and  allow\nconnections over the MongoDB wire protocol . If one is\nenabled, you cannot enable the other. By default, private Endpoints only handle requests that originate from within your VPC.\nYou can configure your App to also accept requests from the public internet. To learn how,\nsee  Change Access Restrictions . All  outbound  requests, such as an an App Services trigger calling an\nexternal API, go over the public internet. To configure a Private Endpoint connection you need to have the\nfollowing information: Once you have this information, you can set up the Private Endpoint\nconnection using either the App Services UI or the Admin API. Your  VPC ID , which you can find in the AWS VPC dashboard or\nby running the  describe-vpcs  command in the AWS\nCLI. Your  Subnet IDs , which you can find in the AWS VPC dashboard\nor by running the  describe-subnets  command in the AWS\nCLI. The App Services  Endpoint Service Name  for your region. If\nyou're using the App Services UI, this value will be provided to\nyou. If you're using the Admin API, you can find this value by\ncalling the  List VPC Private Endpoint Services  endpoint and\nthen locating the entry for your App's deployment region in the\nresponse. To use a Private Endpoint, you must first create a VPC\ninterface endpoint in AWS. You can do this using the\n create-vpc-endpoint  command in the\nAWS CLI. The UI will walk you through creating this command. Once you've entered this information, the fully constructed\nAWS CLI command is displayed in the UI. Copy the command and\nrun it in your terminal. It should resemble the following: After you run the command, click  Next . In the left navigation, click  App Settings . Click the  Private Endpoints  tab. Click the  Add Private Endpoint  button. Enter your  VPC ID  in the input box. Enter your  Subnet IDs  as a comma-separated list in the input box. Now that you've created the interface endpoint in your VPC, you\nneed to add it to your App. Find the  VPC Endpoint ID , either\nin the output of the CLI command you ran in the previous step or\non the endpoint details screen of the AWS VPC dashboard. This\nvalue starts with  vpce- . Once you have the  VPC Endpoint ID , enter it in the input box. Optionally, enter a comment that describes the purpose of the\nendpoint. Then click  Next . Use the AWS CLI to modify your VPC private DNS settings. This\nensures that your App can resolve the private endpoint's DNS name\nto the correct IP address. If this is the first Private Endpoint you are creating, you will\nsee a toggle you can use to either restrict or open all access from\nrequests outside your PrivateLink connection. By default, App\nServices blocks all requests coming from outside your PrivateLink\nconnection. This setting applies to this endpoint and  all\nadditional endpoints you create . Refer to  change access\nrestrictions  for more information. Click  Finish Endpoint Creation  to create the endpoint. To use a Private Endpoint, you must first create a VPC\ninterface endpoint in AWS. You can do this using the\n create-vpc-endpoint  command in the\nAWS CLI. Modify the following command template with your private\nendpoint information. Then, copy the command and run it in\nyour terminal: Now that you've created the interface endpoint in your VPC, you\nneed to add it to your App. Find the  VPC Endpoint ID , either\nin the output of the CLI command you ran in the previous step or\non the endpoint details screen of the AWS VPC dashboard. This\nvalue starts with  vpce- . Once you have the  VPC Endpoint ID , call\n Create a VPC Private Endpoint  with the Endpoint\nID and cloud deployment region (e.g.  aws-us-east-1 ): Use the AWS CLI to modify your VPC private DNS settings. This\nensures that your App can resolve the private endpoint's DNS name\nto the correct IP address. By default, all private Endpoints only handle internal requests. You can choose\nto open access from all requests outside your PrivateLink connection. You can toggle this setting in three ways: This setting is global to all of your endpoints. In the UI, from the list of endpoints on the  Private Endpoints \ntab, click the  Restrict Access  button. In the UI, when creating your first Private Endpoint, you have the option to\ntoggle this setting in the  Modify Your VPC Private DNS  step. In the Admin API, call the  Toggle Non-VPC Requests  with a boolean value ( true  to remove\nthe default restriction, or  false  to reinstate it.)",
            "code": [
                {
                    "lang": "sh",
                    "value": "curl https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/security/private_endpoint_service_infos \\\n  -H \"Authorization: Bearer <access_token>\""
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    \"cloud_provider_region\": \"aws-us-east-1\",\n    \"name\": \"eps_baas-pl-prod_us-east-1_cloud\",\n    \"dns\": \"*.aws.services.cloud.mongodb.com\",\n    \"service_name\": \"com.amazonaws.vpce.us-east-1.vpce-svc-0f12ab34cd56ef789\"\n  },\n  ...\n]"
                },
                {
                    "lang": "sh",
                    "value": "aws ec2 create-vpc-endpoint \\\n  --vpc-endpoint-type Interface\n  --service-name <App Services Endpoint Service Name> \\\n  --vpc-id <your-vpc-id> \\\n  --region <your-aws-deployment-region> \\\n  --subnet-ids <your-subnet-ids>"
                },
                {
                    "lang": "sh",
                    "value": "aws ec2 modify-vpc-endpoint \\\n  --region <your-aws-deployment-region> \\\n  --vpc-endpoint-id <your-vpc-endpoint-id> \\\n  --private-dns-enabled"
                },
                {
                    "lang": "sh",
                    "value": "aws ec2 create-vpc-endpoint \\\n  --vpc-endpoint-type Interface\n  --service-name <App Services Endpoint Service Name> \\\n  --vpc-id <your-vpc-id> \\\n  --region <your-aws-deployment-region> \\\n  --subnet-ids <your-subnet-ids>"
                },
                {
                    "lang": "sh",
                    "value": "curl https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/security/private_endpoints \\\n  -X POST \\\n  -H \"Authorization: Bearer <accessToken>\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"cloud_provider_region\": \"<your-cloud-deployment-region>\",\n    \"cloud_provider_endpoint_id\": \"<your-vpc-endpoint-id>\"\n  }'"
                },
                {
                    "lang": "sh",
                    "value": "aws ec2 modify-vpc-endpoint \\\n  --region <your-aws-deployment-region> \\\n  --vpc-endpoint-id <your-vpc-endpoint-id> \\\n  --private-dns-enabled"
                }
            ],
            "preview": "You can use a Private Endpoint to access Atlas App Services from within\nyour Virtual Private Cloud (VPC). This is a private connection to Atlas\nApp Services that does not go over the public internet.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "edge-server/configure",
            "title": "Configure Edge Server",
            "headings": [
                "Prerequisites",
                "Create an App Services App",
                "Enable Device Sync",
                "Configure Data Access Permissions",
                "Configure an Authentication Provider",
                "Get Edge Server Enabled",
                "Edge Server Host Requirements",
                "Required Network Access",
                "OS Compatibility",
                "Hardware Requirements",
                "Install and Configure the Edge Server",
                "Get the Edge Server Code",
                "Complete the Edge Server Configuration Details",
                "Install Dependencies",
                "Specify Additional Configuration Details (Optional)",
                "Start and Stop the Edge Server",
                "Check the Edge Server Status",
                "Upgrade the Edge Server Version",
                "Prerequisites",
                "Upgrade Edge Server Commands",
                "Upgrade Edge Server Procedure",
                "Run the upgrade command",
                "Start the Edge Server",
                "(Optional) Reset the Edge Server"
            ],
            "paragraphs": "Edge Server is a \"local\" server that sits between your client devices and\nMongoDB Atlas App Services. For most use cases, you'll provision hardware\non-premises to host the Edge Server. While you're developing your Edge\nServer application, you may host it on a cloud server for convenience. Client devices on-prem connect to Edge Server. Your Edge Server connects\nto App Services. Clients do not connect to App Services directly. This page details how to configure and start the Edge Server. Edge Server connects to a Device Sync-enabled App Services App. Before\nyou can use Edge Server, you must create an App Services App, enable\nDevice Sync, and configure an authentication provider and permissions\nfor your App. Create an  App Services App using the UI, CLI, or Admin API . You can use an Edge Server with any Flexible Sync-enabled\nApp Services App. For a quick proof-of-concept, you can\n create an app from a template .\nAny of the starter templates whose name includes\n Atlas Device Sync Starter  can quick-start your\nEdge Server proof-of-concept. If you create a template app from\nthe Atlas UI, choose  Real-time Sync . These templates set up a Device Sync-enabled task tracker app with a\ndownloadable front-end client. The template app comes pre-configured\nwith permissions for the user to write their own Items to an  Item \ncollection in a  todo  database. It also has\n Development Mode  and\n email/password authentication \nenabled. On the  Connect to an Edge Server  page, you can learn how to\nmodify the client to connect to the Edge Server. If you have not started with a template app,  enable Device Sync  with Flexible Sync. Configure any  permissions  your\nApp needs to enforce data access rules. Edge Server supports these authentication providers: Enable an authentication provider for clients to connect to the Edge Server. Anonymous Authentication Email/Password Authentication Custom JWT Authentication API Key Authentication If you created a  template app , email/password\nauthentication is already enabled in the template app configuration. At this time, Edge Server is in Private Preview. Your MongoDB\nrepresentative must enable Edge Server for your App. Give\nyour  App Services App ID  to your\nProduct or Account Representative. They will enable Edge Server and\ngenerate an authorization secret for your App. The host that runs Edge Server may be an EC2 instance or other cloud\nserver, or hardware that you provision on-site. For proof-of-concept testing, you can launch an EC2 instance running\nUbuntu Server 18.x. Under Network settings, add a rule to allow access\nvia HTTP/Port 80. Free tier settings are sufficient for small PoC projects. To accept connections from sync clients or connections via the wireprotocol,\nthe host that runs the Edge Server must allow access to incoming connections. If you run Edge Server on your own infrastructure, you don't need to open\nthe instance to the general internet. You only need to ensure your client\ndevices can access it. For cloud infrastructure, such as running a development environment on\nEC2, you must open ports to accept incoming connections: Edge Server supports TLS. You can find configuration details in the\n\"Complete the Edge Server Configuration Details\" section on this page. Development: Use HTTP on port 80 for simplicity and speed of development. Production: Use SSL with port 443 to secure access to production data. The Edge Server has been verified with the following operating systems: Other operating systems may be possible, but have not yet been verified. Ubuntu Server 18+ macOS 11.x and newer running on M1 or Intel 2018+ MacBook Pros Edge Server benchmarking is not yet complete, so minimum required hardware\nis not yet available. Set up and run the Edge Server on the host. After you start the Edge Server, it attempts to maintain a websocket\nconnection to Atlas App Services to remain in sync, even if no client\napplication is connected. If you are running a demo or otherwise working\non a PoC, remember to stop the Edge Server using  make down  when\nyou're done using it. Otherwise, you'll continue using the host's\ncompute and Device Sync time. Use  wget  to get the current Edge Server code as a  .tar  file: Unzip the tar to get the files. You now have an  edge_server  directory containing the server files. The  edge_server  directory contains a  config.json  file you\nmust edit with the appropriate values to configure the server. Field name Value clientAppId The unique ID of your App Services App. For information about\nfinding the App ID, refer to  Find Your App ID . query The  query  field defaults to  \"*\" , which automatically\nsyncs all data in all of the collections in your  App\nServices schema . If you would like to filter the data that syncs to your Edge\nServer, pass an object containing one or more collections\nand queries to sync a subset of your data with Edge Server.\nThis may resemble: YOUR-COLLECTION-NAME If you're passing a query object to filter the data that syncs\nto the Edge Server, this is the name of a collection in your\ndatabase. If you're using a template app, this is the  Item \ncollection. Your Edge Server query may contain many collections. YOUR-QUERY If you're passing a query object to filter the data that syncs\nto the Edge Server, this is the query that determines what\ndocuments the Edge Server can sync with a collection. You can\nuse  Realm Query Language  to define this query,\nwith the same caveats around  Flexible Sync RQL\nLimitations . For example,  truepredicate  means the Edge Server syncs\nall documents with the collection. You might only want to sync\na subset of documents with an Edge Server, so you could pass a\nRQL query similar to  \u201cstore_id == 42\u201d . cloudSyncServerAuthSecret The authorization secret that your Product or Account\nRepresentative provided when you contacted them to get\nEdge Server enabled for your App. httpListenPort The port on which Edge Server listens for HTTP connections. The\ndefault value is  80 . wireprotocolListenPort The port on which Edge Server listens for MongoDB wireprotocol\nconnections, such as connections from a MongoDB Driver. The\ndefault value is  27021 . tls A configuration object that includes the Transport Layer\nSecurity options for your Edge Server. The default value is\n false . When  enabled  is  true , Edge Server uses the TLS\ncertificates you specify in the  certificates  object to\nencrypt your data. You must set the  publicKeyPath  and\n privateKeyPath  values to the locations of your TLS\ncertificates. When  false , the certificates are ignored\nentirely, even if they point to valid certificates. To use TLS, you must provide certificates signed by a certificate\nauthority; for example, Let's Encrypt; or create self-signed\ncertificates and install them on the devices that connect to\nEdge Server. During development, you may bypass certificate\nmanagement by disabling certificate verification in the device\nconnection code. The following example shows an Edge Server configuration file that: With this example, we explicitly sync data from the  Item  and\n Project  collections, because the App Services schema also\nhas a  Team  collection that we don't want to sync to the\nEdge Server. If we wanted to sync all the data in the App Services schema, we\ncould use the default value for the query:  query: \"*\" . We could also use more granular queries to sync a subset of\ndocuments with a collection. For example, only sync Projects where\n facility_id == 42 . For more details on what you could include in a query, refer to\n Realm Query Language . Syncs data with two collections:  Item  and  Project .\nThe  Project  collection has a list property that represents\na  to-many relationship  to the\n Item  collection. Has TLS enabled, with certificates signed by Let's Encrypt. The Edge Server requires several dependencies, which are listed\nin the  README.md  in the  edge_server  directory. Follow the\ninstructions to install the required dependencies. You can run  make  to build the Edge Server. This creates a\n build/  directory that contains an  edge_config.json \nfile. This configuration contains many additional fields you can change\nif you need to customize Edge Server configuration details. For example, an Edge Server running on EC2 would set the  \"baseURL\" \nfor applications to connect to the Edge Server to the public DNS of the\nEC2 instance, similar to\n \"http://ec2-13-58-70-88.us-east-2.compute.amazonaws.com\" . The default configuration values let you run an Edge Server instance\non local hardware, and connect to it through  http://localhost:80 . While Edge Server is still in Private Preview, the configuration and\naccepted values are expected to change. When Edge Server is available\nfor Public Preview, the configuration details are expected to be more\nstable and documentation will be added with these details. To start the server, from the  edge_server  directory: Now you can connect clients directly to the Edge Server. In the App Services logs, you should see an  Authentication -> Login \nentry whose  Name  is  tiered-sync-token . This is the Edge Server\nconnecting to your App Services App. To stop the server, from the  edge_server  directory: You don't have to run the Docker commands as root. However, if you\ndon't, you may get a message similar to this: If this occurs, you can switch to the root user with  sudo su . After you start the Edge Server, it attempts to maintain a websocket\nconnection to Atlas App Services to remain in sync, even if no client\napplication is connected. If you are running a demo or otherwise working\non a PoC, remember to stop the Edge Server using  make down  when\nyou're done using it. Otherwise, you'll continue using the host's\ncompute and Device Sync time. When the Edge Server is running, you can see the status of the\nEdge Server with: This gives you information about the number of clients connected to the\nserver, as well as errors that may have occurred. The status object of an Edge Server may look similar to: As the Edge Server evolves, you may want to upgrade your Edge Server\nversion to take advantage of new functionality. Upgrading the Edge Server version requires the host machine to have\n jq  installed. If you do not already\nhave  jq  installed, running an Edge Server upgrade command tells you\nto install  jq . You can upgrade the Edge Server version using one of three commands: make upgrade-major : Upgrade to the latest major version of Edge Server. make upgrade-minor : Upgrade to the latest minor version of Edge Server. make upgrade-force : Upgrade to the latest version of Edge Server,\nregardless of major or minor version status. From the  edge_server  directory, run\n the relevant upgrade command \nto upgrade to your preferred version of the Edge Server: Depending on your hardware and network connection, the upgrade\nprocess may take seconds to minutes. After upgrading, from the  edge_server  directory, run the command\nto start the Edge Server: If you encounter unexpected behavior after upgrading the Edge Server,\nyou may want to reset it and start it again. Run the following\ncommand:",
            "code": [
                {
                    "lang": "shell",
                    "value": "wget --content-disposition https://services.cloud.mongodb.com/api/client/v2.0/tiered-sync/package/latest"
                },
                {
                    "lang": "shell",
                    "value": "tar -xvf *.tgz"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"clientAppId\": \"YOUR-APP-ID\",\n  \"query\": \"*\",\n  \"cloudSyncServerAuthSecret\": \"YOUR-AUTH-SECRET\",\n  \"httpListenPort\": 80,\n  \"wireprotocolListenPort\": 27021,\n  \"tls\" : {\n    \"enabled\": false,\n    \"certificates\": [\n      {\n        \"publicKeyPath\": \"YOUR_PUBLIC_KEY_PATH/cert.pem\",\n        \"privateKeyPath\": \"YOUR_PRIVATE_KEY_PATH/certkey.pem\"\n      }\n    ]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"clientAppId\": \"sync-template-app-zkiee\",\n  \"query\": {\n    \"YOUR-COLLECTION-NAME\": \"YOUR-QUERY\",\n    \"YOUR-COLLECTION-NAME\": \"YOUR-QUERY\"\n  },\n  ... additional configuration fields...\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"clientAppId\": \"sync-template-app-zkiee\",\n   \"query\": {\n      \"Item\": \"truepredicate\",\n      \"Project\": \"truepredicate\"\n   },\n   \"cloudSyncServerAuthSecret\": \"3814f3fb7befe2eef66ee01781ae245a\",\n   \"httpListenPort\": 80,\n   \"wireprotocolListenPort\": 27021,\n   \"tls\" : {\n     \"enabled\": true,\n     \"certificates\": [\n       {\n         \"publicKeyPath\": \"certs/cert.pem\",\n         \"privateKeyPath\": \"certs/certkey.pem\"\n       }\n     ]\n   }\n}"
                },
                {
                    "lang": "shell",
                    "value": "make up"
                },
                {
                    "lang": "shell",
                    "value": "make down"
                },
                {
                    "lang": "console",
                    "value": "Couldn't connect to Docker daemon at http+docker://localunixsocket"
                },
                {
                    "lang": "shell",
                    "value": "make status"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"status\": \"ACTIVE\",\n   \"cloud_connected\": true,\n   \"num_local_clients\": 1,\n   \"query\": {\n      \"Item\": \"truepredicate\"\n   }\n}"
                },
                {
                    "lang": "shell",
                    "value": "make upgrade-minor"
                },
                {
                    "lang": "shell",
                    "value": "make up"
                },
                {
                    "lang": null,
                    "value": "make clean up"
                }
            ],
            "preview": "Learn how to download, install, configure, and run a MongoDB Edge Server.",
            "tags": "code example",
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "mongodb/data-access",
            "title": "Query Atlas from Client Apps",
            "headings": [
                "When to Use MongoDB Data Access",
                "Set Up",
                "Create an App Services App",
                "Add an Authentication Provider",
                "Add Rules to a Collection",
                "Add a Schema and Filters to Collection (Optional)",
                "Query from Client Apps"
            ],
            "paragraphs": "You can query data in linked MongoDB Atlas clusters from client applications\nusing standard MongoDB query language (MQL) syntax with  MongoDB Data Access \nin the Realm SDKs. MongoDB Data Access secures data by enforcing your App's role-based access permissions, query filters, and document schemas.\nThese features let you construct complex client-side queries with MQL\nwhile your data remains secure on the server. You might want to use MongoDB Data Access in your application if: You might want to use MongoDB Data Access  instead of   Atlas Device Sync \nin an application in these scenarios: As an alternative to MongoDB Data Access in  client-side  scenarios,\nyou can use the  Atlas Data API . You can also use these APIs without a Realm SDK for your client app's programming\nlanguage as long as the language has a HTTPS client.\nHowever, these APIs do not feature the same type-safe MQL syntax\nas the Realm SDKs' MongoDB Data Access. As alternative to MongoDB Data Access in  server-side  scenarios,\nyou can use the following: You want to use an API like the MongoDB drivers You want to write database queries in your client code instead of on a server You want to construct ad hoc queries from the client device. You are querying documents that are not or cannot be modeled in Realm Database. You want to access collections that are not synced. You do not want to persist data locally. You are using the Realm Web SDK, which does not support Realm Database or Device Sync. MongoDB driver . Using a MongoDB driver to connect to Atlas\nis the generally recommended server-side approach, as the drivers are\nmore performant and flexible than connecting through App Services.\nIf you would like to apply App Services Rules\nto driver operations, you can use the  App Services Wire Protocol . Atlas Data API To use MongoDB Data Access from a Realm SDK, you must perform the following set up. Create an App Services App with a linked MongoDB data source. To learn more about App creation, refer to  Create an App \nand  Link a Data Source . Add at least one authentication provider to the App. All MongoDB Data Access\nqueries run in the context of an authenticated user. To learn more about\nenabling user authentication, refer to  Authentication Providers . To secure your client-side queries, you must add App Services Rules\nto any collections that you use with MongoDB Data Access. To learn how to\nadd rules to collections, refer to  Define Roles & Permissions . If you do not add rules to a collection, then every request to it\nfrom the client will fail. In addition to rules, you can also add a schema and filters to collections\nthat you query with MongoDB Data Access. Schemas  define your App's data model and validate documents\nagainst it. Use filters to ensure that the clients only write data matching\nthe schema to the database. Filters  modify MongoDB queries before they are sent to\nthe database so that queries only return a subset of results. For example,\nyou could define a filter that modifies queries so that a user can only\naccess data that they've written. While schemas and filters are not necessary to use MongoDB Data Access,\nyou should add them to most production use cases. For documentation on how to query MongoDB from the Realm SDKs,\nrefer to the Realm MongoDB Data Access documentation: MongoDB Data Access  is not  supported in these Realm SDKs: Query MongoDB - Java SDK Query MongoDB - .NET SDK Query MongoDB - Node.js SDK Query MongoDB - React Native SDK Query MongoDB - Swift SDK Query MongoDB - Web SDK C++ SDK Flutter SDK Kotlin SDK",
            "code": [],
            "preview": "You can query data in linked MongoDB Atlas clusters from client applications\nusing standard MongoDB query language (MQL) syntax with MongoDB Data Access\nin the Realm SDKs.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "mongodb/read-preference",
            "title": "Read Preference",
            "headings": [
                "Overview",
                "When To Specify Read Preference",
                "Procedure",
                "Navigate to the Cluster Configuration Screen",
                "Specify a Cluster Read Preference",
                "Specify Read Preference Tags",
                "Save the Cluster Configuration",
                "Pull the Latest Version of Your App",
                "Specify a Cluster Read Preference",
                "Specify Read Preference Tags",
                "Deploy the Updated Data Source Configuration"
            ],
            "paragraphs": "You can configure the  read preference \nfor a  linked MongoDB Atlas cluster  to control the type\nof  replica set member  that Atlas App Services\nroutes database read requests to. You can also specify a  tag\nset  to\ntarget specific members of the replica set. You cannot set the read preference on a  Federated data source . By default, App Services uses a read preference of  primary , which routes\nall read requests through the  primary node  of a replica set. The default read preference ( primary ) should be sufficient for most\nuse cases. Consider specifying a cluster read preference when you need\nto do the following: Read from a specific secondary that has a custom configuration, such\nas an analytics node with special indexes optimized for reporting\nworkloads. Read from a node in a specific region of a globally distributed\nreplica set. Maintain read availability during a replica set failover, i.e.,\ncontinue to read potentially stale data when there is no  primary \nnode. You can configure the read preference for a linked cluster from the cluster's\nconfiguration screen in the App Services UI. To get to the configuration screen, click\n Linked Data Sources  beneath  Manage  in the left-hand\nnavigation.\nIn the list of data sources, select the cluster on which you want to\nconfigure read preference. On the cluster configuration screen, select a mode from the\n Read Preference  dropdown. The following read preference modes are available: Mode Description primary App Services routes all read operations to the current replica set\n primary node . This is the\ndefault read preference mode. primaryPreferred App Services routes all read operations to the current replica set\n primary node  if it's\navailable. If the primary is unavailable, such as during an\n automatic failover , read requests are routed\nto a  secondary node \ninstead. secondary App Services routes all read operations to one of the current replica\nset  secondary nodes . secondaryPreferred App Services routes all read operations to one of the replica set's\navailable  secondary nodes . If no secondary is available,\nread requests are routed to the replica set  primary  instead. nearest App Services routes read operations to the  replica set member  that has the lowest network\nlatency relative to the client. If you specify a read preference other than  primary , you can also\nspecify one or more  read preference tags . To\nserve a read request, a  replica set member  must include all of the specified read\npreference tags in its node configuration. To specify a read preference tag, click  Add Tag  and then\nenter the tag's  Key  and  Value  in the new row\nof the  Read Preference Tags  table. MongoDB Atlas  clusters\nare configured with pre-defined tag sets for each\nmember node depending on the member's type. For detailed information\non which tags are defined nodes in an Atlas cluster, see  Atlas\nReplica Set Tags . Once you have specified a  Read Preference  and any\n Read Preference Tags , click  Save . Once saved,\nApp Services routes all incoming database read requests for the cluster\naccording to your preference. To define the read preference for a linked cluster with the  App Services CLI , you need\na local copy of your application's configuration files. To pull a local copy of the latest version of your app, run the following: You can also download a copy of your application's configuration files from\nthe  Deploy > Export App  screen in the App Services UI. To configure the read preference for a linked cluster, open the cluster's\n config.json  file and set the value of  config.readPreference : The following read preference modes are available: Mode Description primary App Services routes all read operations to the current replica set\n primary node . This is the\ndefault read preference mode. primaryPreferred App Services routes all read operations to the current replica set\n primary node  if it's\navailable. If the primary is unavailable, such as during an\n automatic failover , read requests are routed\nto a  secondary node \ninstead. secondary App Services routes all read operations to one of the current replica\nset  secondary nodes . secondaryPreferred App Services routes all read operations to one of the replica set's\navailable  secondary nodes . If no secondary is available,\nread requests are routed to the replica set  primary  instead. nearest App Services routes read operations to the  replica set member  that has the lowest network\nlatency relative to the client. If you specify a read preference other than  primary , you can also\nspecify one or more  read preference tags . To\nserve a read request, a  replica set member  must include all of the specified read\npreference tags in its node configuration. To specify a read\npreference tag set for a cluster, ensure that you have specified a\n readPreference  other than  primary , and then add one or more\ntag definition objects to  config.readPreferenceTagSets : Atlas clusters are configured with pre-defined tag sets for each\nmember node depending on the member's type. For detailed information\non which tags are defined nodes in an Atlas cluster, see  Atlas\nReplica Set Tags . Once you've set the read preference for the cluster, you can push the updated\nconfig to your remote app. App Services CLI immediately deploys the update on push.",
            "code": [
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "json",
                    "value": "{\n   \"name\": \"<MongoDB Service Name>\",\n   \"type\": \"mongodb-atlas\",\n   \"config\": {\n      \"readPreference\": \"<Read Preference Mode>\"\n   }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"name\": \"<MongoDB Service Name>\",\n   \"type\": \"mongodb-atlas\",\n   \"config\": {\n      \"readPreference\": \"<Read Preference Mode (other than primary)>\",\n      \"readPreferenceTagSets\": [\n         { <Tag Key>: <Tag Value> },\n         ...\n      ],\n   }\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=\"<Your App ID>\""
                }
            ],
            "preview": "You can configure the read preference\nfor a linked MongoDB Atlas cluster to control the type\nof replica set member that Atlas App Services\nroutes database read requests to. You can also specify a tag\nset to\ntarget specific members of the replica set.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "mongodb/configure-advanced-rules",
            "title": "Configure Advanced Rules",
            "headings": [
                "Overview",
                "When To Use Advanced Mode",
                "Procedure",
                "Navigate to the Collection Rules Screen",
                "Convert to Advanced Mode",
                "Define the Collection Rules",
                "Save the Updated Collection Rules"
            ],
            "paragraphs": "You can manually configure all aspects of a collection's rules by\nediting the underlying configuration document directly through the\nApp Services UI.  Advanced Mode  refers to editing rules in this manner. The default or \"Basic Mode\" rules editor in the App Services UI covers the\nmajority of use cases for collection rules. However, there are times\nwhen you need more fine-grained control than the UI interface provides.\nConsider using Advanced Mode if you need to do the following: Configure a role that can  only insert documents . Define field-level read or write permissions for a  field in an\nembedded document . Determine field-level write permissions dynamically using a  rule\nexpression . Once you convert a collection's rules to Advanced Mode, you may not\nbe able to switch back to editing that collection's rules through the\nBasic Mode interface. You can edit collection rules in Advanced Mode from the MongoDB rules\nscreen in the App Services UI. To get to the rules screen for a collection,\nclick  Rules  beneath  Data Access  in the\nleft navigation menu and then select the collection from the list. Click the  </>  symbol by  VIEW  in the top\nleft corner of the collection rules interface to enter Advanced View.\nThe UI will switch an editor that shows a preview of the underlying\nAdvanced Mode rule configuration document.\nClick  Convert to Advanced Mode . Type \"convert\" and then click\n Confirm  to confirm your action. Once you have converted the collection to Advanced Mode you can edit\nthe underlying configuration document. Collection rule configuration\ndocuments have the following form: Field Description roles An array of  Role configuration documents  that each define a single role's\n Apply When  condition and associated CRUD\npermissions. App Services evaluates roles for each query in the order that\nthey're defined. Ensure that each role configuration\ndocument's array index matches its desired position in the\nevaluation order. filters An array of  Filter configuration documents  that each define a filter on the collection. schema A  schema  that configures the shape and contents of all documents in the collection. The root of all collection schemas must be an  object\nschema document . You can embed other\n schema types  inside the  properties \nfield of the root schema. For an example of how to define relationships in advanced\nmode, see:  Configure Relationships . Once you have finished editing the collection rules, click\n Save Draft  in the top right corner. App Services will immediately\nbegin using the new rule configuration you defined for all incoming queries\non the collection.",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"roles\": [\n    <Role Configuration>,\n    ...\n  ],\n  \"filters\": [\n    <Filter Configuration>,\n    ...\n  ],\n  \"schema\": {\n    \"properties\": {\n      \"<Field Name>\": <Schema Document>,\n      ...\n    }\n  }\n}"
                }
            ],
            "preview": "You can manually configure all aspects of a collection's rules by\nediting the underlying configuration document directly through the\nApp Services UI. Advanced Mode refers to editing rules in this manner.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "security/network",
            "title": "Configure Network Security",
            "headings": [
                "Overview",
                "Transport Layer Security (TLS)",
                "Firewall Configuration",
                "AWS",
                "Azure",
                "GCP",
                "DNS Filtering",
                "Allowed Request Origins",
                "IP Access List",
                "Find Your IP Address",
                "View IP Access List Entries",
                "Create an IP Access List Entry",
                "Edit an IP Access List Entry",
                "Delete an IP Access List Entry",
                "API Access List"
            ],
            "paragraphs": "App Services uses a range of network security protocols to prevent\nunauthorized access to your data. You can: Configure TLS to secure network requests to and from your application. Define IP addresses from which all outbound requests originate. Define and manage URLs and IP addresses from which inbound requests may originate. App Services uses  TLS 1.3 \nto secure all network requests to and from your application, including: The TLS certificate is pre-defined and cannot be customized or disabled. Apps that connect from a Realm SDK. Data API requests sent over HTTPS. Queries and operations on a linked MongoDB Atlas data source. All internal communication between App Services and Atlas is encrypted with\nx509 certificates. App Services only sends outbound requests from a set list of IP\naddresses. The exact list depends on the cloud provider that the app\nserver is deployed to. You can copy the IP addresses listed in this\nsection to an allowlist for incoming requests on your firewall. You can download a computer-friendly list of all IP addresses used by\nApp Services in  JSON \nor  CSV \nformat. You can also find cloud-provider-specific JSON and CSV files in\nthe following sections. If you run a function from the Atlas App Services UI, the request originates from\nthe server nearest to you, not the region the app is deployed to. Download AWS IP Addresses:  JSON ,  CSV Outbound requests from an app deployed to AWS will originate from one of\nthe following IP addresses: Download Azure IP Addresses:  JSON ,  CSV Outbound requests from an app deployed to Azure will originate from one\nof the following IP addresses: Download GCP IP Addresses:  JSON ,  CSV Outbound requests from an app deployed to GCP will originate from one of\nthe following IP addresses: The above IP lists only apply to outgoing requests from Atlas Functions,\nincluding triggers and HTTPS endpoints that make outgoing requests. For requests that originate from the Sync server, we recommend allowlisting\nthe entire subnet of the App's Deployment Region and cloud provider. You can\nfind the Deployment Region in the App Services UI under  App Settings >\nGeneral > Deployment Region . You can use DNS filtering to specifically allow connections from client\napplications, including Device Sync clients, to the server. Access  *.services.cloud.mongodb.com  via HTTPS or port 443. You can define this  configuration option  in the\napp-level  realm_config.json  file. This field accepts an array of\nURLs that incoming requests may originate from. If you define any allowed\nrequest origins, then App Services blocks any incoming request from\nan origin that is not listed. App Services allows client requests from the enabled entries in the app's\n IP access list . Allowed requests will still use\nApp Services's authentication and authorization rules. When you add IP\naccess list entries, App Services blocks any request originating from\nan IP that is not on the access list. By default, any newly-created App allows access from any client IP\nby adding an access list entry for 0.0.0.0/0. If you delete this entry,\nno client can access your App from any IP address. Run the  ipconfig  command to find the IP address\nof a Windows machine. Run the  ifconfig  command to find the IP address\nof a Linux or Mac machine. To view your IP Access List settings, navigate to  App Settings \nin the App Services UI, and then click the  IP Access List  tab. To view your IP access list entries, call  appservices accessList list .\nThe CLI will then prompt you for your App ID. To view your IP access list entries, create a  GET  request\nin the following format. You must specify the Group and App ID. Admin API Documentation To add an IP address to the list of enabled entries, click the green\n Add IP address  button to open the  Add IP Access List Entry  modal.\nThis modal allows you to specify your current IP address or use a custom one.\nYou can also specify a comment about the entry. To create an IP access list entry, call  appservices accessList create .\nThe CLI will prompt you to input an IP address and to select an App\nfrom a list of all your Apps. Optionally, you can specify any of the following arguments when you call\nthe program: To create an IP access list entry, create a  POST  request\nin the following format. You must specify the IP address in the request\nbody and the Group and App ID in the request URL. Admin API Documentation To edit an entry, click the  edit  button on the right-hand side of\nthe entry, and the  Update IP Access List Entry  modal will open. This\nmodal allows you to specify your current IP address or use a custom one. You can\nalso specify a comment about the entry. You can edit an IP access list entry by updating the entry's IP address or by adding\na comment to the entry. To update an entry's IP address, call: To add or update a comment, call: The CLI will then prompt you to select an App from a list of all your Apps\nand the IP address to update. To update the IP address for an entry, create a  PATCH  request\nin the following format. You must specify the new IP address in the request\nbody and the Group, App, and IP address ID in the request URL. The IP address\nID refers to the entry that you want to update. To get the ID for an IP address,\n view your IP access list entries . Admin API Documentation To delete an entry, click the  delete  button on the right-hand side\nof the entry, and a modal will open requesting you to confirm that you want to\ndelete the entry. Click the red  delete  button on the modal to\ncomplete the deletion of the entry. To delete an IP access list entry, call  appservices accessList delete .\nThe CLI will prompt you to input the App ID and to select which IP address\nto delete. To delete an IP access list entry, create a  DELETE  request\nin the following format. You must specify the Group, App, and IP address ID.\nTo get the ID for an IP address,  view your IP access list entries . Admin API Documentation When you  create an Atlas API key \nfor project or organization access from the Realm CLI or the App Services\nAdmin API, you can specify IP addresses that can use this API key. If you specify\nan IP address, App Services blocks any request originating from an IP address that\nis not on the access list.",
            "code": [
                {
                    "lang": "text",
                    "value": "13.236.189.10\n18.202.2.23\n18.210.66.32\n18.211.240.224\n18.213.24.164\n52.63.26.53\n54.203.157.107\n54.69.74.169\n54.76.145.131\n18.192.255.128\n18.157.138.240\n18.158.38.156\n52.220.57.174\n18.140.123.126\n13.251.182.174\n65.0.112.137\n3.6.231.140\n13.234.189.107\n13.232.212.70\n65.0.113.75\n3.7.215.88\n3.6.255.136\n65.0.188.79\n13.233.17.88\n18.136.226.22\n122.248.203.228\n54.251.109.67\n54.255.78.248\n54.179.247.236\n13.251.170.158\n3.105.146.190\n52.65.242.206\n54.79.24.107\n13.238.106.70\n52.28.11.211\n3.121.9.73\n52.29.205.189\n3.122.49.121\n3.121.58.147\n3.121.97.130\n108.128.63.52\n108.128.66.245\n108.128.51.69\n108.128.45.118\n52.213.157.241\n108.128.66.107\n3.9.6.254\n3.9.74.211\n3.9.61.59\n35.176.121.115\n3.9.85.190\n3.9.47.47\n13.36.132.152\n15.188.240.49\n13.37.29.138\n15.188.152.56\n13.39.52.19\n15.188.159.135\n177.71.159.160\n52.67.231.12\n18.230.146.14\n52.67.94.32\n18.230.109.192\n18.229.199.232\n3.212.79.116\n3.92.113.229\n34.193.91.42\n34.237.40.31\n3.215.10.168\n34.236.228.98\n3.214.203.147\n3.208.110.31\n100.26.2.217\n3.215.143.88\n18.119.73.75\n3.136.153.91\n3.128.101.143\n35.166.246.78\n35.161.40.209\n54.149.249.153\n35.161.32.231\n52.34.65.236\n35.163.245.143"
                },
                {
                    "lang": "text",
                    "value": "20.105.25.17\n20.212.99.191\n20.24.112.135\n20.53.104.226\n20.84.232.59\n20.96.47.95\n40.112.209.0\n52.149.111.83"
                },
                {
                    "lang": "text",
                    "value": "34.150.239.218\n34.69.118.121\n34.78.133.163\n34.82.246.143\n34.93.58.231"
                },
                {
                    "lang": "sh",
                    "value": "curl --request GET \\\n  --header 'Authorization: Bearer <access_token>' \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/<groupId>/apps/<appId>/security/access_list"
                },
                {
                    "lang": "bash",
                    "value": "appservices accessList create \\\n  --app=<Your App ID> \\\n  --ip=<IP Address> \\\n  --comment=<Optional Comment> \\\n  --use-current \\\n  --allow-all"
                },
                {
                    "lang": "sh",
                    "value": "curl --request POST \\\n  --header 'Authorization: Bearer <access_token>' \\\n  --data '{ \"address\": \"<IP Address>\" }' \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/<groupId>/apps/<appId>/security/access_list"
                },
                {
                    "lang": "sh",
                    "value": "``appservices accessList update --new-ip <IP Address>``"
                },
                {
                    "lang": "sh",
                    "value": "``appservices accessList update --comment <Optional Comment>``"
                },
                {
                    "lang": "sh",
                    "value": "curl --request PATCH \\\n  --header 'Authorization: Bearer <access_token>' \\\n  --data '{ \"address\": \"<IP Address>\" }' \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/<groupId>/apps/<appId>/security/access_list/<ipID>"
                },
                {
                    "lang": "sh",
                    "value": "curl --request DELETE \\\n  --header 'Authorization: Bearer <access_token>' \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/<groupId>/apps/<appId>/security/access_list/<ipID>"
                }
            ],
            "preview": "Learn about the network security protocols used by Atlas App Services.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "mongodb/preimages",
            "title": "Document Preimages",
            "headings": [
                "Overview",
                "Performance",
                "Enable Document Preimages",
                "View Preimage-Enabled Collections",
                "Disable Preimages for a Collection"
            ],
            "paragraphs": "Every  database trigger  execution has a related\nchange event. You can configure these change events to include\n document preimages . A preimage is a snapshot of a document  before \na change. Consider  updateOne  in a collection with preimages: Preimages add the following data to write operation change events the document preimage in the  fullDocumentBeforeChange  field the document postimage in the  fullDocument  field For clusters running MongoDB version 6.0 and newer, triggers use the\ncluster's built-in  change stream preimages \nfeature. Older versions of MongoDB store preimages directly in the\noplog. Both of these add storage and compute overhead for each operation\non the collection which may cause performance issues for collections\nwith a high write throughput. You can enable preimages when you  configure a database trigger .  Document Preimage  is a setting\non trigger configuration. Toggle this to enable document preimages for the\ncollection. To view the list of collections that store document preimages: This section contains the  Preimage Preferences by Collection \ntable. This table lists every collection that stores preimages in the oplog.\nEnabling preimages for a collection applies to all collections. This\nincludes collections tied to triggers in a different App. Collections\nwith no triggers that use preimages also appear here. Navigate to the  Linked Data Source  configuration screen. Expand the  Advanced Configuration  section. You can disable collection-level preimages in the App Services UI. To disable preimages for a collection: (Optional) Disable document preimages for triggers in the collection.\nTriggers with preimages continue to fire after disabling preimages.\nBut they don't have the  fullDocumentBeforeChange  field in change\nevents. This includes triggers in other App Services Apps. Terminate sync , if the collection is part\nof a synced cluster. View  the Preimage Preferences By Collection  table. Press the\n Disable  button for a collection. This turns off preimages for\nthat collection. Re-enable Sync , if the collection is part\nof a synced cluster. When you terminate and re-enable Atlas Device Sync, clients can no longer Sync.\nYour client must implement a client reset handler to restore Sync. This\nhandler can discard or attempt to recover unsynchronized changes. Client Reset - Flutter SDK Client Reset - Java SDK Client Reset - Kotlin SDK Client Reset - .NET SDK Client Reset - Node SDK Client Reset - React Native SDK Client Reset - Swift SDK",
            "code": [
                {
                    "lang": "js",
                    "value": "pets.updateOne(\n   { name: \"Fido\" },\n   { $inc: { age: 1 } }\n)"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"name\": \"Fido\",\n   \"age\": 3\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"name\": \"Fido\",\n   \"age\": 4\n}"
                }
            ],
            "preview": "Every database trigger execution has a related\nchange event. You can configure these change events to include\ndocument preimages. A preimage is a snapshot of a document before\na change.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "mongodb/internal-database-usage",
            "title": "Internal Database Usage",
            "headings": [
                "System-Generated Cluster Users",
                "Transactional Locks",
                "Unsynced Documents"
            ],
            "paragraphs": "Atlas App Services uses your linked MongoDB Atlas cluster to manage some\ninternal operations. In general, you do not need to know about these\noperations to use App Services. App Services automatically creates a  MongoDB user  for each app linked to a cluster. These users\nare for internal use only and cannot be edited or manually deleted. If\nyou delete an App, the associated user will also be deleted. Users generated by App Services have names of the form:\n mongodb-realm-<your app id> App Services connects to standard MongoDB Atlas clusters, which\nmeans that you can connect directly to a linked cluster\nusing another tool such as the\n mongosh shell  or\nMongoDB Compass. There are no special considerations when reading\ndata from a linked cluster with another tool. While running update operations, App Services temporarily adds a\nreserved field,  _id__baas_transaction , to documents.\nOnce a document is successfully updated, App Services removes this\nfield. If you want to use another tool to modify data in a\ncollection, ensure that you  $unset  this field prior to\nmaking changes. For example, if you are using the  mongosh  shell to\nupdate documents in the products collection, your command\nmight resemble the following code: If a document in a  synced  collection does not conform to\nthe collection's schema, it cannot be synced to client apps. If there\nare 100,000 or more unsyncable documents, App Services pauses sync\nfor the app. App Services stores a information about unsyncable documents in the\n __realm_sync.unsynced_documents  collection. A common cause for unsyncable documents is data added or modified\noutside of the context of your app. Consider the following scenario: You can read from the  __realm_sync.unsynced_documents  collection,\nbut you should not modify it in any way. An app has a  pets  collection with the following schema: Someone adds a document directly to the  pets  collection using a\nMongoDB driver, MongoDB compass, or an Atlas Function running in \"system\" mode.\nThe document's  age  field contains a string, not a\nnumber, which does not match your app's schema. MongoDB does not\nnatively enforce your app's schema, so it allows the insert without a\nwarning or error. Once inserted, the document fails schema validation in your app and\ncannot be synced. The app stores the failure in the\n unsynced_documents  collection: If your data is used by Sync clients but can also be created or\nmodified outside of Atlas Device Sync, you must ensure those creations and\nmodifications match the defined object schema on the collection. For\ndocuments that have failed, you can replace, update, or delete &\nre-add each document.",
            "code": [
                {
                    "lang": "sh",
                    "value": "db.products.update(\n   { sku: \"unknown\" },\n   { $unset: { _id__baas_transaction: \"\" } }\n)"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"Pet\",\n  \"required\": [\"_id\", \"type\", \"name\", \"age\"],\n  \"properties\": {\n    \"_id\": { \"bsonType: \"objectId\" },\n    \"type\": { \"bsonType: \"string\" },\n    \"name\": { \"bsonType: \"string\" },\n    \"age\": { \"bsonType: \"int\" }\n  }\n};"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": \"5ae782e48f25b9dc5c51c4a5\",\n  \"type\": \"dog\",\n  \"name\": \"Fido\",\n  \"age\": \"7\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": \"6183021879247167daacd8dc\",\n  \"appId\": \"6183021373247568dcdcd3ed\",\n  \"documentId\": \"5ae782e48f25b9dc5c51c4a5\",\n  \"ns\": {\n    \"db\": \"myDatabase\",\n    \"coll\": \"pets\"\n  },\n  \"reason\": \"invalid schema\"\n}"
                }
            ],
            "preview": "Atlas App Services uses your linked MongoDB Atlas cluster to manage some\ninternal operations. In general, you do not need to know about these\noperations to use App Services.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "mongodb/wire-protocol",
            "title": "Wire Protocol",
            "headings": [
                "Overview",
                "Compatible Clients",
                "Connection Strings",
                "Credentials",
                "Region",
                "Parameters",
                "Enable Wire Protocol Connections",
                "Navigate to the Data Source Configuration Screen",
                "Enable Connection Strings for the Cluster",
                "Select the Authentication Method",
                "Pull the Latest Version of Your App",
                "Enable Wire Protocol for the Cluster",
                "Deploy the Updated Data Source Configuration",
                "Connect Over the Wire Protocol",
                "Connect to App Services with a Connection String",
                "Read and Modify Data",
                "Call a Function",
                "Call a Service Action [DEPRECATED]",
                "Get the Logged In User's Data"
            ],
            "paragraphs": "Atlas App Services natively implements a subset of the MongoDB wire protocol,\nwhich allows you to connect to an App through one of its  linked\nMongoDB Atlas data sources  using standard MongoDB\ndrivers and tools. Clients use a specialized  App Services\nconnection string  to connect and send requests.\nApp Services supports most client features over the wire protocol,\nincluding  role-based data access rules ,\n functions , and  service actions . This is a good choice for languages that do not currently have a Realm\nSDK. The examples here are for Python, C++11, and the Mongo Shell. Any\n MongoDB driver  that supports the\n appName  connection string parameter can use the wire protocol to\nconnect to App Services. Federated data sources  do not support connections via the wire\nprotocol . You can use the following tools and drivers to communicate with App Services\nusing a connection string: Version 4.0+ of the  mongo shell . Any MongoDB driver that supports the  appName  connection string\nparameter. All official MongoDB drivers support this parameter in\ntheir current releases. Connections to App Services over the wire protocol have access to the full\nfunctionality of the  MongoDB Service . However,\nApp Services does not support all operations and features available in\nstandard tools and clients. For details, see  MongoDB Service\nLimitations . To connect to App Services over the wire protocol, you must construct a\n MongoDB connection string  that\nincludes credentials for an application  user  and an\napplication-specific  appName  query parameter. App Services connection strings have the following form: You must  URL encode  connection\nstrings before you can use them to connect to App Services. Connection\nstrings in the App Services UI are properly encoded by default. All operations that you issue over the wire protocol run in the context\nof a specific application user that you specify in the connection\nstring. The user must be registered with one of the following\nauthentication providers: The contents of connection string credentials depend on the\nauthentication provider with which the user registered: Email/Password API Key Custom JWT Format <email>:<password> Fields <email> The user's registered email address. <password> The user's password. Example Format _:<apiKey> Fields <apiKey> An active application  API Key . Example Format _:<customAuthToken> Fields <customAuthToken> A custom authentication  JSON\nWeb Token . Example The connection string must specify the  deployment region  and cloud provider in which the App is hosted. Global Apps use the  global  region: Local Apps specify the cloud provider and region name using\n <region>.<cloud>  format. For example, an app deployed to\n aws-us-east-1  would use the following connection string: App Services requires specific connection string options that identify\nthe application you want to connect to and the authentication provider\nassociated with the  credentials  that you provide. App Services connection strings have the following query parameters: Parameter Description authMechanism This parameter should always be set to  PLAIN . authSource This parameter should always be set to  $external . appName Uniquely identifies the application, MongoDB service, and\nauthentication provider to which you want to connect. The  appName  parameter has the following form: <app id> The  App ID  of the App. <service> The name of the MongoDB Service that you want to connect\nto. This value will always be  mongodb-atlas . <provider> The  authentication provider  for which you provided\n credentials . Valid values: local-userpass api-key custom-token You must enable wire protocol connections for a linked clusters before\nyou can connect to an App Services App with a connection string. In the  Manage  section of the left navigation menu, click\n Linked Data Sources . In the list of data sources, select the cluster on\nwhich you want to enable wire protocol connections. Set the toggle for  MongoDB Connection String  to\n Enabled . In the Authentication Method section that\nappears, choose and configure how you want to authenticate the\nwire protocol connections. To enable MongoDB wire protocol connections with the  App Services CLI , you need a local\ncopy of your application's configuration files. To pull a local copy of the latest version of your app, run the following: You can also download a copy of your application's configuration files from\nthe  Deploy > Export App  screen in the App Services UI. To enable wire protocol connections for a linked cluster, open the cluster's\n config.json  file and set the value of  config.wireProtocolEnabled  to\n true : Federated data sources  do not support connections via the wire\nprotocol . Once you've enabled wire protocol connections for the cluster in\n config.json , you can push the config to your remote app. App Services CLI\nimmediately deploys the update on push. To connect to App Services over the wire protocol, pass a\n URL-encoded   App Services connection\nstring  when you create a client, just as you would\nwith a regular  connection string . While connected to App Services over the wire protocol you can use\nstandard MongoDB CRUD operations. App Services applies  role-based data\naccess rules  to all queries in the context of the\nauthenticated user specified in the connection string  credentials . You can call functions using the  callFunction  database command. Command Description Prototype Calls the specified  function  and returns any\nresult. You can call service actions using the  callServiceFunction  database\ncommand. Command Description Prototype Calls the specified  service action  and\nreturns any result. You can get the  user object  for the authenticated\nuser using the  userProfile  database command. Command Description Prototype Returns the  user object  for the authenticated\nuser.",
            "code": [
                {
                    "lang": "shell",
                    "value": "mongodb://<credentials>@<region>.services.cloud.mongodb.com:27020/?<parameters>"
                },
                {
                    "lang": "none",
                    "value": "joe.mango@company.com:SuperSecretPassword123"
                },
                {
                    "lang": "none",
                    "value": "_:tOSJwYhLLam1qTAwP8rZ5M9BiHfn69w5xrya52dYeOv1PdTlD68i8gKOaN0Wy24z"
                },
                {
                    "lang": "none",
                    "value": "_:eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.\n  eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.\n  SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c"
                },
                {
                    "lang": "shell",
                    "value": "mongodb://<credentials>@global.services.cloud.mongodb.com:27020/?<parameters>"
                },
                {
                    "lang": "shell",
                    "value": "mongodb://<credentials>@us-east-1.aws.services.cloud.mongodb.com:27020/?<parameters>"
                },
                {
                    "lang": "none",
                    "value": "<app id>:<service>:<provider>"
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"mongodb-atlas\",\n  \"type\": \"mongodb-atlas\",\n  \"config\": {\n    \"wireProtocolEnabled\": true,\n    ...\n  }\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=\"<Your App ID>\""
                },
                {
                    "lang": "shell",
                    "value": "$ mongo \"mongodb://<user>:<password>@services.cloud.mongodb.com:27020/?authMechanism=PLAIN&authSource=%24external&ssl=true&appName=realm-application-abcde:mongodb-atlas:local-userpass\""
                },
                {
                    "lang": "cpp",
                    "value": "mongocxx::instance instance{};\nmongocxx::uri uri(\"mongodb://<user>:<password>@services.cloud.mongodb.com:27020/?authMechanism=PLAIN&authSource=%24external&ssl=true&appName=realm-application-abcde:mongodb-atlas:local-userpass\");\nmongocxx::client client(uri);"
                },
                {
                    "lang": "python",
                    "value": "client = pymongo.MongoClient(\"mongodb://<user>:<password>@services.cloud.mongodb.com:27020/?authMechanism=PLAIN&authSource=%24external&ssl=true&appName=realm-application-abcde:mongodb-atlas:local-userpass\")"
                },
                {
                    "lang": "javascript",
                    "value": "> use HR\n> db.employees.findOne();\n{\n  \"_id\": ObjectId(\"5ae782e48f25b9dc5c51c4a5\"),\n  \"employeeId\": 854271626,\n  \"name\": {\n    \"first\": \"Lucas\",\n    \"last\": \"Lewis\"\n  },\n  \"role\": \"PM\",\n  \"salary\": 200000,\n  \"email\": \"Lucas.Lewis.0271@company.com\",\n  \"password\": \"<password>\",\n  \"manager\": {\n    \"id\": 328892725,\n    \"email\": \"Daniel.Wilson.0474@company.com\",\n    \"name\": {\n      \"first\": \"Daniel\",\n      \"last\": \"Wilson\"\n    }\n  }\n}"
                },
                {
                    "lang": "cpp",
                    "value": "mongocxx::database db = client[\"HR\"];\nmongocxx::collection employees = db[\"employees\"];\nbsoncxx::stdx::optional<bsoncxx::document::value> result =\n    collection.find_one({});\nif(result) {\n    std::cout << bsoncxx::to_json(result) << \"\\n\";\n}"
                },
                {
                    "lang": "python",
                    "value": ">>> db = client[\"HR\"]\n>>> employee = db[\"employees\"].find_one();\n>>> pprint(employee)\n{'_id': ObjectId('5ae782e48f25b9dc5c51c4a5'),\n 'email': 'Lucas.Lewis.0271@company.com',\n 'employeeId': 854271626.0,\n 'manager': {'email': 'Daniel.Wilson.0474@company.com',\n             'id': 328892725.0,\n             'name': {'first': 'Daniel', 'last': 'Wilson'}},\n 'name': {'first': 'Lucas', 'last': 'Lewis'},\n 'password': '<password>',\n 'role': 'PM',\n 'salary': 200000}"
                },
                {
                    "lang": "none",
                    "value": "{\n  callFunction: <function name>,\n  arguments: [<arg1>, <arg2>, ...]\n}"
                },
                {
                    "lang": "shell",
                    "value": "> db.runCommand({\n...  callFunction: \"getEmployeeById\",\n...  arguments: [\"5ae782e48f25b9dc5c51c4a5\"]\n...});\n{\n  \"ok\" : 1,\n  \"response\" : {\n    \"_id\": ObjectId(\"5ae782e48f25b9dc5c51c4a5\"),\n    \"employeeId\": 854271626,\n    \"name\": {\n      \"first\": \"Lucas\",\n      \"last\": \"Lewis\"\n    },\n    \"role\": \"PM\",\n    \"salary\": 200000,\n    \"email\": \"Lucas.Lewis.0271@company.com\",\n    \"password\": \"<password>\",\n    \"manager\": {\n      \"id\": 328892725,\n      \"email\": \"Daniel.Wilson.0474@company.com\",\n      \"name\": {\n        \"first\": \"Daniel\",\n        \"last\": \"Wilson\"\n      }\n    }\n  }\n}"
                },
                {
                    "lang": "cpp",
                    "value": "db.runCommand({\n  callFunction: \"getEmployeeById\",\n  arguments: [\"5ae782e48f25b9dc5c51c4a5\"]\n});"
                },
                {
                    "lang": "python",
                    "value": ">>> function_result = db.command(\"callFunction\", \"getEmployeeById\",\n...     arguments=[\"5ae782e48f25b9dc5c51c4a5\"]\n...)\n>>> pprint.pprint(function_result)\n{'ok': 1,\n 'response': {'_id': ObjectId('5ae782e48f25b9dc5c51c4a5'),\n              'email': 'Lucas.Lewis.0271@company.com',\n              'employeeId': 854271626.0,\n              'manager': {'email': 'Daniel.Wilson.0474@company.com',\n                          'id': 328892725.0,\n                          'name': {'first': 'Daniel', 'last': 'Wilson'}},\n              'name': {'first': 'Lucas', 'last': 'Lewis'},\n              'password': '<password>',\n              'role': 'PM',\n              'salary': 200000}}"
                },
                {
                    "lang": "none",
                    "value": "{\n  callServiceFunction: <function name>,\n  service: <service name>,\n  arguments: [<arg1>, <arg2>, ...]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "> db.runCommand({\n... callServiceFunction: \"get\",\n... service: \"http\",\n... arguments: [{ url: \"https://jsonplaceholder.typicode.com/todos/1\" }]\n... });\n{\n       \"ok\" : 1,\n       \"response\" : {\n               \"status\" : \"200 OK\",\n               \"statusCode\" : 200,\n               \"contentLength\" : NumberLong(-1),\n               \"headers\" : {\n                       \"Content-Type\" : [\"application/json; charset=utf-8\"],\n                       \"Connection\" : [\"keep-alive\"],\n                       \"Vary\" : [\"Origin, Accept-Encoding\"],\n                       \"X-Content-Type-Options\" : [\"nosniff\"],\n                       \"Via\" : [\"1.1 vegur\"],\n                       \"X-Powered-By\" : [\"Express\"],\n                       \"Cf-Cache-Status\" : [\"HIT\"],\n                       \"Expect-Ct\" : [\"max-age=604800, report-uri=\\\"https://example.com/cdn-cgi/beacon/expect-ct\\\"\"],\n                       \"Set-Cookie\" : [\"__cfduid=d7f650e765d41beb7598ce2ab62d0c0191536867096; expires=Fri, 13-Sep-19 19:31:36 GMT; path=/; domain=.typicode.com; HttpOnly\"],\n                       \"Access-Control-Allow-Credentials\" : [\"true\"],\n                       \"Cache-Control\" : [\"public, max-age=14400\"],\n                       \"Pragma\" : [\"no-cache\"],\n                       \"Etag\" : [\"W/\\\"53-hfEnumeNh6YirfjyjaujcOPPT+s\\\"\"],\n                       \"Server\" : [\"example.com\"],\n                       \"Cf-Ray\" : [\"459d08f88e1e56db-IAD\"],\n                       \"Date\" : [\"Thu, 13 Sep 2018 19:31:36 GMT\"],\n                       \"Expires\" : [\"Thu, 13 Sep 2018 23:31:36 GMT\"]\n                },\n                \"cookies\" : {\n                       \"__cfduid\" : {\n                               \"value\" : \"d7f650e765d41beb7598ce2ab62d0c0191536867096\",\n                               \"path\" : \"/\",\n                               \"domain\" : \".typicode.com\",\n                               \"expires\" : \"Mon, 01 Jan 0001 00:00:00 GMT\",\n                               \"maxAge\" : 0,\n                               \"secure\" : false,\n                               \"httpOnly\" : true\n                       }\n                },\n                \"body\" : BinData(0,\"ewogICJ1c2VySWQiOiAxLAogICJpZCI6IDEsCiAgInRpdGxlIjogImRlbGVjdHVzIGF1dCBhdXRlbSIsCiAgImNvbXBsZXRlZCI6IGZhbHNlCn0=\")\n       }\n}"
                },
                {
                    "lang": "cpp",
                    "value": "db.runCommand({\n  callServiceFunction: \"get\",\n  service: \"http\",\n  arguments: [{ url: \"https://jsonplaceholder.typicode.com/todos/1\" }]\n});"
                },
                {
                    "lang": "python",
                    "value": ">>> result = db.command(\"callServiceFunction\", \"get\",\n...    service=\"http\",\n...    arguments=[{\"url\": \"https://jsonplaceholder.typicode.com/todos/1\"}]\n...)\n>>> pprint.pprint(result)\n{'ok': 1,\n 'response': {'body': b'{\\n  \"userId\": 1,\\n  \"id\": 1,\\n  \"title\": \"delectus aut'\n                      b' autem\",\\n  \"completed\": false\\n}',\n              'contentLength': -1,\n              'cookies': {'__cfduid': {'domain': '.typicode.com',\n                                       'expires': 'Mon, 01 Jan 0001 00:00:00 '\n                                                  'GMT',\n                                       'httpOnly': True,\n                                       'maxAge': 0,\n                                       'path': '/',\n                                       'secure': False,\n                                       'value': 'd4b10004e96ca7fee0be03dceebaf2ab71536866400'}},\n              'headers': {'Access-Control-Allow-Credentials': ['true'],\n                          'Cache-Control': ['public, max-age=14400'],\n                          'Cf-Cache-Status': ['HIT'],\n                          'Cf-Ray': ['459cf7fc7e20c1bd-IAD'],\n                          'Connection': ['keep-alive'],\n                          'Content-Type': ['application/json; charset=utf-8'],\n                          'Date': ['Thu, 13 Sep 2018 19:20:00 GMT'],\n                          'Etag': ['W/\"53-hfEnumeNh6YirfjyjaujcOPPT+s\"'],\n                          'Expect-Ct': ['max-age=604800, '\n                                        'report-uri=\"https://example.com/cdn-cgi/beacon/expect-ct\"'],\n              'Expires': ['Thu, 13 Sep 2018 23:20:00 GMT'],\n              'Pragma': ['no-cache'],\n              'Server': ['example.com'],\n              'Set-Cookie': ['__cfduid=d4b10004e96ca7fee0be03dceebaf2ab71536866400; '\n                             'expires=Fri, 13-Sep-19 19:20:00 GMT; '\n                             'path=/; domain=.typicode.com; '\n                             'HttpOnly'],\n              'Vary': ['Origin, Accept-Encoding'],\n              'Via': ['1.1 vegur'],\n              'X-Content-Type-Options': ['nosniff'],\n              'X-Powered-By': ['Express']},\n  'status': '200 OK',\n  'statusCode': 200}}"
                },
                {
                    "lang": "none",
                    "value": "{\n  userProfile: 1\n}"
                },
                {
                    "lang": "javascript",
                    "value": "> db.runCommand({ userProfile: 1 });\n{\n       \"ok\" : 1,\n       \"profile\" : {\n               \"userid\" : \"5ad7a79e8f25b975898d77b8\",\n               \"domainid\" : ObjectId(\"5ad7a69746224c054067c8b1\"),\n               \"identities\" : [\n                       {\n\n                       }\n               ],\n               \"data\" : \"{\\\"email\\\":\\\"joe.mango@company.com\\\"}\",\n               \"type\" : \"normal\",\n               \"roleassignments\" : [ ]\n       }\n}"
                },
                {
                    "lang": "cpp",
                    "value": "db.runCommand({ userProfile: 1 });"
                },
                {
                    "lang": "python",
                    "value": ">>> result = db.command(\"userProfile\", 1)\n>>> pprint.pprint(result)\n{'ok': 1,\n 'profile': {'data': '{\"email\":\"joe.mango@company.com\"}',\n             'domainid': ObjectId('5ad7a69746224c054067c8b1'),\n             'identities': [{}],\n             'roleassignments': [],\n             'type': 'normal',\n             'userid': '5ad7a79e8f25b975898d77b8'}}"
                }
            ],
            "preview": "Atlas App Services natively implements a subset of the MongoDB wire protocol,\nwhich allows you to connect to an App through one of its linked\nMongoDB Atlas data sources using standard MongoDB\ndrivers and tools. Clients use a specialized App Services\nconnection string to connect and send requests.\nApp Services supports most client features over the wire protocol,\nincluding role-based data access rules,\nfunctions, and service actions.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/metrics",
            "title": "App Services Metrics Reference",
            "headings": [
                "Metric Format",
                "Units",
                "Available Metrics"
            ],
            "paragraphs": "App Services provides a variety of metrics related to App Services and Device\nSync. You can access your metrics data with the Admin API's\n Retrieve App Services metrics  endpoint. The exact metrics returned may be subject to change. When you request metrics, you specify a start time, end time, and time\ngranularity such as \"hourly\". The response contains a list of  measurement \nobjects, one for each metric listed below. Each measurement object contains a\nlist of  data points  that represent that metric's value at a specific time. For example, you request hourly metrics from 12:00am to 11:59pm Tuesday. App\nServices sends back a list of measurements. Each measurement contains about 24\ndata points (one for each hour between start and end time). The value field of\neach data point is the value of that metric at the time specified by the\ntimestamp. App Services expresses each metric in one of the following units: A  scalar  is a singular value representing a magnitude or count. Its exact\ninterpretation depends on the metric. A  scalar per second  represents a count over time (\"rate\"). The value is\neffectively the total count since the previous data point divided by the number\nof seconds since the previous data point. Some scalar per second values are expressed as a 95th percentile. This means\nthat the measurement for the given metric is below the given value 95% of the time\nand above it the other 5% of the time. BYTES_PER_SECOND MILLISECONDS SCALAR SCALAR_PER_SECOND Metric Name Unit Description ACTIVE_OPEN_SYNC_SESSIONS SCALAR The number of active, open sync sessions at the time of the data point. AUTH_EGRESS_BYTES BYTES_PER_SECOND The rate of data transferred from the server due to\nauthentication requests. AUTH_FAILED_REQUESTS SCALAR_PER_SECOND The rate of failed authentication requests. AUTH_RESPONSE_MS MILLISECONDS 95th percentile authentication response time. AUTH_SUCCESSFUL_LOGIN SCALAR_PER_SECOND The rate of successful logins. AUTH_SUCCESSFUL_REQUESTS SCALAR_PER_SECOND The rate of successful authentication requests. AUTH_TOTAL_USERS SCALAR The number of existing users at the time of the data point. ENDPOINTS_COMPUTE_MS SCALAR_PER_SECOND The number of milliseconds spent computing endpoint requests since the\nlast data point divided by the total number of seconds since the last data\npoint. ENDPOINTS_EGRESS_BYTES BYTES_PER_SECOND The rate of data transferred from the server due to endpoints\nrequests. ENDPOINTS_FAILED_REQUEST SCALAR_PER_SECOND The rate of failed endpoints requests. ENDPOINTS_RESPONSE_MS MILLISECONDS 95th percentile endpoint response time. ENDPOINTS_SUCCESSFUL_REQUESTS SCALAR_PER_SECOND The rate of successful endpoint requests. GRAPHQL_RESPONSE_MS MILLISECONDS 95th percentile GraphQL response time. GRAPHQL_COMPUTE_MS SCALAR_PER_SECOND The number of milliseconds spent computing GraphQL requests since the last\ndata point divided by the total number of seconds since the last data\npoint. GRAPHQL_EGRESS_BYTES BYTES_PER_SECOND The rate of data transferred from the server for GraphQL. GRAPHQL_FAILED_REQUESTS SCALAR_PER_SECOND The rate of failed GraphQL requests. GRAPHQL_SUCCESSFUL_REQUESTS SCALAR_PER_SECOND The rate of successful GraphQL requests. LF_RESPONSE_MS MILLISECONDS 95th percentile Log Forwarder (LF) response time. OVERALL_COMPUTE_MS SCALAR_PER_SECOND The number of milliseconds spent computing overall since the last data\npoint divided by the total number of seconds since the last data point. OVERALL_EGRESS_BYTES BYTES_PER_SECOND The rate of data transferred from the server overall. OVERALL_FAILED_REQUESTS SCALAR_PER_SECOND Overall rate of failed requests. OVERALL_SUCCESSFUL_REQUESTS SCALAR_PER_SECOND Overall rate of successful requests. OVERALL_SYNC_MINUTES SCALAR_PER_SECOND The total minutes spent syncing since the last data point divided by the\nnumber of seconds since the last data point. SDK_COMPUTE_MS SCALAR_PER_SECOND The number of milliseconds spent computing client SDK functions calls\nsince the last data point divided by the total number of seconds since the\nlast data point. SDK_EGRESS_BYTES BYTES_PER_SECOND The rate of data transferred from the server due to SDK functions calls. SDK_FAILED_REQUESTS SCALAR_PER_SECOND The rate of failed function call requests from client SDKs. SDK_FNS_RESPONSE_MS MILLISECONDS 95th percentile response time for client SDK function calls. SDK_MQL_COMPUTE_MS SCALAR_PER_SECOND The number of milliseconds spent computing client SDK remote MongoDB\naccess requests since the last data point divided by the total number of\nseconds since the last data point. SDK_MQL_EGRESS_BYTES BYTES_PER_SECOND The rate of data transferred from the server due to client SDK remote\nMongoDB access requests. SDK_MQL_FAILED_REQUESTS SCALAR_PER_SECOND The rate of failed client SDK remote MongoDB access requests. SDK_MQL_RESPONSE_MS MILLISECONDS 95th percentile client SDK remote MongoDB access response time. SDK_MQL_SUCCESSFUL_REQUESTS SCALAR_PER_SECOND The rate of successful client SDK remote MongoDB access requests. SDK_SUCCESSFUL_REQUESTS SCALAR_PER_SECOND The rate of successful function call requests from client SDKs. SYNC_CLIENT_BOOTSTRAP_MS MILLISECONDS 95th percentile Device Sync client bootstrap time. SYNC_CLIENT_UPLOADS_INVALID SCALAR_PER_SECOND Device Sync client invalid uploads. SYNC_CURRENT_OPLOG_LAG_MS_SUM MILLISECONDS The sum of Device Sync oplog lag in milliseconds. SYNC_EGRESS_BYTES BYTES_PER_SECOND The rate of data transferred from the server for Device Sync. SYNC_FAILED_REQUESTS SCALAR_PER_SECOND The rate of failed Device Sync requests. SYNC_HISTORY_WRITE_MS MILLISECONDS 95th percentile Device Sync history write time. SYNC_MINUTES SCALAR_PER_SECOND The total minutes spent syncing since the last data point divided by the\nnumber of seconds since the last data point. Equivalent to\nOVERALL_SYNC_MINUTES. SYNC_NUM_INTEGRATION_ATTEMPTS SCALAR_PER_SECOND Number of Device Sync integration attempts. SYNC_NUM_UNSYNCABLE_DOC SCALAR The number of unsyncable documents at the time of the data point. SYNC_OT_MS MILLISECONDS 95th percentile time spent doing operational transform (OT) for\nDevice Sync. SYNC_SESSIONS_ENDED SCALAR_PER_SECOND The rate of ended Device Sync sessions. SYNC_SESSIONS_STARTED SCALAR_PER_SECOND The rate of started Device Sync sessions. SYNC_SUCCESSFUL_REQUESTS SCALAR_PER_SECOND The rate of successful Device Sync requests. SYNC_UPLOAD_PROPS_MS MILLISECONDS 95th percentile Device Sync upload propagation time. TRIGGERS_COMPUTE_MS SCALAR_PER_SECOND The number of milliseconds spent computing for Triggers since the last\ndata point divided by the total number of seconds since the last data\npoint. TRIGGERS_CURRENT_OPLOG_LAG_MS_SUM MILLISECONDS Sum of Triggers oplog lag. TRIGGERS_EGRESS_BYTES BYTES_PER_SECOND The rate of data transferred from the server for Triggers. TRIGGERS_FAILED_REQUESTS SCALAR_PER_SECOND The rate of failed Triggers requests. TRIGGERS_RESPONSE_MS MILLISECONDS 95th percentile Triggers response time. TRIGGERS_SUCCESSFUL_REQUESTS SCALAR_PER_SECOND The rate of successful Triggers requests.",
            "code": [],
            "preview": "App Services provides a variety of metrics related to App Services and Device\nSync. You can access your metrics data with the Admin API's\nRetrieve App Services metrics endpoint.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/upgrade-shared-cluster",
            "title": "Upgrade a Shared Tier Cluster",
            "headings": [
                "Overview",
                "Procedure",
                "Shut Down All Running Emulators",
                "Terminate Atlas Device Sync",
                "Upgrade Your Cluster"
            ],
            "paragraphs": "Atlas App Services allows you to upgrade your shared tier cluster ( M0 ,  M2 , and\n M5 ) to a dedicated cluster. Upgrade your cluster before releasing an Atlas Device Sync\napplication by completing the following steps. Using a shared tier cluster on a production application is not recommended.\nTo avoid data loss, upgrade to a dedicated tier cluster before releasing your\napplication. When you terminate and re-enable Atlas Device Sync, clients can no longer Sync.\nYour client must implement a client reset handler to restore Sync. This\nhandler can discard or attempt to recover unsynchronized changes. Client Reset - Flutter SDK Client Reset - Java SDK Client Reset - Kotlin SDK Client Reset - .NET SDK Client Reset - Node SDK Client Reset - React Native SDK Client Reset - Swift SDK Before upgrading your cluster, end any client application's Sync Connection.\nTo do this, shut down all emulators, simulators, and terminals that are running\nyour realm application. To terminate Atlas Device Sync, follow the instructions for  Terminating Atlas Device Sync . Click the  Upgrade  green button under the  Enhance Your\nExperience  heading of the  Data Services  tab. Configure your upgraded cluster settings in the following screen and then\nclick  Review Changes . To learn more about the pricing of the various\nAtlas cluster tiers, see the  billing  page. Review your upgraded cluster changes, and click the green  Apply Changes  button.\nA blue header will appear, indicating the progress of the cluster migration operation. Finally, reenable  Atlas Device Sync  to resume your\napplication development, testing, and roll-out.",
            "code": [],
            "preview": "Atlas App Services allows you to upgrade your shared tier cluster (M0, M2, and\nM5) to a dedicated cluster. Upgrade your cluster before releasing an Atlas Device Sync\napplication by completing the following steps.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/services",
            "title": "Third-Party Services [Deprecated]",
            "headings": [
                "Introduction",
                "Concepts",
                "Service Clients [Deprecated]",
                "Service Actions [Deprecated]",
                "Service Rules [Deprecated]",
                "Incoming Webhooks [Deprecated]",
                "Guides",
                "Built-In Services",
                "Reference Documentation"
            ],
            "paragraphs": "Third party services and push notifications in App Services have been deprecated\nin favor of creating HTTP endpoints that use  external dependencies  in functions. Webhooks have been renamed to  HTTPS Endpoints  with no change in behavior. You should\n migrate  existing Webhooks. Existing services will continue to work until  November 1, 2024 . Because third party services and push notifications are now deprecated, they have\nbeen removed by default from the App Services UI. If you need to manage an existing third party\nservice or push notification, you can add the configurations back to the UI by doing\nthe following: In the left navigation, under the  Manage  section, click\n App Settings . Enable the toggle switch next to\n Temporarily Re-Enable 3rd Party Services , and then save your\nchanges. Modern applications often use multiple external services to handle complex use\ncases, such as messaging, analytics, and data management. You can connect to\nthese services through Atlas App Services by creating and configuring service\ninterfaces. Service interfaces specify the connection details for a specific\nexternal service and enable you to define the scope of that service's\ncapabilities with custom  service rules .\nOnce you've configured an interface for a service, you can instantiate a\n service client  that connects to the interface\nand exposes the service's  actions  as methods.\nYou can also create  incoming webhooks  that\nenable external services to communicate directly with your App\nover HTTP. App Services includes clients for certain external services, such\nas  Twilio  and  AWS .\nYou can also use the  HTTP service  to create a\ncustom interface for any external service that offers a REST API. A service client is an object that connects to a service interface and\nenables you to call  actions  associated with\nthe service. You can instantiate service clients in  functions  (using  function context ) as well as in your client\napplication code. For details on instantiating and using service clients, see  Call a\nService Action . A service action is a method that handles a specific use\ncase for a particular service, such as  sending a text\nmessage  with Twilio or\n putting an object  to an AWS S3 bucket.\nActions encapsulate implementation details like request\nauthentication and HTTP methods behind semantic methods that\nare specific to each service. When you  call a service action ,\nApp Services evaluates each  service rule  that\napplies to the action and prevents execution unless one of the rules\nevaluates to  true . App Services blocks all service actions by default.  You must\n configure a service rule  that enables a\nparticular action before you can call it. A service rule is an  expression  that App Services evaluates\nto determine whether or not a user can execute one or more  actions  in a service. For example, you could create a  Twilio  rule\nthat only lets users send a text message from a specific phone number or\nan  AWS  rule that prevents users from putting\nobjects to an S3 bucket that is not included in a list of approved\nbuckets. Expression variables  are variables that you can include\nin service rules to represent dynamic information about your application and\nan action's execution. You can configure service rules based on the\nauthenticated user that called an action ( %%user ) and the\narguments that they provided ( %%args ). You can also create\ncomplex rules that  call a Function \n( %function ) and evaluate based on the Function's return\nvalue. An incoming webhook is a custom handler for events that originate from\nan external service, such as when someone opens a new pull request on\n GitHub  or sends a text message to a\n Twilio  phone number. Get started with incoming\nwebhooks by  configuring a service webhook . Incoming webhooks consist of two primary components: the webhook URL and\nthe webhook Function. Component Description Webhook URL A URL that uniquely identifies the incoming webhook. External\nservices can interact with the webhook by sending an HTTP request\nthat matches the webhook's configuration to the webhook URL. To use a webhook, provide the webhook URL to an external\nservice's HTTP request handler, which may also be referred to as\nan outgoing webhook, callback URL, or similar. If an incoming webhook requires a  secret query parameter , make sure that you append the\nquery parameter to webhook URL before you provide it to the\nexternal service. Webhook Function A webhook function is an  Atlas Function  that\naccepts an incoming HTTP request with data from the external\nservice as its argument and optionally returns an HTTP response. Guide Description Configure Third-Party Services Learn how to create a new external service interface. Configure Service Webhooks Learn how to configure and execute an incoming webhook to handle\nevents in external services. Configure Service Rules Learn how to safely expose a service action for use in a function\nor client application. Call a Service Action Learn how to call a service action from a function or client\napplication. Subject Description Twilio Service Includes service configuration parameters, directions for adding\na webhook to Twilio, and additional information about Twilio\nservice actions. HTTP Service Includes additional information about HTTP service actions and\nwebhooks. AWS Service Includes service configuration parameters, additional information\nabout specifc AWS service actions, and generic directions for\nconnecting to any AWS service. GitHub Service Includes service configuration parameters, directions for\nadding a webhook to GitHub, and guidance on validating incoming\nrequests from GitHub. Subject Description Webhook Requests & Responses Describes how to verify incoming requests,\nparse a request payload, and send a response in service webhook\nfunctions.",
            "code": [],
            "preview": "Modern applications often use multiple external services to handle complex use\ncases, such as messaging, analytics, and data management. You can connect to\nthese services through Atlas App Services by creating and configuring service\ninterfaces.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config",
            "title": "App Configuration",
            "headings": [],
            "paragraphs": "Every component of an Atlas App Services App is fully defined and configured\nusing structured JSON configuration and JavaScript source code files. You'll work directly with configuration files if you prefer to develop and\n deploy  locally instead of through the App Services UI. A complete directory of configuration files has the following root-level files\nand directories: For detailed descriptions and examples of each component type's configuration\nand source code files, refer to the type's page in this section: Static Hosting is deprecated.  Learn More . .json  files define and configure specific components in your app. Every\nJSON configuration file conforms to a specific schema for the type of\ncomponent it configures. .js  files define serverless application logic used in functions, triggers,\nHTTPS endpoints, and custom resolvers. You can include arbitrarily named files in your realm app config, with a few exceptions.\nThe following filenames are used to define and configure components in your App:\n stitch.json ,  config.json ,  realm_config.json , and  app_config.json .\nFiles with these names must conform to a specific schema for their component type.\nUsing these file names when the file's content fails to conform to that schema\nthrows an error when you import the app using the CLI. Refer to this section for reference information about configuring these files. App Services App Users & Authentication Providers MongoDB Data Sources Environment Values Functions GraphQL Static Hosting HTTPS Endpoints Log Forwarders Atlas Device Sync Triggers Values",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u251c\u2500\u2500 root_config.json\n\u251c\u2500\u2500 auth/\n\u251c\u2500\u2500 data_sources/\n\u251c\u2500\u2500 environments/\n\u251c\u2500\u2500 functions/\n\u251c\u2500\u2500 graphql/\n\u251c\u2500\u2500 hosting/\n\u251c\u2500\u2500 https_endpoints/\n\u251c\u2500\u2500 log_forwarders/\n\u251c\u2500\u2500 sync/\n\u251c\u2500\u2500 triggers/\n\u2514\u2500\u2500 values/"
                }
            ],
            "preview": "Every component of an Atlas App Services App is fully defined and configured\nusing structured JSON configuration and JavaScript source code files.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "mongodb/crud-and-aggregation-apis",
            "title": "CRUD & Aggregation APIs",
            "headings": [
                "Overview",
                "CRUD Operations",
                "Query Operators",
                "Comparison Operator Availability",
                "Logical Operator Availability",
                "Array Operator Availability",
                "Element Operator Availability",
                "Evaluation Operator Availability",
                "Geospatial Operator Availability",
                "Bitwise Operator Availability",
                "Update Operators",
                "Field Update Operator Availability",
                "Array Update Operator Availability",
                "Array Update Operator Modifiers Availability",
                "Bitwise Update Operator Availability",
                "Bulk Write Operations",
                "Ordered Bulk Write Operation Availability",
                "Unordered Bulk Write Operation Availability",
                "Query Options",
                "Query Option Availability",
                "Aggregation",
                "Aggregation Methods",
                "Aggregation Pipeline Stage Availability",
                "Aggregation Pipeline Operator Availability",
                "Database Commands",
                "Database Command Availability"
            ],
            "paragraphs": "This page lists  MongoDB data source  support for\nMongoDB commands, operators, options, and aggregation stages. For information on how to read and write data in MongoDB from a\nfunction, see  Query MongoDB Atlas . For more information on supported features and limitations, see\n Service Limitations . Support for an operation may depend on whether you're running the\noperation as an application user or as a system user. A  user function  runs in the context of a\nspecific application user and enforces data access permissions and\nschema validation, which makes some operations untenable. A  system function  runs as a generic\nsystem user and is not subject to rules or schema validation. Some\noperations are only available in system functions. Atlas App Services supports most  query operators  for queries that run in a\n user function . However, some evaluation\noperators and all geospatial and bitwise operators are only available\nin  system functions . Operation User Function System Function $eq Yes Yes $gt Yes Yes $gte Yes Yes $in Yes Yes $lt Yes Yes $lte Yes Yes $ne Yes Yes $nin Yes Yes Operation User Function System Function $and Yes Yes $not Yes Yes $nor Yes Yes $or Yes Yes Operation User Function System Function $all Yes Yes $elemMatch Yes Yes $size Yes Yes Operation User Function System Function $exists Yes Yes $type Yes Yes Operation User Function System Function $mod Yes Yes $expr No Yes $jsonSchema No Yes $regex No Yes $text No Yes $where No Yes Operation User Function System Function $geoIntersects No Yes $geoWithin No Yes $near No Yes $nearSphere No Yes Operation User Function System Function $bitsAllClear No Yes $bitsAllSet No Yes $bitsAnyClear No Yes $bitsAnySet No Yes App Services supports most  update operators  for queries that run in a\n user function . However, some array update\noperators are only available in  system functions . Operation User Function System Function $currentDate Yes Yes $inc Yes Yes $min Yes Yes $max Yes Yes $mul Yes Yes $rename Yes Yes $set Yes Yes $setOnInsert Yes Yes $unset Yes Yes Operation User Function System Function $ (Positional Update) Yes Yes $addToSet Yes Yes $pop Yes Yes $pull Yes Yes $push Yes Yes $pullAll Yes Yes $[] (All Positional Update) Yes Yes $[element] (Filtered Positional Update) Yes Yes Modifier User Function System Function $each Yes Yes $position Yes Yes $slice Yes Yes $sort Yes Yes Operator User Function System Function $bit Yes Yes App Services supports bulk write operations using the same API as the\n MongoDB Node.js driver . App Services also provides the  collection.bulkWrite() \nmethod for performing bulk write operations. You define ordered bulk operations by calling\n collection.initializeOrderedBulkOp()  and manipulating the\n OrderedBulkOperation  object that it\nreturns. App Services supports the following  OrderedBulkOperation  methods: Operation User Function System Function execute Yes Yes find Yes Yes insert Yes Yes You define unordered bulk operations by calling\n collection.initializeUnorderedBulkOp()  and manipulating the\n OrderedBulkOperation  object that it\nreturns. App Services supports the following  UnorderedBulkOperation  methods: Operation User Function System Function execute Yes Yes find Yes Yes insert Yes Yes App Services does not support configuring the following options for any\nCRUD operation in  user functions . All query\noptions are available in  system functions . Option User Context System Context Read Isolation (Read Concern) No Yes Write Acknowledgment (Write Concern) No Yes Collation No Yes App Services supports aggregation on the both the database and collection level\nusing the following commands: db.aggregate() db.collection.aggregate() App Services does not support the following  aggregation pipeline stages  when you  run an aggregation\npipeline  in the context of an\n application user . All aggregation pipeline stages are\navailable to the system user except for  $indexStats . Stage User Context System Context $collStats No Yes $currentOp No Yes $facet No Yes $geoNear No Yes $graphLookup No Yes $indexStats No No $lookup Yes Yes $merge No Yes $out No Yes $search App Services performs  $search  operations as a system user and\nenforces field-level rules on the returned search results. This means that a\nuser may search on a field for which they do not have read access. In this\ncase, the search is based on the specified field but no returned documents\ninclude the field. The  $$SEARCH_META \naggregation variable is only available for functions that  run as system  or if the first role on the searched collection has its\n apply_when  and  read  expressions set to  true . If neither of these two scenarios apply,  $$SEARCH_META  is undefined and\nthe aggregation will fail. Yes Yes $unionWith Yes Yes By default,  $merge  and  $out  route the entire aggregation operation to\nthe cluster's primary node. If you want to force these stages to respect your\nconfigured cluster read preference, set the  enforceReadPref  option to\n true . App Services supports all  aggregation pipeline operators  when you  run an aggregation pipeline  in the  system user  context. App Services supports all pipeline operators\nin an  application user  context with the following\nexceptions: Operator User Context System Context $function No Yes App Services does not support any  database commands  in the Client SDKs or  Functions . You can, however, call a limited subset of database\ncommands when connected to a MongoDB cluster over the  App Services\nwire protocol . The following  database commands  are\npartially supported over the wire protocol. Unsupported options for each\ncommand are listed below. App Services Apps cannot run commands on the  admin  database. Command Unsupported Options App Services-Specific Options find hint skip batchSize comment maxScan maxTimeMS readConcern max min returnKey showRecordId tailable awaitData oplogReplay noCursorTimeout allowPartialResults collation aggregate explain allowDiskUse   readConcern collation bypassDocumentValidation hint comment writeConcern enforceReadPref count limit skip hint readConcern insert writeConcern bypassDocumentValidation update bypassDocumentValidation collation delete collation App Services supports  allowDiskUse  when run in a Function or\nTrigger with  system-level permissions .",
            "code": [],
            "preview": "This page lists MongoDB data source support for\nMongoDB commands, operators, options, and aggregation stages.",
            "tags": null,
            "facets": {
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/service-limitations",
            "title": "Service Limitations",
            "headings": [
                "Overview",
                "Aggregation",
                "Atlas",
                "Batch Loading",
                "Federated database instances",
                "Serverless Instances",
                "Sharded Clusters",
                "Time Series Collections",
                "Private Endpoints",
                "Change Streams",
                "Database Commands",
                "Device Sync",
                "Concurrent Workloads",
                "Default Message Size Limit",
                "Pause Due to Inactivity",
                "MongoDB Version Requirements",
                "HTTP Version Requirement",
                "Query Options",
                "Query Results",
                "Request Traffic",
                "Connection Pooling",
                "Static Hosting"
            ],
            "paragraphs": "Atlas App Services has several guidelines to keep in mind when architecting how\nyour tools and clients interact with MongoDB through App Services.\nKeep this guidance in mind when deciding how to structure queries,\nselecting which CRUD and aggregation operations to use, and determining\nhow to handle concurrent workloads. App Services supports all  aggregation pipeline\nstages  in\n system functions  except for  $currentOp \nand  $indexStats . For security reasons, only a subset of aggregation pipeline stages are\nsupported in  user functions . For a list of pipeline stages that are available and their allowed function\ncontext, see  Aggregation . When data is bulk/batch loaded into MongoDB Atlas, you may see a delay in\ndata appearing on devices while App Services processes changes. You can link a  Federated database instance  to your app\nas a MongoDB data source. However, there are some caveats to keep in\nmind when working with Atlas Data Federation: Federated data sources  do not support write operations . You can only access a Federated data source from a  system function . You cannot connect to a Federated data source via the  wire protocol . You cannot define  roles and permissions  for a Federated data source. You cannot set a  read preference  for a Federated data source. You cannot create a  database trigger  on a Federated data source. You cannot use a Federated data source as your app's  Device Sync  cluster. You can link a  serverless instance  to\nyour app as a MongoDB data source. However, serverless instances do not\ncurrently support change streams, so the following features are limited: You cannot create a  database trigger  on a serverless instance. You cannot use a serverless instance as your app's  Device Sync  cluster. You cannot watch collections for changes data sources that are serverless MongoDB Atlas instances. Atlas Device Sync does not currently support sharded MongoDB Atlas clusters. However,\nyou can add a sharded cluster as a data source and access it from a\n function . Time series collections  do not\nyet support  change streams . Therefore,\nyou cannot define triggers on a time series collection. You can use time series collections with Atlas Device Sync, but only\n Data Ingest  is supported. App Services supports private endpoints for securely connecting to an\nAWS Virtual Private Cloud with AWS PrivateLink. Apps must be deployed\nlocally to a region in AWS to use a private endpoint. App Services does\nnot support private endpoints for Global Apps or Apps deployed to GCP or\nAzure. For more information, see  Private Endpoints . App Services limits the total number of change streams open against a given\ncluster across all Apps based on the cluster's size. The following table lists\nthe limitations for each cluster size: Cluster Size Maximum Number of Change Streams Free Tier ( M0 ) 5 Shared Clusters ( M2 / M5 ) 10 Small, Dedicated Clusters ( M10 / M20 ) 100 Standard Clusters ( M30 / M40 ) 1000 Standard Clusters ( M50  -  M90 ) 1000 High-Power Clusters ( M100+ ) 1000 App Services opens a single change stream on each collection that is\nassociated with a  Database Trigger  or\n Device Sync  operation. To minimize the number of open change streams: Start Sync operations only when necessary. Close any open\nSync streams immediately when they're no longer necessary. Avoid data models that require you to Sync an unbounded\nnumber of collections. App Services does not support any  database commands  in the Client SDKs or  Functions . You can, however, call a limited subset of database\ncommands when when connected to a MongoDB cluster over the  wire\nprotocol . For a list of supported commands, see\n Database Commands . You can have up to 5,000 concurrent Sync\nconnections. See  Request Traffic . For improved performance, you may want to limit the number of concurrent writes\nto a single MongoDB document. App Services has no hard limit to the number of\nusers concurrently writing data to a MongoDB document. However, with more than\n30 concurrent writers, you may see delays in syncing or conflict resolution. Frontend clients reading from a global realm can scale past tens\nof thousands of concurrent users. When you intend to have many concurrent writers, have each writer\nwork on a separate document. Device Sync has a default message size limit of 20 MB\ncompressed for any messages sent from the client application. The message size\ndepends on the compressed size of changes sent from the client device. For\nmore information about changesets and the compression that Sync uses, refer\nto  Changeset . The compressed size of a changeset is not directly equivalent to the size of\nany document in the upload. In practice, however, the changeset size is\ngenerally on the same order of magnitude of the size of documents in the\nupload. This limit acts as a countermeasure against connections consuming too many\nresources. You may see this limit as an error message similar to  failed to read: read\nlimited at 20971521 bytes . You can request a higher limit by  filing a support ticket . After 30 days with no activity, Device Sync\npauses. Refer to  Re-Enable Sync  for information about how to re-enable\nDevice Sync. You can access most of the CRUD and Aggregation functionality of MongoDB\nversion 3.6 with the  MongoDB service ; however, App Services\ndoes not support all operations and features available in standard tools\nand clients. For a list of specific MongoDB operations that\nare available when you connect to MongoDB through App Services, see\n the CRUD & Aggregation API reference . Device Sync requires MongoDB Atlas clusters to run specific versions of MongoDB.\nFlexible Sync requires MongoDB 5.0.0 or greater. Data API and HTTPS Endpoints require HTTP/1.1 or greater when making requests. App Services supports all query options in  system functions . App Services does not support certain options for\nCRUD operations run from  user functions . For a\nlist of specific options that are available when you connect to MongoDB through\nApp Services, see  Query Options . MongoDB queries executed through App Services can return a maximum of 50,000\ndocuments. If you need to return more documents, consider paginating\nyour query. App Services limits request traffic to the following defaults: App Services can handle requests many times the above limits. However, these limits are\nput in place to ensure applications scale responsibly and to prevent DOS attacks\nand unintended billing charges. You can request a higher limit by  filing a support ticket . 10,000 concurrent requests. Any requests made beyond\nthe non-Sync concurrent request limit receive an HTTP response status\ncode of  429 - Too Many Requests . 5,000 concurrent Device Sync connections. App Services uses connection pooling to reduce the overhead of frequently\nopening and closing connections between requests and trigger executions.\nConnections are opened as needed. Connection pooling is dependent on\nseveral factors: Cluster Tier. The higher the cluster tier, the more connections available in\nthe pool. Deployment Mode. Global deployments use multiple servers in each region, and\ntherefore have an overall larger connection pool. Services. Each service has an independent connection pool, so the number of\nservices in your app does not impact the number of connections available. App Services enforces a 25MB maximum file size constraint on\n static hosting . Static Hosting is deprecated.  Learn More .",
            "code": [],
            "preview": "Follow these guidelines to avoid issues related to service limitations.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/third-party-licenses",
            "title": "Third-Party Licenses",
            "headings": [
                "Overview",
                "IntelRDFPMathLib20U2"
            ],
            "paragraphs": "Atlas App Services uses third-party libraries or other resources that may be\ndistributed under licenses different than the MongoDB software.\nApp Services depends upon the following third-party packages. These\npackages are licensed as shown in the following list. The Intel\u00ae Decimal Floating-Point Math Library uses the following\nlicense:",
            "code": [
                {
                    "lang": "text",
                    "value": "Copyright (c) 2018, Intel Corp.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n  * Redistributions of source code must retain the above copyright notice, this\n    list of conditions and the following disclaimer.\n  * Redistributions in binary form must reproduce the above copyright notice,\n    his list of conditions and the following disclaimer in the documentation\n    and/or other materials provided with the distribution.\n  * Neither the name of Intel Corporation nor the names of its contributors\n    may be used to endorse or promote products derived from this software\n    without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED.\nIN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,\nINDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\nBUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\nLIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\nOR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\nADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
                }
            ],
            "preview": "Atlas App Services uses third-party libraries or other resources that may be\ndistributed under licenses different than the MongoDB software.\nApp Services depends upon the following third-party packages. These\npackages are licensed as shown in the following list.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/push-notifications",
            "title": "Push Notifications [Deprecated]",
            "headings": [
                "Overview",
                "Management Interface",
                "Send a Push Notification",
                "Set Up Clients to Receive Push Notifications",
                "Rule Templates",
                "Notifications Must Have a Specific Title",
                "Notifications Must Be for a Specific Topic",
                "Notifications Can Only Be Sent To a Limited Set of User IDs",
                "Push Notification Rules",
                "Notification Document Fields"
            ],
            "paragraphs": "Third party services and push notifications in App Services have been deprecated\nin favor of creating HTTP endpoints that use  external dependencies  in functions. Webhooks have been renamed to  HTTPS Endpoints  with no change in behavior. You should\n migrate  existing Webhooks. Existing services will continue to work until  November 1, 2024 . Because third party services and push notifications are now deprecated, they have\nbeen removed by default from the App Services UI. If you need to manage an existing third party\nservice or push notification, you can add the configurations back to the UI by doing\nthe following: In the left navigation, under the  Manage  section, click\n App Settings . Enable the toggle switch next to\n Temporarily Re-Enable 3rd Party Services , and then save your\nchanges. Atlas App Services supports integrating mobile (Android and iOS) apps with\nthe Firebase Cloud Messaging (FCM) service to provide push\nnotifications. You configure and send notification messages from within\nthe App Services console, while clients register with  Firebase Cloud\nMessaging (FCM)  for messages sent to specific topics. The  Push Notifications  screen in the App Services UI lets you\nwrite notification drafts, send notifications to users, and define\n notification rules . There are four tabs in the  Push Notifications  screen: Tab Name Description Draft In the Draft tab, you can view a list of the draft messages. For\nthe draft messages, you can: Duplicate draft messages Edit draft messages Delete draft messages Sent The Sent tab shows the messages that App Services has already sent to registered clients. From here, you can view a list of the sent messages. For the sent messages, you can: Resend sent messages. Duplicate sent messages. Config In the Config tab, you provide the  FCM (Firebase Cloud\nMessaging)  credentials (Sender ID and Legacy API key). Rules You can optionally specify  Push Notification Rules  to restrict the types of push notifications that your application sends. You can send new Push Notifications from the  Push\nNotifications  page. Click the  Send New Notification  button\nto open the  Send New Notification  dialog. In this dialog,\nyou provide the notification message, label, and the relevant topic\naudience. The general process of integrating Push Notifications in an Android\napplication is as follows: To see the latest information and detailed steps on FCM integration with Android Clients, see the official  Set up a Firebase Cloud Messaging client app on Android  guide. Add a dependency for  FCM (Firebase Cloud Messaging) . Create an instance of  Push . Use the  push.registerDevice() \nor  push.registerDeviceAsync() \nmethods to register the client for push notifications. Use FirebaseMessaging's  subscribeToTopic() \nto subscribe to topics. Create a class that extends the abstract\n FirebaseMessagingService \nand implements the  onMessageReceived() \nmethod. This class handles communication with  FCM\n(Firebase Cloud Messaging) . Update  AndroidManifest.xml  file for your Android project to\nregister your  FirebaseMessagingService -derived class and a receiver\nfor your application. To use FCM with App Services in an iOS app, follow these\ngeneral steps: To see the latest information and detailed steps on FCM integration with iOS Clients, see the official  Set up a Firebase Cloud Messaging client app on iOS  guide. Create a valid Apple Push Notification service (APNs) certificate through the  Apple Developer Member center . Copy the  GoogleService-Info.plist  file to your XCode project. Configure FCM and add your iOS app. When you register your app, FCM will return a registration token. This template calls an example function named\n allUserIdsAreValid  that does the following: Accepts the list of User IDs provided in the  userIds  argument Queries MongoDB for a user document that matches the current user's id Compares the provided phone number to the number listed in the user document Returns the boolean result of the comparison To specify rules for Push Notifications, click on the  Rules \ntab on the  Push Notifications  page. The Push Notifications rules permit the following arguments. You can\naccess them with the  \"%%args\"  expansion: Unlike other services in App Services, rules for push notifications\nare optional. By default, all push notifications are allowed.\nHowever, once you specify a rule, the restrictions imposed by that\nrule will take effect. Field Type Description userIds Array of strings. The user ids of the message recipients. to String The recipient of the message. The value can be a device's registration\ntoken, a device group's notification key, or a single topic (prefixed\nwith  /topics/ ). registrationTokens Array of strings. The list of registration tokens for the devices receiving the\nmulticast message. priority string The priority of the notification. Value is either  \"high\"  or\n \"normal\" . Corresponds to the  priority  option for HTTP\nJSON messages via FCM. See  the FCM HTTP protocol reference . collapseKey string The collapse key associated with collapsible messages.\nCorresponds to the  collapse_key  option for HTTP JSON\nmessages via FCM. See  the FCM HTTP protocol reference . contentAvailable boolean A flag that determines whether to awake idle client apps upon\nreceipt of the message. Corresponds to the  content_available \noption for HTTP JSON messages via FCM. See  the FCM\nHTTP protocol reference . mutableContent boolean A flag that determines whether the notification content can be\nmodified before being displayed to the user. Corresponds to the\n mutable_content  option for HTTP JSON messages via FCM.\nSee  the FCM HTTP protocol reference . timeToLive int Maximum time (in milliseconds) to retain the message if the\ndevice is offline. Valid value range from 0 to 2419200. Corresponds to the  time_to_live  option for HTTP JSON\nmessages via FCM. See  the FCM HTTP protocol reference . data JSON document Payload for data message.  data  document consists of custom\nkey-value pairs. Corresponds to the  data  option for HTTP\nJSON messages via FCM. See  the FCM HTTP protocol reference . notification JSON document Payload for notification.  notification  document consists of\n predefined fields . Corresponds to\nthe  notification  option for HTTP JSON messages via FCM.\nSee  the FCM HTTP protocol reference . The following table lists the fields of the  notification  document\nthat is available as a permitted field for Push Notifications rules. To\naccess one of these fields in a rule, use\n \"%%args.notification.<field>\" . Field Type Description title string The title of the notification. Corresponds to the  title \noption for HTTP JSON messages via FCM. See  the FCM\nHTTP protocol reference . body string The body of the notification. Corresponds to the  body \noption for HTTP JSON messages via FCM. See  the FCM\nHTTP protocol reference . sound string The sound to play upon receipt of the notification. Corresponds\nto the  sound  option for HTTP JSON messages via FCM. See\n the FCM HTTP protocol reference . clickAction string The action to take when a user click on the notification.\nCorresponds to the  click_action  option for HTTP JSON\nmessages via FCM. See  the FCM HTTP protocol reference . bodyLocKey string The key for localization of the body string. Corresponds to the\n body_loc_key  option for HTTP JSON messages via FCM. See\n the FCM HTTP protocol reference . bodyLocArgs string The string values to replace format specifiers for localization\nin the body string. Corresponds to the  body_loc_args  option\nfor HTTP JSON messages via FCM. See  the FCM\nHTTP protocol reference . titleLocKey string The key for localization of the title string. Corresponds to the\n title_loc_key  option for HTTP JSON messages via FCM. See\n the FCM HTTP protocol reference . titleLocArgs string The string values to replace format specifiers for localization\nin the title string. Corresponds to the  title_loc_args  option\nfor HTTP JSON messages via FCM. See  the FCM HTTP protocol\nreference . icon string For Android only. The notification icon. Corresponds to the\n icon  option for HTTP JSON messages via FCM. See\n the FCM HTTP protocol reference . color string For Android only. Indicates the icon color in #rrggbb format.\nCorresponds to the  color  option for HTTP JSON messages\nvia FCM. See  the FCM HTTP protocol reference . tag string For Android only. If specified, each notification does not\nresult in a new entry but replaces an existing entry with the\nspecified tag. If unset, each notificaiton results in a new\nentry. Corresponds to the  tag  option for HTTP JSON\nmessages via FCM. See  the FCM HTTP protocol reference . badge string For iOS only. The badge on the gclient app home icon. Corresponds to\nthe  badge  option for HTTP JSON messages via FCM. See\n the FCM HTTP protocol reference .",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.notification.title\": \"Test Notification Please Ignore\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.to\": \"%%values.validTopics\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%true\": {\n    \"%function\": {\n      \"name\": \"allUserIdsAreValid\",\n      \"arguments\": [\n        \"%%args.userIds\"\n      ]\n    }\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function(toPhone) {\n  const mdb = context.services.get('mongodb-atlas');\n  const users = mdb.db('demo').collection('users');\n  const user = users.findOne({ _id: context.user.id });\n  return user.phoneNumber === toPhone;\n}"
                }
            ],
            "preview": "Atlas App Services supports integrating mobile (Android and iOS) apps with\nthe Firebase Cloud Messaging (FCM) service to provide push\nnotifications. You configure and send notification messages from within\nthe App Services console, while clients register with Firebase Cloud\nMessaging (FCM) for messages sent to specific topics.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/hosting",
            "title": "Static Hosting Configuration Files",
            "headings": [
                "Hosting Configuration",
                "File Metadata"
            ],
            "paragraphs": "Static Hosting is deprecated.  Learn More . You can enable and configure  static file hosting  for your\napplication in  hosting/config.json . Field Name Description If  true ,  static hosting  is enabled for your app. The  custom domain name  for\nyour application's hosted files. The default domain for your application's hosted files. Atlas App Services\nautomatically sets this value and you cannot change it. You can define metadata for any hosted file by adding an entry to\n hosting/metadata.json . Field Description The  resource path  of the file. An array of documents where each document represents a single\nmetadata attribute. Attribute documents have the following form: Field Description The name of the metadata attribute. This should be one of\nthe  file metadata attributes  that App Services supports. The value of the metadata attribute. If you do not specify a  Content-Type  metadata attribute for a hosted\nfile, Atlas App Services will attempt to automatically add a  Content-Type \nattribute to it based on the file extension. For example, App Services would automatically add the attribute\n Content-Type: application/html  to the file  myPage.html .",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 hosting/\n    \u251c\u2500\u2500 config.json\n    \u251c\u2500\u2500 metadata.json\n    \u2514\u2500\u2500 files/\n        \u2514\u2500\u2500 <files to host>"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"enabled\": <Boolean>,\n  \"custom_domain\": \"<Custom Domain Name>\",\n  \"app_default_domain\": \"<Default Domain Name>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    \"path\": \"<File Resource Path>\",\n    \"attrs\": [\n      {\n        \"name\": \"<Attribute Type>\",\n        \"value\": \"<Attribute Value>\"\n      },\n      ...\n    ]\n  },\n  ...\n]"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Attribute Type>\",\n  \"value\": \"<Attribute Value>\"\n}"
                }
            ],
            "preview": "You can enable and configure static file hosting for your\napplication in hosting/config.json.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/template-apps",
            "title": "Template Apps",
            "headings": [
                "Overview",
                "Create a Template App",
                "Get a Template App Client",
                "Template Apps Available"
            ],
            "paragraphs": "You can get up and running with an Atlas App Services App using one of our premade template apps.\nTemplate apps bring together many of the building blocks available in Atlas App Services\nand start you off with a prebuilt app that you can customize. You can create a template app using the UI on the Atlas App Services\nsite, the Realm CLI, or the Admin API. Choose the tab for whichever\noption is most convenient for you. You can create a template App using the same command as you would\nuse to create a blank App. To learn how to create an App Services\nApp, see  Create an App  and follow the\ninstructions for a template App. After you create a template app, the UI includes a\n Welcome to Your Application  section that offers\nseveral resources for getting started: Information about what your chosen template app includes. On-screen guides to customize your app. Template app client code that you can download as a  .zip  file. You can create a template App using the same command as you would\nuse to create a blank App. To learn how to create an App Services\nApp, see  Create an App  and follow the\ninstructions for a template App. The command must include the  --template  flag with a valid\ntemplate App ID value: You can create a template App using the same endpoint as you would\nuse to create a blank App. To learn how to create an App Services\nApp, see  Create an App  and follow the\ninstructions for a template App. Your Admin API request must include a valid  template_id  value\nin the request body. Some of the template apps come with working clients to explore and build\non when creating your applications. You can access these clients through\nthe Atlas App Services UI, Realm CLI, or GitHub. When you download client code through the App Services UI, the client\ncode is not pre-populated with your App Services App ID. To use\none of these clients, you must  find your App ID \nto use in the client. Follow the  Configuration  and\n Download the Client as a Zip File  instructions in the client\n README.md  to learn where to insert your App ID. Download the client when you create the template app When you create a template app using the App Services UI, the UI\nprompts you with two options to get the client code immediately\nafter creating the template: After selecting the  .zip  or App Services CLI method, follow the on-screen\ninstructions to get the client code. Download the client at some point after creating the template app If you don't download the client code when you first create the app,\nyou can download it later. Your App Services App\ndisplays a \"Welcome to Your Application\" pane on the main dashboard.\nThis pane contains a button labeled  </> Pull front-end code .\nWhen you click this button, you see the same dialogue as when you\nfirst create an app, which gives you the option to download the client\nas a  .zip  or pull it with the App Services CLI. Download your preferred client as a  .zip  file. Use the App Services CLI to pull your preferred client to a local directory. When you download client code through App Services CLI, the client code is\npre-populated with your App Services App ID. Download the client when you create the template app When you use App Services CLI to create the template app, it automatically\ncreates a directory wherever you run  the create command  that contains both the backend and client\ncode. Alternately, you can use the  --local  option to specify a\ndirectory where it should download the app code. The directory name is the name of your app. Inside this directory,\nyou'll see a  backend  directory that contains the App Services code,\nand a  frontend  directory that contains the client application code. Download the client at some point after creating the template app At any point after creating a template app, you can use the App Services CLI\nto download the client code. Use the  App Services CLI pull command  with the  --template  option to specify which\nclient template you want to download. Use  the ID of an available template below \nthat offers a client application. The Device Sync template app clients are available in GitHub. If you\nwant just the client code without the backend code, explore the\nrelevant GitHub repository for your preferred framework or language: If you clone one of these repositories, the client code is not\npre-populated with your App Services App ID. To use one of these\nclients, you must  create a template app \nand  find your App ID  to use in the client.\nFollow the  Configuration  and  Cloning from GitHub \ninstructions in the client  README.md  to learn where to insert\nyour App ID. Flutter Device Sync Client Kotlin Device Sync Client MAUI Device Sync Client React Native Device Sync Client SwiftUI Device Sync Client The following templates are available. The Realm CLI accepts the following IDs\nto the  --template  flag of the  appservices apps create  and\n appservices pull  commands. ID Name Description Client flex-sync-guides.add-collaborators Flexible Sync Permissions Guide: 'Add Collaborators' Demo Demo with Node.js client and App Services App that implements the\n Dynamic Collaboration permissions strategy \nfor Device Sync using Flexible Sync. None flex-sync-guides.restricted-feed Flexible Sync Permissions Guide: 'Restricted Feed' Demo Demo with Node.js client and App Services App that implements the\n Restricted News Feed permissions strategy \nfor Device Sync using Flexible Sync. None flex-sync-guides.tiered Flexible Sync Permissions Guide: 'Tiered' Demo Demo with Node.js client and App Services App that implements the\n Tiered Privileges permissions strategy \nfor Device Sync using Flexible Sync. None flutter.todo.flex Realm Flutter SDK + Atlas Device Sync Starter Cross-platform to-do list app for Android, iOS, Windows, MacOS, and Linux written in Dart using the  Realm Flutter SDK . Syncs local data to MongoDB Atlas using Device Sync with  Flexible Sync . Flutter kotlin.todo.flex Kotlin SDK + Atlas Device Sync Starter Android to-do list app written in Kotlin using the  Realm Kotlin SDK . Syncs local data to MongoDB Atlas using Device Sync with  Flexible Sync . Kotlin maui.todo.flex MAUI + Atlas Device Sync Starter Cross-platform to-do list mobile app using the  Realm C# SDK . Syncs local data to MongoDB Atlas using Device Sync with  Flexible Sync . MAUI react-native.todo.flex React Native + Atlas Device Sync Starter Cross-platform to-do list mobile app using the  Realm JS SDK  and the  @realm/react  library. Syncs local data to MongoDB Atlas using Device Sync with  Flexible Sync . Realm React swiftui.todo.flex SwiftUI + Atlas Device Sync Starter iOS to-do list app using SwiftUI and the  Realm Swift SDK . Syncs local data to MongoDB Atlas using Device Sync with  Flexible Sync . SwiftUI sync.todo Flexible Sync Todo App Backend Backend-only App that contains the configuration used by the  <Language or Framework>.todo.flex  template apps. None triggers Manage Database Views Event-driven  Database Trigger  template to update a view in a separate collection. None web.mql.todo Realm Web SDK + React Starter Hosted to-do list web app using the  Realm Web SDK . React/ MQL web.data-api.todo Atlas Data API + React Starter Hosted to-do list app using the  Data API . React/ HTTPS",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices apps create \\\n  --name \"<App Name>\" \\\n  --template \"<Template App ID>\""
                },
                {
                    "lang": "bash",
                    "value": "curl -X POST \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps \\\n  -H 'Authorization: Bearer <access_token>' \\\n  -d '{\n    \"name\": \"<App Name>\",\n    \"template_id\": \"<Template App ID>\",\n    \"data_source\": {\n      \"name\": \"mongodb-atlas\",\n      \"type\": \"mongodb-atlas\",\n      \"config\": {\n        \"clusterName\": \"<Atlas Cluster Name>\"\n      }\n    }\n  }'"
                },
                {
                    "lang": "shell",
                    "value": "appservices apps create -n \"<App Name>\" --template \"<Chosen Template App ID>\""
                },
                {
                    "lang": "shell",
                    "value": "appservices pull --remote \"<App Name>\" --template \"<Chosen Template App ID>\""
                }
            ],
            "preview": "You can get up and running with an Atlas App Services App using one of our premade template apps.\nTemplate apps bring together many of the building blocks available in Atlas App Services\nand start you off with a prebuilt app that you can customize.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/environments",
            "title": "Environment Value Configuration Files",
            "headings": [
                "Environment Configuration"
            ],
            "paragraphs": "You can define variable values for each  environment \nin a  .json  file within the  /environments  directory that uses the\nenvironment name as its file name. Atlas App Services supports the following environments: \"\" \"development\" \"testing\" \"qa\" \"production\" Field Description An object where each property maps the name of an environment value name\nto its value in the current environment.",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 environments/\n    \u251c\u2500\u2500 no-environment.json\n    \u251c\u2500\u2500 development.json\n    \u251c\u2500\u2500 testing.json\n    \u251c\u2500\u2500 qa.json\n    \u2514\u2500\u2500 production.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"values\": {\n    \"<Value Name>\": <Value>\n  }\n}"
                }
            ],
            "preview": "You can define variable values for each environment\nin a .json file within the /environments directory that uses the\nenvironment name as its file name.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/graphql",
            "title": "GraphQL Configuration Files",
            "headings": [
                "Service Configuration",
                "Custom Resolver Configuration"
            ],
            "paragraphs": "You can configure the  GraphQL API  for your application in\nthe  graphql  directory: GraphQL is deprecated.  Learn More . Field Description true  by default for new apps. You can only set the value to  false  when you  create a new app with the Admin API endpoint \nand pass the property into the body of the request. The value\ncannot be changed to  false  once the app is created. If  true , generated schema type names use common English\npluralization whenever possible. If  false , or if a natural pluralization cannot be determined,\nthen plural types use the singular type name with an  \"s\" \nappended to the end. App Services can use either a natural plural or a default plural for a\ngenerated \"mouse\" type: Natural: \"mice\" Default: \"mouses\" This value is  false  by default for new apps. If  true , the GraphQL API blocks  introspection\nqueries  from clients. This setting is useful for production Apps that do not want to\nexpose their GraphQL schema to the public. When introspection is\ndisabled, clients like GraphiQL cannot show docs for the API\nschema or autocomplete queries and mutations. Field Description The parent type that exposes the custom resolver as one of its fields. Valid Options: \"Query\"  (for  custom query  operations) \"Mutation\"  (for  custom mutation  operations) A singular  exposed  type name (for  computed properties ) The name of the field on the parent type that exposes the custom\nresolver. The field name must be unique among all custom resolver on its\nparent type. If the field name matches a field in the parent type's schema, the custom\nresolver overrides the schema type. The name of the  function  that runs when the\nresolver is called. The function arguments may accept a single argument\n(configured by  input_type  and  input_type_format ) and must return\na payload value (configured by  payload_type  and\n payload_type_format ). The type of the resolver's  input  argument (if it accepts input). You\ncan specify either the name of another type in your GraphQL schema or a\ncustom JSON schema specific to the resolver. A metadata description of the  input_type . Valid Options: \"scalar\"  (for a single value of a specific BSON type) \"scalar-list\"  (for multiple values of a specific BSON type) \"generated\"  (for a single value of a specific  exposed  type) \"generated-list\"  (for multiple values of a specific  exposed  type) \"custom\"  (for a custom JSON schema) The type of the value returned in the resolver's payload. You can specify\neither the name of another type in your GraphQL schema or a custom JSON\nschema specific to the resolver. If you do not specify a payload type, the resolver returns a\n DefaultPayload  object: A metadata description of the  payload_type . Valid Options: \"scalar\"  (for a single value of a specific BSON type) \"scalar-list\"  (for multiple values of a specific BSON type) \"generated\"  (for a single value of a specific  exposed  type) \"generated-list\"  (for multiple values of a specific  exposed  type) \"custom\"  (for a custom JSON schema)",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 graphql/\n    \u251c\u2500\u2500 config.json\n    \u2514\u2500\u2500 custom_resolvers\n        \u2514\u2500\u2500 <resolver name>.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"use_natural_pluralization\": <Boolean>,\n  \"disable_schema_introspection\": <Boolean>\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"on_type\": \"<Parent Type Name>\",\n  \"field_name\": \"<Resolver Field Name>\",\n  \"function_name\": \"<Resolver Function Name>\",\n  \"input_type\": \"<Input Type Name>\" | { <JSON Schema> },\n  \"input_type_format\": \"<Input Type Format>\",\n  \"payload_type\": \"<Payload Type Name>\" | { <JSON Schema> },\n  \"payload_type_format\": \"<Payload Type Format>\",\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type DefaultPayload {\n  status: String!\n}\n"
                }
            ],
            "preview": "You can configure the GraphQL API for your application in\nthe graphql directory:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/sync",
            "title": "Atlas Device Sync Configuration Files",
            "headings": [
                "Sync Configuration"
            ],
            "paragraphs": "You can configure  Atlas Device Sync  for your application in the  sync \ndirectory: Field Description The sync mode. There are two Sync modes: Flexible Sync and the older\nPartition-Based Sync. We recommend using Flexible Sync. For more\ninformation about Partition-Based Sync, refer to\n Partition-Based Sync . Valid Options for a Flexible Sync Configuration: \"flexible\" If  true ,  Development Mode  is enabled\nfor the application. While enabled, App Services automatically stores synced\nobjects in a specific database (specified in  database_name ) and\nmirrors objects types in that database's collection schemas. The name of the Atlas cluster  data source \nto sync. You cannot use sync with a  serverless instance . The name of a database in the synced cluster where App Services stores data in\n Development Mode . App Services automatically\ngenerates a schema for each synced type and maps each object type to a\ncollection within the database. The current state of the sync protocol for the application. Valid Options: \"enabled\" \"disabled\" The number of days that the  backend compaction \nprocess waits before aggressively pruning metadata that some clients\nrequire to synchronize from an old version of a realm. If  false ,  Recovery Mode  is enabled\nfor the application. While enabled, Realm SDKs that support this feature\nattempt to recover unsynced changes upon performing a client reset.\nRecovery mode is enabled by default. A list of field names to use as  global queryable fields . A list of field names to use as the  indexed queryable field . While this property is an array,\nSync currently supports only one one indexed queryable field.\nTherefore, this array may contain at most one element. The indexed queryable field must be present in the schema and be\nthe same  eligible field type  in every collection you\nsync. The indexed queryable field name must  also  appear in\n queryable_fields_names  since this is a global queryable\nfield. A map from collection names to a list of  collection-level\nqueryable fields  for each collection. The date and time that sync was last paused or disabled, represented by\nthe number of seconds since the Unix epoch (January 1, 1970, 00:00:00\nUTC).",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 sync/\n    \u2514\u2500\u2500 config.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"flexible\",\n  \"development_mode_enabled\": <Boolean>,\n  \"service_name\": \"<Data Source Name>\",\n  \"database_name\": \"<Development Mode Database Name>\",\n  \"state\": <\"enabled\" | \"disabled\">,\n  \"client_max_offline_days\": <Number>,\n  \"is_recovery_mode_disabled\": <Boolean>,\n  \"queryable_fields_names\": [\"<Field Name>\", ...],\n  \"indexed_queryable_fields_names\": [\"<Field Name>\", ...],\n  \"collection_queryable_fields_names\": {\n    \"<Collection Name>\": [\"<Field Name>\", ...],\n    ...\n  }\n}"
                }
            ],
            "preview": "You can configure Atlas Device Sync for your application in the sync\ndirectory:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/partition-based-sync",
            "title": "Partition-Based Sync",
            "headings": [
                "Overview",
                "Key Terms",
                "Partition",
                "Partition Key",
                "Change a Partition Key",
                "Partition Value",
                "Enable Partition-Based Sync",
                "Before You Begin",
                "Procedure",
                "Navigate to the Sync Configuration Screen",
                "Select Development Details",
                "Choose a Partition Key",
                "Define Read & Write Permissions",
                "Check Default Advanced Configuration Behaviors",
                "Turn On Sync",
                "Authenticate a MongoDB Atlas User",
                "Pull the Latest Version of Your App",
                "Add a Sync Configuration",
                "Choose a Partition Key",
                "Define Read & Write Permissions",
                "Specify Values for Sync Optimization",
                "Deploy the Sync Configuration",
                "Select a Cluster to Sync",
                "Choose a Partition Key",
                "Define Read & Write Permissions",
                "Specify Values for Sync Optimization",
                "Deploy the Sync Configuration",
                "Partition-Based Rules and Permissions",
                "Partition-Based Sync Rule Behavior",
                "Partition-Based Sync Permissions",
                "Key Concepts",
                "Read Permissions",
                "Write Permissions",
                "Permission Strategies",
                "Global Permissions",
                "Permissions for Specific Partitions",
                "Permissions for Specific Users",
                "Permissions Based on User Data",
                "Function Rules",
                "Migrate Partition-Based Sync Rules to App Services Rules",
                "Partition Strategies",
                "Firehose Strategy",
                "User Strategy",
                "Team Strategy",
                "Channel Strategy",
                "Region Strategy",
                "Bucket Strategy",
                "Data Ingest",
                "Partition-Based Sync Configuration",
                "Alter Your Partition-Based Sync Configuration",
                "Partition-Based Sync Errors",
                "Backend Compaction"
            ],
            "paragraphs": "Atlas Device Sync has two sync modes:  Flexible Sync  and\nthe older Partition-Based Sync. We recommend using Flexible Sync for all new\napps. The information on this page is for users who are still using\nPartition-Based Sync. In  Partition-Based Sync , documents in a synced\ncluster form a \"partition\" by having the same value for a field designated as\n\"partition key\". All documents in a partition have the same read/write\npermissions for a given user. In Partition-Based Sync, you define a partition key whose value determines\nwhether the user can read or write a document. App Services evaluates a set of\nrules to determine whether users can read or write to a given partition. App\nServices directly maps a partition to individual synced  .realm  files. Each\nobject in a synced realm has a corresponding document in the partition. In the client, you pass a value for the partition key when opening a synced\nrealm. App Services then syncs objects whose partition key value matches the\nvalue passed in from the client application. Device Sync requires MongoDB Atlas clusters to run specific versions of MongoDB.\nPartition-Based Sync requires MongoDB 4.4.0 or greater. Consider an inventory management application using Partition-Based Sync. If\nyou use  store_number  as the partition key, each store can read and write\ndocuments pertaining to its inventory. An example of the permissions for this type of app might be: Store employees could have read and write access to documents whose store\nnumber is Store 42. Customers, meanwhile, could have read-only access to store inventory. Based on the store inventory example above, the SDK might pass in\n store42  as the  partitionValue  in the sync configuration.\nThis would sync any InventoryItem whose  partitionValue  was\n store42 . You can use  custom user data  to indicate\nwhether a logged-in user is a store employee or a customer. Store\nemployees would have read and write permission for the  store42 \ndata set, while customers would have only read permission for the\nsame data set. A  partition  is a collection of objects that share the same partition key\nvalue. A MongoDB Atlas cluster consists of several remote servers. These servers\nprovide the storage for your synced data. The Atlas cluster stores\ndocuments in collections. Each MongoDB collection maps to a different Realm\nobject type. Each document in a collection represents a specific Realm object. A synced realm is a local file on a device. A synced realm may contain\nsome or all the objects relevant to the end user. A client app\nmay use more than one synced realm to access all the objects that the\napplication needs. Partitions link objects in Realm Database to documents in MongoDB. When\nyou initialize a synced realm file, one of its parameters is a partition\nvalue. Your client app creates objects in the synced realm. When those\nobjects sync, the partition value becomes a field in the MongoDB documents.\nThe value of this field determines which MongoDB documents the client\ncan access. At a high level: A partition maps to a subset of the documents in a synced cluster. All documents in a partition have the same read/write permissions for a given user. Atlas App Services maps each partition to an individual synced  .realm  file. Each object in a synced realm has a corresponding document in the partition. Partitions shape data in an Atlas cluster. Each shape represents an object type.\nPartitions are determined by each shape's  color : red, green, or blue. A  partition key  is a named field that you specify when you\n configure Atlas Device Sync . Device Sync uses this key to\ndetermine which partition contains a given document. Depending on your data model and the complexity of your app, your partition\nkey could be either: You can make the partition key required or optional on your objects.\nApp Services maps any object that does not include a partition key to a default\npartition -  null partition . Consider the following when choosing a partition key: A property that already exists in each document and that logically\npartitions data. For example, consider an app where every document is\nprivate to a specific user. You might store the user's ID value in an\n owner_id  field and then use that as the partition key. A synthetic property that you create to partition data (e.g.\n _partition ). You might use this when your app does not include a\nnatural partition key. Partition keys must be one of these types:  String ,  ObjectID ,\n Long , or  UUID . App Services clients should never modify the partition value directly. You\ncannot use any field that clients can modify as a partition key. Partition keys use the same field name in every synced document. The\npartition key should not collide with any field name in any object's model. The following schemas demonstrate natural and synthetic partition keys: An app can only specify a single partition key, but either of these fields\ncould work depending on your use case: The  owner_id  field is a natural key because it's already part of the\napp's data model. The  _partition  field is a synthetic key because its only purpose is to\nserve as a partition key. Your app's partition key is a central part of a Sync-enabled app's data\nmodel. When you set a partition key in your Sync configuration, you cannot\nlater reassign the key's field. If you need to change a partition key, you\nmust terminate sync. Then, you can enable it again with the new partition\nkey. However, this requires all client applications to reset and sync data\ninto new realms. When you terminate and re-enable Atlas Device Sync, clients can no longer Sync.\nYour client must implement a client reset handler to restore Sync. This\nhandler can discard or attempt to recover unsynchronized changes. Client Reset - Flutter SDK Client Reset - Java SDK Client Reset - Kotlin SDK Client Reset - .NET SDK Client Reset - Node SDK Client Reset - React Native SDK Client Reset - Swift SDK A  partition value  is a value of the partition key field for a given\ndocument. Documents with the same partition value belong to the same\npartition. They sync to the same realm file, and share user-level data\naccess permissions. A partition's value is the identifier for its corresponding synced realm.\nYou specify the partition's value when you open it as a synced realm in\nthe client. If you change a partition value in Atlas, App Services interprets\nthe change as two operations: A delete from the old partition. An insert into the new partition. If you change a document's partition value, you lose client-side unsynced\nchanges to the object. You will need the following to enable Partition-Based Sync: An App Services App. To learn how to create an App, see  Create\nan App . A linked Atlas data source. To learn how to add a data source, see\n Link a Data Source . To define Device Sync rules and enable Device Sync for your application, navigate to the\n Device Sync  configuration screen through the left navigation menu. You define Device Sync rules at the same time as you enable Device Sync. Once Device Sync is\nenabled, you can't change your app's Device Sync rules unless you terminate Device Sync\nand re-enable it with new rules. You can enable Device Sync for a single linked cluster in your application.\nDetermine which cluster you want to use and then select it from the\n Select a Cluster To Sync  dropdown menu. The Device Sync partition key is a field in every synced document that maps\neach document to a client-side realm. Device Sync rules apply at the partition\nlevel, so it's particularly important to consider your data model and access\npatterns. For more information on partition keys and how to choose one, see\n Partitions . The partition key can either be required or optional. If the partition key\nfield is optional, then all MongoDB documents that either exclude the\npartition key or have a null value for the partition key will be sent to the\nnull partition. If the partition key field is required, Device Sync\nignores any MongoDB documents that lack a valid value for the partition\nkey. We recommend a required partition key unless you want to Device Sync\npre-existing data with invalid or missing partition values from a\nMongoDB collection. The  Define Permissions  section allows you to define  JSON\nexpressions  that App Services dynamically evaluates to determine a\nuser's read and write permissions for the data in a given partition. Device Sync rule expressions have access to  %%user , which resolves\nto the user that opened the realm, and  %%partition , which\nresolves to the partition key value for the realm. You can also use operators,\nincluding  %function , to handle more complex cases. For\nexamples, see  Role-based Permissions . Once you've determined how to decide a user's read and write permissions for a\ngiven partition, define the corresponding JSON expressions in the\n Read  and  Write  inputs. Expand the  Advanced Configuration  section. App Services Apps provide advanced optimization features\nthat are enabled by default: If you want to change the length of time that a client can be\noffline, or disable either of these features, you can do so\nin the  Advanced Configuration  section. Client Max Offline Time Client Recovery Now that you've configured rules for your synced cluster, all that's left to\ndo is turn on Device Sync for client applications. Click  Enable Sync ,\ntake note of any recommendations that appear, and then confirm your choice. Use your MongoDB Atlas Admin API Key to log in to the CLI: Get a local copy of your App's configuration files. To get the latest\nversion of your App, run the following command: You can also export a copy of your application's configuration files\nfrom the UI or with the Admin API. To learn how, see  Export an App . You can enable sync for a single linked cluster in your application. The App Services App has a  sync  directory where you can find\nthe  sync configuration file . If you have not\nyet enabled Sync, this directory is empty. Add a  config.json  similar to: The sync partition key is a field in every synced document that maps\neach document to a client-side realm. Sync rules apply at the partition level,\nso it's particularly important to consider your data model and access\npatterns. For more information on partition keys and how to choose one, see\n Partitions . Once you have decided which field to use, update  sync.partition \nwith the partition key field name in the  key  field and the partition key\ntype in the  type  field, similar to: The partition key can either be required or optional. If the partition key\nfield is optional, then all MongoDB documents that either exclude the\npartition key or have a null value for the partition key will be sent to the\nnull partition. If the partition key field is required, Device Sync\nignores any MongoDB documents that lack a valid value for the partition\nkey. We recommend a required partition key unless you want to sync\npre-existing data with invalid or missing partition values from a\nMongoDB collection. App Services allows you to define  JSON expressions  that it\ndynamically evaluates whenever a user opens a realm to determine if the user\nhas read or write permissions for data in the partition. Sync rule expressions have access to  %%user , which resolves\nto the user that opened the realm, and  %%partition , which\nresolves to the partition key value for the realm. You can also use operators,\nincluding  %function , to handle more complex cases. For\nexamples, see  Sync Permissions . Once you've determined how to decide a user's read and write permissions for a\ngiven partition, define the corresponding JSON expressions in the\n read  and  write  fields of  partition.permissions , similar to: Sync provides features that enable you to optimize Sync\nperformance and improve the data recovery process on the client.\nThese features are represented by additional settings: You can set a numerical value for  client_max_offline_days .\nWhen you enable Sync via the App Services UI, the default\nvalue is  30 , which represents 30 days. For more information, see:  Client Maximum Offline Time . Recovery Mode enables Sync to attempt to recover data that\nhas not yet been synced when a client reset occurs. When you\nenable Sync via the App Services UI, Recovery Mode is enabled\nby default. We recommend enabling Recovery Mode for automatic\nclient reset handling. If you have disabled recovery mode and\nwant to re-enable it, set  is_recovery_mode_disabled  to  false . For more information, see:  Recover Unsynced Changes . client_max_offline_days is_recovery_mode_disabled Now that you've defined the sync configuration, including read and write\npermissions, you can deploy your changes to start syncing data and enforcing\nsync rules. To deploy your changes, import your app configuration to your App Services App: You can enable Sync for a single linked cluster in your application. You'll need the cluster's service configuration file to configure sync. You\ncan find the configuration file by  listing all services\nthrough the Admin API : Identify the service whose configuration you need to update to enable\nSync. If you have accepted the default names when configuring\nyour App, this should be a service whose  name  is  mongodb-atlas \nand  type  is  mongodb-atlas . You need this service's  _id . Now you can  get the configuration file for\nthis service : Once you have the configuration, add the  sync  object with the\nfollowing template configuration: The sync partition key is a field in every synced document that maps\neach document to a client-side realm. Sync rules apply at the partition level,\nso it's particularly important to consider your data model and access\npatterns. For more information on partition keys and how to choose one, see\n Partitions . You can  get a list of all valid partition\nkeys  and their corresponding types\nthrough the Admin API: Once you have decided which field to use, update  sync.partition \nwith the partition key field name in the  key  field and the partition key\ntype in the  type  field. The partition key can either be required or optional. If the partition key\nfield is optional, then all MongoDB documents that either exclude the\npartition key or have a null value for the partition key will be sent to the\nnull partition. If the partition key field is required, Device Sync\nignores any MongoDB documents that lack a valid value for the partition\nkey. We recommend a required partition key unless you want to sync\npre-existing data with invalid or missing partition values from a\nMongoDB collection. App Services allows you to define  JSON expressions  that it\ndynamically evaluates whenever a user opens a realm to determine if the user\nhas read or write permissions for data in the partition. Sync rule expressions have access to  %%user , which resolves\nto the user that opened the realm, and  %%partition , which\nresolves to the partition key value for the realm. You can also use operators,\nincluding  %function , to handle more complex cases. For\nexamples, see  Sync Permissions . Once you've determined how to decide a user's read and write permissions for a\ngiven partition, define the corresponding JSON expressions in the\n read  and  write  fields of  sync.partition.permissions . Sync provides features that enable you to optimize Sync\nperformance and improve the client data recovery process. These\nfeatures are represented by additional settings: You can set a numerical value for  client_max_offline_days .\nWhen you enable Sync via the App Services UI, the default\nvalue is  30 , which represents 30 days. For more information, see:  Client Maximum Offline Time . Recovery Mode enables Sync to attempt to recover unsynced data\nwhen a client reset occurs. When you enable Sync via the App\nServices UI, Recovery Mode is enabled by default. We recommend\nenabling Recovery Mode for automatic client reset handling.\nTo enable Recovery Mode, set  is_recovery_mode_disabled  to\n false . For more information, see:  Recover Unsynced Changes . client_max_offline_days is_recovery_mode_disabled Now that you've defined the sync configuration, including read and write\npermissions, you can deploy your changes to start syncing data and enforcing\nsync rules. To deploy your changes, send an  Admin API request that updates the cluster\nconfiguration with your sync configuration : Whenever a user opens a synced realm from a client app, App Services evaluates your\napp's rules and determines if the user has read and write permissions for the\n partition . Users must have read permission to sync\nand read data in a realm and must have write permission to create, modify,\nor delete objects. Write permission implies read permission, so if a user\nhas write permission then they also have read permission. Sync rules apply to specific partitions and are coupled to your app's data model\nby the partition key. Consider the following behavior when designing\nyour schemas to ensure that you have appropriate data access granularity and\ndon't accidentally leak sensitive information. Sync rules apply dynamically based on the user. One user may have full read &\nwrite access to a partition while another user has only read permissions or is\nunable to access the partition entirely. You control these permissions by\ndefining  JSON expressions . Sync rules apply equally to all objects in a partition. If a user has read\nor write permission for a given partition then they can read or modify all\nsynced fields of any object in the partition. Write permissions require read permissions, so a user with write access to a\npartition also has read access regardless of the defined read permission\nrules. App Services enforces dynamic, user-specific read and write permissions to secure the\ndata in each  partition . You define permissions with\n JSON rule expressions  that control whether or not a given\nuser has read or write access to the data in a given partition. App Services evaluates\na user's permissions every time they open a synced realm. Your rule expressions can use JSON expansions like\n %%partition  and  %%user  to dynamically\ndetermine a user's permissions based on the context of their request. A user with read permissions for a given partition can view all fields of any\nobject in the corresponding synced realm. Read permissions do not permit a user\nto modify data. A user with write permissions for a given partition can modify the value of any\nfield of any object in the corresponding synced realm. Write permissions require\nread permissions, so any user that can modify data can also view that data\nbefore and after it's modified. You can structure your read and write permission expressions as a set of\npermission strategies that apply to your  partition strategy . The following strategies outline common approaches that\nyou might take to define sync read and write permissions for your app. In an app that uses\n Partition-Based Sync , an object can only have\na relationship with other objects  in the same partition . The objects can\nexist in different databases and collections (within the same cluster) as\nlong as the partition key value matches. You can define global permissions that apply to all users for all partitions.\nThis is, in essence, a choice to not implement user-specific permissions in\nfavor of universal read or write permissions that apply to all users. To define a global read or write permission, specify a boolean value or a\n JSON expression  that always evaluates to the same boolean\nvalue. Example Description The expression  true  means that all users have the given access\npermissions for all partitions. The expression  false  means that no users have the given access\npermissions for any partitions. This expression always evaluates to  true , so it's effectively the\nsame as directly specifying  true . You can define permissions that apply to a specific partition or a groups of\npartitions by explicitly specifying their partition values. Example Description This expression means that all users have the given access permissions\nfor data with a partition value of  \"Public\" . This expression means that all users have the given access permissions\nfor data with any of the specified partition values. You can define permissions that apply to a specific user or a group of users by\nexplicitly specifying their ID values. Example Description This expression means that the user with id\n \"5f4863e4d49bd2191ff1e623\"  has the given access permissions for data\nin any partition. This expression means that any user with one of the specified user ID\nvalues has the given access permissions for data in any partition. You can define permissions that apply to users based on specific data defined in\ntheir  custom user data  document, metadata fields, or\nother data from an authentication provider. Example Description This expression means that a user has read access to a\npartition if the partition value is listed in the  readPartitions \nfield of their custom user data. This expression means that a user has write access to a\npartition if the partition value is listed in the\n data.writePartitions  field of their user object. You can define complex dynamic permissions by evaluating a  function  that returns a boolean value. This is useful for permission schemes\nthat require you to access external systems or other custom logic that you\ncannot express solely in JSON expressions. Example Description This expression calls the  canReadPartition  function and\npasses in the partition value as its first and only argument. The\nfunction looks up the user's permissions for the partition from a MongoDB\ncollection and then returns a boolean that indicates if the user can read\ndata in the requested partition. If you migrate your Partition-Based Sync App to Flexible Sync, your data access\nrules also need to migrate. Some Partition-Based Sync rule strategies can't translate directly to App Services\nRules. You may need to manually migrate permissions that include: See the list of  Flexible Sync-compatible expansions \nfor all supported expansions. You should also check out the\n Device Sync Permissions Guide  for more\ninformation about how to work with permissions. %function  operator. Function rules  are not compatible with Flexible Sync\nand cannot be migrated. %%partition  expansion. App Services Rules do not have an equivalent expansion for\n %%partition , so it cannot be migrated. %or ,  %not ,  %nor ,  %and  expansions. These permissions   may   work, but there is enough nuance that you should test them\nto ensure expected behavior. Testing new permissions won't work on the App\nyou are migrating. You'll need a new app to test manually-migrated permissions. A  partition strategy  is a key/value pattern to divide objects into\npartitions. Different use cases call for different partition strategies.\nYou can compose partition strategies within the same app to form a larger\nstrategy. This enables you to handle arbitrarily complex use cases. Which strategy to use depends on your app's data model and access patterns.\nConsider an app with both public and private data. You might put some data,\nlike announcements, in a public partition. You might restrict other data,\nlike personal information, to privileged users. When developing a partition strategy, consider: Data Security:  Users may need different read and write access\npermissions to subsets of data. Consider the permissions users need for\ntypes of documents. A user has the same permissions for every document\nin a partition. Storage Capacity:  Your client apps may have limited on-device storage\non some devices and platforms. A partition strategy should take storage\nlimitations into account. Make sure a user can store their synced data\non their device. You can use a partition key name like  _partition  with a query string\nas the value. This lets you use multiple strategies in the same app.\nFor example: You can use functions and rule expressions to parse the string.\nSync can determine whether a user has access to a partition based on\nthe combined strategy. A firehose partition strategy groups all documents into a single partition.\nWith this structure, every user syncs all of your app's data to their device.\nThis approach is functionally a decision to not partition data. This works\nfor basic applications or small public data sets. One way to create a firehose is to set the partition key as an optional\nfield. Don't include a value for this field in any document. App Services maps\nany document that doesn't include a partition value to the  null partition . You can also set a  static partition value . This is when the partition\nvalue doesn't change based on the user or data. For example, realms\nacross all clients could use the partition value  \"MyPartitionValue\" . Data Security.  All data is public to clients with a realm that\nuses the partition. If a user has read or write access to the partition,\nthey can read or write  any  document. Storage Capacity.  Every user syncs every document in the partition.\nAll data must fit within your most restrictive storage limitation. Use\nthis strategy only when data sets are small and do not grow quickly. An app lets users browse scores and statistics for local high school\nbaseball games. Consider the following documents in the  games  and\n teams  collections: The total number of games is small. A small number of local teams only\nplay a few games each year. Most devices should be able to download all\ngame data for easy offline access. In this case, the firehose strategy\nis appropriate. The data is public and documents don't need a partition\nkey. The strategy maps the collections to the following realms: A user partition strategy groups private documents for each user. These\ndocuments go into a partition specific to that user. This works when each\ndocument has an owner, and nobody else needs the data. A username or ID that\nidentifies the owner makes a natural partition key. Data Security.  Data in a user partition is specific to a given user.\nIt may contain information private to that user. Each user syncs only\ntheir own partition. Other users cannot access documents in the partition. Storage Capacity.  Each user only syncs data from their own partition.\nTheir data must fit within their device's storage constraints. Use this\nstrategy only when each user has a manageable amount of data. A music streaming app stores data about playlists and song ratings for each\nuser. Consider the following documents in the  playlists  and  ratings \ncollections: Every document includes the  owner_id  field. This is a good partition\nkey for a user partition strategy. It naturally maps documents to\nindividual users. This limits the data on each device to playlists and\nratings for the device's user. The strategy maps the collections to the following realms: Users have read and write access to their user realm. This contains\nplaylists they have created and ratings that they've given to songs. Every user has read access to the realm for partition value  PUBLIC .\nThis contains playlists that are available to all users. A team partition strategy groups private documents shared by a team of users.\nA team might include a store location's employees or a band's members. Each\nteam has a partition specific to that group. All users in the team share\naccess and ownership of the team's documents. Data Security.  Data in a team partition is specific to a given team.\nIt may contain data private to the team but not to a member of the team.\nEach user syncs partitions for the teams they belong to. Users not in a\nteam cannot access documents in the team's partition. Storage Capacity:  Each user only syncs data from their own teams. The\ndata from a user's teams must fit within their device's storage constraints.\nUse this strategy when users belong to a manageable number of teams. If\nusers belong to many teams, the combined realms could contain a lot of\ndata. You may need to limit the number of team partitions synced at a\ntime. An app lets users create projects to collaborate with other users.\nConsider the following documents in the  projects \nand  tasks  collections: Every document includes the  owner_id  field. This is a good partition\nkey for a team partition strategy. It naturally maps documents to\nindividual teams. This limits the data on each device. Users only have\nprojects and tasks that are relevant to them. The strategy maps the collections to the following realms: Users have read and write access to partitions owned by teams where\nthey're a member. Data stored in a  teams  or  users  collection can map users to\nteam membership: A channel partition strategy groups documents for a common topic or domain.\nEach topic or domain has its own partition. Users can choose to access or\nsubscribe to specific channels. A name or ID may identify these channels\nin a public list. Data Security.  Data in a channel partition is specific to a topic or\narea. Users can choose to access these channels. You can limit a user's\naccess to a subset of channels. You could prevent users from accessing\nchannels entirely. When user has read or write permission for a channel,\nthey can access any document in the partition. Storage Capacity.  Users can choose to sync data from any allowed\nchannel. All data from a user's channels must fit within their device's\nstorage constraints. Use this strategy to partition public or semi-private\ndata sets. This strategy breaks up data sets that would not fit within\nstorage constraints. An app lets users create chatrooms based on topics. Users can search\nfor and join channels for any topic that interests them. Consider\nthese documents in the  chatrooms  and  messages  collections: Every document includes the  topic  field. This is a good partition\nkey for a channel partition strategy. It naturally maps documents to\nindividual channels. This reduces the data on each device. Data only\ncontains messages and metadata for channels the user has chosen. The strategy maps the collections to the following realms: Users have read and write access to chatrooms where they're subscribed.\nUsers can change or delete any message - even those sent by other users.\nTo limit write permissions, you could give users read-only access.\nThen, handle sending messages with a serverless function. Store the use's subscribed channels in either the  chatrooms \nor  users  collection: A region partition strategy groups documents related to a location or region.\nEach partition contains documents specific to those areas. Data Security.  Data is specific to a given geographic area. You can\nlimit a user's access to their current region. Alternately, give access\nto data on a region-by-region basis. Storage Capacity.  Storage needs vary depending on size and usage\npatterns for the region. Users might only sync data in their own region.\nThe data for any region should fit within a device's storage constraints.\nIf users sync multiple regions, partition into smaller subregions. This\nhelps avoid syncing unneeded data. An app lets users search for nearby restaurants and order from their\nmenus. Consider the following documents in the  restaurants \ncollection: Every document includes the  city  field. This is a good partition\nkey for a region partition strategy. It naturally maps documents to\nspecific physical areas. This limits data to messages and metadata\nfor a user's current city. Users have read access to restaurants in\ntheir current region. You could determine the user's region in your\napplication logic. The strategy maps the collections to the following realms: A bucket partition strategy groups documents by range. When documents range\nalong a dimension, a partition contains a subrange. Consider time-based\nbucket ranges. Triggers move documents to new partitions when they fall\nout of their bucket range. Data Security.  Limit users to read or write only specific buckets.\nData may flow between buckets. Consider access permissions for a document\nacross all possible buckets. Storage Capacity.  Storage needs vary based on size and usage patterns\nfor each bucket. Consider which buckets users need to access. Limit the\nsize of buckets to fit within a device's storage constraints. If users\nsync many buckets, partition into smaller buckets. This helps avoid\nsyncing unneeded data. An IoT app shows real-time views of sensor readings several times a\nsecond. Buckets derive from the number of seconds since the reading\ncame in. Consider these documents in the  readings  collection: Every document includes the  bucket  field. This field maps documents\nto specific time ranges. A user's device only contains sensor readings\nfor the window they view. The strategy maps the collections to the following realms: Users have read access to sensor readings for any time bucket. Sensors use client apps with write access to upload readings. Data Ingest  is a feature of Flexible Sync\nand cannot be enabled on apps that use Partition-Based Sync. When you use  Partition-Based Sync , your Atlas App Services\napp uses this Sync configuration: Field Description The sync mode. There are two Sync modes:  Flexible Sync  and the older Partition-Based Sync. Valid Options for a Partition-Based Sync Configuration: \"partition\" The current state of the sync protocol for the application. Valid Options: \"enabled\" \"disabled\" The name of the  MongoDB data source \nto sync. You cannot use sync with a  serverless instance  or  Federated database instance . If  true ,  Development Mode  is enabled\nfor the application. While enabled, App Services automatically stores synced\nobjects in a specific database (specified in  database_name ) and\nmirrors objects types in that database's collection schemas. The name of a database in the synced cluster where App Services stores data in\n Development Mode . App Services automatically\ngenerates a schema for each synced type and maps each object type to a\ncollection within the database. The field name of your app's  partition key . This\nfield must be defined in the schema for object types that you want to\nsync. The value type of the partition key field. This must match the type\ndefined in the object schema. Valid Options: \"string\" \"objectId\" \"long\" An  expression  that evaluates to  true  when a user\nhas permission to read objects in a partition. If the expression\nevaluates to  false , App Services does not allow the user to read\nan object or its properties. An  expression  that evaluates to  true  when a user\nhas permission to write objects in a partition. If the expression\nevaluates to  false , App Services does not allow the user to\nmodify an object or its properties. Write permission requires read\npermission. A user cannot write to a document that they cannot read. The date and time that sync was last paused or disabled, represented by\nthe number of seconds since the Unix epoch (January 1, 1970, 00:00:00\nUTC). Controls how long the  backend compaction \nprocess waits before aggressively pruning metadata that some clients\nrequire to synchronize from an old version of a realm. If  false ,  Recovery Mode  is enabled\nfor the application. While enabled, Realm SDKs that support this feature\nattempt to recover unsynced changes upon performing a client reset.\nRecovery mode is enabled by default. You must  Terminate Device Sync  and\n Re-enable Device Sync  to make changes to your\nPartition-Based Device Sync Configuration. While you are re-enabling\nAtlas Device Sync, you can specify a new Partition Key, or changes to your Read/Write\nPermissions. Making changes to your Device Sync configuration while\nterminating and re-enabling Device Sync will trigger a client reset. To learn\nmore about handling client resets, read the  client reset \ndocumentation. The following errors may occur when your App uses  Partition-Based Sync . Error Name Description ErrorIllegalRealmPath This error indicates that the client attempted to open a realm with a\npartition value of the wrong type. For example, you might see the error\nmessage \"attempted to bind on illegal realm partition: expected partition\nto have type objectId but found string\". To recover from this error, ensure that the type of the partition value\nused to open the realm matches the partition key type in your\nDevice Sync configuration. Atlas Device Sync uses space in your app's synced Atlas cluster to store metadata\nfor synchronization. This includes a history of changes to each realm.\nAtlas App Services minimizes this space usage in your Atlas cluster.\nMinimizing metadata is necessary to reduce the time and data needed for sync. Apps using Partition-Based Sync perform  backend compaction  to reduce the amount\nof metadata stored in an Atlas Cluster. Backend compaction is an  maintenance process\nthat automatically runs for all Apps using Partition-Based Sync.\nUsing compaction, the backend optimizes a realm's history by removing unneeded\nchangeset metadata. The process removes any instruction whose effect is later\noverwritten by a newer instruction. Consider the following realm history: The following history would also converge to the same state, but\nwithout unneeded interim changes: Partition Based Sync  uses backend compaction\nto reduce Device Sync history stored in Atlas.\nFlexible Sync achieves this with  trimming  and\n client maximum offline time .",
            "code": [
                {
                    "lang": "json",
                    "value": "{ \"%%partition\": \"Store 42\" }"
                },
                {
                    "lang": "javascript",
                    "value": "const config = {\n  schema: [InventoryItem], // predefined schema\n  sync: {\n    user: app.currentUser, // already logged in user\n    partitionValue: \"store42\",\n  },\n};\ntry {\n  const realm = await Realm.open(config);\n  realm.close();\n} catch (err) {\n  console.error(\"failed to open realm\", err.message);\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"note\",\n  \"bsonType\": \"object\",\n  \"required\": [\n    \"_id\",\n    \"_partition\",\n    \"owner_id\",\n    \"text\"\n  ],\n  \"properties\": {\n    \"_id\": { \"bsonType\": \"objectId\" },\n    \"_partition\": { \"bsonType\": \"string\" },\n    \"owner_id\": { \"bsonType\": \"string\" },\n    \"text\": { \"bsonType\": \"string\" }\n  }\n}\n\n{\n  \"title\": \"notebook\",\n  \"bsonType\": \"object\",\n  \"required\": [\n    \"_id\",\n    \"_partition\",\n    \"owner_id\",\n    \"notes\"\n  ],\n  \"properties\": {\n    \"_id\": { \"bsonType\": \"objectId\" },\n    \"_partition\": { \"bsonType\": \"string\" },\n    \"owner_id\": { \"bsonType\": \"string\" },\n    \"notes\": {\n      \"bsonType\": \"array\",\n      \"items\": { \"bsonType\": \"objectId\" }\n    }\n  }\n}"
                },
                {
                    "lang": "shell",
                    "value": "appservices login --api-key=\"<my api key>\" --private-api-key=\"<my private api key>\""
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"partition\",\n  \"state\": <\"enabled\" | \"disabled\">,\n  \"development_mode_enabled\": <Boolean>,\n  \"service_name\": \"<Data Source Name>\",\n  \"database_name\": \"<Database Name>\",\n  \"partition\": {\n    \"key\": \"<Partition Key Field Name>\",\n    \"type\": \"<Partition Key Type>\",\n    \"permissions\": {\n      \"read\": { <Expression> },\n      \"write\": { <Expression> }\n    }\n  },\n  \"client_max_offline_days\": <Number>,\n  \"is_recovery_mode_disabled\": <Boolean>\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  ...\n  \"partition\": {\n    \"key\": \"owner_id\",\n    \"type\": \"string\",\n    \"permissions\": {\n      \"read\": {},\n      \"write\": {}\n    }\n  }\n  ...\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  ...\n  \"partition\": {\n    \"key\": \"owner_id\",\n    \"type\": \"string\",\n    \"permissions\": {\n      \"read\": {\n        \"$or\": [\n          { \"%%user.id\": \"%%partition\" },\n          { \"%%user.custom_data.shared\": \"%%partition\" }\n        ]\n      },\n      \"write\": {\n        \"%%user.id\": \"%%partition\"\n      }\n    }\n  }\n  ...\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"partition\",\n  \"state\": \"enabled\",\n  \"development_mode_enabled\": true,\n  \"service_name\": \"mongodb-atlas\",\n  \"database_name\": \"my-test-database\",\n  \"partition\": {\n    ...\n  },\n  \"client_max_offline_days\": 30,\n  \"is_recovery_mode_disabled\": false\n}"
                },
                {
                    "lang": "shell",
                    "value": "appservices push --remote=\"<Your App ID>\""
                },
                {
                    "lang": "shell",
                    "value": "curl https://services.cloud.mongodb.com/api/admin/v3.0/groups/{GROUP_ID}/apps/{APP_ID}/services \\\n  -X GET \\\n  -h 'Authorization: Bearer <Valid Access Token>'"
                },
                {
                    "lang": "shell",
                    "value": "curl https://services.cloud.mongodb.com/api/admin/v3.0/groups/{GROUP_ID}/apps/{APP_ID}/services/{MongoDB_Service_ID}/config \\\n  -X GET \\\n  -h 'Authorization: Bearer <Valid Access Token>'"
                },
                {
                    "lang": "json",
                    "value": "{\n  ...\n  \"sync\": {\n    \"type\": \"partition\",\n    \"state\": \"enabled\",\n    \"development_mode_enabled\": <Boolean>,\n    \"database_name\": \"<Database Name>\",\n    \"partition\": {\n      \"key\": \"<Partition Key Field Name>\",\n      \"type\": \"<Partition Key Type>\",\n      \"permissions\": {\n        \"read\": { <Expression> },\n        \"write\": { <Expression> }\n      }\n    },\n    \"client_max_offline_days\": <Number>,\n    \"is_recovery_mode_disabled\": <Boolean>\n  }\n  ...\n}"
                },
                {
                    "lang": "shell",
                    "value": "curl https://services.cloud.mongodb.com/api/admin/v3.0/groups/{GROUP_ID}/apps/{APP_ID}/sync/data?service_id=<MongoDB Service ID> \\\n  -X GET \\\n  -h 'Authorization: Bearer <Valid Access Token>'"
                },
                {
                    "lang": "json",
                    "value": "{\n  ...\n  \"sync\": {\n    \"type\": \"partition\",\n    \"state\": \"enabled\",\n    \"development_mode_enabled\": <Boolean>,\n    \"database_name\": \"<Database Name>\",\n    \"partition\": {\n      \"key\": \"owner_id\",\n      \"type\": \"string\",\n      \"permissions\": {\n        \"read\": { <Expression> },\n        \"write\": { <Expression> }\n      }\n    },\n    \"client_max_offline_days\": <Number>,\n    \"is_recovery_mode_disabled\": <Boolean>\n  }\n  ...\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  ...\n  \"sync\": {\n    \"type\": \"partition\",\n    \"state\": \"enabled\",\n    \"development_mode_enabled\": <Boolean>,\n    \"database_name\": \"<Database Name>\",\n    \"partition\": {\n      \"key\": \"owner_id\",\n      \"type\": \"string\",\n      \"permissions\": {\n        \"read\": {\n          \"$or\": [\n            { \"%%user.id\": \"%%partition\" },\n            { \"%%user.custom_data.shared\": \"%%partition\" }\n          ]\n        },\n        \"write\": {\n          \"%%user.id\": \"%%partition\"\n        }\n      }\n    },\n    \"client_max_offline_days\": <Number>,\n    \"is_recovery_mode_disabled\": <Boolean>\n  }\n  ...\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  ...\n  \"sync\": {\n    \"type\": \"partition\",\n    \"state\": \"enabled\",\n    \"development_mode_enabled\": true,\n    \"database_name\": \"my-test-database\",\n    \"partition\": {\n      ...\n    },\n    \"client_max_offline_days\": 30,\n    \"is_recovery_mode_disabled\": false\n  }\n  ...\n}"
                },
                {
                    "lang": "shell",
                    "value": "curl https://services.cloud.mongodb.com/api/admin/v3.0/groups/{GROUP_ID}/apps/{APP_ID}/services/{MongoDB_Service_ID}/config \\\n  -X PATCH \\\n  -h 'Authorization: Bearer <Valid Access Token>' \\\n  -d @/sync/config.json"
                },
                {
                    "lang": "json",
                    "value": "true"
                },
                {
                    "lang": "json",
                    "value": "false"
                },
                {
                    "lang": "json",
                    "value": "{ \"%%true\": true }"
                },
                {
                    "lang": "json",
                    "value": "{ \"%%partition\": \"PUBLIC\" }"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%partition\": {\n    \"$in\": [\n      \"PUBLIC (NA)\",\n      \"PUBLIC (EMEA)\",\n      \"PUBLIC (APAC)\"\n    ]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"%%user.id\": \"5f4863e4d49bd2191ff1e623\" }"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%user.id\": {\n    \"$in\": [\n      \"5f4863e4d49bd2191ff1e623\",\n      \"5f48640dd49bd2191ff1e624\",\n      \"5f486417d49bd2191ff1e625\"\n    ]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"%%user.custom_data.readPartitions\" : \"%%partition\" }"
                },
                {
                    "lang": "json",
                    "value": "{ \"%%user.data.writePartitions\" : \"%%partition\" }"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%true\": {\n    \"%function\": {\n      \"name\": \"canReadPartition\",\n      \"arguments\": [\"%%partition\"]\n    }\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// canReadPartition\nexports = async (partition) => {\n  const cluster = context.services.get(\"mongodb-atlas\");\n  const permissions = cluster\n    .db(\"myApp\")\n    .collection(\"permissions\");\n  const { canRead } = await permissions.findOne({ partition });\n  return canRead.includes(context.user.id);\n}"
                },
                {
                    "lang": "javascript",
                    "value": "_partitionKey: \"user_id=abcdefg\"\n_partitionKey: \"team_id=1234&city='New York, NY'\""
                },
                {
                    "lang": "javascript",
                    "value": "collection games: [\n  { teams: [\"Brook Ridge Miners\", \"Southside Rockets\"], score: { ... }, date: ... }\n  { teams: [\"Brook Ridge Miners\", \"Uptown Bombers\"], score: { ... }, date: ... }\n  { teams: [\"Brook Ridge Miners\", \"Southside Rockets\"], score: { ... }, date: ... }\n  { teams: [\"Southside Rockets\", \"Uptown Bombers\"], score: { ... }, date: ... }\n  { teams: [\"Brook Ridge Miners\", \"Uptown Bombers\"], score: { ... }, date: ... }\n  { teams: [\"Southside Rockets\", \"Uptown Bombers\"], score: { ... }, date: ... }\n]\n\ncollection teams: [\n  { name: \"Brook Ridge Miners\" }\n  { name: \"Southside Rockets\" }\n  { name: \"Uptown Bombers\" }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "realm null: [\n  Game { teams: [\"Brook Ridge Miners\", \"Southside Rockets\"], score: { ... }, date: ... }\n  Game { teams: [\"Brook Ridge Miners\", \"Uptown Bombers\"], score: { ... }, date: ... }\n  Game { teams: [\"Brook Ridge Miners\", \"Southside Rockets\"], score: { ... }, date: ... }\n  Game { teams: [\"Southside Rockets\", \"Uptown Bombers\"], score: { ... }, date: ... }\n  Game { teams: [\"Brook Ridge Miners\", \"Uptown Bombers\"], score: { ... }, date: ... }\n  Game { teams: [\"Southside Rockets\", \"Uptown Bombers\"], score: { ... }, date: ... }\n  Team { name: \"Brook Ridge Miners\" }\n  Team { name: \"Southside Rockets\" }\n  Team { name: \"Uptown Bombers\" }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "collection playlists: [\n  { name: \"Work\", owner_id: \"dog_enthusiast_95\", song_ids: [ ... ] }\n  { name: \"Party\", owner_id: \"cat_enthusiast_92\", song_ids: [ ... ] }\n  { name: \"Soup Tunes\", owner_id: \"dog_enthusiast_95\", song_ids: [ ... ] }\n  { name: \"Disco Anthems\", owner_id: \"PUBLIC\", song_ids: [ ... ] }\n  { name: \"Deep Focus\", owner_id: \"PUBLIC\", song_ids: [ ... ] }\n]\n\ncollection ratings: [\n  { owner_id: \"dog_enthusiast_95\", song_id: 3, rating: -1 }\n  { owner_id: \"cat_enthusiast_92\", song_id: 1, rating: 1 }\n  { owner_id: \"dog_enthusiast_95\", song_id: 1, rating: 1 }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "realm dog_enthusiast_95: [\n  Playlist { name: \"Work\", owner_id: \"dog_enthusiast_95\", song_ids: [ ... ] }\n  Playlist { name: \"Soup Tunes\", owner_id: \"dog_enthusiast_95\", song_ids: [ ... ] }\n  Rating { owner_id: \"dog_enthusiast_95\", song_id: 3, rating: -1 }\n  Rating { owner_id: \"dog_enthusiast_95\", song_id: 1, rating: 1 }\n]\n\nrealm cat_enthusiast_92: [\n  Playlist { name: \"Party\", owner_id: \"cat_enthusiast_92\", song_ids: [ ... ] }\n  Rating { owner_id: \"cat_enthusiast_92\", song_id: 1, rating: 1 }\n]\n\nrealm PUBLIC: [\n  Playlist { name: \"Disco Anthems\", owner_id: \"PUBLIC\", song_ids: [ ... ] }\n  Playlist { name: \"Deep Focus\", owner_id: \"PUBLIC\", song_ids: [ ... ] }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "collection projects: [\n  { name: \"CLI\", owner_id: \"cli-team\", task_ids: [ ... ] }\n  { name: \"API\", owner_id: \"api-team\", task_ids: [ ... ] }\n]\n\ncollection tasks: [\n  { status: \"complete\", owner_id: \"api-team\", text: \"Import dependencies\" }\n  { status: \"inProgress\", owner_id: \"api-team\", text: \"Create app MVP\" }\n  { status: \"inProgress\", owner_id: \"api-team\", text: \"Investigate off-by-one issue\" }\n  { status: \"todo\", owner_id: \"api-team\", text: \"Write tests\" }\n  { status: \"inProgress\", owner_id: \"cli-team\", text: \"Create command specifications\" }\n  { status: \"todo\", owner_id: \"cli-team\", text: \"Choose a CLI framework\" }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "realm cli-team: [\n  Project { name: \"CLI\", owner_id: \"cli-team\", task_ids: [ ... ] }\n  Task { status: \"inProgress\", owner_id: \"cli-team\", text: \"Create command specifications\" }\n  Task { status: \"todo\", owner_id: \"cli-team\", text: \"Choose a CLI framework\" }\n]\n\nrealm api-team: [\n  Project { name: \"API\", owner_id: \"api-team\", task_ids: [ ... ] }\n  Task { status: \"complete\", owner_id: \"api-team\", text: \"Import dependencies\" }\n  Task { status: \"inProgress\", owner_id: \"api-team\", text: \"Create app MVP\" }\n  Task { status: \"inProgress\", owner_id: \"api-team\", text: \"Investigate off-by-one issue\" }\n  Task { status: \"todo\", owner_id: \"api-team\", text: \"Write tests\" }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "collection teams: [\n  { name: \"cli-team\", member_ids: [ ... ] }\n  { name: \"api-team\", member_ids: [ ... ] }\n]\n\ncollection users: [\n  { name: \"Joe\", team_ids: [ ... ] }\n  { name: \"Liz\", team_ids: [ ... ] }\n  { name: \"Matt\", team_ids: [ ... ] }\n  { name: \"Emmy\", team_ids: [ ... ] }\n  { name: \"Scott\", team_ids: [ ... ] }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "collection chatrooms: [\n  { topic: \"cats\", description: \"A place to talk about cats\" }\n  { topic: \"sports\", description: \"We like sports and we don't care who knows\" }\n]\n\ncollection messages: [\n  { topic: \"cats\", text: \"Check out this cute pic of my cat!\", timestamp: 1625772933383 }\n  { topic: \"cats\", text: \"Can anybody recommend a good cat food brand?\", timestamp: 1625772953383 }\n  { topic: \"sports\", text: \"Did you catch the game last night?\", timestamp: 1625772965383 }\n  { topic: \"sports\", text: \"Yeah what a comeback! Incredible!\", timestamp: 1625772970383 }\n  { topic: \"sports\", text: \"I can't believe how they scored that goal.\", timestamp: 1625773000383 }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "realm cats: [\n  Chatroom { topic: \"cats\", description: \"A place to talk about cats\" }\n  Message { topic: \"cats\", text: \"Check out this cute pic of my cat!\", timestamp: 1625772933383 }\n  Message { topic: \"cats\", text: \"Can anybody recommend a good cat food brand?\", timestamp: 1625772953383 }\n]\n\nrealm sports: [\n  Chatroom { topic: \"sports\", description: \"We like sports and we don't care who knows\" }\n  Message { topic: \"sports\", text: \"Did you catch the game last night?\", timestamp: 1625772965383 }\n  Message { topic: \"sports\", text: \"Yeah what a comeback! Incredible!\", timestamp: 1625772970383 }\n  Message { topic: \"sports\", text: \"I can't believe how they scored that goal.\", timestamp: 1625773000383 }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "collection chatrooms: [\n  { topic: \"cats\", subscriber_ids: [ ... ] }\n  { topic: \"sports\", subscriber_ids: [ ... ] }\n]\n\ncollection users: [\n  { name: \"Joe\", chatroom_ids: [ ... ] }\n  { name: \"Liz\", chatroom_ids: [ ... ] }\n  { name: \"Matt\", chatroom_ids: [ ... ] }\n  { name: \"Emmy\", chatroom_ids: [ ... ] }\n  { name: \"Scott\", chatroom_ids: [ ... ] }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "collection restaurants: [\n  { city: \"New York, NY\", name: \"Joe's Pizza\", menu: [ ... ] }\n  { city: \"New York, NY\", name: \"Han Dynasty\", menu: [ ... ] }\n  { city: \"New York, NY\", name: \"Harlem Taste\", menu: [ ... ] }\n  { city: \"Chicago, IL\", name: \"Lou Malnati's\", menu: [ ... ] }\n  { city: \"Chicago, IL\", name: \"Al's Beef\", menu: [ ... ] }\n  { city: \"Chicago, IL\", name: \"Nando's\", menu: [ ... ] }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "realm New York, NY: [\n { city: \"New York, NY\", name: \"Joe's Pizza\", menu: [ ... ] }\n { city: \"New York, NY\", name: \"Han Dynasty\", menu: [ ... ] }\n { city: \"New York, NY\", name: \"Harlem Taste\", menu: [ ... ] }\n]\n\nrealm Chicago, IL: [\n { city: \"Chicago, IL\", name: \"Lou Malnati's\", menu: [ ... ] }\n { city: \"Chicago, IL\", name: \"Al's Beef\", menu: [ ... ]  }\n { city: \"Chicago, IL\", name: \"Nando's\", menu: [ ... ]  }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "collection readings: [\n  { bucket: \"0s<t<=60s\", timestamp: 1625773000383 , data: { ... } }\n  { bucket: \"0s<t<=60s\", timestamp: 1625772970383 , data: { ... } }\n  { bucket: \"0s<t<=60s\", timestamp: 1625772965383 , data: { ... } }\n  { bucket: \"60s<t<=300s\", timestamp: 1625772953383 , data: { ... } }\n  { bucket: \"60s<t<=300s\", timestamp: 1625772933383 , data: { ... } }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "realm 0s<t<=60s: [\n  Reading { bucket: \"0s<t<=60s\", timestamp: 1625773000383 , data: { ... } }\n  Reading { bucket: \"0s<t<=60s\", timestamp: 1625772970383 , data: { ... } }\n  Reading { bucket: \"0s<t<=60s\", timestamp: 1625772965383 , data: { ... } }\n]\n\nrealm 60s<t<=300s: [\n  Reading { bucket: \"60s<t<=300s\", timestamp: 1625772953383 , data: { ... } }\n  Reading { bucket: \"60s<t<=300s\", timestamp: 1625772933383 , data: { ... } }\n]"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"partition\",\n  \"state\": <\"enabled\" | \"disabled\">,\n  \"development_mode_enabled\": <Boolean>,\n  \"service_name\": \"<Data Source Name>\",\n  \"database_name\": \"<Development Mode Database Name>\",\n  \"partition\": {\n    \"key\": \"<Partition Key Field Name>\",\n    \"type\": \"<Partition Key Type>\",\n    \"permissions\": {\n      \"read\": { <Expression> },\n      \"write\": { <Expression> }\n    }\n  },\n  \"last_disabled\": <Number>,\n  \"client_max_offline_days\": <Number>,\n  \"is_recovery_mode_disabled\": <Boolean>\n}"
                },
                {
                    "lang": null,
                    "value": "CREATE table1.object1\nUPDATE table1.object1.x = 4\nUPDATE table1.object1.x = 10\nUPDATE table1.object1.x = 42"
                },
                {
                    "lang": null,
                    "value": "CREATE table1.object1\nUPDATE table1.object1.x = 42"
                }
            ],
            "preview": "Atlas Device Sync has two sync modes: Flexible Sync and\nthe older Partition-Based Sync. We recommend using Flexible Sync for all new\napps. The information on this page is for users who are still using\nPartition-Based Sync.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/template-expansions",
            "title": "Create Template Configurations with Expansions",
            "headings": [
                "Define a Template",
                "Define Environment Values",
                "Define an Expansion Replacement",
                "Specify an Environment",
                "Create a New Templated App",
                "Development Workflow"
            ],
            "paragraphs": "Atlas App Services supports a replacement operator ( %() ) that dynamically resolves JSON\nexpansions like  %%environment  in your  configuration\nfiles . When you create a new app with configuration files\nthat include expansions (either with  App Services CLI  or through\n GitHub ), App Services automatically resolves the expansions and\nuses the resolved configuration files to create the app. Configuration files that use the replacement operator are  templates .\nOnce you create an app from a template, the app uses a copy of the\nconfiguration files that reflect the resolved values at the time the app\nwas created. You can use the same template to create multiple apps with\ndifferent configurations. The replacement operator is currently only supported for\nconfiguration fields with string values. You cannot use expansions\nfor boolean, number, object, or array values. For example, you can use expansions to template the  clusterName  field: But you cannot use expansions to template the  wireProtocolEnabled \nfield because it has a boolean value: You can use expansions to template any environment. For each environment that\nyou use, define values that you reference in your configuration files. The following environments configure different MongoDB Atlas cluster names: You can reference any environment value from a configuration file with the\n %()  replacement operator and the  %%environment \nexpansion. Define expansions for each configuration file you want to template. The following template configuration file for the  mongodb-atlas  data\nsource uses expansions to dynamically resolve  clusterName  based on the\nenvironment: Choose which environment to deploy your app to and then update the\napp configuration with the environment name: If you do not specify an environment,  %%environment \nexpansions resolve to values in\n environments/no-environment.json . You can now use your templated configuration files to create a new\napp in your chosen environment. App Services uses the template configuration to generate\nnew, resolved configuration files on import: You can use configuration expansions for any App, but the most useful case\nis for teams that develop a production application with source code files stored\nin an external version control system like GitHub. You can create independent\napps to develop features or test changes and use expansions to customize the\napps based on their environment. Create a copy  of the common template with the\nenvironment set to  development . If you're using Git, create a new branch\non GitHub and check out a local copy. Make changes and test them locally against the development app. Any changes\nyou make to the app won't affect the production app, though they may still\nread and write production data sources and services unless you configure\nalternatives. Commit changes and merge them back to production. You can set up a CI/CD\npipeline, which may itself create an app in the  test  environment, to\nvalidate and merge your changes into the production branch. Once your changes are merged, you can safely delete the development app and\nclean up any other associated services.",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"mongodb-atlas\",\n  \"type\": \"mongodb-atlas\",\n  \"config\": {\n    \"clusterName\": \"%(%%environment.values.clusterName)\",\n    \"readPreference\": \"primaryPreferred\",\n    \"wireProtocolEnabled\": false\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"mongodb-atlas\",\n  \"type\": \"mongodb-atlas\",\n  \"config\": {\n    \"clusterName\": \"Cluster0\",\n    \"readPreference\": \"primaryPreferred\",\n    \"wireProtocolEnabled\": %(%%environment.values.wireProtocolEnabled)\n   }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"values\": {\n    \"clusterName\": \"atlas-development\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"values\": {\n    \"clusterName\": \"atlas-production\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"mongodb-atlas\",\n  \"type\": \"mongodb-atlas\",\n  \"config\": {\n    \"clusterName\": \"%(%%environment.values.clusterName)\",\n    \"readPreference\": \"primaryPreferred\",\n    \"wireProtocolEnabled\": true\n  }\n}"
                },
                {
                    "lang": null,
                    "value": "{\n  \"name\": \"MyApp\",\n  \"environment\": \"development\",\n  \"deployment_model\": \"LOCAL\",\n  \"location\": \"aws-us-east-1\"\n}"
                },
                {
                    "lang": "shell",
                    "value": "appservices push"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"mongodb-atlas\",\n  \"type\": \"mongodb-atlas\",\n  \"config\": {\n    \"clusterName\": \"atlas-development\",\n    \"readPreference\": \"primaryPreferred\",\n    \"wireProtocolEnabled\": true\n  }\n}"
                }
            ],
            "preview": "Atlas App Services supports a replacement operator (%()) that dynamically resolves JSON\nexpansions like %%environment in your configuration\nfiles. When you create a new app with configuration files\nthat include expansions (either with App Services CLI or through\nGitHub), App Services automatically resolves the expansions and\nuses the resolved configuration files to create the app.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/log_forwarders",
            "title": "Log Forwarder Configuration Files",
            "headings": [],
            "paragraphs": "You define log forwarder configuration files in the  /log_forwarders \ndirectory. Field Description A unique name for the log forwarder. An array of one or more log types that the forwarder should\nsend to a service. Atlas App Services only forwards a log if its type is\nlisted  and  its status is listed in  log_statuses . The array may contain the following log types: auth endpoint function graphql push schema service sync trigger trigger_error_handler An array of one or more log statuses that the forwarder should\nsend to a service. App Services only forwards a log if its type is\nlisted  and  its type is listed in  log_types . The array may contain the following log statuses: error success An object that configures the forwarder's batching policy. To forward logs individually: To group logs into batches: An object that configures where and how the forwarder sends logs. To forward logs to a linked MongoDB collection: To forward logs with a custom function:",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 log_forwarders/\n    \u2514\u2500\u2500 <Name>.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<name>\",\n  \"log_types\": [ \"<type>\", ... ],\n  \"log_statuses\": [ \"<status>\", ... ],\n  \"policy\": { batching policy },\n  \"action\": { action configuration }\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"type\": \"single\" }"
                },
                {
                    "lang": "json",
                    "value": "{ \"type\": \"batch\" }"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"collection\",\n  \"data_source\": \"<data source name>\",\n  \"database\": \"<database name>\",\n  \"collection\": \"<collection name>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"function\",\n  \"name\": \"<function name>\"\n}"
                }
            ],
            "preview": "You define log forwarder configuration files in the /log_forwarders\ndirectory.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/values",
            "title": "Value Configuration Files",
            "headings": [
                "Configuration"
            ],
            "paragraphs": "You can define static  values  in the  /values  directory.\nEach value is defined in its own JSON file with the same name as the value. Field Description A string that uniquely identifies the value. Atlas App Services automatically\ngenerates a unique ID for a value when you create it. A unique name for the value. This name is how you refer to\nthe value in functions and rules. Default:  false . If  true , the value exposes a\n Secret  instead of a plain-text JSON value. The stored data that App Services exposes when the value is referenced. If  from_secret  is  false ,  value  can be a standard\nJSON string, number, array, or object. If  from_secret  is  true ,  value  is a string that\ncontains the name of the  Secret  that the\nvalue exposes.",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 values/\n    \u2514\u2500\u2500 <value name>.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<Value ID>\",\n  \"name\": \"<Value Name>\",\n  \"from_secret\": <boolean>,\n  \"value\": <Stored JSON Value|Secret Name>\n}"
                }
            ],
            "preview": "You can define static values in the /values directory.\nEach value is defined in its own JSON file with the same name as the value.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/auth",
            "title": "User & Authentication Provider Configuration Files",
            "headings": [
                "Authentication Providers",
                "Configuration",
                "Custom User Data"
            ],
            "paragraphs": "You can enable and configure  authentication providers  in  /auth/providers.json . Each field of the configuration is the name of a provider type and contains a\nconfiguration object for that provider. Authentication provider configurations\nshare a common structure but each provider type uses a unique set of\nconfiguration fields. You can find detailed information on a specific provider's configuration on\nthat provider's reference page. For a list of all provider reference pages,\nsee  Authentication Providers . Field Description The name of the authentication provider. This will always be the same as\nthe the provider's  type . The authentication provider type. Valid options: \"anon-user\"  for  Anonymous  authentication. \"local-userpass\"  for  Email/Password  authentication. \"api-key\"  for  API Key  authentication. \"custom-token\"  for  Custom JWT  authentication. \"custom-function\"  for  Custom Function  authentication. \"oauth2-google\"  for  Google  authentication. \"oauth2-facebook\"  for  Facebook  authentication. \"oauth2-apple\"  for  Apple ID  authentication. If  true , this authentication provider is not enabled for your\napplication. Users cannot log in using credentials for a disabled\nprovider. A document that contains configuration values that are specific\nto the authentication provider. The following provider configurations include  config : Email/Password Custom JWT Custom Function Google Facebook Apple ID A document where each field name is a private configuration field\nfor the provider and the value of each field is the name of a\n Secret  that stores the configuration value. The following provider configurations include  redirect_uris : Custom JWT Google Facebook Apple ID An array of documents where each document defines a metadata\nfield that describes the user. The existence of this field and\nthe exact format of each metadata field document depends on the\nprovider type. The following provider configurations include  metadata_fields : Custom JWT Google Facebook A list of URLs that Atlas App Services may redirect user back to after\nthey complete an OAuth authorization. The following provider configurations include  redirect_uris : Google Facebook Apple ID You can configure the  custom user data \ncollection for your app in  /auth/custom_user_data.json . Field Name Description If  true , App Services associates each user with a document in the specified\ncollection that contains their custom data. The name of the  data source  that contains\nthe custom user data collection. The name of the database that contains the custom user data collection. The name of the collection that contains the custom user data. The name of the field in custom user data documents that\ncontains the user ID of the application user the document\ndescribes. The name of the  user creation function .",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 auth/\n    \u251c\u2500\u2500 providers.json\n    \u2514\u2500\u2500 custom_user_data.json"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"<Provider Name>\": {\n    \"name\": \"<Provider Name>\",\n    \"type\": \"<Provider Type>\",\n    \"disabled\": <Boolean>,\n    \"config\": {\n      \"<Configuration Option>\": \"<Configuration Value>\"\n    },\n    \"secret_config\": {\n      \"<Configuration Option>\": \"<Secret Name>\"\n    },\n    \"metadata_fields\": [\n      {\n        \"required\": <Boolean>,\n        \"name\": \"Field Name\"\n      },\n      ...\n    ],\n    \"redirect_uris\": [\"<Application Redirect URI>\", ...]\n  },\n  ...\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"enabled\": <Boolean>,\n  \"mongo_service_name\": \"<MongoDB Data Source Name>\",\n  \"database_name\": \"<Database Name>\",\n  \"collection_name\": \"<Collection Name>\",\n  \"user_id_field\": \"<Field Name>\",\n  \"on_user_creation_function_name\": \"<Function Name>\"\n}"
                }
            ],
            "preview": "You can configure the custom user data\ncollection for your app in /auth/custom_user_data.json.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/legacy",
            "title": "Application Configuration Files (Legacy)",
            "headings": [
                "Overview",
                "When Will I Use Configuration Files?",
                "Directory Structure",
                "Application Configuration",
                "Configuration",
                "Authentication Providers",
                "Configuration",
                "Functions",
                "Configuration",
                "Source Code",
                "MongoDB Services",
                "Service Configuration",
                "Synced Cluster Configuration",
                "MongoDB Collection Rules (Non-Sync)",
                "External Services",
                "Service Configuration",
                "Service Rules",
                "Incoming Webhooks",
                "Configuration",
                "Source Code",
                "Triggers",
                "Configuration",
                "Hosting",
                "Metadata Configuration",
                "Values",
                "Configuration"
            ],
            "paragraphs": "This page describes the legacy configuration file format used by\n realm-cli  version 1. For an up-to-date description of Atlas App Services\nconfiguration files, see  App Configuration . App Services uses JSON files and source code files to define and configure every\ncomponent of an application. Each component has a specific configuration file\nschema and every application uses a standard file structure to organize the\nconfiguration files. Every App is composed of a collection of configuration files, so you\nuse application configuration files whenever you create or modify an App. If\nyou use the App Services UI then you rarely deal directly with the configuration files\nthemselves, but other deployment methods, like  App Services CLI  and  GitHub  allow you to define and edit the\nconfiguration files directly. The following figure shows a high-level view of an App's directory structure: Application-level configuration information is defined in a single\ndocument named  config.json  stored in your application's root\ndirectory. Field Description The application's  App ID . The application's name. Application names must be between 1 and 32 characters and may\nonly contain ASCII letters, numbers, underscores, and hyphens. A document that contains configuration options for\napplication-level security features. Field Name Description An array of URLs that incoming requests may originate\nfrom. If you define any allowed request origins, then\nAtlas App Services blocks any incoming request from an origin that is\nnot listed. Request origins are URLs with the following form: A document that contains configuration options for all\n hosted files : Field Name Description If  true , indicates that your application can\n host static files . A  custom domain name  for your\napplication's hosted files. The default domain for your application's hosted files.\nApp Services automatically sets this value and you cannot change\nit. The schema version that all configuration files in the\napplication conform to. This value is machine generated and\nyou typically should not manually set or modify it. A document that contains configuration options for\n custom user data . Field Name Description If  true , App Services associates each user with a document\nthat contains their data stored in the specified\ncollection. The service ID of the  MongoDB Atlas data source  that contains the custom user\ndata. You can find this value in the  id  field of the\nservice configuration file. The name of the database that contains the custom user\ndata collection. The name of the collection that contains the\ncustom user data. The name of the field in each custom data document that\ncontains the user ID of the application user the document\ndescribes. The application's  deployment model . The following values are valid: Deployment Model Value Global Deployment \"GLOBAL\" Local Deployment \"LOCAL\" The name of the  cloud region \nthat the application is deployed in. application requests and database writes in this region. Global applications  process\nall database writes in this region, but serve other application\nrequests in the nearest deployment region. Authentication providers \nare defined in your application's  /auth_providers \ndirectory. Each provider is defined in its own JSON file named after the provider.\nFor detailed information on configuring and using a specific\nauthentication provider, see that provider's reference page. Field Description A value that uniquely identifies the authentication\nprovider. Atlas App Services automatically generates a unique ID for a\nprovider when you create it. The name of the authentication provider. The  type  of the authentication\nprovider. Valid Options: \"anon-user\" \"local-userpass\" \"api-key\" \"oauth2-apple\" \"oauth2-google\" \"oauth2-facebook\" \"custom-token\" \"custom-function\" A document that contains configuration values that are specific\nto the authentication provider. The existence of this field and\nits exact configuration fields depend on the provider type. A document where each field name is a private configuration field\nfor the provider and the value of each field is the name of a\n Secret  that stores the configuration value. An array of documents where each document defines a metadata\nfield that describes the user. The existence of this field and\nthe exact format of each metadata field document depends on the\nprovider type. If  true , this authentication provider is not enabled for your\napplication and cannot be used. Atlas Functions are defined in a sub-directory of your application's\n /functions  directory. Each function maps to its own subdirectory\nwith the same name as the function. Each function is configured in  config.json  and has its source code\ndefined in  source.js . Field Description A value that uniquely identifies the function. App Services\nautomatically generates a unique ID for a function when you\ncreate it. The name of the function. The name must be unique among all\nfunctions in your application. If  true , this function may only be accessed from\nHTTPS endpoints, rules, and named functions. A  rule expression  that evaluates to  true  when\nthe function is allowed to execute in response to a given request. If  true , App Services omits the arguments provided to a function\nfrom the  function execution log entry . If  true , this function  runs as the system user . This overrides any values defined for\n run_as_user_id  and  run_as_user_id_script_source . The unique ID of a  App Services User  that the\nfunction always executes as. Cannot be used with\n run_as_user_id_script_source . A stringified  function  that runs whenever the\nfunction is called and returns the unique ID of a  App Services\nUser  that the function executes as. Cannot be used with\n run_as_user_id . Every  MongoDB Atlas data source \nlinked to your app is configured as a service in the  /services \ndirectory. Each data source maps to its own sub-directory with the same\nname as the service. The primary service configuration for a MongoDB Atlas data source is\n config.json , which defines connection parameters and sync rules. If the data source is not a  synced cluster  or\n Federated database instance , then you can\ndefine collection-level rules in the  /rules  sub-directory. MongoDB Service names are not necessarily the same as their linked\ndata source's name in Atlas. You define the service name for a data\nsource when you link it to your application. For linked clusters, the\ndefault MongoDB service name is  mongodb-atlas . For\nFederated data sources, the default service name is\n mongodb-datafederation . The configuration file to link an Atlas cluster should have the\nfollowing form: The configuration file for a Federated data source should have the following form: Exactly one of  config.dataLakeName  and  config.clusterName  is\nrequired, depending on whether you are linking a cluster or a\nFederated data source. Field Description A string that uniquely identifies the service. App Services\nautomatically generates a unique ID for a MongoDB service when you create\nit. The name of the service. The name may be at most 64 characters\nlong and can only contain ASCII letters, numbers, underscores,\nand hyphens. For clusters, the default name is  mongodb-atlas .\nFor Federated data sources, it is  mongodb-datafederation . For MongoDB Atlas clusters, this value is always  \"mongodb-atlas\" .\nFor Federated data sources, this value is  \"datalake\" . Required when linking a cluster. The name of the service's linked cluster in MongoDB Atlas. Required when linking a Federated data sources. The name of the instance that you want to link to your application. The  read preference  mode for queries sent\nthrough the service. Not available for Federated data sources. Mode Description primary App Services routes all read operations to the current replica set\n primary node . This is the\ndefault read preference mode. primaryPreferred App Services routes all read operations to the current replica set\n primary node  if it's\navailable. If the primary is unavailable, such as during an\n automatic failover , read requests are routed\nto a  secondary node \ninstead. secondary App Services routes all read operations to one of the current replica\nset  secondary nodes . secondaryPreferred App Services routes all read operations to one of the replica set's\navailable  secondary nodes . If no secondary is available,\nread requests are routed to the replica set  primary  instead. nearest App Services routes read operations to the  replica set member  that has the lowest network\nlatency relative to the client. A configuration document that determines if a cluster is  synced  and, if it is, defines the rules for sync operations on the\ncluster. Not available for Federated data sources. For detailed information on sync configuration documents, see\n Synced Cluster Configuration . The  config.sync  field of  config.json  determines if a cluster is\n synced  and, if it is, defines the rules for sync operations on\nthe cluster. Field Description If  true ,  Sync  is enabled for the cluster, which means\nthat client applications can sync data in the cluster with Realm Database\nand that  non-sync collection rules  do not\napply. If  true , Development Mode is enabled for the cluster. While\nenabled, Atlas App Services stores synced objects in a specific database within\nthe cluster, and mirrors object types in that database's collection\nschemas. The name of the database in the synced cluster where App Services should store\nsynced objects. When Development Mode is enabled, App Services stores synced objects in\nthis database. Each object type maps to its own collection in the\ndatabase with a schema that matches the synced objects. The name of the  partition key  field that maps data\ninto individual synced realms. The type of the partition key field value. A document that defines the  read  and  write  permissions for the\nsynced cluster. Permissions are defined with  rule expressions  that App Services evaluates per-user, per-partition. The\nexpressions have access to the  %%user  and\n %%partition  expansions. For non-synced clusters, you can define collection-level rules that App Services\nevaluates dynamically for each request. Each collection's rules are stored in a\n rules.json  file in that collection's configuration subdirectory, which is\n data_sources/<data-source-name>/<database-name>/<collection-name>/ . Federated data sources  do not support rules or schemas . You can only access a Federated data source\nfrom a system function. Field Description A string that uniquely identifies the trigger. App Services\nautomatically generates a unique ID for a trigger when you create\nit. The name of the database that holds the collection. The name of the collection. An array of  Role configuration documents , which have the following form: Field Description The name of the role. Role names identify and distinguish between\nroles in the same collection. Limited to 100 characters or fewer. An  expression  that evaluates to true when\nthis  role  applies to a user. When Device Sync (Flexible Mode) is not enabled, App Services assigns a\nrole on a per-document basis. When Device Sync (Flexible Mode) is\nenabled, App Services assigns roles on a per-collection, per-session\nbasis -- that is, for each synced collection when a client opens a sync\nconnection. To assign a role, App Services evaluates the  apply_when  of each\npotential role until one evaluates to true. Potential roles are any roles\nspecified in the  rules.json  configuration file for the given\ncollection or, if no  rules.json  file is found for the given\ncollection, the default role(s). App Services evaluates roles in the\norder that you specify them in your configuration. If no role matches,\nthen access is denied. For more information, see  Role-based Permissions . If Device Sync (Flexible Mode) is enabled, the assigned roles must be\n Sync compatible . If the role is not Sync\ncompatible, but its  apply_when  evaluated to true, others roles are\nnot considered; access is denied. A document with read and write expressions that determine whether\nthe role's other permissions may be evaluated. If Device Sync is enabled, both  document_filters.read  and\n document_filters.write   must  be defined to make the role\n Sync compatible . Sync incompatible roles\ndeny all access to Sync requests. If Device Sync is not enabled,  document_filters ,\n document_filters.read , and  document_filters.write  are all\noptional; an undefined  document_filters.read  or\n document_filters.write  defaults to true, allowing subsequent\npermissions to be evaluated. An  expression  that specifies whether  read , read\npermissions in  fields , and read permissions in  additional_fields \nmay be evaluated. If false (and  document_filters.write  is undefined\nor false), read access is denied for the entire document. To maintain  Sync compatibility , the\nexpression must be defined and may only reference  queryable fields . An  expression  that specifies whether  write ,\nwrite permissions in  fields , and write permissions in\n additional_fields  may be evaluated. If false, then write access is\ndenied for the entire document. To maintain  Sync compatibility , the\nexpression must be defined and may only reference  queryable fields . An  expression  that evaluates to true if the\nrole has permission to read all fields in the document. To maintain  Sync compatibility , the\nexpression must be a boolean literal (either  true  or  false ). Document-level read permissions take priority over any field-level\npermissions. If a role has document-level  read  permissions, it\napplies to all fields in the document. Read permissions specified by\n fields  or  additional_fields  do not override document-level\n read  permissions. To define a default fallback alongside field-level rules, leave  read \nundefined and use  additional_fields . An  expression  that evaluates to true if the\nrole has permission to add, modify, or remove all fields in the document. To maintain  Sync compatibility , the\nexpression must be a boolean literal (either  true  or  false ). Document-level write permissions take priority over any field-level\npermissions. If a role has document-level  write  permissions, it\napplies to all fields in the document. Write permissions specified by\n fields  or  additional_fields  do not override document-level\n write  permissions. To define a default fallback alongside field-level rules, leave  write \nundefined and use  additional_fields . You can use expansions like  %%root  and\n %%prevRoot  in  write  JSON expressions. Any time a role has  write  permission for a particular\nscope it also has  read  permission even if that is not\nexplicitly defined. An  expression  that evaluates to\n true  if the role has permission to insert a new document into the\ncollection. App Services only evaluates this expression for insert operations and\nonly after determining that the role has  write  permission for all\nfields in the new document. An  expression  that evaluates to true if the\nrole has permission to delete a document from the collection. App Services only evaluates this expression for delete operations and\nonly after determining that the role has  write  permission for all\nfields in the document to be deleted. An  expression  that evaluates to true if the\nrole has permission to search the collection using  Atlas Search . App Services performs  $search  operations as a system user and\nenforces field-level rules on the returned search results. This means that a\nuser may search on a field for which they do not have read access. In this\ncase, the search is based on the specified field but no returned documents\ninclude the field. A document where each key corresponds to a field name, and each value\ndefines the role's field-level  read  and  write  permissions for the\ncorresponding field in a queried document. To maintain  Sync compatibility , the inner\n read  and  write  expressions must be boolean literals (either\n true  or  false ). Document-level  read  or  write  permissions override all\nfield-level permissions of the same type. If permissions are\ndefined for a field that contains an embedded document, those\npermissions override any permissions defined for the\ndocument's embedded fields. An  expression  that evaluates to true if the\nrole has permission to read the field. To maintain  Sync compatibility , the\nexpression must be a boolean literal (either  true  or  false ). An  expression  that evaluates to true if the\nrole has permission to add, modify, or remove the field. To maintain  Sync compatibility , the\nexpression must be a boolean literal (either  true  or  false ). A  fields  document that defines  read  and  write \npermissions for fields that are embedded within this field in a\nqueried document. See the  Field-level Permissions for Embedded Documents  role pattern for more\ninformation. A document that defines the role's field-level  read  and\n write  permissions for any fields in a queried document that\ndon't have explicitly defined permissions in the  fields \ndocument. To maintain  Sync compatibility , the\ninner  read  and  write  expressions must be boolean literals (either\n true  or  false ). An  expression  that evaluates to true if the\nrole has permission to read any field that does not have a field-level\npermission definition in  fields . To maintain  Sync compatibility , the\nexpression must be boolean (either  true  or  false ). An  expression  that evaluates to true if the\nrole has permission to add, modify, or remove any field that does not\nhave a field-level permission definition in  fields . To maintain  Sync compatibility , the\nexpression must be boolean (either  true  or  false ). A  document schema . The root level\nschema must be an  object schema ,\nwhich has the following form: An array of  Filter configuration documents , which have the following form: Field Description Required. The name of the filter. Filter names are\nuseful for identifying and distinguishing between filters.\nLimited to 100 characters or fewer. An  expression  that determines when this filter\napplies to an incoming MongoDB operation. Atlas App Services evaluates and applies filters before it reads any\ndocuments, so you cannot use  MongoDB document expansions  in a filter's Apply When expression.\nHowever, you can use other expansions like  %%user ,\n %%values , and  %function . A  MongoDB query  that App Services merges\ninto a filtered operation's existing query. A filter withholds documents that have a  score  below  20  using\nthe following query: A  MongoDB projection \nthat App Services merges into a filtered operation's existing projection. MongoDB projections can be either inclusive or exclusive, i.e.\nthey can either return only specified fields or withhold\nfields that are not specified. If multiple filters apply to a\nquery, the filters must all specify the same type of\nprojection, or the query will fail. A filter withholds the  _internal  field from all documents using\nthe following projection: 3rd party services  are defined in the  /services \ndirectory. Each service maps to its own sub-directory with the same name as the\nservice. Each service directory contains the following: config.json : a service configuration file /rules : a sub-directory of service rule configurations /incoming_webhooks : a sub-directory of webhook configurations (if the\nservice supports webhooks, i.e. HTTP, GitHub, or Twilio) Field Description A string that uniquely identifies the service. Atlas App Services\nautomatically generates a unique ID for a service when you create\nit. The name of the service. The name may be at most 64 characters\nlong and can only contain ASCII letters, numbers, underscores,\nand hyphens. The type of the service. Valid Options: \"http\" \"aws\" \"twilio\" \"github\" \"gcm\" A document with fields that map to additional configuration\noptions for the service. The exact configuration fields depend on\nthe service  type . HTTP Service Configuration AWS Service Configuration Twilio Service Configuration GitHub Service Configuration A document where each field name is a private configuration field\nfor the service and the value of each field is the name of a\n Secret  that stores the configuration value. Rules for a specific external service are defined in the  /<service\nname>/rules  sub-directory. Each rule maps to its own JSON file with the same name as the rule. Field Description A string that uniquely identifies the rule. App Services automatically\ngenerates a unique ID for a rule when you create it. The name of the service rule. The name may be at most 64\ncharacters long and can only contain ASCII letters, numbers,\nunderscores, and hyphens. A list of service actions that the rule applies to. The specific\nactions available depend on the service  type . A  rule expression  that evaluates to  true  when\nthe rule applies to a given request. Incoming webhooks for a specific service are defined in the\n /<service name>/incoming_webhooks/  sub-directory. Incoming webhooks use the  same configuration format as function  but have additional configuration parameters. Field Description A value that uniquely identifies the function. App Services\nautomatically generates a unique ID for a function when you\ncreate it. The name of the function. The name must be unique among all\nfunctions in your application. If  true , this function may only be accessed from incoming\nwebhooks, rules, and named functions. A  rule expression  that evaluates to  true  if\nthe function is allowed to execute in response to a given request. If  true , App Services omits the arguments provided to a function\nfrom the  function execution log entry . If  true , the webhook function  runs as the system user . This overrides any values defined for\n run_as_user_id  and  run_as_user_id_script_source . The unique ID of a  App Services User  that the\nfunction always executes as. Cannot be used with\n run_as_user_id_script_source . A stringified  function  that runs whenever the\nwebhook is called and returns the unique ID of a  App Services\nUser  that the function executes as. Cannot be used with\n run_as_user_id . If  true , App Services includes the webhook function return value as\nthe body of the HTTP response it sends to the client that\ninitiated the webhook request. A document that contains configuration options for the webhook. Field Description The HTTP method type that the webhook accepts. Incoming\nwebhook requests must use this method. The name of the  request validation method  that the webhook uses. Valid options: \"VERIFY_PAYLOAD\" \"SECRET_AS_QUERY_PARAM\" \"NO_VALIDATION\" The secret value used to  validate incoming webhook\nrequests . Triggers  are defined in your application's\n /triggers  directory. Each trigger is defined in its own JSON file with the same name as the\ntrigger. Field Description A string that uniquely identifies the Trigger. Atlas App Services\nautomatically generates a unique ID for a trigger when you create\nit. The name of the Trigger. The name may be at most 64 characters\nlong and can only contain ASCII letters, numbers, underscores,\nand hyphens. The  type  of application event that the trigger\nlistens for. Valid Options: \"DATABASE\" \"AUTHENTICATION\" \"SCHEDULED\" The name of the Atlas Function that the Trigger\nexecutes whenever it fires. The Trigger automatically passes\narguments to the function depending on the Trigger  type . A document with fields that map to additional configuration\noptions for the trigger. The exact configuration fields depend on\nthe trigger  type . Database Trigger Configuration Authentication Trigger Configuration Scheduled Trigger Configuration If  true , the trigger will not listen for any events and will\nnot fire. Files that you want to  host on Atlas App Services  should be\nincluded in your application's  /hosting  directory. Each file will be\nuploaded with the metadata defined in  metadata.json . You can  configure the metadata \nfor each hosted file in  metadata.json . This metadata configuration\nfile is an array of documents that each correspond to a single hosted\nfile's metadata attributes. Field Description The  resource path  of the file. An array of documents where each document represents a single\nmetadata attribute. Attribute documents have the following form: Field Description The name of the metadata attribute. This should be one of\nthe  file metadata attributes  that App Services supports. The value of the metadata attribute. If you do not specify a  Content-Type  metadata attribute for a hosted\nfile, Atlas App Services will attempt to automatically add a  Content-Type \nattribute to it based on the file extension. For example, App Services would automatically add the attribute\n Content-Type: application/html  to the file  myPage.html . Values are defined in your application's  /values  directory. Each value is defined in its own JSON file named after the value. Field Description A string that uniquely identifies the value. Atlas App Services automatically\ngenerates a unique ID for a value when you create it. A unique name for the value. This name is how you refer to\nthe value in functions and rules. Default:  false . If  true , the value exposes a\n Secret  instead of a plain-text JSON value. The stored data that App Services exposes when the value is referenced. If  from_secret  is  false ,  value  can be a standard\nJSON string, array, or object. If  from_secret  is  true ,  value  is a string that\ncontains the name of the  Secret  that the\nvalue exposes.",
            "code": [
                {
                    "lang": "none",
                    "value": "yourRealmApp/\n\u251c\u2500\u2500 config.json\n\u251c\u2500\u2500 secrets.json\n\u251c\u2500\u2500 auth_providers/\n\u2502   \u2514\u2500\u2500 <provider name>.json\n\u251c\u2500\u2500 functions/\n\u2502   \u2514\u2500\u2500 <function name>/\n\u2502       \u251c\u2500\u2500 config.json\n\u2502       \u2514\u2500\u2500 source.js\n\u251c\u2500\u2500 services/\n\u2502   \u2514\u2500\u2500 <service name>/\n\u2502       \u251c\u2500\u2500 config.json\n\u2502       \u251c\u2500\u2500 incoming_webhooks/\n\u2502       \u2502   \u251c\u2500\u2500 config.json\n\u2502       \u2502   \u2514\u2500\u2500 source.js\n\u2502       \u2514\u2500\u2500 rules/\n\u2502           \u2514\u2500\u2500 <rule name>.json\n\u251c\u2500\u2500 triggers/\n\u2502   \u2514\u2500\u2500 <trigger name>.json\n\u251c\u2500\u2500 hosting/\n\u2502   \u251c\u2500\u2500 metadata.json\n\u2502   \u2514\u2500\u2500 files/\n\u2502       \u2514\u2500\u2500 <files to host>\n\u2514\u2500\u2500 values/\n    \u2514\u2500\u2500 <value name>.json"
                },
                {
                    "lang": "none",
                    "value": "yourRealmApp/\n\u2514\u2500\u2500 config.json"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"app_id\": \"\",\n  \"name\": \"\",\n  \"security\": {\n    \"allowed_request_origins\": [\"<Origin URL>\"]\n  },\n  \"hosting\": {\n    \"enabled\": <boolean>,\n    \"custom_domain\": \"<Custom Domain Name>\",\n    \"app_default_domain\": \"<Default Domain Name>\"\n  },\n  \"custom_user_data_config\": {\n    \"enabled\": <Boolean>\n    \"mongo_service_id\": \"<MongoDB Service ID>\",\n    \"database_name\": \"<Database Name>\",\n    \"collection_name\": \"<Collection Name>\",\n    \"user_id_field\": \"<Field Name>\"\n  }\n  \"deployment_model\": \"<Deployment Model Type>\",\n  \"location\": \"<Deployment Cloud Region Name>\",\n  \"config_version\": <Version Number>\n}"
                },
                {
                    "lang": "json",
                    "value": "\"security\": {\n  \"allowed_request_origins\": [\"<Origin URL>\"]\n}"
                },
                {
                    "lang": "text",
                    "value": "<scheme>://<host>[:port]"
                },
                {
                    "lang": "json",
                    "value": "\"hosting\": {\n  \"enabled\": <boolean>,\n  \"custom_domain\": \"<Custom Domain Name>\",\n  \"app_default_domain\": \"<Default Domain Name>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "\"custom_user_data_config\": {\n  \"enabled\": <Boolean>\n  \"mongo_service_id\": \"<MongoDB Service ID>\",\n  \"database_name\": \"<Database Name>\",\n  \"collection_name\": \"<Collection Name>\",\n  \"user_id_field\": \"<Field Name>\"\n}"
                },
                {
                    "lang": "none",
                    "value": "yourRealmApp/\n\u2514\u2500\u2500 auth_providers/\n    \u2514\u2500\u2500 <provider name>.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<Provider ID>\",\n  \"name\": \"<Provider Name>\",\n  \"type\": \"<Provider Type>\",\n  \"disabled\": <Boolean>,\n  \"config\": {\n    \"<Configuration Option>\": <Configuration Value>\n  },\n  \"secret_config\": {\n    \"<Configuration Option>\": \"<Secret Name>\"\n  },\n  \"metadata_fields\": [{\n    \"required\": <Boolean>,\n    \"name\": \"Field Name\"\n  }]\n}"
                },
                {
                    "lang": "none",
                    "value": "yourRealmApp/\n\u2514\u2500\u2500 functions/\n    \u2514\u2500\u2500 <function name>/\n        \u251c\u2500\u2500 config.json\n        \u2514\u2500\u2500 source.js"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<Function ID>\",\n  \"name\": \"<Function Name>\",\n  \"private\": <Boolean>,\n  \"can_evaluate\": <Rule Expression>,\n  \"disable_arg_logs\": <Boolean>,\n  \"run_as_system\": <Boolean>,\n  \"run_as_user_id\": \"<App Services User ID>\",\n  \"run_as_user_id_script_source\": \"<Function Source Code>\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  // function code\n};"
                },
                {
                    "lang": "none",
                    "value": "yourRealmApp/\n\u2514\u2500\u2500 services/\n    \u2514\u2500\u2500 <MongoDB Service Name>/\n        \u251c\u2500\u2500 config.json\n        \u2514\u2500\u2500 rules/\n            \u2514\u2500\u2500 <rule name>.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<Service ID>\",\n  \"name\": \"<Service Name>\",\n  \"type\": \"mongodb-atlas\",\n  \"config\": {\n    \"clusterName\": \"<Atlas Cluster Name>\",\n    \"readPreference\": \"<Read Preference>\",\n    \"wireProtocolEnabled\": <Boolean>,\n    \"sync\": <Sync Configuration>\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<Service ID>\",\n  \"name\": \"<Service Name>\",\n  \"type\": \"datalake\",\n  \"config\": {\n     \"dataLakeName\": \"<Federated database instance name>\"\n   }\n }"
                },
                {
                    "lang": "json",
                    "value": "{\n  ...,\n  \"config\": {\n    ...,\n    \"sync\": {\n      \"state\": <Boolean>,\n      \"development_mode_enabled\": <Boolean>,\n      \"database_name\": \"<Development Mode Database Name>\",\n      \"partition\": {\n        \"key\": \"<Partition Key Field Name>\",\n        \"type\": \"<Partition Key Value Type>\",\n        \"permissions\": {\n          \"read\": <JSON Expression>,\n          \"write\": <JSON Expression>\n        }\n      }\n    }\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<Rule ID>\",\n  \"database\": \"<Database Name>\",\n  \"collection\": \"<Collection Name>\",\n  \"roles\": [<Role>],\n  \"schema\": <Document Schema>,\n  \"filters\": [<Filter>],\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"name\": \"<Role Name>\",\n   \"apply_when\": { Expression },\n   \"document_filters\": {\n     \"read\": { Expression },\n     \"write\": { Expression }\n   },\n   \"read\": { Expression },\n   \"write\": { Expression },\n   \"insert\": { Expression },\n   \"delete\": { Expression },\n   \"search\": <Boolean>,\n   \"fields\": {\n      \"<Field Name>\": {\n         \"read\": { Expression },\n         \"write\": { Expression },\n         \"fields\": { Embedded Fields }\n      },\n      ...\n   },\n   \"additional_fields\": {\n     \"read\": { Expression },\n     \"write\": { Expression }\n   }\n}"
                },
                {
                    "lang": "json",
                    "value": "\"document_filters\": {\n  \"read\": { Expression },\n  \"write\": { Expression }\n}"
                },
                {
                    "lang": "json",
                    "value": "\"fields\": {\n  \"<Field Name>\": {\n     \"read\": { Expression },\n     \"write\": { Expression },\n     \"fields\": <Fields Document>\n  },\n  ...\n}"
                },
                {
                    "lang": "json",
                    "value": "\"additional_fields\": {\n  \"read\": { Expression },\n  \"write\": { Expression }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"bsonType\": \"object\",\n  \"properties\": {\n    \"<Field Name>\": <Schema Document>\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"name\": \"<Filter Name>\",\n  \"apply_when\": { Expression },\n  \"query\": { MongoDB Query },\n  \"projection\": { MongoDB Projection }\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"score\": { \"$gte\": 20 } }"
                },
                {
                    "lang": "json",
                    "value": "{ \"_internal\": 0 }"
                },
                {
                    "lang": "none",
                    "value": "yourRealmApp/\n\u2514\u2500\u2500 services/\n    \u2514\u2500\u2500 <services name>/\n        \u251c\u2500\u2500 config.json\n        \u251c\u2500\u2500 incoming_webhooks/\n        \u2502   \u251c\u2500\u2500 config.json\n        \u2502   \u2514\u2500\u2500 source.js\n        \u2514\u2500\u2500 rules/\n            \u2514\u2500\u2500 <rule name>.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<Service ID>\",\n  \"name\": \"<Service Name>\",\n  \"type\": \"<Service Type>\",\n  \"config\": {\n    \"<Configuration Option>\": <Configuration Value>\n  },\n  \"secret_config\": {\n    \"<Configuration Option>\": \"<Secret Name>\"\n  },\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<Rule ID>\",\n  \"name\": \"<Rule Name>\",\n  \"actions\": [\"<Service Action Name>\"],\n  \"when\": <JSON Rule Expression>\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<Function ID>\",\n  \"name\": \"<Function Name>\",\n  \"private\": <Boolean>,\n  \"can_evaluate\": <Rule Expression>,\n  \"disable_arg_logs\": <Boolean>,\n  \"run_as_system\": <Boolean>,\n  \"run_as_user_id\": \"<App Services User ID>\",\n  \"run_as_user_id_script_source\": \"<Function Source Code>\",\n  \"respond_result\": <Boolean>,\n  \"options\": {\n    \"httpMethod\": \"<HTTP Method>\",\n    \"validationMethod\": \"<Webhook Validation Method>\",\n    \"secret\": \"<Webhook Secret>\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"httpMethod\": \"<HTTP Method>\",\n  \"validationMethod\": \"<Webhook Validation Method>\",\n  \"secret\": \"<Webhook Secret>\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  // webhook function code\n};"
                },
                {
                    "lang": "none",
                    "value": "yourRealmApp/\n\u2514\u2500\u2500 triggers/\n    \u2514\u2500\u2500 <trigger name>.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<Trigger ID>\",\n  \"name\": \"<Trigger Name>\",\n  \"type\": \"<Trigger Type>\",\n  \"function_name\": \"<Trigger Function Name>\",\n  \"config\": {\n    \"<Configuration Option>\": <Configuration Value>\n  },\n  \"disabled\": <Boolean>,\n}"
                },
                {
                    "lang": "none",
                    "value": "yourRealmApp/\n\u2514\u2500\u2500 hosting/\n    \u251c\u2500\u2500 metadata.json\n    \u2514\u2500\u2500 files/\n        \u2514\u2500\u2500 <files to host>"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    \"path\": \"<File Resource Path>\",\n    \"attrs\": [{\n      \"name\": \"<Attribute Type>\",\n      \"value\": \"<Attribute Value>\"\n    }]\n  }\n]"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Attribute Type>\",\n  \"value\": \"<Attribute Value>\"\n}"
                },
                {
                    "lang": "text",
                    "value": "yourRealmApp/\n\u2514\u2500\u2500 values/\n    \u2514\u2500\u2500 <value name>.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<Value ID>\",\n  \"name\": \"<Value Name>\",\n  \"from_secret\": <boolean>,\n  \"value\": <Stored JSON Value|Secret Name>\n}"
                }
            ],
            "preview": "App Services uses JSON files and source code files to define and configure every\ncomponent of an application. Each component has a specific configuration file\nschema and every application uses a standard file structure to organize the\nconfiguration files.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/app",
            "title": "App Configuration Files",
            "headings": [
                "Root Configuration"
            ],
            "paragraphs": "You configure Application-level settings like your App name,\nenvironment, and deployment configuration in  root_config.json . If you're using the App Services CLI, you can find metadata information about\nyour specific deployed App instance in  the .mdb directory . Your configuration directory must include a  root_config.json  file\nwith the following structure: Field Description The application's name. Application names must be between 1 and 32 characters and may\nonly contain ASCII letters, numbers, underscores, and hyphens. The application's  deployment model . Valid options: \"GLOBAL\"  for  Global Deployment \"LOCAL\"  for  Local Deployment The name of the  cloud region  that the\napplication is deployed in. Global applications  process\nall database writes in this region, but serve other application\nrequests in the nearest deployment region. Local applications  process all\napplication requests and database writes in this region. The name of the environment the app should use when evaluating\n environment values . Valid options: Default:  \"\" \"\" \"development\" \"testing\" \"qa\" \"production\" An array of URLs that incoming requests may originate from. If you define\nany allowed request origins, then Atlas App Services blocks any incoming\nrequest from an origin that is not listed. Request origins are URLs with the following form: For example, the following are both valid request origins:",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 root_config.json"
                },
                {
                    "lang": "typescript",
                    "value": "{\n  \"name\": \"<App Name>\",\n  \"deployment_model\": \"<Deployment Model Type>\",\n  \"provider_region\": \"<Cloud Provider Region Name>\",\n  \"environment\": \"<Environment Name>\",\n  \"allowed_request_origins\": [\"<Request Origin URL>\", ...]\n}"
                },
                {
                    "lang": "text",
                    "value": "<scheme>://<host>[:port]"
                },
                {
                    "lang": "text",
                    "value": "https://www.example.com\nhttps://www.example.com:443"
                }
            ],
            "preview": "You configure Application-level settings like your App name,\nenvironment, and deployment configuration in root_config.json.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/functions",
            "title": "Function Configuration Files",
            "headings": [
                "Function Manifest",
                "Function Source Code"
            ],
            "paragraphs": "Every  function  in your app has a corresponding metadata entry\nin the function manifest file:  /functions/config.json . Atlas App Services automatically adds functions to the manifest on import if\nthey don't already have a configuration defined. If you're okay with the\ndefault settings, you can skip defining the configuration for a function and\nlet App Services do it for you. The manifest will include the generated\nconfigurations the next time you export or pull your app. Field Description The name of the function. The name must match the file name of a\n source code file  and be unique among\nall functions in your application. If  true , this function may only be called from other functions or in\n %function  rule expressions. You cannot call a private\nfunction directly from a client application or with an SDK. A  JSON expression  that evaluates to  true  if the\nfunction is allowed to execute. App Services evaluates this\nexpression for every incoming request. If  true , App Services omits the arguments provided to the\nfunction from the  function execution log entry . If  true , this function  runs as the system user . This overrides any values defined for\n run_as_user_id  and  run_as_user_id_script_source . The unique ID of a  App Services User  that the\nfunction always executes as. Cannot be used with\n run_as_user_id_script_source . A stringified  function  that runs whenever the\nfunction is called and returns the unique ID of a  App Services\nUser  that the function executes as. Cannot be used with\n run_as_user_id . You define a function's source code in a  .js  file within the  /functions \ndirectory that uses the function name as its file name. Each file must export\nthe main function that runs whenever a request calls the function. All of your function source code files must be in the  /functions \ndirectory.",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 functions/\n    \u251c\u2500\u2500 config.json\n    \u2514\u2500\u2500 <function>.js"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    \"name\": \"<Function Name>\",\n    \"private\": <Boolean>,\n    \"can_evaluate\": { <JSON Expression> },\n    \"disable_arg_logs\": <Boolean>,\n    \"run_as_system\": <Boolean>,\n    \"run_as_user_id\": \"<App Services User ID>\",\n    \"run_as_user_id_script_source\": \"<Function Source Code>\"\n  },\n  ...\n]"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function addOne(input) {\n  if(typeof input !== \"number\") {\n    throw new Error(\"You must call addOne() with a number\");\n  }\n  return input + 1;\n};"
                }
            ],
            "preview": "Every function in your app has a corresponding metadata entry\nin the function manifest file: /functions/config.json.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/data_sources",
            "title": "MongoDB Data Source Configuration Files",
            "headings": [
                "Service Configuration",
                "MongoDB Clusters",
                "Federated database instances",
                "Databases & Collections",
                "Collection Schema",
                "Relationships",
                "Default Rules",
                "Collection Rules",
                "Rule Configurations",
                "Roles",
                "Filters"
            ],
            "paragraphs": "Field Description Required. Default:  mongodb-atlas The service name used to refer to the cluster within this Atlas App Services\napp. The name may be at most 64 characters long and must only\ncontain ASCII letters, numbers, underscores, and hyphens. Required. For MongoDB Atlas clusters, this value is always  \"mongodb-atlas\" . Required. The name of the cluster in Atlas. The  read preference  mode for queries sent\nthrough the service. Mode Description primary App Services routes all read operations to the current replica set\n primary node . This is the\ndefault read preference mode. primaryPreferred App Services routes all read operations to the current replica set\n primary node  if it's\navailable. If the primary is unavailable, such as during an\n automatic failover , read requests are routed\nto a  secondary node \ninstead. secondary App Services routes all read operations to one of the current replica\nset  secondary nodes . secondaryPreferred App Services routes all read operations to one of the replica set's\navailable  secondary nodes . If no secondary is available,\nread requests are routed to the replica set  primary  instead. nearest App Services routes read operations to the  replica set member  that has the lowest network\nlatency relative to the client. If  true , clients may  connect to the app over the MongoDB Wire\nProtocol . Field Description Required. Default:  mongodb-datafederation The service name used to refer to the Federated database instance within this App Services\napp. The name may be at most 64 characters long and must only\ncontain ASCII letters, numbers, underscores, and hyphens. Required. For a Federated database instance, this value is always  \"datalake\" . Required. The name of the Federated database instance in Atlas. If you want to enforce a  schema  for a collection, define a\n schema.json  configuration file that contains a JSON schema for the\ndocuments. The root level schema must be an  object schema , which has the following form: Field Description A JSON schema  $ref  string that specifies the foreign collection. The\nstring has the following form: The name of the field in this collection's schema that specifies which\ndocuments in the foreign collection to include in the relationship. A\nforeign document is included if  source_key  contains the value of its\n foreign_key  field. The name of the field in the foreign collection's schema that contains\nthe value referenced by  source_key . If  true , the relationship may refer to multiple foreign documents\n(i.e. a \"to-many\" relationship). The  source_key  field must be an\narray with elements of the same type as the  foreign_key  field. If  false , the relationship may refer to zero or one foreign documents\n(i.e. a \"to-one\" relationship). The  source_key  field must be the same\ntype as the  foreign_key  field. An ecommerce app defines a relationship between two collections: each\ndocument in  store.orders  references one or more documents in the\n store.items  collection by including item  _id  values in the order's\n items  array. Both collection are in the same linked cluster\n( mongodb-atlas ) and database ( store ). The relationship is defined for the  orders  collection: You can define default rules that apply to all collections in a data\nsource that don't have more specific  collection-level rules  defined. You define default rules in the data source's  default_rule.json \nconfiguration file at  data_sources/<data-source-name>/default_rule.json . Field Description An array of  Role  configurations. An array of  Filter  configurations. If the data source is not a  Federated data source ,\nthen you can define collection-level rules in a collection's  rules.json \nconfiguration file. Field Description The name of the database that holds the collection. The name of the collection. An array of  Role  configurations. An array of  Filter  configurations. Field Description The name of the role. Role names identify and distinguish between\nroles in the same collection. Limited to 100 characters or fewer. An  expression  that evaluates to true when\nthis  role  applies to a user. When Device Sync (Flexible Mode) is not enabled, App Services assigns a\nrole on a per-document basis. When Device Sync (Flexible Mode) is\nenabled, App Services assigns roles on a per-collection, per-session\nbasis -- that is, for each synced collection when a client opens a sync\nconnection. To assign a role, App Services evaluates the  apply_when  of each\npotential role until one evaluates to true. Potential roles are any roles\nspecified in the  rules.json  configuration file for the given\ncollection or, if no  rules.json  file is found for the given\ncollection, the default role(s). App Services evaluates roles in the\norder that you specify them in your configuration. If no role matches,\nthen access is denied. For more information, see  Role-based Permissions . If Device Sync (Flexible Mode) is enabled, the assigned roles must be\n Sync compatible . If the role is not Sync\ncompatible, but its  apply_when  evaluated to true, others roles are\nnot considered; access is denied. A document with read and write expressions that determine whether\nthe role's other permissions may be evaluated. If Device Sync is enabled, both  document_filters.read  and\n document_filters.write   must  be defined to make the role\n Sync compatible . Sync incompatible roles\ndeny all access to Sync requests. If Device Sync is not enabled,  document_filters ,\n document_filters.read , and  document_filters.write  are all\noptional; an undefined  document_filters.read  or\n document_filters.write  defaults to true, allowing subsequent\npermissions to be evaluated. An  expression  that specifies whether  read , read\npermissions in  fields , and read permissions in  additional_fields \nmay be evaluated. If false (and  document_filters.write  is undefined\nor false), read access is denied for the entire document. To maintain  Sync compatibility , the\nexpression must be defined and may only reference  queryable fields . An  expression  that specifies whether  write ,\nwrite permissions in  fields , and write permissions in\n additional_fields  may be evaluated. If false, then write access is\ndenied for the entire document. To maintain  Sync compatibility , the\nexpression must be defined and may only reference  queryable fields . An  expression  that evaluates to true if the\nrole has permission to read all fields in the document. To maintain  Sync compatibility , the\nexpression must be a boolean literal (either  true  or  false ). Document-level read permissions take priority over any field-level\npermissions. If a role has document-level  read  permissions, it\napplies to all fields in the document. Read permissions specified by\n fields  or  additional_fields  do not override document-level\n read  permissions. To define a default fallback alongside field-level rules, leave  read \nundefined and use  additional_fields . An  expression  that evaluates to true if the\nrole has permission to add, modify, or remove all fields in the document. To maintain  Sync compatibility , the\nexpression must be a boolean literal (either  true  or  false ). Document-level write permissions take priority over any field-level\npermissions. If a role has document-level  write  permissions, it\napplies to all fields in the document. Write permissions specified by\n fields  or  additional_fields  do not override document-level\n write  permissions. To define a default fallback alongside field-level rules, leave  write \nundefined and use  additional_fields . You can use expansions like  %%root  and\n %%prevRoot  in  write  JSON expressions. Any time a role has  write  permission for a particular\nscope it also has  read  permission even if that is not\nexplicitly defined. An  expression  that evaluates to\n true  if the role has permission to insert a new document into the\ncollection. App Services only evaluates this expression for insert operations and\nonly after determining that the role has  write  permission for all\nfields in the new document. An  expression  that evaluates to true if the\nrole has permission to delete a document from the collection. App Services only evaluates this expression for delete operations and\nonly after determining that the role has  write  permission for all\nfields in the document to be deleted. An  expression  that evaluates to true if the\nrole has permission to search the collection using  Atlas Search . App Services performs  $search  operations as a system user and\nenforces field-level rules on the returned search results. This means that a\nuser may search on a field for which they do not have read access. In this\ncase, the search is based on the specified field but no returned documents\ninclude the field. A document where each key corresponds to a field name, and each value\ndefines the role's field-level  read  and  write  permissions for the\ncorresponding field in a queried document. To maintain  Sync compatibility , the inner\n read  and  write  expressions must be boolean literals (either\n true  or  false ). Document-level  read  or  write  permissions override all\nfield-level permissions of the same type. If permissions are\ndefined for a field that contains an embedded document, those\npermissions override any permissions defined for the\ndocument's embedded fields. An  expression  that evaluates to true if the\nrole has permission to read the field. To maintain  Sync compatibility , the\nexpression must be a boolean literal (either  true  or  false ). An  expression  that evaluates to true if the\nrole has permission to add, modify, or remove the field. To maintain  Sync compatibility , the\nexpression must be a boolean literal (either  true  or  false ). A  fields  document that defines  read  and  write \npermissions for fields that are embedded within this field in a\nqueried document. See the  Field-level Permissions for Embedded Documents  role pattern for more\ninformation. A document that defines the role's field-level  read  and\n write  permissions for any fields in a queried document that\ndon't have explicitly defined permissions in the  fields \ndocument. To maintain  Sync compatibility , the\ninner  read  and  write  expressions must be boolean literals (either\n true  or  false ). An  expression  that evaluates to true if the\nrole has permission to read any field that does not have a field-level\npermission definition in  fields . To maintain  Sync compatibility , the\nexpression must be boolean (either  true  or  false ). An  expression  that evaluates to true if the\nrole has permission to add, modify, or remove any field that does not\nhave a field-level permission definition in  fields . To maintain  Sync compatibility , the\nexpression must be boolean (either  true  or  false ). Field Description Required. The name of the filter. Filter names are\nuseful for identifying and distinguishing between filters.\nLimited to 100 characters or fewer. An  expression  that determines when this filter\napplies to an incoming MongoDB operation. Atlas App Services evaluates and applies filters before it reads any\ndocuments, so you cannot use  MongoDB document expansions  in a filter's Apply When expression.\nHowever, you can use other expansions like  %%user ,\n %%values , and  %function . A  MongoDB query  that App Services merges\ninto a filtered operation's existing query. A filter withholds documents that have a  score  below  20  using\nthe following query: A  MongoDB projection \nthat App Services merges into a filtered operation's existing projection. MongoDB projections can be either inclusive or exclusive, i.e.\nthey can either return only specified fields or withhold\nfields that are not specified. If multiple filters apply to a\nquery, the filters must all specify the same type of\nprojection, or the query will fail. A filter withholds the  _internal  field from all documents using\nthe following projection:",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 data_sources/\n    \u2514\u2500\u2500 <service name>/\n        \u251c\u2500\u2500 config.json\n        \u2514\u2500\u2500 <database>/\n            \u2514\u2500\u2500 <collection>/\n                \u251c\u2500\u2500 schema.json\n                \u251c\u2500\u2500 relationships.json\n                \u2514\u2500\u2500 rules.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Service Name>\",\n  \"type\": \"mongodb-atlas\",\n  \"config\": {\n    \"clusterName\": \"<Atlas Cluster Name>\",\n    \"readPreference\": \"<Read Preference>\",\n    \"wireProtocolEnabled\": <Boolean>\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Service Name>\",\n  \"type\": \"datalake\",\n  \"config\": {\n     \"dataLakeName\": \"<Federated database instance name>\"\n   }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"<Object Type Name>\",\n  \"bsonType\": \"object\",\n  \"properties\": {\n    \"<Property Name>\": { <Schema> },\n    ...\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"<Source Field Name>\": {\n    \"ref\": \"#/relationship/<Data Source Name>/<Database Name>/<Collection Name>\",\n    \"source_key\": \"<Source Field Name>\",\n    \"foreign_key\": \"<Foreign Field Name>\",\n    \"is_list\": <Boolean>\n  },\n  ...\n}"
                },
                {
                    "lang": "text",
                    "value": "#/relationship/<Data Source Name>/<Database Name>/<Collection Name>"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"items\": {\n    \"ref\": \"#/relationship/mongodb-atlas/store/items\",\n    \"source_key\": \"items\",\n    \"foreign_key\": \"_id\",\n    \"is_list\": true\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"roles\": [<Role>],\n  \"filters\": [<Filter>]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"database\": \"<Database Name>\",\n  \"collection\": \"<Collection Name>\",\n  \"roles\": [<Role>],\n  \"filters\": [<Filter>]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"name\": \"<Role Name>\",\n   \"apply_when\": { Expression },\n   \"document_filters\": {\n     \"read\": { Expression },\n     \"write\": { Expression }\n   },\n   \"read\": { Expression },\n   \"write\": { Expression },\n   \"insert\": { Expression },\n   \"delete\": { Expression },\n   \"search\": <Boolean>,\n   \"fields\": {\n      \"<Field Name>\": {\n         \"read\": { Expression },\n         \"write\": { Expression },\n         \"fields\": { Embedded Fields }\n      },\n      ...\n   },\n   \"additional_fields\": {\n     \"read\": { Expression },\n     \"write\": { Expression }\n   }\n}"
                },
                {
                    "lang": "json",
                    "value": "\"document_filters\": {\n  \"read\": { Expression },\n  \"write\": { Expression }\n}"
                },
                {
                    "lang": "json",
                    "value": "\"fields\": {\n  \"<Field Name>\": {\n     \"read\": { Expression },\n     \"write\": { Expression },\n     \"fields\": <Fields Document>\n  },\n  ...\n}"
                },
                {
                    "lang": "json",
                    "value": "\"additional_fields\": {\n  \"read\": { Expression },\n  \"write\": { Expression }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"name\": \"<Filter Name>\",\n  \"apply_when\": { Expression },\n  \"query\": { MongoDB Query },\n  \"projection\": { MongoDB Projection }\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"score\": { \"$gte\": 20 } }"
                },
                {
                    "lang": "json",
                    "value": "{ \"_internal\": 0 }"
                }
            ],
            "preview": "If you want to enforce a schema for a collection, define a\nschema.json configuration file that contains a JSON schema for the\ndocuments. The root level schema must be an object schema, which has the following form:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/v20210101",
            "title": "Application Configuration Files (v20210101) [Deprecated]",
            "headings": [],
            "paragraphs": "For detailed descriptions and examples of each component type's configuration\nand source code files, refer to the type's page in this section: This page describes a legacy configuration file format. You should\nonly use this information if you're using the deprecated\n realm-cli . Any configuration files you pull with App Services CLI or export from the UI\nuse the latest configuration version. For detailed information on the\ncurrent configuration file format, see  App Configuration . App Services App Users & Authentication Providers MongoDB Data Sources Environment Values Functions GraphQL Static Hosting HTTP Endpoints Log Forwarders Third-Party Services Atlas Device Sync Triggers Values",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u251c\u2500\u2500 realm_config.json\n\u251c\u2500\u2500 auth/\n\u251c\u2500\u2500 data_sources/\n\u251c\u2500\u2500 environments/\n\u251c\u2500\u2500 functions/\n\u251c\u2500\u2500 graphql/\n\u251c\u2500\u2500 hosting/\n\u251c\u2500\u2500 http_endpoints/\n\u251c\u2500\u2500 log_forwarders/\n\u251c\u2500\u2500 services/\n\u251c\u2500\u2500 sync/\n\u251c\u2500\u2500 triggers/\n\u2514\u2500\u2500 values/"
                }
            ],
            "preview": "For detailed descriptions and examples of each component type's configuration\nand source code files, refer to the type's page in this section:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/https_endpoints",
            "title": "HTTPS Endpoint Configuration Files",
            "headings": [
                "Custom HTTPS Endpoint Configuration",
                "Data API Configuration"
            ],
            "paragraphs": "Define the configurations for all of your app's  custom HTTPS\nendpoints  as an array in  https_endpoints/config.json . Field Description The endpoint  route . The type of  HTTP method  that the\nendpoint handles. Specify  *  to handle all methods with a\nsingle endpoint. One of: \"GET\" \"POST\" \"PUT\" \"PATCH\" \"DELETE\" \"DELETE\" \"*\" The name of the  function  associated\nwith the endpoint. The function should use the  endpoint\nfunction signature . The  endpoint authorization scheme \nused to validate incoming requests. One of: \"SECRET_AS_QUERY_PARAM\" \"VERIFY_PAYLOAD\" \"NO_VALIDATION\" The name of a  secret  that contains a string.\nIf  validation_method  is set to  SECRET_AS_QUERY_PARAM  or\n VERIFY_PAYLOAD , this secret is used to authorize requests. If  true , the endpoint returns a customizable HTTP response to\nthe client. You configure the response by calling the methods on\nthe  Response  object. If you do\nnot configure the response, the endpoint returns a  200 - Ok \nresponse with the value returned from the endpont function as the\nrequest body. If  false , requests return a  204 - No Content  response\nwith no data in the body. If  true , the authenticated user's  custom user data  document is available via\n context.user.custom_data . If  false , the user's custom data is not queried and\n context.user.custom_data  is an empty object. If  true , your app automatically creates a new user\nif the provided user credentials authenticate successfully but\naren't associated with an existing user. This setting is useful for apps that integrate with external\nauthentication system through the Custom JWT authentication\nprovider. If a request includes a valid JWT from the external system\nthat doesn't correspond to a registered user, this creates a new\nuser with the JWT as an identity. Enables ( false ) or disables ( true ) the endpoint. Define the configuration for your app's generated  Data API\nendpoints  in  https_endpoints/data_api_config.json . Field Description If  false , the Data API is not enabled. Generated endpoints\nwill not handle or respond to requests. A list of Data API versions that your app supports. The list may\ninclude a subset of all possible versions but must list the\nversions in ascending order. You cannot enable a version other\nthan the most recent version but any previously enabled versions\nlisted here will continue to work. Available Versions: \"v1\" The data format to use for data returned by endpoints in HTTPS\nresponse bodies. One of: \"EJSON\" \"JSON\" If  true , your app automatically creates a new user\nif the provided user credentials authenticate successfully but\naren't associated with an existing user. This setting is useful for apps that integrate with external\nauthentication system through the Custom JWT authentication\nprovider. If a request includes a valid JWT from the external system\nthat doesn't correspond to a registered user, this creates a new\nuser with the JWT as an identity. An application user's account ID. If defined, endpoints will\nalways run as the specified user. Cannot be used with  run_as_user_id_script_source . Stringified source code for a  function  that\nreturns an application user's account ID. If defined, endpoints\nexecute the function on every request and run as the user with\nthe ID returned from the function. Cannot be used with  run_as_user_id .",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 https_endpoints/\n    \u251c\u2500\u2500 config.json\n    \u2514\u2500\u2500 data_api_config.json"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    \"route\": \"<Endpoint Route Name>\",\n    \"http_method\": \"<HTTP method>\",\n    \"function_name\": \"<Endpoint function name\",\n    \"validation_method\": \"<Authorization scheme>\",\n    \"secret_name\": \"<Validation Secret Name>\",\n    \"respond_result\": <boolean>,\n    \"fetch_custom_user_data\": <boolean>,\n    \"create_user_on_auth\": <boolean>,\n    \"disabled\": <boolean>\n  }\n]"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"disabled\": <boolean>,\n  \"versions\": [\"v1\"],\n  \"return_type\": \"EJSON\" | \"JSON\",\n  \"create_user_on_auth\": <boolean>,\n  \"run_as_system\": <boolean>,\n  \"run_as_user_id\": \"<User Account ID>\",\n  \"run_as_user_id_script_source\": \"<Function Source Code>\"\n}"
                }
            ],
            "preview": "Define the configurations for all of your app's custom HTTPS\nendpoints as an array in https_endpoints/config.json.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/v20210101/services",
            "title": "Third-Party Service Configuration Files",
            "headings": [
                "Service Configuration",
                "Service Rules"
            ],
            "paragraphs": "This page describes a legacy configuration file format. You should\nonly use this information if you're using the deprecated\n realm-cli . Any configuration files you pull with App Services CLI or export from the UI\nuse the latest configuration version. For detailed information on the\ncurrent configuration file format, see  App Configuration . Field Description The name of the service. The name may be at most 64 characters\nlong and can only contain ASCII letters, numbers, underscores,\nand hyphens. The type of the service. Valid Options: \"aws\" \"twilio\" \"github\" \"gcm\" A document with fields that map to additional configuration\noptions for the service. The exact configuration fields depend on\nthe service  type . AWS Service Configuration Twilio Service Configuration GitHub Service Configuration A document where each field name is a private configuration field\nfor the service and the value of each field is the name of a\n Secret  that stores the configuration value. Rules for a specific external service are defined in the  /<service\nname>/rules  sub-directory. Each rule maps to its own JSON file with the same name as the rule. Field Description The name of the service rule. The name may be at most 64\ncharacters long and can only contain ASCII letters, numbers,\nunderscores, and hyphens. A list of  service actions  that the rule applies\nto. The specific actions available depend on the service  type . A  rule expression  that evaluates to  true  when\nthe rule applies to a given request.",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 services/\n    \u2514\u2500\u2500 <Service Name>/\n        \u251c\u2500\u2500 config.json\n        \u2514\u2500\u2500 rules/\n            \u2514\u2500\u2500 <Rule Name>.json"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"name\": \"<Service Name>\",\n   \"type\": \"<Service Type>\",\n   \"config\": {\n      \"<Configuration Option>\": <Configuration Value>\n   },\n   \"secret_config\": {\n      \"<Configuration Option>\": \"<Secret Name>\"\n   },\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Rule Name>\",\n  \"actions\": [\"<Service Action Name>\"],\n  \"when\": { <JSON Expression> }\n}"
                }
            ],
            "preview": "Rules for a specific external service are defined in the /<service\nname>/rules sub-directory.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/v20210101/hosting",
            "title": "Static Hosting Configuration Files",
            "headings": [
                "Hosting Configuration",
                "File Metadata"
            ],
            "paragraphs": "This page describes a legacy configuration file format. You should\nonly use this information if you're using the deprecated\n realm-cli . Any configuration files you pull with App Services CLI or export from the UI\nuse the latest configuration version. For detailed information on the\ncurrent configuration file format, see  App Configuration . Static Hosting is deprecated.  Learn More . You can enable and configure  static file hosting  for your\napplication in  hosting/config.json . Field Name Description If  true ,  static hosting  is enabled for your app. The  custom domain name  for\nyour application's hosted files. The default domain for your application's hosted files. Atlas App Services\nautomatically sets this value and you cannot change it. You can define metadata for any hosted file by adding an entry to\n hosting/metadata.json . Field Description The  resource path  of the file. An array of documents where each document represents a single\nmetadata attribute. Attribute documents have the following form: Field Description The name of the metadata attribute. This should be one of\nthe  file metadata attributes  that App Services supports. The value of the metadata attribute. If you do not specify a  Content-Type  metadata attribute for a hosted\nfile, Atlas App Services will attempt to automatically add a  Content-Type \nattribute to it based on the file extension. For example, App Services would automatically add the attribute\n Content-Type: application/html  to the file  myPage.html .",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 hosting/\n    \u251c\u2500\u2500 config.json\n    \u251c\u2500\u2500 metadata.json\n    \u2514\u2500\u2500 files/\n        \u2514\u2500\u2500 <files to host>"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"enabled\": <Boolean>,\n  \"custom_domain\": \"<Custom Domain Name>\",\n  \"app_default_domain\": \"<Default Domain Name>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    \"path\": \"<File Resource Path>\",\n    \"attrs\": [\n      {\n        \"name\": \"<Attribute Type>\",\n        \"value\": \"<Attribute Value>\"\n      },\n      ...\n    ]\n  },\n  ...\n]"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Attribute Type>\",\n  \"value\": \"<Attribute Value>\"\n}"
                }
            ],
            "preview": "You can enable and configure static file hosting for your\napplication in hosting/config.json.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/v20210101/environments",
            "title": "Environment Value Configuration Files",
            "headings": [
                "Environment Configuration"
            ],
            "paragraphs": "This page describes a legacy configuration file format. You should\nonly use this information if you're using the deprecated\n realm-cli . Any configuration files you pull with App Services CLI or export from the UI\nuse the latest configuration version. For detailed information on the\ncurrent configuration file format, see  App Configuration . You can define variable values for each  environment \nin a  .json  file within the  /environments  directory that uses the\nenvironment name as its file name. Atlas App Services supports the following environments: \"\" \"development\" \"testing\" \"qa\" \"production\" Field Description An object where each property maps the name of an environment value name\nto its value in the current environment.",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 environments/\n    \u251c\u2500\u2500 no-environment.json\n    \u251c\u2500\u2500 development.json\n    \u251c\u2500\u2500 testing.json\n    \u251c\u2500\u2500 qa.json\n    \u2514\u2500\u2500 production.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"values\": {\n    \"<Value Name>\": <Value>\n  }\n}"
                }
            ],
            "preview": "You can define variable values for each environment\nin a .json file within the /environments directory that uses the\nenvironment name as its file name.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/v20210101/sync",
            "title": "Atlas Device Sync Configuration Files",
            "headings": [],
            "paragraphs": "You can configure  Atlas Device Sync  for your application in the  sync \ndirectory: Refer to  Sync Configuration File Reference  for details. This page describes a legacy configuration file format. You should\nonly use this information if you're using the deprecated\n realm-cli . Any configuration files you pull with App Services CLI or export from the UI\nuse the latest configuration version. For detailed information on the\ncurrent configuration file format, see  App Configuration .",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 sync/\n    \u2514\u2500\u2500 config.json"
                }
            ],
            "preview": "You can configure Atlas Device Sync for your application in the sync\ndirectory:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/v20210101/log_forwarders",
            "title": "Log Forwarder Configuration Files",
            "headings": [],
            "paragraphs": "You define log forwarder configuration files in the  /log_forwarders \ndirectory. This page describes a legacy configuration file format. You should\nonly use this information if you're using the deprecated\n realm-cli . Any configuration files you pull with App Services CLI or export from the UI\nuse the latest configuration version. For detailed information on the\ncurrent configuration file format, see  App Configuration . Field Description A unique name for the log forwarder. An array of one or more log types that the forwarder should\nsend to a service. Atlas App Services only forwards a log if its type is\nlisted  and  its status is listed in  log_statuses . The array may contain the following log types: auth endpoint function graphql push schema service sync trigger trigger_error_handler An array of one or more log statuses that the forwarder should\nsend to a service. App Services only forwards a log if its type is\nlisted  and  its type is listed in  log_types . The array may contain the following log statuses: error success An object that configures the forwarder's batching policy. To forward logs individually: To group logs into batches: An object that configures where and how the forwarder sends logs. To forward logs to a linked MongoDB collection: To forward logs with a custom function:",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 log_forwarders/\n    \u2514\u2500\u2500 <Name>.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<name>\",\n  \"log_types\": [ \"<type>\", ... ],\n  \"log_statuses\": [ \"<status>\", ... ],\n  \"policy\": { batching policy },\n  \"action\": { action configuration }\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"type\": \"single\" }"
                },
                {
                    "lang": "json",
                    "value": "{ \"type\": \"batch\" }"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"collection\",\n  \"data_source\": \"<data source name>\",\n  \"database\": \"<database name>\",\n  \"collection\": \"<collection name>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"function\",\n  \"name\": \"<function name>\"\n}"
                }
            ],
            "preview": "You define log forwarder configuration files in the /log_forwarders\ndirectory.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/v20210101/auth",
            "title": "User & Authentication Provider Configuration Files",
            "headings": [
                "Authentication Providers",
                "Configuration",
                "Custom User Data"
            ],
            "paragraphs": "This page describes a legacy configuration file format. You should\nonly use this information if you're using the deprecated\n realm-cli . Any configuration files you pull with App Services CLI or export from the UI\nuse the latest configuration version. For detailed information on the\ncurrent configuration file format, see  App Configuration . You can enable and configure  authentication providers  in  /auth/providers.json . Each field of the configuration is the name of a provider type and contains a\nconfiguration object for that provider. Authentication provider configurations\nshare a common structure but each provider type uses a unique set of\nconfiguration fields. You can find detailed information on a specific provider's configuration on\nthat provider's reference page. For a list of all provider reference pages,\nsee  Authentication Providers . Field Description The name of the authentication provider. This will always be the same as\nthe the provider's  type . The authentication provider type. Valid options: \"anon-user\"  for  Anonymous  authentication. \"local-userpass\"  for  Email/Password  authentication. \"api-key\"  for  API Key  authentication. \"custom-token\"  for  Custom JWT  authentication. \"custom-function\"  for  Custom Function  authentication. \"oauth2-google\"  for  Google  authentication. \"oauth2-facebook\"  for  Facebook  authentication. \"oauth2-apple\"  for  Apple ID  authentication. If  true , this authentication provider is not enabled for your\napplication. Users cannot log in using credentials for a disabled\nprovider. A document that contains configuration values that are specific\nto the authentication provider. The following provider configurations include  config : Email/Password Custom JWT Custom Function Google Facebook Apple ID A document where each field name is a private configuration field\nfor the provider and the value of each field is the name of a\n Secret  that stores the configuration value. The following provider configurations include  redirect_uris : Custom JWT Google Facebook Apple ID An array of documents where each document defines a metadata\nfield that describes the user. The existence of this field and\nthe exact format of each metadata field document depends on the\nprovider type. The following provider configurations include  metadata_fields : Custom JWT Google Facebook A list of URLs that Atlas App Services may redirect user back to after\nthey complete an OAuth authorization. The following provider configurations include  redirect_uris : Google Facebook Apple ID You can configure the  custom user data \ncollection for your app in  /auth/custom_user_data.json . Field Name Description If  true , App Services associates each user with a document in the specified\ncollection that contains their custom data. The name of the  data source  that contains\nthe custom user data collection. The name of the database that contains the custom user data collection. The name of the collection that contains the custom user data. The name of the field in custom user data documents that\ncontains the user ID of the application user the document\ndescribes. The name of the  user creation function .",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 auth/\n    \u251c\u2500\u2500 providers.json\n    \u2514\u2500\u2500 custom_user_data.json"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"<Provider Name>\": {\n    \"name\": \"<Provider Name>\",\n    \"type\": \"<Provider Type>\",\n    \"disabled\": <Boolean>,\n    \"config\": {\n      \"<Configuration Option>\": \"<Configuration Value>\"\n    },\n    \"secret_config\": {\n      \"<Configuration Option>\": \"<Secret Name>\"\n    },\n    \"metadata_fields\": [\n      {\n        \"required\": <Boolean>,\n        \"name\": \"Field Name\"\n      },\n      ...\n    ],\n    \"redirect_uris\": [\"<Application Redirect URI>\", ...]\n  },\n  ...\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"enabled\": <Boolean>,\n  \"mongo_service_name\": \"<MongoDB Data Source Name>\",\n  \"database_name\": \"<Database Name>\",\n  \"collection_name\": \"<Collection Name>\",\n  \"user_id_field\": \"<Field Name>\",\n  \"on_user_creation_function_name\": \"<Function Name>\"\n}"
                }
            ],
            "preview": "You can configure the custom user data\ncollection for your app in /auth/custom_user_data.json.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/triggers",
            "title": "Trigger Configuration Files",
            "headings": [
                "General Configuration",
                "Database Triggers",
                "Authentication Triggers",
                "Scheduled Triggers"
            ],
            "paragraphs": "All triggers conform to a base schema with specific variations depending on the\ntrigger type. The following fields exist in  all  trigger configuration files: Field Description The trigger name. This may be at most 64 characters\nlong and can only contain ASCII letters, numbers, underscores,\nand hyphens. The trigger type. The value of this field determines the exact\nconfiguration file schema. Valid Options: \"DATABASE\" \"AUTHENTICATION\" \"SCHEDULED\" Defaults to  false . If  true , the trigger will not listen for any events and will\nnot fire. A document with fields that map to additional configuration options for\nthe trigger. The exact configuration fields depend on the trigger\n type : Database Triggers Authentication Triggers Scheduled Triggers A document that configures the trigger to send events to an event\nprocessor whenever it fires. Valid Options: For more information on Functions, see  Atlas Functions . For more information on AWS EventBridge, see  Send Trigger Events to AWS EventBridge . \"FUNCTION\" \"AWS_EVENTBRIDGE\" A document with fields that map to additional configuration options for\nthe event processor. The exact configuration fields depend on the event\nprocessor type: Function AWS EventBridge Database trigger configurations conform to the base trigger schema with\nadditional configuration options that specify which collection to watch and when\nto fire the trigger. The following fields exist in  database  trigger configuration files. There are\ntwo possible configurations depending on the event processor type: Database Trigger with a Function Event Processor Database Trigger with an AWS EventBridge Event Processor FUNCTION AWS_EVENTBRIDGE Field Description The name of the  MongoDB data source \nthat contains the watched collection. You cannot define a database\ntrigger on a  serverless instance  or\n Federated database instance . The name of the MongoDB database that contains the watched collection. The name of the collection that the trigger watches. A list of one or more  database operation\ntypes  that cause the trigger to fire. Valid operations types for all triggers: Valid operations types for database and deployment triggers: Valid operations types for deployment triggers only: \"INSERT\" \"UPDATE\" \"REPLACE\" \"DELETE\" \"CREATE_COLLECTION\" \"MODIFY_COLLECTION\" \"RENAME_COLLECTION\" \"SHARD_COLLECTION\" \"DROP_COLLECTION\" \"RESHARD_COLLECTION\" \"REFINE_COLLECTION_SHARD_KEY\" \"CREATE_INDEXES\" \"DROP_INDEXES\" \"DROP_DATABASE\" Update operations executed from MongoDB Compass or the MongoDB Atlas Data\nExplorer fully replace the previous document. As a result, update\noperations from these clients will generate  REPLACE  change events\nrather than  UPDATE  events. If  true ,  UPDATE  change events include the latest\n majority-committed  version\nof the modified document  after  the change was applied in the\n fullDocument  field. Regardless of this setting,  INSERT  and  REPLACE  events always\ninclude the``fullDocument`` field.  DELETE  events never include the\n fullDocument  field. If  true , change events include a copy of the modified document\nfrom immediately  before  the change was applied in the\n fullDocumentBeforeChange  field. All change events except for\n INSERT  events include the document preimage. Document preimages use extra information stored in the oplog.\nThe extra data may have performance implications for some apps. Once you've enabled document preimages for any trigger on a\ngiven collection, that collection will include preimage data in\nthe oplog and other triggers on the collection can use preimages\nwith no additonal overhead. You can disable document preimages on a per-trigger basis to\nexclude the preimage from change events. Regardless of your\ntrigger-level settings, a collection's oplog entries will\ncontinue to include preimage data unless you explicitly disable\npreimages for the collection. For more information, see  Document Preimages . If  true , the Trigger automatically resumes if the token\nrequired to process change stream events cannot be found. For more information on resuming suspended Triggers, see\n Suspended Triggers . If enabled, when this Trigger's resume token\ncannot be found in the cluster's oplog, the Trigger automatically resumes\nprocessing events at the next relevant change stream event.\nAll change stream events from when the Trigger was suspended until the Trigger\nresumes execution do not have the Trigger fire for them. If  true , indicates that event ordering is disabled for this trigger. If event ordering is enabled, multiple executions of this Trigger will occur\nsequentially based on the timestamps of the change events. If event ordering is\ndisabled, multiple executions of this Trigger will occur independently. Improve performance for Triggers that respond to bulk database operations\nby disabling event ordering.\n Learn more. A  $match  expression document\nthat App Services uses to filter which change events cause the Trigger to\nfire. The Trigger evaluates all change event objects that it receives against\nthis match expression and only executes if the expression evaluates to  true \nfor a given change event. MongoDB performs a full equality match for embedded documents in a match\nexpression. If you want to match a specific field in an embedded document,\nrefer to the field directly using  dot-notation . For more information, see\n Query on Embedded Documents  in\nthe MongoDB server manual. Limit the number of fields that the Trigger processes by using a\n $match  expression.\n Learn more. Defaults to  false . If  true , you can increase the maximum throughput\nbeyond the default 10,000 concurrent processes. For more information,\nsee  Maximize Throughput Triggers . Defaults to  false . If  true , enabling the Trigger after it was disabled\nwill not invoke events that occurred while the Trigger was disabled. A  $project \nexpression that selects a subset of fields from each event in the change\nstream. You can use this to  optimize the trigger's execution . The expression is an object that maps the name of fields in the change\nevent to either a  0 , which excludes the field, or a  1 , which\nincludes it. An expression can have values of either  0  or  1  but\nnot both together. This splits projections into two categories,\ninclusive and exclusive: An  inclusive  project expression specifies fields to include in each\nchange event document. The expression is an object that maps the name\nof fields to include to a  1 . If you don't include a field, it is\nnot included in the projected change event. The following projection includes only the  _id  and\n fullDocument  fields: An  exclusive  project expression specifies fields to exclude from\neach change event document. The expression is an object that maps the\nname of fields to include to a  0 . If you don't exclude a field, it\nis included in the projected change event. The following projection excludes the  _id  and\n fullDocument  fields: You cannot exclude the  operation_type  field with a projection.\nThis ensures that the trigger can always check if it should run for\na given event's operation type. An AWS account ID. For more information on how to find the account ID,\nrefer to  Setup the MongoDB Partner Event Source . An AWS region. false  by default. If  true , extended JSON is enabled. Triggers convert the BSON types in event objects into standard JSON types.\nTo preserve BSON type information, you can serialize event objects into\n Extended JSON format  instead.\nExtended JSON preserves type information at the expense of readability and\ninteroperability. If  true , error handling is enabled for the AWS EventBridge trigger. For more information on\nconfiguring error handling, refer to  Custom Error Handling . The name of the error handler function invoked when the AWS EventBridge\ntrigger fails and cannot be successfully retried. Authentication trigger configurations conform to the base trigger schema with\nadditional configuration options that specify which auth providers to watch and\nwhen to fire the trigger. The following fields exist in  authentication  trigger\nconfiguration files: Field Description The  authentication operation type  that causes the trigger to fire. Valid operations types: \"LOGIN\" \"CREATE\" \"DELETE\" A list of  authentication provider  types\nthat the trigger watches. Valid provider types: \"anon-user\" \"local-userpass\" \"api-key\" \"custom-token\" \"custom-function\" \"oauth2-facebook\" \"oauth2-google\" \"oauth2-apple\" Scheduled trigger configurations conform to the base trigger schema with\nadditional configuration options that specify the schedule on which the trigger\nfires. The following fields exist in  scheduled  trigger configuration files: Field Description The  CRON expression  that\nschedules the trigger's execution.",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 triggers/\n    \u2514\u2500\u2500 <trigger name>.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Trigger Name>\",\n  \"type\": \"<Trigger Type>\",\n  \"disabled\": <Boolean>,\n  \"config\": {},\n  \"event_processors\": {\n    \"<Type of Event Processor>\": {\n      \"config\": {}\n    }\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Trigger Name>\",\n  \"type\": \"DATABASE\",\n  \"disabled\": <boolean>,\n  \"config\": {\n    \"service_name\": \"<MongoDB Service Name>\",\n    \"database\": \"<Database Name>\",\n    \"collection\": \"<Collection Name>\",\n    \"operation_types\": [\"<Operation Type>\", ...],\n    \"full_document\": <boolean>,\n    \"full_document_before_change\": <boolean>,\n    \"tolerate_resume_errors\": <boolean>,\n    \"unordered\": <boolean>,\n    \"match\": { <Match Filter> },\n    \"maximum_throughput\": <boolean>,\n    \"skip_catchup_events\": <boolean>,\n    \"project\": { <Projection Filter> },\n  },\n  \"event_processors\": {\n     \"FUNCTION\": {\n         \"config\": {\n             \"function_name\": \"<Function Name>\"\n         }\n     }\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Trigger Name>\",\n  \"type\": \"DATABASE\",\n  \"disabled\": <boolean>,\n  \"config\": {\n    \"service_name\": \"<MongoDB Service Name>\",\n    \"database\": \"<Database Name>\",\n    \"collection\": \"<Collection Name>\",\n    \"operation_types\": [\"<Operation Type>\", ...],\n    \"full_document\": <boolean>,\n    \"full_document_before_change\": <boolean>,\n    \"tolerate_resume_errors\": <boolean>,\n    \"unordered\": <boolean>,\n    \"match\": { <Match Filter> },\n    \"maximum_throughput\": <boolean>,\n    \"skip_catchup_events\": <boolean>,\n    \"project\": { <Projection Filter> },\n  },\n  \"event_processors\": {\n    \"AWS_EVENTBRIDGE\": {\n      \"config\": {\n        \"account_id\": \"<AWS Account ID>\",\n        \"region\": \"<AWS Region>\",\n        \"extended_json_enabled\": <boolean>\n      }\n    }\n  },\n  \"error_handler\": {\n    \"config\": {\n      \"enabled\": <boolean>,\n      \"function_name\": \"<Error Handler Function Name>\"\n    }\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  _id: 1,\n  fullDocument: 1\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  _id: 0,\n  fullDocument: 0\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Trigger Name>\",\n  \"type\": \"AUTHENTICATION\",\n  \"config\": {\n    \"operation_type\": [\"<Operation Type>\", ...],\n    \"providers\": [\"<Provider Type>\", ...],\n  },\n  \"function_name\": \"<Trigger Function Name>\",\n  \"disabled\": <Boolean>\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Trigger Name>\",\n  \"type\": \"SCHEDULED\",\n  \"config\": {\n    \"schedule\": \"<CRON expression>\"\n  },\n  \"function_name\": \"<Trigger Function Name>\",\n  \"disabled\": <Boolean>\n}"
                }
            ],
            "preview": "Learn about Atlas Trigger configuration parameters.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "programming_language": [
                    "json"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/v20210101/graphql",
            "title": "GraphQL Configuration Files",
            "headings": [
                "Service Configuration",
                "Custom Resolver Configuration"
            ],
            "paragraphs": "You can configure the  GraphQL API  for your application in\nthe  graphql  directory: This page describes a legacy configuration file format. You should\nonly use this information if you're using the deprecated\n realm-cli . Any configuration files you pull with App Services CLI or export from the UI\nuse the latest configuration version. For detailed information on the\ncurrent configuration file format, see  App Configuration . GraphQL is deprecated.  Learn More . Field Description true  by default for new apps. You can only set the value to  false  when you  create a new app with the Admin API endpoint \nand pass the property into the body of the request. The value\ncannot be changed to  false  once the app is created. If  true , generated schema type names use common English\npluralization whenever possible. If  false , or if a natural pluralization cannot be determined,\nthen plural types use the singular type name with an  \"s\" \nappended to the end. App Services can use either a natural plural or a default plural for a\ngenerated \"mouse\" type: Natural: \"mice\" Default: \"mouses\" This value is  false  by default for new apps. If  true , the GraphQL API blocks  introspection\nqueries  from clients. This setting is useful for production Apps that do not want to\nexpose their GraphQL schema to the public. When introspection is\ndisabled, clients like GraphiQL cannot show docs for the API\nschema or autocomplete queries and mutations. Field Description The parent type that exposes the custom resolver as one of its fields. Valid Options: \"Query\"  (for  custom query  operations) \"Mutation\"  (for  custom mutation  operations) A singular  exposed  type name (for  computed properties ) The name of the field on the parent type that exposes the custom\nresolver. The field name must be unique among all custom resolver on its\nparent type. If the field name matches a field in the parent type's schema, the custom\nresolver overrides the schema type. The name of the  function  that runs when the\nresolver is called. The function arguments may accept a single argument\n(configured by  input_type  and  input_type_format ) and must return\na payload value (configured by  payload_type  and\n payload_type_format ). The type of the resolver's  input  argument (if it accepts input). You\ncan specify either the name of another type in your GraphQL schema or a\ncustom JSON schema specific to the resolver. A metadata description of the  input_type . Valid Options: \"scalar\"  (for a single value of a specific BSON type) \"scalar-list\"  (for multiple values of a specific BSON type) \"generated\"  (for a single value of a specific  exposed  type) \"generated-list\"  (for multiple values of a specific  exposed  type) \"custom\"  (for a custom JSON schema) The type of the value returned in the resolver's payload. You can specify\neither the name of another type in your GraphQL schema or a custom JSON\nschema specific to the resolver. If you do not specify a payload type, the resolver returns a\n DefaultPayload  object: A metadata description of the  payload_type . Valid Options: \"scalar\"  (for a single value of a specific BSON type) \"scalar-list\"  (for multiple values of a specific BSON type) \"generated\"  (for a single value of a specific  exposed  type) \"generated-list\"  (for multiple values of a specific  exposed  type) \"custom\"  (for a custom JSON schema)",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 graphql/\n    \u251c\u2500\u2500 config.json\n    \u2514\u2500\u2500 custom_resolvers\n        \u2514\u2500\u2500 <resolver name>.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"use_natural_pluralization\": <Boolean>,\n  \"disable_schema_introspection\": <Boolean>\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"on_type\": \"<Parent Type Name>\",\n  \"field_name\": \"<Resolver Field Name>\",\n  \"function_name\": \"<Resolver Function Name>\",\n  \"input_type\": \"<Input Type Name>\" | { <JSON Schema> },\n  \"input_type_format\": \"<Input Type Format>\",\n  \"payload_type\": \"<Payload Type Name>\" | { <JSON Schema> },\n  \"payload_type_format\": \"<Payload Type Format>\",\n}"
                },
                {
                    "lang": "graphql",
                    "value": "type DefaultPayload {\n  status: String!\n}\n"
                }
            ],
            "preview": "You can configure the GraphQL API for your application in\nthe graphql directory:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/v20210101/app",
            "title": "App Configuration Files",
            "headings": [
                "Configuration"
            ],
            "paragraphs": "You can configure high-level features of your application in\n realm_config.json . This page describes a legacy configuration file format. You should\nonly use this information if you're using the deprecated\n realm-cli . Any configuration files you pull with App Services CLI or export from the UI\nuse the latest configuration version. For detailed information on the\ncurrent configuration file format, see  App Configuration . Field Description The application's  App ID . The application's name. Application names must be between 1 and 32 characters and may\nonly contain ASCII letters, numbers, underscores, and hyphens. The schema version that all configuration files in the application\nconform to. This value is machine generated and you typically should not\nmanually set or modify it. The name of the environment the app should\nuse when evaluating  environment values . Valid options: Default:  \"\" \"\" \"development\" \"testing\" \"qa\" \"production\" An array of URLs that incoming requests may originate from. If you define\nany allowed request origins, then Atlas App Services blocks any incoming\nrequest from an origin that is not listed. Request origins are URLs with the following form: The application's  deployment model . Valid options: \"GLOBAL\"  for  Global Deployment \"LOCAL\"  for  Local Deployment The name of the  cloud region  that the application\nis deployed in. Global applications  process\nall database writes in this region, but serve other application\nrequests in the nearest deployment region. Local applications  process all\napplication requests and database writes in this region.",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 realm_config.json"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"app_id\": \"<App ID>\",\n  \"name\": \"<App Name>\",\n  \"config_version\": <Version Number>,\n  \"environment\": \"<Environment Name>\",\n  \"allowed_request_origins\": [\"<Origin URL>\", ...],\n  \"deployment_model\": \"<Deployment Model Type>\",\n  \"location\": \"<Deployment Cloud Region Name>\"\n}"
                },
                {
                    "lang": "text",
                    "value": "<scheme>://<host>[:port]"
                }
            ],
            "preview": "You can configure high-level features of your application in\nrealm_config.json.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/v20210101/values",
            "title": "Value Configuration Files",
            "headings": [
                "Configuration"
            ],
            "paragraphs": "You can define static  values  in the  /values  directory.\nEach value is defined in its own JSON file with the same name as the value. This page describes a legacy configuration file format. You should\nonly use this information if you're using the deprecated\n realm-cli . Any configuration files you pull with App Services CLI or export from the UI\nuse the latest configuration version. For detailed information on the\ncurrent configuration file format, see  App Configuration . Field Description A string that uniquely identifies the value. Atlas App Services automatically\ngenerates a unique ID for a value when you create it. A unique name for the value. This name is how you refer to\nthe value in functions and rules. Default:  false . If  true , the value exposes a\n Secret  instead of a plain-text JSON value. The stored data that App Services exposes when the value is referenced. If  from_secret  is  false ,  value  can be a standard\nJSON string, number, array, or object. If  from_secret  is  true ,  value  is a string that\ncontains the name of the  Secret  that the\nvalue exposes.",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 values/\n    \u2514\u2500\u2500 <value name>.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<Value ID>\",\n  \"name\": \"<Value Name>\",\n  \"from_secret\": <boolean>,\n  \"value\": <Stored JSON Value|Secret Name>\n}"
                }
            ],
            "preview": "You can define static values in the /values directory.\nEach value is defined in its own JSON file with the same name as the value.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/v20210101/functions",
            "title": "Function Configuration Files",
            "headings": [
                "Function Manifest",
                "Function Source Code"
            ],
            "paragraphs": "This page describes a legacy configuration file format. You should\nonly use this information if you're using the deprecated\n realm-cli . Any configuration files you pull with App Services CLI or export from the UI\nuse the latest configuration version. For detailed information on the\ncurrent configuration file format, see  App Configuration . Every  function  in your app has a corresponding metadata entry\nin the function manifest file:  /functions/config.json . Atlas App Services automatically adds functions to the manifest on import if\nthey don't already have a configuration defined. If you're okay with the\ndefault settings, you can skip defining the configuration for a function and\nlet App Services do it for you. The manifest will include the generated\nconfigurations the next time you export or pull your app. Field Description The name of the function. The name must match the file name of a\n source code file  and be unique among\nall functions in your application. If  true , this function may only be called from other functions or in\n %function  rule expressions. You cannot call a private\nfunction directly from a client application or with an SDK. A  JSON expression  that evaluates to  true  if the\nfunction is allowed to execute. App Services evaluates this\nexpression for every incoming request. If  true , App Services omits the arguments provided to the\nfunction from the  function execution log entry . If  true , this function  runs as the system user . This overrides any values defined for\n run_as_user_id  and  run_as_user_id_script_source . The unique ID of a  App Services User  that the\nfunction always executes as. Cannot be used with\n run_as_user_id_script_source . A stringified  function  that runs whenever the\nfunction is called and returns the unique ID of a  App Services\nUser  that the function executes as. Cannot be used with\n run_as_user_id . You define a function's source code in a  .js  file within the  /functions \ndirectory that uses the function name as its file name. Each file must export\nthe main function that runs whenever a request calls the function. All of your function source code files must be in the  /functions \ndirectory.",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 functions/\n    \u251c\u2500\u2500 config.json\n    \u2514\u2500\u2500 <function>.js"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    \"name\": \"<Function Name>\",\n    \"private\": <Boolean>,\n    \"can_evaluate\": { <JSON Expression> },\n    \"disable_arg_logs\": <Boolean>,\n    \"run_as_system\": <Boolean>,\n    \"run_as_user_id\": \"<App Services User ID>\",\n    \"run_as_user_id_script_source\": \"<Function Source Code>\"\n  },\n  ...\n]"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function addOne(input) {\n  if(typeof input !== \"number\") {\n    throw new Error(\"You must call addOne() with a number\");\n  }\n  return input + 1;\n};"
                }
            ],
            "preview": "Every function in your app has a corresponding metadata entry\nin the function manifest file: /functions/config.json.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/v20210101/data_sources",
            "title": "MongoDB Data Source Configuration Files",
            "headings": [
                "Service Configuration",
                "MongoDB Clusters",
                "Federated database instances",
                "Databases & Collections",
                "Collection Schema",
                "Relationships",
                "Default Rules",
                "Collection Rules",
                "Rule Configurations",
                "Roles",
                "Filters"
            ],
            "paragraphs": "This page describes a legacy configuration file format. You should\nonly use this information if you're using the deprecated\n realm-cli . Any configuration files you pull with App Services CLI or export from the UI\nuse the latest configuration version. For detailed information on the\ncurrent configuration file format, see  App Configuration . Field Description Required. Default:  mongodb-atlas The service name used to refer to the cluster within this Atlas App Services\napp. The name may be at most 64 characters long and must only\ncontain ASCII letters, numbers, underscores, and hyphens. Required. For MongoDB Atlas clusters, this value is always  \"mongodb-atlas\" . Required. The name of the cluster in Atlas. The  read preference  mode for queries sent\nthrough the service. Mode Description primary App Services routes all read operations to the current replica set\n primary node . This is the\ndefault read preference mode. primaryPreferred App Services routes all read operations to the current replica set\n primary node  if it's\navailable. If the primary is unavailable, such as during an\n automatic failover , read requests are routed\nto a  secondary node \ninstead. secondary App Services routes all read operations to one of the current replica\nset  secondary nodes . secondaryPreferred App Services routes all read operations to one of the replica set's\navailable  secondary nodes . If no secondary is available,\nread requests are routed to the replica set  primary  instead. nearest App Services routes read operations to the  replica set member  that has the lowest network\nlatency relative to the client. If  true , clients may  connect to the app over the MongoDB Wire\nProtocol . Field Description Required. Default:  mongodb-datafederation The service name used to refer to the Federated database instance within this App Services\napp. The name may be at most 64 characters long and must only\ncontain ASCII letters, numbers, underscores, and hyphens. Required. For a Federated database instance, this value is always  \"datalake\" . Required. The name of the Federated database instance in Atlas. If you want to enforce a  schema  for a collection, define a\n schema.json  configuration file that contains a JSON schema for the\ndocuments. The root level schema must be an  object schema , which has the following form: Field Description A JSON schema  $ref  string that specifies the foreign collection. The\nstring has the following form: The name of the field in this collection's schema that specifies which\ndocuments in the foreign collection to include in the relationship. A\nforeign document is included if  source_key  contains the value of its\n foreign_key  field. The name of the field in the foreign collection's schema that contains\nthe value referenced by  source_key . If  true , the relationship may refer to multiple foreign documents\n(i.e. a \"to-many\" relationship). The  source_key  field must be an\narray with elements of the same type as the  foreign_key  field. If  false , the relationship may refer to zero or one foreign documents\n(i.e. a \"to-one\" relationship). The  source_key  field must be the same\ntype as the  foreign_key  field. An ecommerce app defines a relationship between two collections: each\ndocument in  store.orders  references one or more documents in the\n store.items  collection by including item  _id  values in the order's\n items  array. Both collection are in the same linked cluster\n( mongodb-atlas ) and database ( store ). The relationship is defined for the  orders  collection: You can define default rules that apply to all collections in a data\nsource that don't have more specific  collection-level rules  defined. You define default rules in the data source's  default_rule.json \nconfiguration file at  data_sources/<data-source-name>/default_rule.json . Field Description An array of  Role  configurations. An array of  Filter  configurations. If the data source is not a  Federated data source ,\nthen you can define collection-level rules in a collection's  rules.json \nconfiguration file. Field Description The name of the database that holds the collection. The name of the collection. An array of  Role  configurations. An array of  Filter  configurations. Field Description The name of the role. Role names identify and distinguish between\nroles in the same collection. Limited to 100 characters or fewer. An  expression  that evaluates to true when\nthis  role  applies to a user. When Device Sync (Flexible Mode) is not enabled, App Services assigns a\nrole on a per-document basis. When Device Sync (Flexible Mode) is\nenabled, App Services assigns roles on a per-collection, per-session\nbasis -- that is, for each synced collection when a client opens a sync\nconnection. To assign a role, App Services evaluates the  apply_when  of each\npotential role until one evaluates to true. Potential roles are any roles\nspecified in the  rules.json  configuration file for the given\ncollection or, if no  rules.json  file is found for the given\ncollection, the default role(s). App Services evaluates roles in the\norder that you specify them in your configuration. If no role matches,\nthen access is denied. For more information, see  Role-based Permissions . If Device Sync (Flexible Mode) is enabled, the assigned roles must be\n Sync compatible . If the role is not Sync\ncompatible, but its  apply_when  evaluated to true, others roles are\nnot considered; access is denied. A document with read and write expressions that determine whether\nthe role's other permissions may be evaluated. If Device Sync is enabled, both  document_filters.read  and\n document_filters.write   must  be defined to make the role\n Sync compatible . Sync incompatible roles\ndeny all access to Sync requests. If Device Sync is not enabled,  document_filters ,\n document_filters.read , and  document_filters.write  are all\noptional; an undefined  document_filters.read  or\n document_filters.write  defaults to true, allowing subsequent\npermissions to be evaluated. An  expression  that specifies whether  read , read\npermissions in  fields , and read permissions in  additional_fields \nmay be evaluated. If false (and  document_filters.write  is undefined\nor false), read access is denied for the entire document. To maintain  Sync compatibility , the\nexpression must be defined and may only reference  queryable fields . An  expression  that specifies whether  write ,\nwrite permissions in  fields , and write permissions in\n additional_fields  may be evaluated. If false, then write access is\ndenied for the entire document. To maintain  Sync compatibility , the\nexpression must be defined and may only reference  queryable fields . An  expression  that evaluates to true if the\nrole has permission to read all fields in the document. To maintain  Sync compatibility , the\nexpression must be a boolean literal (either  true  or  false ). Document-level read permissions take priority over any field-level\npermissions. If a role has document-level  read  permissions, it\napplies to all fields in the document. Read permissions specified by\n fields  or  additional_fields  do not override document-level\n read  permissions. To define a default fallback alongside field-level rules, leave  read \nundefined and use  additional_fields . An  expression  that evaluates to true if the\nrole has permission to add, modify, or remove all fields in the document. To maintain  Sync compatibility , the\nexpression must be a boolean literal (either  true  or  false ). Document-level write permissions take priority over any field-level\npermissions. If a role has document-level  write  permissions, it\napplies to all fields in the document. Write permissions specified by\n fields  or  additional_fields  do not override document-level\n write  permissions. To define a default fallback alongside field-level rules, leave  write \nundefined and use  additional_fields . You can use expansions like  %%root  and\n %%prevRoot  in  write  JSON expressions. Any time a role has  write  permission for a particular\nscope it also has  read  permission even if that is not\nexplicitly defined. An  expression  that evaluates to\n true  if the role has permission to insert a new document into the\ncollection. App Services only evaluates this expression for insert operations and\nonly after determining that the role has  write  permission for all\nfields in the new document. An  expression  that evaluates to true if the\nrole has permission to delete a document from the collection. App Services only evaluates this expression for delete operations and\nonly after determining that the role has  write  permission for all\nfields in the document to be deleted. An  expression  that evaluates to true if the\nrole has permission to search the collection using  Atlas Search . App Services performs  $search  operations as a system user and\nenforces field-level rules on the returned search results. This means that a\nuser may search on a field for which they do not have read access. In this\ncase, the search is based on the specified field but no returned documents\ninclude the field. A document where each key corresponds to a field name, and each value\ndefines the role's field-level  read  and  write  permissions for the\ncorresponding field in a queried document. To maintain  Sync compatibility , the inner\n read  and  write  expressions must be boolean literals (either\n true  or  false ). Document-level  read  or  write  permissions override all\nfield-level permissions of the same type. If permissions are\ndefined for a field that contains an embedded document, those\npermissions override any permissions defined for the\ndocument's embedded fields. An  expression  that evaluates to true if the\nrole has permission to read the field. To maintain  Sync compatibility , the\nexpression must be a boolean literal (either  true  or  false ). An  expression  that evaluates to true if the\nrole has permission to add, modify, or remove the field. To maintain  Sync compatibility , the\nexpression must be a boolean literal (either  true  or  false ). A  fields  document that defines  read  and  write \npermissions for fields that are embedded within this field in a\nqueried document. See the  Field-level Permissions for Embedded Documents  role pattern for more\ninformation. A document that defines the role's field-level  read  and\n write  permissions for any fields in a queried document that\ndon't have explicitly defined permissions in the  fields \ndocument. To maintain  Sync compatibility , the\ninner  read  and  write  expressions must be boolean literals (either\n true  or  false ). An  expression  that evaluates to true if the\nrole has permission to read any field that does not have a field-level\npermission definition in  fields . To maintain  Sync compatibility , the\nexpression must be boolean (either  true  or  false ). An  expression  that evaluates to true if the\nrole has permission to add, modify, or remove any field that does not\nhave a field-level permission definition in  fields . To maintain  Sync compatibility , the\nexpression must be boolean (either  true  or  false ). Field Description Required. The name of the filter. Filter names are\nuseful for identifying and distinguishing between filters.\nLimited to 100 characters or fewer. An  expression  that determines when this filter\napplies to an incoming MongoDB operation. Atlas App Services evaluates and applies filters before it reads any\ndocuments, so you cannot use  MongoDB document expansions  in a filter's Apply When expression.\nHowever, you can use other expansions like  %%user ,\n %%values , and  %function . A  MongoDB query  that App Services merges\ninto a filtered operation's existing query. A filter withholds documents that have a  score  below  20  using\nthe following query: A  MongoDB projection \nthat App Services merges into a filtered operation's existing projection. MongoDB projections can be either inclusive or exclusive, i.e.\nthey can either return only specified fields or withhold\nfields that are not specified. If multiple filters apply to a\nquery, the filters must all specify the same type of\nprojection, or the query will fail. A filter withholds the  _internal  field from all documents using\nthe following projection:",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 data_sources/\n    \u2514\u2500\u2500 <service name>/\n        \u251c\u2500\u2500 config.json\n        \u2514\u2500\u2500 <database>/\n            \u2514\u2500\u2500 <collection>/\n                \u251c\u2500\u2500 schema.json\n                \u251c\u2500\u2500 relationships.json\n                \u2514\u2500\u2500 rules.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Service Name>\",\n  \"type\": \"mongodb-atlas\",\n  \"config\": {\n    \"clusterName\": \"<Atlas Cluster Name>\",\n    \"readPreference\": \"<Read Preference>\",\n    \"wireProtocolEnabled\": <Boolean>\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Service Name>\",\n  \"type\": \"datalake\",\n  \"config\": {\n     \"dataLakeName\": \"<Federated database instance name>\"\n   }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"<Object Type Name>\",\n  \"bsonType\": \"object\",\n  \"properties\": {\n    \"<Property Name>\": { <Schema> },\n    ...\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"<Source Field Name>\": {\n    \"ref\": \"#/relationship/<Data Source Name>/<Database Name>/<Collection Name>\",\n    \"source_key\": \"<Source Field Name>\",\n    \"foreign_key\": \"<Foreign Field Name>\",\n    \"is_list\": <Boolean>\n  },\n  ...\n}"
                },
                {
                    "lang": "text",
                    "value": "#/relationship/<Data Source Name>/<Database Name>/<Collection Name>"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"items\": {\n    \"ref\": \"#/relationship/mongodb-atlas/store/items\",\n    \"source_key\": \"items\",\n    \"foreign_key\": \"_id\",\n    \"is_list\": true\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"roles\": [<Role>],\n  \"filters\": [<Filter>]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"database\": \"<Database Name>\",\n  \"collection\": \"<Collection Name>\",\n  \"roles\": [<Role>],\n  \"filters\": [<Filter>]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"name\": \"<Role Name>\",\n   \"apply_when\": { Expression },\n   \"document_filters\": {\n     \"read\": { Expression },\n     \"write\": { Expression }\n   },\n   \"read\": { Expression },\n   \"write\": { Expression },\n   \"insert\": { Expression },\n   \"delete\": { Expression },\n   \"search\": <Boolean>,\n   \"fields\": {\n      \"<Field Name>\": {\n         \"read\": { Expression },\n         \"write\": { Expression },\n         \"fields\": { Embedded Fields }\n      },\n      ...\n   },\n   \"additional_fields\": {\n     \"read\": { Expression },\n     \"write\": { Expression }\n   }\n}"
                },
                {
                    "lang": "json",
                    "value": "\"document_filters\": {\n  \"read\": { Expression },\n  \"write\": { Expression }\n}"
                },
                {
                    "lang": "json",
                    "value": "\"fields\": {\n  \"<Field Name>\": {\n     \"read\": { Expression },\n     \"write\": { Expression },\n     \"fields\": <Fields Document>\n  },\n  ...\n}"
                },
                {
                    "lang": "json",
                    "value": "\"additional_fields\": {\n  \"read\": { Expression },\n  \"write\": { Expression }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"name\": \"<Filter Name>\",\n  \"apply_when\": { Expression },\n  \"query\": { MongoDB Query },\n  \"projection\": { MongoDB Projection }\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"score\": { \"$gte\": 20 } }"
                },
                {
                    "lang": "json",
                    "value": "{ \"_internal\": 0 }"
                }
            ],
            "preview": "If you want to enforce a schema for a collection, define a\nschema.json configuration file that contains a JSON schema for the\ndocuments. The root level schema must be an object schema, which has the following form:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/v20210101/triggers",
            "title": "Trigger Configuration Files",
            "headings": [
                "General Configuration",
                "Database Triggers",
                "Authentication Triggers",
                "Scheduled Triggers"
            ],
            "paragraphs": "This page describes a legacy configuration file format. You should\nonly use this information if you're using the deprecated\n realm-cli . Any configuration files you pull with App Services CLI or export from the UI\nuse the latest configuration version. For detailed information on the\ncurrent configuration file format, see  App Configuration . All triggers conform to a base schema with specific variations depending on the\ntrigger type. The following fields exist in  all  trigger configuration files: Field Description The trigger name. This may be at most 64 characters\nlong and can only contain ASCII letters, numbers, underscores,\nand hyphens. The trigger type. The value of this field determines the exact\nconfiguration file schema. Valid Options: \"DATABASE\" \"AUTHENTICATION\" \"SCHEDULED\" A document with fields that map to additional configuration options for\nthe trigger. The exact configuration fields depend on the trigger\n type : Database Triggers Authentication Triggers Scheduled Triggers The name of the  Atlas Function  that the\ntrigger executes whenever it fires. A document that configures the trigger to send events to external event\nprocessors whenever it fires. Cannot be used with  function_name . For more information, see  Send Trigger Events to AWS EventBridge . If  true , the trigger will not listen for any events and will\nnot fire. Database trigger configurations conform to the base trigger schema with\nadditional configuration options that specify which collection to watch and when\nto fire the trigger. The following fields exist in  database  trigger\nconfiguration files: Field Description The name of the  MongoDB data source \nthat contains the watched collection. You cannot define a database\ntrigger on a  serverless instance  or\n Federated database instance . The name of the MongoDB database that contains the watched collection. The name of the collection that the trigger watches. A list of one or more  database operation\ntypes  that cause the trigger to fire. Valid operations types: \"INSERT\" \"UPDATE\" \"REPLACE\" \"DELETE\" Update operations executed from MongoDB Compass or the MongoDB Atlas Data\nExplorer fully replace the previous document. As a result, update\noperations from these clients will generate  REPLACE  change events\nrather than  UPDATE  events. If  true ,  UPDATE  change events include the latest\n majority-committed  version\nof the modified document  after  the change was applied in the\n fullDocument  field. Regardless of this setting: INSERT  and  REPLACE  events always include the\n fullDocument  field. DELETE  events never include the  fullDocument  field. If  true , change events include a copy of the modified document\nfrom immediately  before  the change was applied in the\n fullDocumentBeforeChange  field. All change events except for\n INSERT  events include the document preimage. Document preimages use extra information stored in the oplog.\nThe extra data may have performance implications for some apps. Once you've enabled document preimages for any trigger on a\ngiven collection, that collection will include preimage data in\nthe oplog and other triggers on the collection can use preimages\nwith no additonal overhead. You can disable document preimages on a per-trigger basis to\nexclude the preimage from change events. Regardless of your\ntrigger-level settings, a collection's oplog entries will\ncontinue to include preimage data unless you explicitly disable\npreimages for the collection. For more information, see  Document Preimages . If  true , the Trigger automatically resumes if the token\nrequired to process change stream events cannot be found. For more information on resuming suspended Triggers, see\n Suspended Triggers . If enabled, when this Trigger's resume token\ncannot be found in the cluster's oplog, the Trigger automatically resumes\nprocessing events at the next relevant change stream event.\nAll change stream events from when the Trigger was suspended until the Trigger\nresumes execution do not have the Trigger fire for them. If  true , indicates that event ordering is disabled for this trigger. If event ordering is enabled, multiple executions of this Trigger will occur\nsequentially based on the timestamps of the change events. If event ordering is\ndisabled, multiple executions of this Trigger will occur independently. Improve performance for Triggers that respond to bulk database operations\nby disabling event ordering.\n Learn more. A  $match  expression document\nthat App Services uses to filter which change events cause the Trigger to\nfire. The Trigger evaluates all change event objects that it receives against\nthis match expression and only executes if the expression evaluates to  true \nfor a given change event. MongoDB performs a full equality match for embedded documents in a match\nexpression. If you want to match a specific field in an embedded document,\nrefer to the field directly using  dot-notation . For more information, see\n Query on Embedded Documents  in\nthe MongoDB server manual. Limit the number of fields that the Trigger processes by using a\n $match  expression.\n Learn more. A  $project \nexpression that selects a subset of fields from each event in the change\nstream. You can use this to  optimize the trigger's execution . The expression is an object that maps the name of fields in the change\nevent to either a  0 , which excludes the field, or a  1 , which\nincludes it. An expression can have values of either  0  or  1  but\nnot both together. This splits projections into two categories,\ninclusive and exclusive: An  inclusive  project expression specifies fields to include in each\nchange event document. The expression is an object that maps the name\nof fields to include to a  1 . If you don't include a field, it is\nnot included in the projected change event. The following projection includes only the  _id  and\n fullDocument  fields: An  exclusive  project expression specifies fields to exclude from\neach change event document. The expression is an object that maps the\nname of fields to include to a  0 . If you don't exclude a field, it\nis included in the projected change event. The following projection excludes the  _id  and\n fullDocument  fields: You cannot exclude the  operation_type  field with a projection.\nThis ensures that the trigger can always check if it should run for\na given event's operation type. Authentication trigger configurations conform to the base trigger schema with\nadditional configuration options that specify which auth providers to watch and\nwhen to fire the trigger. The following fields exist in  authentication  trigger\nconfiguration files: Field Description The  authentication operation type  that causes the trigger to fire. Valid operations types: \"LOGIN\" \"CREATE\" \"DELETE\" A list of  authentication provider  types\nthat the trigger watches. Valid provider types: \"anon-user\" \"local-userpass\" \"api-key\" \"custom-token\" \"custom-function\" \"oauth2-facebook\" \"oauth2-google\" \"oauth2-apple\" Scheduled trigger configurations conform to the base trigger schema with\nadditional configuration options that specify the schedule on which the trigger\nfires. The following fields exist in  scheduled  trigger configuration files: Field Description The  CRON expression  that\nschedules the trigger's execution.",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 triggers/\n    \u2514\u2500\u2500 <trigger name>.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Trigger Name>\",\n  \"type\": \"<Trigger Type>\",\n  \"config\": {},\n  \"function_name\": \"<Trigger Function Name>\",\n  \"disabled\": <Boolean>\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Trigger Name>\",\n  \"type\": \"DATABASE\",\n  \"config\": {\n    \"service_name\": \"<MongoDB Service Name>\",\n    \"database\": \"<Database Name>\",\n    \"collection\": \"<Collection Name>\",\n    \"operation_types\": [\"<Operation Type>\", ...],\n    \"full_document\": <boolean>,\n    \"full_document_before_change\": <boolean>,\n    \"tolerate_resume_errors\": <boolean>,\n    \"unordered\": <boolean>,\n    \"match\": { <Match Filter> },\n    \"project\": { <Projection Filter> },\n  },\n  \"function_name\": \"<Trigger Function Name>\",\n  \"disabled\": <Boolean>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  _id: 1,\n  fullDocument: 1\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  _id: 0,\n  fullDocument: 0\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Trigger Name>\",\n  \"type\": \"AUTHENTICATION\",\n  \"config\": {\n    \"operation_type\": [\"<Operation Type>\", ...],\n    \"providers\": [\"<Provider Type>\", ...],\n  },\n  \"function_name\": \"<Trigger Function Name>\",\n  \"disabled\": <Boolean>\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Trigger Name>\",\n  \"type\": \"SCHEDULED\",\n  \"config\": {\n    \"schedule\": \"<CRON expression>\"\n  },\n  \"function_name\": \"<Trigger Function Name>\",\n  \"disabled\": <Boolean>\n}"
                }
            ],
            "preview": "All triggers conform to a base schema with specific variations depending on the\ntrigger type. The following fields exist in all trigger configuration files:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "reference/config/v20210101/http_endpoints",
            "title": "HTTP Endpoint Configuration Files",
            "headings": [
                "Custom HTTPS Endpoint Configuration",
                "Data API Configuration",
                "[Deprecated] HTTP Service Configuration",
                "[Deprecated] Service Action Rules",
                "[Deprecated] Incoming Webhooks",
                "Configuration",
                "Source Code"
            ],
            "paragraphs": "This page describes a legacy configuration file format. You should\nonly use this information if you're using the deprecated\n realm-cli . Any configuration files you pull with App Services CLI or export from the UI\nuse the latest configuration version. For detailed information on the\ncurrent configuration file format, see  App Configuration . Define the configurations for your all of your app's  custom HTTPS\nendpoints  as an array in  http_endpoints/config.json . Field Description The endpoint  route . The type of  HTTP method  that the\nendpoint handles. Specify  *  to handle all methods with a\nsingle endpoint. One of: \"GET\" \"POST\" \"PUT\" \"PATCH\" \"DELETE\" \"DELETE\" \"*\" The name of the  function  associated\nwith the endpoint. The function should use the  endpoint\nfunction signature . The  endpoint authorization scheme \nused to validate incoming requests. One of: \"SECRET_AS_QUERY_PARAM\" \"VERIFY_PAYLOAD\" \"NO_VALIDATION\" The name of a  secret  that contains a string.\nIf  validation_method  is set to  SECRET_AS_QUERY_PARAM  or\n VERIFY_PAYLOAD , this secret is used to authorize requests. If  true , the endpoint returns a customizable HTTP response to\nthe client. You configure the response by calling the methods on\nthe  Response  object. If you do\nnot configure the response, the endpoint returns a  200 - Ok \nresponse with the value returned from the endpont function as the\nrequest body. If  false , requests return a  204 - No Content  response\nwith no data in the body. If  true , the authenticated user's  custom user data  document is available via\n context.user.custom_data . If  false , the user's custom data is not queried and\n context.user.custom_data  is an empty object. If  true , your app automatically creates a new user\nif the provided user credentials authenticate successfully but\naren't associated with an existing user. This setting is useful for apps that integrate with external\nauthentication system through the Custom JWT authentication\nprovider. If a request includes a valid JWT from the external system\nthat doesn't correspond to a registered user, this creates a new\nuser with the JWT as an identity. Enables ( false ) or disables ( true ) the endpoint. Define the configuration for your app's generated  Data API\nendpoints  in  http_endpoints/data_api_config.json . Field Description If  false , the Data API is not enabled. Generated endpoints\nwill not handle or respond to requests. An list of Data API versions that your app supports. The list may\ninclude a subset of all possible versions but must list the\nversions in ascending order. You cannot enable a version other\nthan the most recent version but any previously enabled versions\nlisted here will continue to work. Available Versions: \"v1\" The data format to use for data returned by endpoints in HTTPS\nresponse bodies. One of: \"EJSON\" \"JSON\" If  true , your app automatically creates a new user\nif the provided user credentials authenticate successfully but\naren't associated with an existing user. This setting is useful for apps that integrate with external\nauthentication system through the Custom JWT authentication\nprovider. If a request includes a valid JWT from the external system\nthat doesn't correspond to a registered user, this creates a new\nuser with the JWT as an identity. An application user's account ID. If defined, endpoints will\nalways run as the specified user. Cannot be used with  run_as_user_id_script_source . Stringified source code for a  function  that\nreturns an application user's account ID. If defined, endpoints\nexecute the function on every request and run as the user with\nthe ID returned from the function. Cannot be used with  run_as_user_id . Deprecated legacy HTTP services are grouped into named services within\n /http_endpoints . Field Description The name of the HTTP endpoints service. This must be unique among all\nHTTP endpoint services in the app and match the name of its containing\ndirectory. For HTTP endpoints, this value is always  \"http\" . Additional configuration options for the service. HTTP Endpoints\ncurrently have no additional configuration options. You define  service action rules  in the service's\n rules/  sub-directory. Each rule maps to its own  .json  configuration file\nwith the same name as the rule. Field Description The name of the service rule. The name may be at most 64\ncharacters long and can only contain ASCII letters, numbers,\nunderscores, and hyphens. A list of  HTTP actions  that the rule\napplies to. A  rule expression  that evaluates to  true  when\nthe rule applies to a given request. Field Description The name of the webhook. This must be unique among all webhooks in the\nHTTP endpoints service and match the name of its containing directory. A  JSON expression  that evaluates to  true  if the\nwebhook is allowed to execute. Atlas App Services evaluates this\nexpression for every incoming request. If  true , App Services omits the arguments provided to the webhook\nfrom the  function execution log entry . If  true , the webhook function runs in the context of an existing\napplication user specified by each incoming request. Incoming requests\nmust include the user's authentication provider credentials in either the\nrequest body or the request headers. For an example of how to specify credentials, see\n Configure Service Webhooks [Deprecated] . The unique ID of a  App Services User  that\nthe function always executes as. Cannot be used with\n run_as_user_id_script_source  or  run_as_authed_user . A stringified  function  that runs whenever the\nwebhook is called and returns the unique ID of a  App Services\nUser  that the function executes as. Cannot be used with\n run_as_user_id  or  run_as_authed_user . If  true , App Services includes the webhook function return value as\nthe body of the HTTP response it sends to the client that initiated the\nwebhook request. If  true , App Services queries the requesting user's  custom\nuser data  and, if it exists, exposes the data as an\nobject on the  context.user.custom_data  property. This option is only available if  run_as_authed_user  is set to\n true . If  true , App Services automatically creates a new user based\non the provided user credentials if they don't match an already existing\nuser (e.g. no other user has the specified email address). The\nauthentication provider that corresponds to the credentials must be\nenabled at the time of the request to create a new user. This option is only available if  run_as_authed_user  is set to\n true . A document that contains configuration options for the webhook. Field Description The HTTP method type that the webhook accepts. Incoming\nwebhook requests must use this method. The name of the  request validation method  that the webhook uses. Valid options: \"VERIFY_PAYLOAD\" \"SECRET_AS_QUERY_PARAM\" \"NO_VALIDATION\" The secret value used to  validate incoming webhook\nrequests . You define a webhook function's source code in a  source.js  file within the\nwebhook directory. Each file must export the main function that runs whenever a\nrequest calls the webhook.",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 http_endpoints/\n    \u251c\u2500\u2500 config.json\n    \u251c\u2500\u2500 data_api_config.json\n    \u2514\u2500\u2500 [Deprecated] <Service Name>/\n        \u251c\u2500\u2500 config.json\n        \u251c\u2500\u2500 rules/\n        \u2502   \u2514\u2500\u2500 <Rule Name>.json\n        \u2514\u2500\u2500 incoming_webhooks/\n            \u2514\u2500\u2500 <Webhook Name>/\n                \u251c\u2500\u2500 config.json\n                \u2514\u2500\u2500 source.js"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    \"route\": \"<Endpoint route name>\",\n    \"http_method\": \"<HTTP method>\",\n    \"function_name\": \"<Endpoint function name\",\n    \"validation_method\": \"<Authorization scheme>\",\n    \"secret_name\": \"<Validation Secret Name>\",\n    \"respond_result\": <boolean>,\n    \"fetch_custom_user_data\": <boolean>,\n    \"create_user_on_auth\": <boolean>,\n    \"disabled\": <boolean>\n  }\n]"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"disabled\": <boolean>,\n  \"versions\": [\"v1\"],\n  \"return_type\": \"EJSON\" | \"JSON\",\n  \"create_user_on_auth\": <boolean>,\n  \"run_as_system\": <boolean>,\n  \"run_as_user_id\": \"<User Account ID>\",\n  \"run_as_user_id_script_source\": \"<Function Source Code>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Service Name>\",\n  \"type\": \"http\",\n  \"config\": {}\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Rule Name>\",\n  \"actions\": [\"<Service Action Name>\"],\n  \"when\": { <JSON Rule Expression> }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Webhook Name>\",\n  \"can_evaluate\": { <JSON Expression> },\n  \"run_as_authed_user\": <Boolean>,\n  \"run_as_user_id\": \"<App Services User ID>\",\n  \"run_as_user_id_script_source\": \"<Function Source Code>\",\n  \"fetch_custom_user_data\": <Boolean>,\n  \"create_user_on_auth\": <Boolean>,\n  \"respond_result\": <Boolean>,\n  \"options\": {\n    \"httpMethod\": \"<HTTP Method>\",\n    \"validationMethod\": \"<Webhook Validation Method>\",\n    \"secret\": \"<Webhook Secret>\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"httpMethod\": \"<HTTP Method>\",\n  \"validationMethod\": \"<Webhook Validation Method>\",\n  \"secret\": \"<Webhook Secret>\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function (payload, response) {\n  // Convert the webhook body from BSON to an EJSON object\n  const body = EJSON.parse(payload.body.text());\n\n  // Execute application logic, such as working with MongoDB\n  if (body.someField) {\n    const mdb = context.services.get(\"mongodb-atlas\");\n    const requests = mdb.db(\"demo\").collection(\"requests\");\n    const { insertedId } = await requests.insertOne({\n      someField: body.someField,\n    });\n    // Respond with an affirmative result\n    response.setStatusCode(200);\n    response.setBody(`Successfully saved \"someField\" with _id: ${insertedId}.`);\n  } else {\n    // Respond with a malformed request error\n    response.setStatusCode(400);\n    response.setBody(`Could not find \"someField\" in the webhook request body.`);\n  }\n  // This return value does nothing because we already modified the response object.\n  // If you do not modify the response object and you enable *Respond with Result*,\n  // App Services will include this return value as the response body.\n  return { msg: \"finished!\" };\n};\n"
                }
            ],
            "preview": "Define the configurations for your all of your app's custom HTTPS\nendpoints as an array in http_endpoints/config.json.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "data-api/generated-endpoints",
            "title": "Data API Endpoints",
            "headings": [
                "When to Use the Data API",
                "Base URL",
                "Set Up the Data API",
                "Authentication",
                "Authorization",
                "Built-In Authorization Schemes",
                "Custom Authorization Schemes",
                "Response Type",
                "Access Permissions",
                "API Versions",
                "Call a Data API Endpoint",
                "Choose an API Version",
                "Specify the Request Data Format",
                "Choose a Response Data Format",
                "Authenticate the Request",
                "Authorize the Request"
            ],
            "paragraphs": "The Data API lets you securely read and write data using standard HTTPS\nrequests. The API includes automatically generated endpoints that each\nrepresent a MongoDB operation. You can use the endpoints to create,\nread, update, delete, and aggregate documents in a MongoDB data source. For example, this  POST  request stores a\ndocument in a linked cluster by calling the  insertOne  endpoint: For a detailed API reference of all available endpoints, see the\n Data API OpenAPI Reference . You can use the Data API to integrate with any app or service that\nsupports HTTPS requests. For example, you might: An operation called through an API endpoint will likely take longer than\nthe corresponding MongoDB operation called through a connected MongoDB\nDriver. For high-load use-cases and latency sensitive applications, we\nrecommend connecting directly to your database with a MongoDB driver. To\nlearn more, visit the  MongoDB Drivers  documentation. call the API from a serverless edge function query Atlas from a mobile application access test data and log events in a CI/CD workflow integrate Atlas into a federated API gateway connect from an environment not currently supported via a MongoDB\nDriver or Realm SDK The Data API endpoints in an app share a base URL. The URL uses your App\nID to uniquely point to your app and specifies which version of the Data\nAPI to call. Each endpoint has a unique URL formed by appending the\nendpoint's route to your app's base URL. Globally deployed apps use the following format: Endpoints in a locally deployed app use a base URL specific to the app's\n deployment region  (e.g.  us-east-1.aws ): You can configure the Data API for your app from the App Services UI or by\ndeploying configuration files with App Services CLI: Click  HTTPS Endpoints  in the left navigation menu and then\nselect the  Data API  tab. Enable the Data API for your app. This generates endpoints that\ncan access any MongoDB data source linked to your app. Choose an  authentication \nmethod and enable authentication providers. Choose a  response type . Save the Data API configuration. Configure  access permissions  by\ndefining rules for collections in your linked data sources to\nallow requests to securely read and write data. Save and deploy your app. Pull the latest version of your app. Define a Data API configuration file. Deploy your app. Data API endpoints run in the context of a specific user, which allows\nyour app to enforce rules and validate document schemas for each\nrequest. By default, endpoints use Application Authentication, which requires\neach request to include credentials for one of your application users,\nlike an API key or JWT. You can also configure other custom\nauthentication schemes to fit your application's needs. For examples of how to authenticate requests, see  Authenticate\nData API Requests . Application authentication requires users to log in with an\nauthentication provider that you have enabled for your App.\nRequests can either include an access token granted by the\nauthentication provider or the credentials the user would log in\nwith (e.g. their API key or email and password). User ID authentication runs all requests as a single, pre-selected\napplication user. This is useful if all requests should have the\nsame permissions regardless of who called the endpoint. To select the user, specify their User Account ID in your app's\nData API configuration. Script authentication calls a function to determine which\napplication user a request runs as. You can use this to implement\ncustom authentication and authorization schemes. The function must return an existing application user's Account ID\nas a string or  { \"runAsSystem\": true }  to run the request\nas a  system user  that has full access to\nMongoDB CRUD and Aggregation APIs and is not affected by any\nrules, roles, or permissions. To define the function, specify the source code in your app's Data\nAPI configuration. You can require authenticated users to provide additional authorization\ninformation in each request. You define the authorization scheme for\nall generated Data API endpoints in your Data API configuration. Endpoints natively support a set of built-in authorization schemes that\nuse a secret string to prove that the request is authorized. You can\nalso define a custom authorization scheme that you can use together with\nor instead of the built-in schemes. Endpoints support the following built-in authorization schemes: All authenticated users are authorized to call the endpoint.\nAuthenticated requests do not need to include authorization\ninformation. Authenticated users must prove that they are authorized to call\nthe endpoint by including a specific string as the value of the\n secret  query parameter. You define the string in a  secret  and\nreference the secret by name in the endpoint configuration. To learn how to include the secret in a request, see  Authorize the Request . Authenticated users must prove that they are authorized to call\nthe endpoint by including an  Endpoint-Signature  header that\ncontains a hexadecimal-encoded  HMAC (Hash-based Message\nAuthentication Code)  SHA-256 hash generated from the request body\nand a secret string. You define the string in a  secret  and\nreference the secret by name in the endpoint configuration. To learn how to sign your requests, see  Authorize the Request . You can define a custom authorization  expression  to\ndetermine if an incoming authenticated request is allowed to run. The\nexpression is evaluated for each request and must evaluate to  true \nto allow the request. If the expression evaluates to  false , the\nrequest is not authorized and fails with an error. Requests that fail\nauthorization are not counted toward your App's billed usage. Authorization expressions can use variables like\n %%user  to authorize based on the calling user's data\nor  %%request  to make decisions based on the specifics\nof each incoming request. To define a custom authorization scheme, specify the expression in your\napp's Data API configuration: Endpoints can return data in one of two  data formats , either JSON or EJSON. By default, endpoints return JSON, which is a standard data format that\nis widely supported modern langauges and platforms. However, JSON cannot\nrepresent every data type that you can store in MongoDB and loses type\ninformation for some data types. You can also configure endpoints to return EJSON, which uses structured\nJSON objects to fully represent the types that MongoDB supports. This\npreserves type information in responses but requires that your\napplication understands how to parse and use EJSON. The official MongoDB drivers include methods for working with EJSON.\nYou can also download a standalone parser like  bson  on npm. The Data API uses your app's  data access rules  to\ndetermine if a user can read and write data. To allow Data API requests\nto access a specific collection, you must first define rules for the\ncollection. You can also set up an  IP Access List  for\nadditional security. The Data API uses a built-in versioning scheme to upgrade endpoints over\ntime while maintaining backwards compatibility. Incoming requests can\nspecify which version of an endpoint to use in the request URL and the\nData API can serve any version that you have enabled. You must enable a new version before users can call endpoints with that\nversion. You can always enable the most recent Data API version.\nHowever, you cannot enable an older version after a newer version has\nbeen released. The following versions are currently supported: beta v1 You can call a Data API endpoint from any standard HTTP client. Each\nrequest can include configuration headers and arguments in the request\nbody. HTTP/1.1 or greater is required when making requests. Data API requests specify which version of the API to use in the request\nURL. A request can specify any version that is enabled for your app. Data API requests must include a  Content-Type  header to specify the\n data format  used in the request body. Use  Content-Type: application/json  to represent standard JSON types in a\nData API request body. Use  Content-Type: application/ejson  to represent standard JSON\ntypes  and  additional EJSON types in a Data API request body. A request can include an  Accept  header to request a specific data\nformat for the response body, either JSON or EJSON. If a request does\nnot include a valid  Accept  header, the response uses the data format\nspecified in your  Data API configuration . If an endpoint is configured to use Application Authentication then you\nmust include a valid user access token or login credentials with every\nrequest. To use an access token, first authenticate the user through an App\nServices authentication provider. Then, get the access token returned\nfrom App Services and include it in the request's Authorization header\nusing a Bearer token scheme. For more information on how to acquire and\nuse an access token, see  Bearer Token Authentication . Alternatively, you can include valid login credentials for the user in\nthe request headers. In general, bearer authentication with an access token has higher\nthroughput and is more secure than credential headers. Use an access\ntoken instead of credential headers when possible. The token lets you\nrun multiple requests without re-authenticating the user. It also lets\nyou send requests from a web browser that enforces  CORS . If you're authenticating from a browser or another user-facing client application,\navoid using an API key to log in. Instead, use another authentication\nprovider that takes user-provided credentials. Never store API keys or other\nsensitive credentials locally. Depending on your Data API  authorization configuration , your requests may need to include additional\nauthorization information. All authenticated users are authorized to call the endpoint.\nAuthenticated requests do not need to include authorization\ninformation. Authenticated users must prove that they are authorized to call\nthe endpoint by including the endpoint's secret string as the\nvalue of the  secret  query parameter. The following  curl  request uses secret query parameter\nvalidation with the secret string  \"12345\" : Authenticated users must prove that they are authorized to call\nthe endpoint by including an  Endpoint-Signature  header that\ncontains a hexadecimal-encoded  HMAC (Hash-based Message\nAuthentication Code)  SHA-256 hash generated from the request body\nand the endpoint's secret string. You could use the following function to generate the payload\nsignature: The following  curl  includes a payload signature header\nsigned with the secret value  12345 :",
            "code": [
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/data/v1/action/insertOne\" \\\n  -X POST \\\n  -H \"Content-Type: application/ejson\" \\\n  -H \"Accept: application/json\" \\\n  -H \"apiKey: TpqAKQgvhZE4r6AOzpVydJ9a3tB1BLMrgDzLlBLbihKNDzSJWTAHMVbsMoIOpnM6\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"learn-data-api\",\n    \"collection\": \"hello\",\n    \"document\": {\n      \"text\": \"Hello, world!\"\n    }\n  }'\n"
                },
                {
                    "lang": "json",
                    "value": "{ \"insertedId\": \"5f1a785e1536b6e6992fd588\" }"
                },
                {
                    "lang": "none",
                    "value": "https://data.mongodb-api.com/app/<App ID>/endpoint/data/<API Version>"
                },
                {
                    "lang": "none",
                    "value": "https://<Region>.<Cloud>.data.mongodb-api.com/app/<App ID>/endpoint/data/<API Version>"
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "bash",
                    "value": "{\n  \"disabled\": false,\n  \"versions\": [\"v1\"],\n  \"can_evaluate\": {},\n  \"return_type\": <\"JSON\" | \"EJSON\">\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices push"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"run_as_user_id\": \"628e47baf4c2ac2796fc8a91\"\n}"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    ...,\n    \"run_as_user_id_script_source\": \"exports = () => {return \\\"628e47baf4c2ac2796fc8a91\\\"}\"\n  }\n]"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    ...,\n    \"verification_method\": \"SECRET_AS_QUERY_PARAM\",\n    \"secret_name\": \"secret_verification_string\"\n  }\n]"
                },
                {
                    "lang": "json",
                    "value": "{\n  ...,\n  \"can_evaluate\": {\n    \"%%request.requestHeaders.x-secret-key\": \"my-secret\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"return_type\": \"JSON\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"return_type\": \"EJSON\"\n}"
                },
                {
                    "lang": "bash",
                    "value": "https://data.mongodb-api.com/app/<App ID>/endpoint/data/<API Version>"
                },
                {
                    "lang": "bash",
                    "value": "curl -X GET \\\n  https://data.mongodb-api.com/app/myapp-abcde/endpoint/data/v1/action/insertOne \\\n  -H 'apiKey: <API Key>' \\\n  -H 'Content-Type: application/ejson' \\\n  -H 'Accept: application/ejson' \\\n  --data-raw '{\n      \"dataSource\": \"mongodb-atlas\",\n      \"database\": \"learn-data-api\",\n      \"collection\": \"hello\",\n      \"document\": {\n        \"_id\": { \"$oid\": \"629971c0d71aad65bd59c595\" },\n        \"greeting\": \"Hello, EJSON!\",\n        \"date\": { \"$date\": { \"$numberLong\": \"1654589430998\" } }\n      }\n  }'"
                },
                {
                    "lang": "json",
                    "value": "{ \"insertedId\": { \"$oid\": \"629971c0d71aad65bd59c595\" } }"
                },
                {
                    "lang": "bash",
                    "value": "curl -X POST \\\n  https://data.mongodb-api.com/app/myapp-abcde/endpoint/data/v1/action/updateOne \\\n  -H 'apiKey: <API Key>' \\\n  -H 'Content-Type: application/json' \\\n  --data-raw '{\n     \"dataSource\": \"mongodb-atlas\",\n     \"database\": \"learn-data-api\",\n     \"collection\": \"hello\",\n     \"filter\": { \"greeting\": \"Hello, world!\" },\n     \"update\": {\n         \"$set\": { \"greeting\": \"Hello, universe!\" }\n     }\n  }'"
                },
                {
                    "lang": "json",
                    "value": "{ \"matchedCount\": 1, \"modifiedCount\": 1 }"
                },
                {
                    "lang": "bash",
                    "value": "curl -X GET \\\n  https://data.mongodb-api.com/app/myapp-abcde/endpoint/data/v1/action/findOne \\\n  -H 'apiKey: <API Key>' \\\n  -H 'Content-Type: application/json' \\\n  -H 'Accept: application/ejson' \\\n  --data-raw '{\n     \"dataSource\": \"mongodb-atlas\",\n     \"database\": \"learn-data-api\",\n     \"collection\": \"hello\",\n     \"projection\": { \"greeting\": 1, \"date\": 1 }\n  }'"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": { \"$oid\": \"629971c0d71aad65bd59c545\"},\n  \"greeting\": \"Hello, Leafie!\",\n  \"date\": { \"$date\": { \"$numberLong\": \"1654589430998\" } }\n}"
                },
                {
                    "lang": "bash",
                    "value": "curl -X POST \\\n  https://data.mongodb-api.com/app/myapp-abcde/endpoint/data/v1/action/findOne \\\n  -H 'apiKey: <API Key>' \\\n  -H 'Content-Type: application/json' \\\n  -H 'Accept: application/json' \\\n  --data-raw '{\n     \"dataSource\": \"mongodb-atlas\",\n     \"database\": \"learn-data-api\",\n     \"collection\": \"hello\",\n     \"projection\": { \"greeting\": 1, \"date\": 1 }\n  }'"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": \"629971c0d71aad65bd59c545\",\n  \"greeting\": \"Hello, Leafie!\",\n  \"date\": \"2022-06-07T08:10:30.998Z\"\n}"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/data/v1/action/findOne\" \\\n  -X POST \\\n  -H \"Accept: application/json\" \\\n  -H \"Authorization: Bearer $ACCESS_TOKEN\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"sample_mflix\",\n    \"collection\": \"movies\",\n    \"filter\": {\n      \"title\": \"The Matrix\"\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/data/v1/action/findOne\" \\\n  -X POST \\\n  -H \"Accept: application/json\" \\\n  -H \"email: bob@example\" \\\n  -H \"password: Pa55w0rd!\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"sample_mflix\",\n    \"collection\": \"movies\",\n    \"filter\": {\n      \"title\": \"The Matrix\"\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/data/v1/action/findOne\" \\\n  -X POST \\\n  -H \"Accept: application/json\" \\\n  -H \"apiKey: TpqAKQgvhZE4r6AOzpVydJ9a3tB1BLMrgDzLlBLbihKNDzSJWTAHMVbsMoIOpnM6\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"sample_mflix\",\n    \"collection\": \"movies\",\n    \"filter\": {\n      \"title\": \"The Matrix\"\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/data/v1/action/findOne\" \\\n  -X POST \\\n  -H \"Accept: application/json\" \\\n  -H \"jwtTokenString: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJteWFwcC1hYmNkZSIsInN1YiI6IjEyMzQ1Njc4OTAiLCJuYW1lIjoiSm9obiBEb2UiLCJleHAiOjIxNDU5MTY4MDB9.E4fSNtYc0t5XCTv3S8W89P9PKLftC4POLRZdN2zOICI\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"sample_mflix\",\n    \"collection\": \"movies\",\n    \"filter\": {\n      \"title\": \"The Matrix\"\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -X POST \\\n  https://data.mongodb-api.com/app/myapp-abcde/endpoint/data/v1/action/findOne?secret=12345 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"learn-data-api\",\n    \"collection\": \"tasks\",\n    \"filter\": { \"text\": \"Do the dishes\" }\n  }'"
                },
                {
                    "lang": "none",
                    "value": "Endpoint-Signature: sha256=<hex encoded hash>"
                },
                {
                    "lang": "javascript",
                    "value": "/**\n * Generate an HMAC request signature.\n * @param {string} secret - The secret validation string, e.g. \"12345\"\n * @param {object} body - The endpoint request body e.g. { \"message\": \"MESSAGE\" }\n * @returns {string} The HMAC SHA-256 request signature in hex format.\n */\nexports = function signEndpointRequest(secret, body) {\n  const payload = EJSON.stringify(body);\n  return utils.crypto.hmac(payload, secret, \"sha256\", \"hex\");\n};\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -X POST \\\n  https://data.mongodb-api.com/app/myapp-abcde/endpoint/data/v1/action/findOne \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Endpoint-Signature: sha256=769cd86855787f476afc8b0d2cf9837ab0318181fca42f45f34b6dffd086dfc7\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"learn-data-api\",\n    \"collection\": \"tasks\",\n    \"filter\": { \"text\": \"Do the dishes\" }\n  }'"
                }
            ],
            "preview": "The Data API lets you securely read and write data using standard HTTPS\nrequests. The API includes automatically generated endpoints that each\nrepresent a MongoDB operation. You can use the endpoints to create,\nread, update, delete, and aggregate documents in a MongoDB data source.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "data-api/examples",
            "title": "Data API Examples",
            "headings": [
                "Create, Read, Update, and Delete (CRUD) Operations",
                "Find a Single Document",
                "Find Multiple Documents",
                "Insert a Single Document",
                "Insert Multiple Documents",
                "Update a Single Document",
                "Update Multiple Documents",
                "Replace a Single Document",
                "Delete a Single Document",
                "Delete Multiple Documents",
                "Run an Aggregation Pipeline",
                "Specify BSON Types in Requests",
                "Binary",
                "Date",
                "Decimal128",
                "Double",
                "Int32",
                "Int64",
                "ObjectId",
                "Query Examples",
                "Find All Documents",
                "Find a Document by ID",
                "Find By Date",
                "Data Access Permissions",
                "Define Permissions for Specific API Keys",
                "Define Permissions for a Specific Collection"
            ],
            "paragraphs": "The following examples demonstrate how to send requests to the Atlas\nData API. The following examples demonstrate how to perform CRUD operations using\nthe Atlas Data API. Data API requests can specify BSON types that don't exist in JSON by\ninstead using the EJSON  data format  in\nthe request body. You can use EJSON to match BSON types in query filters\nor write BSON types in insert and update operations. To specify that a request body uses EJSON, set the  Content-Type  header: To specify a binary value, use  $binary  with the value encoded in\nBase64 and a  BSON subtype  encoded as\na two-character hexadecimal string: To specify a date, use a  $date  object. The value depends on which\nEJSON format you want to use. Canonical EJSON The value is a  UNIX timestamp in milliseconds  as a 64-bit integer: Relaxed EJSON The value is an  ISO 8601  date string with a time\ncomponent: To specify a 128-bit decimal, use  $numberDecimal  with the decimal\nvalue as a string: To specify a 64-bit signed floating point value (commonly referred to as\na \"double\"), use a canonical  $numberDouble  with the integer value as\na string: Relaxed EJSON An EJSON value that contains a raw JSON  number  with a decimal point\nis automatically cast to a  $numberDouble  object: To specify a 32-bit signed integer value, use a canonical  $numberInt \nobject with the integer value as a string: Relaxed EJSON An EJSON value that contains a raw JSON  number  without a decimal\npoint is automatically cast to a  $numberInt  object: To specify a 64-bit signed integer value, use  $numberLong  with the\ninteger value as a string: To specify an ObjectId value, use  $oid  with the ID as a byte string: The code snippets in this section demonstrate common patterns you can\nuse in your read and write operations. To match all documents in a collection, use an empty query object: MongoDB stores each document with a unique identifier in the  _id \nfield. For most apps, this is a BSON ObjectID value. You can match any\nObjectID field, including a document using an EJSON  $oid  object: You can match documents on a specific date by matching an EJSON\n $date  object with a field that contains a date value: You can query a range of dates with  $gt ,  $gte ,  $lt , and\n $lte : You can secure Data API requests using your app's built-in role-based\npermissions. The examples in this section demonstrate how to set up\ncommon permissions schemes. You can mix and match these and more complex\nschemes to meet your API's needs. For more examples and information on\nhow permissions work, see  Role-based Permissions . You can define multiple roles with different data access permissions and\nassign specific API keys to each role. For example, you can create a\n read-only  role that allows users to read all data but not insert,\ndelete, or modify data. You map each role to an API key in the role's  apply_when  expression. Each API key corresponds to a separate user account with a unique\naccount ID. You can access the account ID and other data of the user\nmaking a request with the  %%user  expansion. To associate a role with a single API key, set the  apply_when \nexpression to match the API key's account ID: To associate a role with multiple API keys, set the  apply_when \nexpression to match an array of API key account IDs: You can also use a custom system to determine if a user has a role. For\nexample, you can write a function that looks up a user's roles from an\nAtlas cluster. And then call that function from the role's  apply_when  expression\nwith the  %function  operator: This example uses a custom user data collection, but you can use any\nexternal service to determine if a user has a role. The function code\nand arguments you pass are up to you. You can define roles that control data access permissions for a specific\ncollection in your cluster. For example, you can create a  read-only \nrole that allows users to read all data in a collection but not insert,\ndelete, or modify any data. To define roles for a specific collection, update the collection's\nconfiguration file with the  collection-specific role\nconfigurations : If no collection roles match, then the request is also evaluated against\nthe data source's default roles. If you don't want to apply default\nroles, remove any roles and filters from the default rule configuration\nfile.",
            "code": [
                {
                    "lang": "",
                    "value": "curl -s \"https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/findOne\" \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"learn-data-api\",\n    \"collection\": \"tasks\",\n    \"filter\": {\n      \"text\": \"Do the dishes\"\n    }\n  }'\n"
                },
                {
                    "lang": "",
                    "value": "curl -s \"https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/find\" \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"learn-data-api\",\n    \"collection\": \"tasks\",\n    \"filter\": {\n      \"status\": \"complete\"\n    },\n    \"sort\": { \"completedAt\": 1 },\n    \"limit\": 10\n  }'\n"
                },
                {
                    "lang": "",
                    "value": "curl -s \"https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/insertOne\" \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"learn-data-api\",\n    \"collection\": \"tasks\",\n    \"document\": {\n      \"status\": \"open\",\n      \"text\": \"Do the dishes\"\n    }\n  }'\n"
                },
                {
                    "lang": "",
                    "value": "curl -s \"https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/insertMany\" \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"learn-data-api\",\n    \"collection\": \"tasks\",\n    \"documents\": [\n      {\n        \"status\": \"open\",\n        \"text\": \"Mop the floor\"\n      },\n      {\n        \"status\": \"open\",\n        \"text\": \"Clean the windows\"\n      }\n    ]\n  }'\n"
                },
                {
                    "lang": "",
                    "value": "curl -s \"https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/updateOne\" \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"learn-data-api\",\n    \"collection\": \"tasks\",\n    \"filter\": {\n      \"_id\": { \"$oid\": \"64224f4d089104f1766116a5\" }\n    },\n    \"update\": {\n      \"$set\": {\n        \"status\": \"complete\",\n        \"completedAt\": { \"$date\": { \"$numberLong\": \"1680105272788\" } }\n      }\n    }\n  }'\n"
                },
                {
                    "lang": "",
                    "value": "curl -s \"https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/updateMany\" \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"learn-data-api\",\n    \"collection\": \"tasks\",\n    \"filter\": {\n      \"status\": \"open\"\n    },\n    \"update\": {\n      \"$set\": {\n        \"status\": \"complete\",\n        \"completedAt\": { \"$date\": { \"$numberLong\": \"1680105287069\" } }\n      }\n    }\n  }'\n"
                },
                {
                    "lang": "",
                    "value": "curl -s \"https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/replaceOne\" \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"learn-data-api\",\n    \"collection\": \"tasks\",\n    \"filter\": {\n      \"text\": \"Clean the windows\"\n    },\n    \"replacement\": {\n      \"status\": \"open\",\n      \"text\": \"Re-clean the windows\"\n    }\n  }'\n"
                },
                {
                    "lang": "",
                    "value": "curl -s \"https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/deleteOne\" \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"learn-data-api\",\n    \"collection\": \"tasks\",\n    \"filter\": {\n      \"_id\": { \"$oid\": \"64224f3cd79f54ad342dd9b2\" }\n    }\n  }'\n"
                },
                {
                    "lang": "",
                    "value": "curl -s \"https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/deleteMany\" \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"learn-data-api\",\n    \"collection\": \"tasks\",\n    \"filter\": {\n      \"status\": \"complete\"\n    }\n  }'\n"
                },
                {
                    "lang": "",
                    "value": "curl -s \"https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/aggregate\" \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"learn-data-api\",\n    \"collection\": \"tasks\",\n    \"pipeline\": [\n      {\n        \"$match\": { \"status\": \"complete\" }\n      },\n      {\n        \"$group\": {\n          \"_id\": \"$status\",\n          \"count\": { \"$sum\": 1 },\n          \"tasks\": { \"$push\": \"$text\" }\n        }\n      },\n      {\n        \"$sort\": { \"count\": -1 }\n      }\n    ]\n  }'\n"
                },
                {
                    "lang": "none",
                    "value": "Content-Type: application/ejson"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/insertOne\" \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"<cluster name>\",\n    \"database\": \"<database name>\",\n    \"collection\": \"<collection name>\",\n    \"document\": { \"_id\": { \"$oid\":\"645404f4ee8583002fc5a77e\" },\n      \"data\": {\n        \"$binary\": {\n          \"base64\": \"46d989eaf0bde5258029534bc2dc2089\",\n          \"subType\": \"05\"\n        }\n      }\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/insertOne \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"<cluster name>\",\n    \"database\": \"<database name>\",\n    \"collection\": \"<collection name>\",\n    \"document\": {\n      \"createdAt\": { \"$date\": { \"$numberLong\": \"1638551310749\" } }\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/insertOne \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"<cluster name>\",\n    \"database\": \"<database name>\",\n    \"collection\": \"<collection name>\",\n    \"document\": {\n      \"createdAt\": { \"$date\": \"2021-12-03T17:08:30.749Z\" }\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/insertOne \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"<cluster name>\",\n    \"database\": \"<database name>\",\n    \"collection\": \"<collection name>\",\n    \"document\": {\n      \"accountBalance\": { \"$numberDecimal\": \"128452.420523\" }\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/insertOne \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"<cluster name>\",\n    \"database\": \"<database name>\",\n    \"collection\": \"<collection name>\",\n    \"document\": {\n      \"temperatureCelsius\": { \"$numberDouble\": \"23.847\" }\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/insertOne \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"<cluster name>\",\n    \"database\": \"<database name>\",\n    \"collection\": \"<collection name>\",\n    \"document\": {\n      \"temperatureCelsius\": 23.847\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/insertOne \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"<cluster name>\",\n    \"database\": \"<database name>\",\n    \"collection\": \"<collection name>\",\n    \"document\": {\n      \"coins\": { \"$numberInt\": \"2147483647\" }\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/insertOne \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"<cluster name>\",\n    \"database\": \"<database name>\",\n    \"collection\": \"<collection name>\",\n    \"document\": {\n      \"coins\": 2147483647\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/insertOne \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"<cluster name>\",\n    \"database\": \"<database name>\",\n    \"collection\": \"<collection name>\",\n    \"document\": {\n      \"population\": { \"$numberLong\": \"8047923148\" }\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s https://data.mongodb-api.com/app/$CLIENT_APP_ID/endpoint/data/v1/action/insertOne \\\n  -X POST \\\n  -H \"apiKey: $API_KEY\" \\\n  -H 'Content-Type: application/ejson' \\\n  -H \"Accept: application/json\" \\\n  -d '{\n    \"dataSource\": \"<cluster name>\",\n    \"database\": \"<database name>\",\n    \"collection\": \"<collection name>\",\n    \"document\": {\n      \"_id\": { \"$oid\": \"61f02ea3af3561e283d06b91\" }\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl --request POST \\\n  'https://data.mongodb-api.com/app/<App ID>/endpoint/data/v1/action/find' \\\n  --header 'Content-Type: application/ejson' \\\n  --header 'apiKey: <API Key>' \\\n  --data-raw '{\n    \"dataSource\": \"<cluster name>\",\n    \"database\": \"<database name>\",\n    \"collection\": \"<collection name>\",\n    \"filter\": {}\n  }'"
                },
                {
                    "lang": "bash",
                    "value": "curl --request POST \\\n  'https://data.mongodb-api.com/app/<App ID>/endpoint/data/v1/action/find' \\\n  --header 'Content-Type: application/ejson' \\\n  --header 'apiKey: <API Key>' \\\n  --data-raw '{\n    \"dataSource\": \"<cluster name>\",\n    \"database\": \"<database name>\",\n    \"collection\": \"<collection name>\",\n    \"filter\": {\n      \"_id\": { \"$oid\": \"630e51b3f4cd7d9e606caab6\" }\n    }\n  }'"
                },
                {
                    "lang": "bash",
                    "value": "curl --request POST \\\n  'https://data.mongodb-api.com/app/<App ID>/endpoint/data/v1/action/find' \\\n  --header 'Content-Type: application/ejson' \\\n  --header 'apiKey: <API Key>' \\\n  --data-raw '{\n    \"dataSource\": \"<cluster name>\",\n    \"database\": \"<database name>\",\n    \"collection\": \"<collection name>\",\n    \"filter\": {\n      \"createdAt\": { \"$date\": \"2022-08-30T17:52:05.033Z\" }\n    }\n  }'"
                },
                {
                    "lang": "bash",
                    "value": "curl --request POST \\\n  'https://data.mongodb-api.com/app/<App ID>/endpoint/data/v1/action/find' \\\n  --header 'Content-Type: application/ejson' \\\n  --header 'apiKey: <API Key>' \\\n  --data-raw '{\n    \"dataSource\": \"<cluster name>\",\n    \"database\": \"<database name>\",\n    \"collection\": \"<collection name>\",\n    \"filter\": {\n      \"createdAt\": {\n        \"$gte\": { \"$date\": \"2022-01-01T00:00:00.000Z\" },\n        \"$lt\": { \"$date\": \"2023-01-01T00:00:00.000Z\" }\n      }\n    }\n  }'"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"database\": \"<database>\",\n  \"collection\": \"<collection>\",\n  \"roles\": [\n    {\n      \"name\": \"SpecificUser\",\n      \"apply_when\": {\n        \"%%user.id\": \"61f9a5e69cd3c0199dc1bb88\"\n      },\n      \"insert\": true,\n      \"delete\": true,\n      \"read\": true,\n      \"write\": true\n    }\n  ],\n  \"filters\": []\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"database\": \"<database>\",\n  \"collection\": \"<collection>\",\n  \"roles\": [\n    {\n      \"name\": \"MultipleUsers\",\n      \"apply_when\": {\n        \"%%user.id\": {\n          \"$in\": [\n            \"61f9a5e69cd3c0199dc1bb88\",\n            \"61f9a5e69cd3c0199dc1bb89\"\n          ]\n        }\n      },\n      \"insert\": true,\n      \"delete\": true,\n      \"read\": true,\n      \"write\": true\n    }\n  ],\n  \"filters\": []\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function hasRole({\n  userId, // The user account ID, e.g. \"60d0c1bdee9d3c23b677f929\"\n  roleName, // The name of a role the user might have, e.g. \"admin\"\n}) {\n  // 1. Get a reference to a custom user data collection\n  const users = context.services.get(\"mongodb-atlas\").db(\"app\").collection(\"users\")\n  // 2. Query the user's document and make sure it exists\n  const user = await users.findOne({ userId })\n  if(!user) {\n    console.error(`User.id ${userId} not found in custom user data collection`)\n    return false\n  }\n  // 3. Decide if the user has the role we're checking\n  return user.roles.includes(roleName)\n};"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"database\": \"<database>\",\n  \"collection\": \"<collection>\",\n  \"roles\": [\n    {\n      \"name\": \"SomeCustomRole\",\n      \"apply_when\": {\n        \"%%true\": {\n          \"%function\": {\n            \"name\": \"hasRole\",\n            \"arguments\": [{\n              \"userId\": \"%%user.id\",\n              \"roleName\": \"SomeCustomRole\"\n            }]\n          }\n        }\n      },\n      \"insert\": true,\n      \"delete\": true,\n      \"read\": true,\n      \"write\": true\n    }\n  ],\n  \"filters\": []\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"database\": \"<database>\",\n  \"collection\": \"<collection>\",\n  \"roles\": [\n    {\n      \"name\": \"IsOwner\",\n      \"apply_when\": { \"owner_id\": \"%%user.id\" },\n      \"insert\": false,\n      \"delete\": false,\n      \"read\": true,\n      \"write\": true\n    }\n  ],\n  \"filters\": []\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"roles\": [],\n  \"filters\": []\n}"
                }
            ],
            "preview": "The following examples demonstrate how to send requests to the Atlas\nData API.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "data-api/data-formats",
            "title": "Data Formats",
            "headings": [
                "Standard JSON (application/json)",
                "Canonical & Relaxed EJSON (application/ejson)",
                "BSON Types",
                "Array",
                "Binary",
                "Date",
                "Decimal128",
                "Document",
                "Double",
                "Int32",
                "Int64",
                "MaxKey",
                "MinKey",
                "ObjectId",
                "Regular Expression",
                "Timestamp"
            ],
            "paragraphs": "MongoDB stores data in a format called  BSON ,\nwhich is similar to a JSON\nobject in structure but supports additional data types and uses a binary\nencoding. BSON is efficient for computers but is not human readable,\nso you can't work with it directly. Instead, the Data API uses two formats to represent data in requests and\nresponses: JSON and EJSON. You define a single default return type for all generated Data API\nendpoints and individually for each custom endpoint. Incoming requests\ncan also specify a preferred data format that overrides the default\nusing an  Accept  header. You define a single default return type for all generated Data API\nendpoints and individually for each custom endpoint. Incoming requests\ncan also specify a preferred data format that overrides the default\nusing an  Accept  header. This document can be represented in either JSON or EJSON shows BSON types\nrepresented in JSON and EJSON: The  JSON  format uses standard types that any tool can parse and\nunderstand. However, JSON cannot represent all BSON types so JSON\nresponses may lose type information for some fields. For example, BSON\nhas distinct types for 32-bit integers and 64-bit floats but a JSON\nresponse represents both as a  number . The  EJSON  format, short for  MongoDB Extended JSON , is a superset of standard JSON\nthat uses structured fields to represent BSON data that don't have\ncorresponding JSON types. This fully represents your data but requires\nyour client to understand how to work with EJSON. There are two variants of EJSON: You can use either Canonical or Relaxed EJSON in request bodies. Data\nAPI endpoints that are configured to return EJSON always return\nCanonical EJSON. Canonical EJSON  uses a verbose structure that emphasizes type\npreservation at the expense of readability and interoperability. It\nfully represents BSON types but you may need to use a library or\ncustom code to work with it. Relaxed EJSON  uses a more compact structure that is easier to read\nand work with but may lose type information for some BSON types. For\nexample, a number field in an inserted document may be inferred as a\ndifferent numeric BSON type than you expect. This sections lists the BSON types that the Data API supports and shows\nhow each type is represented in JSON and EJSON format. EJSON JSON Canonical EJSON Relaxed EJSON Same as Canonical EJSON JSON Canonical EJSON Relaxed EJSON Same as Canonical EJSON JSON Canonical EJSON Relaxed EJSON EJSON JSON Canonical EJSON Relaxed EJSON Same as Canonical EJSON JSON Canonical EJSON Relaxed EJSON Same as Canonical EJSON JSON Canonical EJSON Relaxed EJSON EJSON JSON Canonical EJSON Relaxed EJSON EJSON JSON Canonical EJSON Relaxed EJSON Same as Canonical EJSON JSON Canonical EJSON Relaxed EJSON Same as Canonical No JSON equivalent EJSON JSON Canonical EJSON Relaxed EJSON Same as Canonical No JSON equivalent EJSON JSON Canonical EJSON Relaxed EJSON Same as Canonical EJSON JSON Canonical EJSON Relaxed EJSON Same as Canonical EJSON JSON Canonical EJSON Relaxed EJSON Same as Canonical",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n   \"Name\": \"Mango\",\n   \"Year\": { \"$numberLong\": \"2022\" },\n   \"Weight\": { \"$numberDecimal\": \"9823.1297\" },\n   \"Date\": { \"$date\": { \"$numberLong\": \"1641954803067\" } }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"Name\": \"Mango\",\n   \"Year\": 2022,\n   \"Weight\": \"9823.1297\",\n   \"Date\": \"2022-01-12T02:33:23.067Z\"\n}"
                },
                {
                    "lang": "json",
                    "value": "[ <elements> ]"
                },
                {
                    "lang": "json",
                    "value": "[ <elements> ]"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"$binary\": {\n      \"base64\": \"e67803a39588be8a95731a21e27d7391\",\n      \"subType\": \"05\"\n   }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"Subtype\": 5,\n   \"Data\": \"e67803a39588be8a95731a21e27d7391\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"$date\": {\n      \"$numberLong\": \"1641954803067\"\n   }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"$date\": \"2022-01-12T02:33:23.067Z\"\n}"
                },
                {
                    "lang": "json",
                    "value": "\"2022-01-12T02:33:23.067Z\""
                },
                {
                    "lang": "json",
                    "value": "{ \"$numberDecimal\": \"9823.1297\" }"
                },
                {
                    "lang": "json",
                    "value": "\"9823.1297\""
                },
                {
                    "lang": "json",
                    "value": "{ <content> }"
                },
                {
                    "lang": null,
                    "value": "{ <content> }"
                },
                {
                    "lang": "json",
                    "value": "{ \"$numberDouble\": \"10.5\" }"
                },
                {
                    "lang": "json",
                    "value": "10.5"
                },
                {
                    "lang": "json",
                    "value": "10.5"
                },
                {
                    "lang": "json",
                    "value": "{ \"$numberInt\": \"10\" }"
                },
                {
                    "lang": "json",
                    "value": "10"
                },
                {
                    "lang": "json",
                    "value": "10"
                },
                {
                    "lang": "json",
                    "value": "{ \"$numberLong\": \"50\" }"
                },
                {
                    "lang": "json",
                    "value": "50"
                },
                {
                    "lang": "json",
                    "value": "{ \"$maxKey\": 1 }"
                },
                {
                    "lang": "json",
                    "value": "{}"
                },
                {
                    "lang": "json",
                    "value": "{ \"$minKey\": 1 }"
                },
                {
                    "lang": "json",
                    "value": "{}"
                },
                {
                    "lang": "json",
                    "value": "{ \"$oid\":\"5d505646cf6d4fe581014ab2\" }"
                },
                {
                    "lang": "json",
                    "value": "\"5d505646cf6d4fe581014ab2\""
                },
                {
                    "lang": "json",
                    "value": "{\n  \"$regularExpression\": {\n    \"pattern\":\"^H\",\n    \"options\":\"i\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"Pattern\": \"^H\",\n   \"Options\": \"i\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"$timestamp\": {\n    \"t\":1565545664,\n    \"i\":1\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"T\": 1565545664,\n   \"I\": 1\n}"
                }
            ],
            "preview": "MongoDB stores data in a format called BSON,\nwhich is similar to a JSON\nobject in structure but supports additional data types and uses a binary\nencoding. BSON is efficient for computers but is not human readable,\nso you can't work with it directly.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "data-api/authenticate",
            "title": "Authenticate Data API Requests",
            "headings": [
                "Overview",
                "Bearer Authentication",
                "Credential Headers",
                "Email/Password",
                "API Key",
                "Custom JWT"
            ],
            "paragraphs": "Data API endpoints generally require that incoming requests include\nauthentication information for the user calling the endpoint. This lets\nthe endpoint enforce rules and validate document schemas for each\nrequest. Requests must include authentication data in specific request headers.\nApp Services uses the following process to authenticate a given request: Check for an  Authorization  header. If it's present, try to use\n Bearer Authentication . The\nheader must contain a valid user access token. If the token is\ninvalid, the request fails. If the  Authorization  header is not present or does not use the\n\"Bearer\" scheme, check for  Credential Headers . The headers must contain valid\nEmail/Password, API Key, or Custom JWT credentials for an App user. You must  enable an authentication provider  before users can authenticate with\nit. The Data API supports Bearer Authentication, which lets you authenticate\na request by including a valid user access token in the request's\n Authorization  header. To learn how to get and manage an access\ntoken, see  Manage User Sessions . The Authorization header uses the following format: For example, the following request uses Bearer Authentication: In general, bearer authentication with an access token has higher\nthroughput and is more secure than credential headers. Use an access\ntoken instead of credential headers when possible. The token lets you\nrun multiple requests without re-authenticating the user. It also lets\nyou send requests from a web browser that enforces  CORS . If you're authenticating from a browser or another user-facing client application,\navoid using an API key to log in. Instead, use another authentication\nprovider that takes user-provided credentials. Never store API keys or other\nsensitive credentials locally. Bearer authentication is useful for: sending requests from a web browser. sending multiple requests without storing user credentials or prompting the user on each request. sending requests from an app that also uses a  Realm SDK  to authenticate users. For security reasons, App Services does not return detailed Bearer\nAuthentication errors to the client app. If you are having problems with\nBearer Authentication, check the  Application Logs . You can authenticate a Data API request by including the user's login\ncredentials in the request headers. The exact headers to include depend\non the authentication provider. Credential headers are useful for: requests sent from a server-side application requests sent from a command-line tool manual or test requests sent from an HTTPS client like Postman You cannot use credential headers to authenticate requests sent from\na web browser due to  Cross-Origin Resource Sharing  restrictions. Instead, to\nauthenticate Data API requests from a browser, use  Bearer\nAuthentication . To authenticate a Data API request as an  email/password  user, include the user's credentials in\nthe request's  email  and  password  headers. To authenticate a Data API request with an  API Key , include the API key in the request's\n apiKey  header. If you're authenticating from a browser or another user-facing client application,\navoid using an API key to log in. Instead, use another authentication\nprovider that takes user-provided credentials. Never store API keys or other\nsensitive credentials locally. To authenticate a Data API request as a  Custom JWT  user, include the JWT string in the\nrequest's  jwtTokenString  header.",
            "code": [
                {
                    "lang": "text",
                    "value": "Authorization: Bearer <AccessToken>"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/data/v1/action/findOne\" \\\n  -X POST \\\n  -H \"Accept: application/json\" \\\n  -H \"Authorization: Bearer $ACCESS_TOKEN\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"sample_mflix\",\n    \"collection\": \"movies\",\n    \"filter\": {\n      \"title\": \"The Matrix\"\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/data/v1/action/findOne\" \\\n  -X POST \\\n  -H \"Accept: application/json\" \\\n  -H \"email: bob@example\" \\\n  -H \"password: Pa55w0rd!\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"sample_mflix\",\n    \"collection\": \"movies\",\n    \"filter\": {\n      \"title\": \"The Matrix\"\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/data/v1/action/findOne\" \\\n  -X POST \\\n  -H \"Accept: application/json\" \\\n  -H \"apiKey: TpqAKQgvhZE4r6AOzpVydJ9a3tB1BLMrgDzLlBLbihKNDzSJWTAHMVbsMoIOpnM6\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"sample_mflix\",\n    \"collection\": \"movies\",\n    \"filter\": {\n      \"title\": \"The Matrix\"\n    }\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/data/v1/action/findOne\" \\\n  -X POST \\\n  -H \"Accept: application/json\" \\\n  -H \"jwtTokenString: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJteWFwcC1hYmNkZSIsInN1YiI6IjEyMzQ1Njc4OTAiLCJuYW1lIjoiSm9obiBEb2UiLCJleHAiOjIxNDU5MTY4MDB9.E4fSNtYc0t5XCTv3S8W89P9PKLftC4POLRZdN2zOICI\" \\\n  -d '{\n    \"dataSource\": \"mongodb-atlas\",\n    \"database\": \"sample_mflix\",\n    \"collection\": \"movies\",\n    \"filter\": {\n      \"title\": \"The Matrix\"\n    }\n  }'\n"
                }
            ],
            "preview": "Data API endpoints generally require that incoming requests include\nauthentication information for the user calling the endpoint. This lets\nthe endpoint enforce rules and validate document schemas for each\nrequest.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "data-api/custom-endpoints",
            "title": "Custom HTTPS Endpoints",
            "headings": [
                "Structure of an Endpoint",
                "Base URL",
                "Endpoint Routes",
                "HTTP Methods",
                "Create a Custom HTTPS Endpoint",
                "Authentication",
                "Authorization",
                "Built-In Authorization Schemes",
                "Custom Authorization Schemes",
                "Write an Endpoint Function",
                "Access Request Data",
                "Return an HTTPS Response",
                "Example",
                "Call a Custom Endpoint",
                "Choose a Response Data Format",
                "Authenticate the Request",
                "Authorize the Request"
            ],
            "paragraphs": "You can define custom HTTPS endpoints to create app-specific API routes\nor webhooks that integrate with external services. A custom endpoint\nuses a serverless function that you write to handle incoming requests\nfor a specific URL and HTTP method. Endpoints use standard, encrypted HTTPS requests, which means that you\ndon't need to install any database drivers or opinionated libraries to\ncall them. Instead, you send requests like this from any HTTP client: An endpoint handles one or more HTTP methods sent to a specific URL. Endpoints in an app share a base URL. The URL uses your App ID to\nuniquely point to your app. Globally deployed apps use the following format: Endpoints in a locally deployed app use a base URL specific to the app's\n deployment region  (e.g.  us-east-1 ) Every HTTPS endpoint has a route that serves as a name for the endpoint.\nAn endpoint's route is arbitrary and specific to your app. However, it\nappears in the endpoint URL path and so should represent the action the\nroute performs. Route names must begin with a forward slash ( / ) and may contain\nadditional forward slashes to indicate a nested path. You call an endpoint by appending its route to your app's base URL and\nsending an HTTP request. Each endpoint in your app handles one or more  HTTP methods  for a given route. For example, you might have a\nsingle route that accepts  POST  requests to create a new resource and\na  GET  request to list existing resources. You can define multiple custom endpoints that serve the same route but\nhandle different request methods. Alternatively, you can define a single\nendpoint for the route that handles all methods. Custom endpoints support the following standard HTTP methods: GET POST PUT PATCH DELETE You can configure the Data API for your app from the App Services UI or by\ndeploying configuration files with App Services CLI: Click  HTTPS Endpoints  in the left navigation menu and then\nclick  Add An Endpoint . Define the endpoint  Route . Route names must begin\nwith a forward slash ( / ) and may contain additional forward\nslashes to indicate a nested path. Choose an  HTTP method  for the\nendpoint from the dropdown. You can choose either a specific\nmethod (e.g.  GET  or  POST ) or configure the endpoint to\naccept  any  HTTP method (i.e.  ANY ). Choose a  response type , either\nJSON or EJSON. You can enable  Respond With Result \nto automatically include the endpoint function's return value\nas the response body. Write an  endpoint function  that\nhandles requests for the endpoint. Alternatively, specify an\nexisting function by name. For additional security, you can configure  request\nauthorization . Save the Data API configuration. Configure  access permissions  to\nallow requests to securely read and write data. Save and deploy your app. Pull the latest version of your app. Define a  configuration  object for\nthe custom endpoint. Define  rules  for one or more\ncollections. Deploy your app. Custom endpoints run in the context of a specific user, which allows\nyour app to enforce rules and validate document schemas for each\nrequest. By default, endpoints use Application Authentication, which requires\neach request to include credentials for one of your application users,\nlike an API key or JWT. You can also configure other custom\nauthentication schemes to fit your application's needs. For examples of how to authenticate requests, see  Authenticate\nData API Requests . Application authentication requires users to log in with an\nauthentication provider that you have enabled for your App.\nRequests can either include an access token granted by the\nauthentication provider or the credentials the user would log in\nwith (e.g. their API key or email and password). User ID authentication runs all requests as a single, pre-selected\napplication user. This is useful if all requests should have the\nsame permissions regardless of who called the endpoint. To select the user, specify their User Account ID in the endpoint\nconfiguration. Script authentication calls a function to determine which\napplication user a request runs as. You can use this to implement\ncustom authentication and authorization schemes. The function must return an existing application user's Account ID\nas a string or  { \"runAsSystem\": true }  to run the request\nas a  system user  that has full access to\nMongoDB CRUD and Aggregation APIs and is not subject to any\nrules, roles, or limited permissions. To define the function, specify the source code in the endpoint\nconfiguration. System authentication configures an endpoint to run as a\n system user  that requires no credentials, has\nfull access to MongoDB CRUD and Aggregation APIs, and is not\nsubject to any rules, roles, or limited permissions. New endpoints that you create in the UI use System authentication by default. An endpoint can require authenticated users to provide additional\nauthorization information in the request. You define the authorization\nscheme for each custom endpoint by configuring the endpoint function. Endpoints natively support a set of built-in authorization schemes that\nuse a secret string to prove that the request is authorized. You can\nalso define a custom authorization scheme that you can use together with\nor instead of the built-in schemes. To learn how to configure authorization for a specific function, see\n Define a Function . Endpoints support the following built-in authorization schemes: All authenticated users are authorized to call the endpoint.\nAuthenticated requests do not need to include authorization\ninformation. Authenticated users must prove that they are authorized to call\nthe endpoint by including a specific string as the value of the\n secret  query parameter. You define the string in a  secret  and\nreference the secret by name in the endpoint configuration. To learn how to include the secret in a request, see  Authorize the Request . Authenticated users must prove that they are authorized to call\nthe endpoint by including an  Endpoint-Signature  header that\ncontains a hexadecimal-encoded  HMAC (Hash-based Message\nAuthentication Code)  SHA-256 hash generated from the request body\nand a secret string. You define the string in a  secret  and\nreference the secret by name in the endpoint configuration. To learn how to sign your requests, see  Authorize the Request . You can define a custom authorization  expression  to\ndetermine if an incoming authenticated request is allowed to run. The\nexpression is evaluated for each request and must evaluate to  true \nto allow the request. If the expression evaluates to  false , the\nrequest is not authorized and fails with an error. Requests that fail\nauthorization are not counted toward your App's billed usage. Authorization expressions can use variables like\n %%user  to authorize based on the calling user's data\nor  %%request  to make decisions based on the specifics\nof each incoming request. To define a custom authorization scheme, specify the expression in the\nendpoint function's configuration: Every custom endpoint is associated with a  function \nthat runs whenever the endpoint receives an incoming request. In the\nfunction, you can import libraries from npm, connect to a linked MongoDB\nAtlas cluster, and call other serverless functions. To define a new function when  creating an endpoint  in the App Services UI, navigate\nto the  Function  section and select\n + New Function  from the dropdown. Depending on your workflow, you can also define and edit endpoint\nhandler functions: Endpoint functions always receive two arguments: For a sample function and an example request and response, see\n Example . On the  Functions  page in the  App Services UI . In the  functions  directory of your application using the\n App Services CLI . From your client using the  Admin API . A  Request  object that lets you access\nincoming request headers, query parameters, and body data. A  Response  object that you use to\nconfigure the HTTPS response sent back to the caller. A custom endpoint  Request  object represents the HTTP request that\ncalled the endpoint. You can access the incoming request's headers,\nquery parameters, and body data. Field Description query An object where each field maps a URL query parameter to its\nvalue. If a key is used multiple times in the query string, only\nthe first occurence is represented in this object. To work with\nthe full query string, use  context.request.rawQueryString . The following  query  object represents the query string\n ?regions=na,eu&currency=USD : headers An object where each field maps a request header name to an array\nof one or more values. body A  BSON.Binary  object that contains the\nrequest body. If the request did not include a body, this value\nis  undefined . To access data in the request body, you need to serialize the\nbinary: A custom endpoint  Response  object lets you configure the HTTPS\nresponse sent back to the caller. You can set the status code, customize\nheaders, and include data in the response body. Method Description Set the HTTP response  status code . Set the HTTP response  body . If  body  is a string, the endpoint automatically encodes it as\na  BSON.Binary . Set the HTTP response  header  specified\nby  name  to the value passed in the  value  argument. This\noverrides any other values that may have already been assigned to\nthat header. Set the HTTP response  header  specified\nby  name  to the value passed in the  value  argument. Unlike\n setHeader , this does not override other values that have\nalready been assigned to the header. Consider an endpoint function that parses the body of an incoming  POST \nrequest, stores the parsed body in a MongoDB collection, and then\nresponds to the caller: The function receives the following  POST  request: After the function verifies that the body of the incoming request\nis defined, it stores the parsed body as a new document in a collection\nnamed  myCollection . The resulting output displays the configured\nresponse, which includes a custom message and the  insertedId . You can call a custom endpoint from any standard HTTPS client. HTTP/1.1 or greater is required when making requests. A request can include an  Accept  header to request a specific data\nformat for the response body, either JSON or EJSON. If a request does\nnot include a valid  Accept  header, the response uses the default\ndata format specified in the endpoint configuration. If an endpoint is configured to use Application Authentication then you\nmust include a valid user access token or login credentials with every\nrequest. To use an access token, first authenticate the user through an App\nServices authentication provider. Then, get the access token returned\nfrom App Services and include it in the request's Authorization header\nusing a Bearer token scheme. For more information on how to acquire and\nuse an access token, see  Bearer Token Authentication . Alternatively, you can include valid login credentials for the user in\nthe request headers. In general, bearer authentication with an access token has higher\nthroughput and is more secure than credential headers. Use an access\ntoken instead of credential headers when possible. The token lets you\nrun multiple requests without re-authenticating the user. It also lets\nyou send requests from a web browser that enforces  CORS . If you're authenticating from a browser or another user-facing client application,\navoid using an API key to log in. Instead, use another authentication\nprovider that takes user-provided credentials. Never store API keys or other\nsensitive credentials locally. Depending on the  endpoint configuration , your requests may need to include additional\nauthorization information. All authenticated users are authorized to call the endpoint.\nAuthenticated requests do not need to include authorization\ninformation. Authenticated users must prove that they are authorized to call\nthe endpoint by including the endpoint's secret string as the\nvalue of the  secret  query parameter. The following  curl  request uses secret query parameter\nvalidation with the secret string  \"Super5ecr3tPa55w0rd\" : Authenticated users must prove that they are authorized to call\nthe endpoint by including an  Endpoint-Signature  header that\ncontains a hexadecimal-encoded  HMAC (Hash-based Message\nAuthentication Code)  SHA-256 hash generated from the request body\nand the endpoint's secret string. You could use the following function to generate the payload\nsignature: The following  curl  request includes a payload signature header\nsigned with the secret value  Super5ecr3tPa55w0rd :",
            "code": [
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/hello\" \\\n  -X POST \\\n  -H \"Accept: application/json\" \\\n  -H \"apiKey: TpqAKQgvhZE4r6AOzpVydJ9a3tB1BLMrgDzLlBLbihKNDzSJWTAHMVbsMoIOpnM6\" \\\n  -d '{\n    \"name\": \"Casey\"\n  }'\n"
                },
                {
                    "lang": "none",
                    "value": "https://data.mongodb-api.com/app/<App ID>/endpoint"
                },
                {
                    "lang": "none",
                    "value": "https://<Region>.aws.data.mongodb-api.com/app/<App ID>/endpoint"
                },
                {
                    "lang": "none",
                    "value": "/my/nested/route"
                },
                {
                    "lang": "none",
                    "value": "https://data.mongodb-api.com/app/<App ID>/endpoint/my/nested/route"
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "bash",
                    "value": "[\n  {\n    \"route\": \"<Endpoint route name>\",\n    \"http_method\": \"<HTTP method>\",\n    \"function_name\": \"<Endpoint function name\",\n    \"validation_method\": \"<Authorization scheme>\",\n    \"respond_result\": <boolean>,\n    \"fetch_custom_user_data\": <boolean>,\n    \"create_user_on_auth\": <boolean>,\n    \"disabled\": <boolean>\n  }\n]"
                },
                {
                    "lang": "bash",
                    "value": "{\n  \"database\": \"<Database Name>\",\n  \"collection\": \"<Collection Name>\",\n  \"roles\": [<Role>],\n  \"filters\": [<Filter>]\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices push"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    ...,\n    \"run_as_user_id\": \"628e47baf4c2ac2796fc8a91\"\n  }\n]"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    ...,\n    \"run_as_user_id_script_source\": \"exports = () => {return \\\"628e47baf4c2ac2796fc8a91\\\"}\"\n  }\n]"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    ...,\n    \"run_as_system\": true\n  }\n]"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n     ...,\n     \"verification_method\": \"SECRET_AS_QUERY_PARAM\",\n     \"secret_name\": \"secret_verification_string\"\n  }\n]"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n     ...,\n     \"can_evaluate\": {\n       \"%%request.requestHeaders.x-secret-key\": \"my-secret\"\n     }\n  }\n]"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"headers\": { \"<Header>\": [\"<Header Value>\"] },\n  \"query\": { \"<Query Parameter>\": \"<Parameter Value>\" },\n  \"body\": <BSON.Binary>\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"regions\": \"na&eu\",\n  \"currency\": \"USD\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"Content-Type\": [\"application/json\"],\n  \"Accept\": [\"application/json\"],\n  \"X-CustomHeader\": [\"some-value\", \"some-other-value\"]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "// Convert the request body to a JSON string\nconst serialized = request.body.text();\n// Parse the string into a usable object\nconst body = JSON.parse(serialized);"
                },
                {
                    "lang": "javascript",
                    "value": "response.setStatusCode(201);"
                },
                {
                    "lang": "javascript",
                    "value": "response.setBody(\n  \"{'message': 'Hello, World!'}\"\n);"
                },
                {
                    "lang": "javascript",
                    "value": "response.setHeader(\n  \"Content-Type\",\n  \"application/json\"\n);"
                },
                {
                    "lang": "javascript",
                    "value": "response.addHeader(\n  \"Cache-Control\",\n  \"max-age=600\"\n);\n\nresponse.addHeader(\n  \"Cache-Control\",\n  \"min-fresh=60\"\n)"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function MyCustomEndpoint(request, response) {\n  try {\n    // 1. Parse data from the incoming request\n    if (request.body === undefined) {\n      throw new Error(`Request body was not defined.`);\n    }\n    const body = JSON.parse(request.body.text());\n    // 2. Handle the request\n    const { insertedId } = await context.services\n      .get(\"mongodb-atlas\")\n      .db(\"myDb\")\n      .collection(\"myCollection\")\n      .insertOne({ date: new Date(), requestBody: body });\n    // 3. Configure the response\n    response.setStatusCode(201);\n    // tip: You can also use EJSON.stringify instead of JSON.stringify.\n    response.setBody(\n      JSON.stringify({\n        insertedId,\n        message: \"Successfully saved the request body\",\n      })\n    );\n  } catch (error) {\n    response.setStatusCode(400);\n    response.setBody(error.message);\n  }\n};\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/custom\" \\\n  -X POST \\\n  -H \"Accept: application/json\" \\\n  -H \"apiKey: TpqAKQgvhZE4r6AOzpVydJ9a3tB1BLMrgDzLlBLbihKNDzSJWTAHMVbsMoIOpnM6\" \\\n  -d '{\n    \"type\": \"event\",\n    \"date\": \"2024-01-01T00:00:00.000Z\",\n    \"name\": \"New Year Begins\",\n    \"comment\": \"Happy New Year!\"\n  }'\n"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"message\": \"Successfully saved the request body\",\n  \"insertedId\": \"639a521bbdec9b85ba94014b\"\n}"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/hello\" \\\n  -X POST \\\n  -H \"Accept: application/json\" \\\n  -H \"apiKey: TpqAKQgvhZE4r6AOzpVydJ9a3tB1BLMrgDzLlBLbihKNDzSJWTAHMVbsMoIOpnM6\" \\\n  -d '{\n    \"name\": \"Casey\"\n  }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/hello/latest\" \\\n  -X GET \\\n  -H \"Accept: application/ejson\" \\\n  -H \"apiKey: TpqAKQgvhZE4r6AOzpVydJ9a3tB1BLMrgDzLlBLbihKNDzSJWTAHMVbsMoIOpnM6\"\n"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"greeting\": \"Hello, Leafie!\",\n  \"date\": { \"$date\": { \"$numberLong\": \"1654589430998\" } }\n}"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/hello/latest\" \\\n  -X GET \\\n  -H \"Accept: application/json\" \\\n  -H \"apiKey: TpqAKQgvhZE4r6AOzpVydJ9a3tB1BLMrgDzLlBLbihKNDzSJWTAHMVbsMoIOpnM6\"\n"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"greeting\": \"Hello, Leafie!\",\n  \"date\": \"2022-06-07T08:10:30.998Z\"\n}"
                },
                {
                    "lang": "shell",
                    "value": "curl -X GET \\\n   -H 'Authorization: Bearer <AccessToken>' \\\n   -H 'Content-Type: application/json' \\\n   https://data.mongodb-api.com/app/myapp-abcde/endpoint/hello"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/hello\" \\\n  -X POST \\\n  -H \"Accept: application/json\" \\\n  -H \"email: bob@example\" \\\n  -H \"password: Pa55w0rd!\" \\\n  -d '{ \"name\": \"Bob\" }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/hello\" \\\n  -X POST \\\n  -H \"Accept: application/json\" \\\n  -H \"apiKey: TpqAKQgvhZE4r6AOzpVydJ9a3tB1BLMrgDzLlBLbihKNDzSJWTAHMVbsMoIOpnM6\" \\\n  -d '{ \"name\": \"Alice\" }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/hello\" \\\n  -X POST \\\n  -H \"Accept: application/json\" \\\n  -H \"jwtTokenString: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJ0ZXN0LWN1c3RvbS1lbmRwb2ludHMtZWhtenQiLCJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiZXhwIjoyMTQ1OTE2ODAwfQ.pIMvnXWrcDvmPzmE33ZPrwkBAFSwy-GxW8sP-qLtYiw\" \\\n  -d '{ \"name\": \"Carlos\" }'\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/passwordRequired?secret=Super5ecr3tPa55w0rd\" \\\n  -X GET \\\n  -H \"Accept: application/json\" \\\n  -H \"apiKey: TpqAKQgvhZE4r6AOzpVydJ9a3tB1BLMrgDzLlBLbihKNDzSJWTAHMVbsMoIOpnM6\" \\\n  -d '{ \"data\": \"VGhpcyBpcyBzb21lIGRhdGEgdGhhdCB3YXMgZW5jb2RlZCBhcyBhIEJhc2U2NCBBU0NJSSBzdHJpbmc=\" }'\n"
                },
                {
                    "lang": "none",
                    "value": "Endpoint-Signature: sha256=<hex encoded hash>"
                },
                {
                    "lang": "javascript",
                    "value": "/**\n * Generate an HMAC request signature.\n * @param {string} secret - The secret validation string, e.g. \"12345\"\n * @param {object} body - The endpoint request body e.g. { \"message\": \"MESSAGE\" }\n * @returns {string} The HMAC SHA-256 request signature in hex format.\n */\nexports = function signEndpointRequest(secret, body) {\n  const payload = EJSON.stringify(body);\n  return utils.crypto.hmac(payload, secret, \"sha256\", \"hex\");\n};\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -s \"https://data.mongodb-api.com/app/myapp-abcde/endpoint/sendMessage\" \\\n  -X POST \\\n  -H \"Accept: application/json\" \\\n  -H \"apiKey: TpqAKQgvhZE4r6AOzpVydJ9a3tB1BLMrgDzLlBLbihKNDzSJWTAHMVbsMoIOpnM6\" \\\n  -H \"Endpoint-Signature: sha256=d4f0537db4e230d7a6028a6f7c3bb1b57c9d16f39176d78697e559ac333e0b36\" \\\n  -d '{ \"message\": \"Hello!\" }'\n"
                }
            ],
            "preview": "You can define custom HTTPS endpoints to create app-specific API routes\nor webhooks that integrate with external services. A custom endpoint\nuses a serverless function that you write to handle incoming requests\nfor a specific URL and HTTP method.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "users/create",
            "title": "Create an App User",
            "headings": [
                "Overview",
                "Identity",
                "Link to Existing Accounts",
                "Create an Email/Password User",
                "Create an Email/Password User in the SDK",
                "Manually Create an Email/Password User",
                "Confirm a User",
                "Re-run the User Confirmation Workflow",
                "Summary"
            ],
            "paragraphs": "Atlas App Services provides various  authentication providers \nto log users into your app. For most providers, App Services automatically creates a\nuser account the first time a user authenticates through the provider. The only exception is\n email/password user authentication , which requires\nregistering and confirming a user before the user can authenticate. Apple  requires that applications distributed through the App Store  must give any user who creates\nan account the option to delete the account. Whether that app uses an\nauthentication method where you must manually register a user, such as\nemail/password authentication, or one that automatically creates a\nuser, such as Sign-In with Apple, an app distributed through the App Store\nmust implement  user account deletion . When you first log in with an authentication provider, App Services creates a\n user object  that contains a unique identity ID and\n provider-specific metadata  about the user. A single user object can have more than one identity. You can use the Realm SDKs\nto link identities to existing user accounts. This allows users to log in to\na single account with more than one provider. For more information, see the\ndocumentation on linking identities for your preferred SDK: Link User Identities - Flutter SDK Link User Identities - Java SDK Link User Identities - Kotlin SDK Link User Identities - .NET SDK Link User Identities - Node.js SDK Link User Identities - React Native SDK Link User Identities - Swift SDK Link User Identities - Web SDK When you use  email/password user authentication , you must first register a user, which creates\nthe user object. You can register users in your client application\nusing your preferred SDK or you can manually create email/password users. After registering the user, you must  confirm the user \nbefore they can authenticate. Each SDK offers an API that enables you to register an email/password user.\nAfter registering the user, you must confirm the user before the they can\nauthenticate. For code examples that demonstrate how to manage email/password\nusers in the client application, see the documentation for the Realm SDKs: Create Email/Password Users - C++ SDK Create Email/Password Users - Flutter SDK Create Email/Password Users - Java SDK Create Email/Password Users - Kotlin SDK Create Email/Password Users - .NET SDK Create Email/Password Users - Node SDK Create Email/Password Users - React Native SDK Create Email/Password Users - Swift SDK Create Email/Password Users - Web SDK You can create a new  Email/Password  user\nfrom the App Services UI, CLI, or Admin API. Manually-created users bypass any\nconfigured user confirmation flows. Manually creating a user can be useful for testing and debugging your application in\ndevelopment. Select  App Users  from the left navigation menu. Click the  Add New User  button. Specify an email address and password for the new user. The Email/Password authentication provider requires passwords to be\nbetween 6 and 128 characters long. Click  Create . You can also create API keys that applications use to connect to your\nApp. Although API Keys are not associated with\na single user, the  Users  tab lists them. To learn more\nabout API keys, see  API Key Authentication . To create a new email/password user, call  appservices users create  and\nspecify  --type=email . The CLI will prompt you for your App ID as well\nas the new user's email and password. You can also specify the arguments when you call the program: The Email/Password authentication provider requires passwords to be\nbetween 6 and 128 characters long. To create a new email/password user, create a  POST  request\nin the following format. You must specify the user credentials\nin the request body and the Group and App ID in the request URL. The Email/Password authentication provider requires passwords to be\nbetween 6 and 128 characters long. Admin API Documentation You must confirm the email address of new\n Email/Password users \nbefore they are permitted to log into App Services. The\nexact method of confirmation depends upon your provider\nconfiguration, but typically involves a\n handshake process  between the\nuser and your application. You can read more about\nEmail/Password user confirmation at  Email/Password\nConfirmation . Sometimes, users are unable to complete the confirmation\nprocess. For example: To help you work around cases like this, you can confirm\nusers manually using the App Services UI or the Admin API: An overzealous spam filter might block App Services email confirmation\nemails. A proxy or web blocker could prevent a user from activating the\n confirmUser  client SDK function via the client application. An implementation error could cause the client application's user\nconfirmation page to fail for specific use cases. To confirm a pending email/password user in the UI: A manually-confirmed user continues to appear in the  PENDING \nuser list until they log in to your application for the first time, at\nwhich point App Services moves them into the list of confirmed users and\ntransitions their  User Status  to  confirmed . Select  App Users  from the left navigation menu. Under the  Users  tab, select the  PENDING \nbutton. Find the user in the list and click on the ellipsis ( ... ). Select the  Confirm User  option from the context menu that\nappears. If the operation succeeds, the banner at the top of the App Services\nadmin console should display a confirmation message. The user's\n User Status  changes from  Pending Confirmation \nto  Pending User Login . To confirm a pending email/password user using the Admin API, create\na  POST  request in the following format. You must specify the Group\nID, App ID, and email address. Admin API Documentation A few circumstances can lead to incomplete Email/Password user\nconfirmation workflows: Users caught in this situation appear stuck in an unconfirmed state.\nSince an existing account has their email registered to it, users cannot\ncreate a new account with the same email address. Also, they cannot log\ninto an account that is not confirmed. Applications that use the built-in email confirmation service of App Services\ncan use the  resendConfirmationEmail  Client SDK method to send a new\nemail with a new confirmation link to the user, allowing them to confirm\ntheir account and log in. Calling this method will result in an error by\nany application using a confirmation flow other than  send a\nconfirmation email . There is no such specific method to re-run a custom confirmation\nfunction. Instead, App Services has a method to trigger a re-run of whatever\nthe current Email/Password user confirmation workflow happens to be. You\ncan manually re-run the currently selected user confirmation flow using\nthe App Services UI or the  App Services Admin API : An email is caught by a spam filter, not delivered due to a bug, or\naccidentally deleted by a prospective App Services user. A custom confirmation function is unable to communicate with an\nunconfirmed user due to a bug or oversight. An unconfirmed user forgot to visit their confirmation link within 30\nminutes of receiving their login tokens, and the tokens expired. To re-run the confirmation workflow for a user in the UI: If the re-run fails or the user's tokens expire again, you can re-run\nthe confirmation function as many times as necessary. Select  App Users  from the left navigation menu. Under the  Users  tab, select the  PENDING \nbutton. Find the user in the list and click on the ellipsis ( ... ). Select the  Run user confirmation  option from the context\nmenu that appears. Select the  Run User Confirmation  button in the dialogue\nbox that appears. If the operation succeeds, the banner at the top of the App Services\nadmin console should display a confirmation message. The user's\n User Status  changes from  Pending Confirmation \nto  Pending User Login . Once the user logs in, they will\nmove into the list of active users automatically. To re-run the confirmation workflow for a user using the Admin API, create a\n POST  request in the following format. You must specify the Group ID, App ID,\nand email address. Admin API Documentation For all providers except for Email/Password authentication,\nApp Services automatically creates a user object the first time a user authenticates. You can log in to a single account with more than one provider by using the Realm SDKs\nto link identities. The Email/Password authentication provider requires users to create an\naccount the first time they connect to your App. Email/Password users must be confirmed manually through the App Services UI or Admin API. Users caught in an incomplete Email/Password confirmation workflow\nmust re-run the confirmation through the App Services UI or Admin API.",
            "code": [
                {
                    "lang": "bash",
                    "value": "appservices users create --type=email"
                },
                {
                    "lang": "bash",
                    "value": "appservices users create --type=email \\\n  --app=<Your App ID> \\\n  --email=<User's Email Address> \\\n  --password=<User's Password>"
                },
                {
                    "lang": "sh",
                    "value": "curl --request POST \\\n  --header 'Authorization: Bearer <access_token>' \\\n  --data '{ \"email\": \"<string>\", \"password\": \"<string>\" }' \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/<groupId>/apps/<appId>/users"
                },
                {
                    "lang": "sh",
                    "value": "curl --request POST \\\n  --header 'Authorization: Bearer <access_token>' \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/<groupId>/apps/<appId>/user_registrations/by_email/<email>/confirm"
                },
                {
                    "lang": "sh",
                    "value": "curl --request POST \\\n  --header 'Authorization: Bearer <access_token>' \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/<groupId>/apps/<appId>/user_registrations/by_email/<email>/run_confirm"
                }
            ],
            "preview": "Atlas App Services provides various authentication providers\nto log users into your app. For most providers, App Services automatically creates a\nuser account the first time a user authenticates through the provider. The only exception is\nemail/password user authentication, which requires\nregistering and confirming a user before the user can authenticate.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "users/manage",
            "title": "Manage User Accounts",
            "headings": [
                "Overview",
                "Delete a User",
                "Manually Delete a User",
                "Delete a User in the SDK",
                "Delete a User with a Custom Function",
                "Disable a User",
                "Enable a User"
            ],
            "paragraphs": "You can manage your application's user accounts with the App Services UI,\nApp Services CLI, Admin API, or Realm SDKs. You can completely remove a user from your application, including any\nmetadata and authentication provider identities. Deleting a user also\nimmediately ends any  sessions  associated with\nthat user. If you don't want to delete the user's account, you can  disable their\naccount  to temporarily suspend their access. You can use the App Services UI, CLI, or Admin API to manually remove a user account. Select  App Users  from the left navigation menu. Select either  Confirmed  or  Pending , depending\non the current state of the user you wish to delete. Under the  Users  tab, find a user in the list and click on\nthe ellipsis ( ... ). Click  Delete User  and confirm your choice. To delete a user, call  appservices users delete . The CLI will prompt you\nfor your App ID and list users in that app for you to select. You can also specify the arguments when you call the program: You can delete multiple users with a single command by specifying\ntheir  id  values as a comma-separated list. To delete a user, create a  DELETE  request\nin the following format. You must specify the Group, App, and User ID. If you want to delete a  pending \nemail/password user, create a request in the following format: Admin API Documentation App Services does not automatically delete any data in your linked\nMongoDB Atlas cluster that you have associated with a deleted user. For example,\nif your application allows users to create data that linked to a user by\nincluding their ID in an  owner_id  field, deleting the user object does\nnot delete the user-created linked data. To remove all traces of a deleted\nuser, you must manually delete or modify any such documents. You can give users the option to delete their own account from a client\napplication when you use the Realm SDKs to delete users. Delete a User - Flutter SDK Delete a User - Kotlin SDK Delete a User - .NET SDK Delete a User - Node.js SDK Delete a User - React Native SDK Delete a User - Swift SDK Delete a User - Web SDK You can write a custom function to delete a user. You might want to do this if\nyour SDK does not yet support the delete users API. Create a function  similar to our example below\nthat uses  Application Authentication . You might want to incorporate\nerror handling in the event that the function does not successfully\nauthenticate, or it cannot delete the calling user. For this example function, we have  created values and secrets for the\nadminApiPublicKey and adminApiPrivateKey .\nWe would then add the  Project and Application IDs  to the  apiUrl . We can then  call this function from the SDK . The example function below does not\ntake any arguments, and deletes the user who calls the function. If your app uses  Email/Password Authentication , consider that you may want to\ndelete pending users, which involves a second endpoint: Delete users Delete pending users You can temporarily disable a user, which prevents the user from logging in and\ninvalidates any of the user's existing access and refresh tokens. You can\n enable  a disabled user to let them log in again. Select  App Users  from the left navigation menu. Select either  Confirmed  or  Pending , depending\non the current state of the user you wish to disable. Under the  Users  tab, find a user in the list and click on\nthe ellipsis ( ... ). Click  Disable User  and confirm your choice. To disable a user, call  appservices users disable . The CLI will prompt you\nfor your App ID and list users in that app for you to select. You can also specify the arguments when you call the program: You can disable multiple users with a single command by specifying\ntheir  id  values as a comma-separated list. To disable a user, create a  PUT  request\nin the following format. You must specify the Group, App, and User ID. Admin API Documentation You can enable a  disabled user  to let them log in again. Select  App Users  from the left navigation menu. Select either  Confirmed  or  Pending , depending\non the current state of the user you wish to enable. Under the  Users  tab, find a user in the list and click on\nthe ellipsis ( ... ). Click  Enable User  and confirm your choice. To enable a user, call  appservices users enable . The CLI will prompt you\nfor your App ID and list users in that app for you to select. You can also specify the arguments when you call the program: You can enable multiple users with a single command by specifying\ntheir  id  values as a comma-separated list. To enable a user, create a  PUT  request\nin the following format. You must specify the Group, App, and User ID. Admin API Documentation",
            "code": [
                {
                    "lang": "bash",
                    "value": "appservices users delete"
                },
                {
                    "lang": "bash",
                    "value": "appservices users delete \\\n  --app=<Your App ID> \\\n  --user=<User ID>"
                },
                {
                    "lang": "bash",
                    "value": "appservices users delete --user=6099694d5debcbcc873ff413,60996996b78eca4a8d615d3a"
                },
                {
                    "lang": "sh",
                    "value": "curl --request DELETE \\\n  --header 'Authorization: Bearer <access_token>' \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/<groupId>/apps/<appId>/users/<userId>"
                },
                {
                    "lang": "sh",
                    "value": "curl --request DELETE \\\n   --header 'Authorization: Bearer <access_token>' \\\n   https://services.cloud.mongodb.com/api/admin/v3.0/groups/<groupId>/apps/<appId>/user_registrations/by_email/<email>"
                },
                {
                    "lang": "javascript",
                    "value": "const apiUrl = \"https://services.cloud.mongodb.com/api/admin/v3.0/groups/{insert-your-project-id}/apps/{insert-your-app-id}\";\n\nexports = async function(){\n\n// This function deletes the user who calls it. It gets this user's ID\n// from the user in the function context. This is safer than accepting\n// a passed-in user ID, as the user can never delete any other user's account.\nconst callersUserId = context.user.id\n\nasync function adminLogIn() {\n   const username = context.values.get(\"adminApiPublicKey\");\n   const apiKey = context.values.get(\"adminApiPrivateKey\");\n   const response = await context.http.post({\n      url: \"https://services.cloud.mongodb.com/api/admin/v3.0/auth/providers/mongodb-cloud/login\",\n      body: {username, apiKey},\n      encodeBodyAsJSON: true,\n   });\n   const body = EJSON.parse(response.body.text());\n   return body.access_token;\n}\n\nconst token = await adminLogIn();\n\nasync function deleteUser(_id) {\n   await context.http.delete({\n      url: `${apiUrl}/users/${_id}`,\n      headers: {\"Authorization\": [`Bearer ${token}`]}\n   });\n   return _id;\n}\n\nreturn deleteUser(callersUserId);\n\n};"
                },
                {
                    "lang": "bash",
                    "value": "appservices users disable"
                },
                {
                    "lang": "bash",
                    "value": "appservices users disable \\\n  --app=<Your App ID> \\\n  --user=<User ID>"
                },
                {
                    "lang": "bash",
                    "value": "appservices users disable --user=6099694d5debcbcc873ff413,60996996b78eca4a8d615d3a"
                },
                {
                    "lang": "sh",
                    "value": "curl --request PUT \\\n  --header 'Authorization: Bearer <access_token>' \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/<groupId>/apps/<appId>/users/<userId>/disable"
                },
                {
                    "lang": "bash",
                    "value": "appservices users enable"
                },
                {
                    "lang": "bash",
                    "value": "appservices users enable \\\n  --app=<Your App ID> \\\n  --user=<User ID>"
                },
                {
                    "lang": "bash",
                    "value": "appservices users enable --user=6099694d5debcbcc873ff413,60996996b78eca4a8d615d3a"
                },
                {
                    "lang": "sh",
                    "value": "curl --request PUT \\\n  --header 'Authorization: Bearer <access_token>' \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/<groupId>/apps/<appId>/users/<userId>/enable"
                }
            ],
            "preview": "You can manage your application's user accounts with the App Services UI,\nApp Services CLI, Admin API, or Realm SDKs.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "users/sessions",
            "title": "Manage User Sessions",
            "headings": [
                "Overview",
                "Get a User Session Access Token",
                "Get a User Access Token Over HTTPS",
                "Find Your App's Client API Base URL",
                "Authenticate a User",
                "Get a User Access Token from a Realm SDK",
                "Refresh a User Session Access Token",
                "Refresh a User Access Token Over HTTPS",
                "Refresh a User Access Token from a Realm SDK",
                "Configure Refresh Token Expiration",
                "Set the Refresh Token Expiration In the Admin UI",
                "Set the Refresh Token Expiration Over HTTPS",
                "Verify & Decode a User Access Token",
                "End a User Session",
                "Revoke a User's Sessions"
            ],
            "paragraphs": "App Services manages  user sessions  with access\ntokens and refresh tokens. An access token allows anyone that has the\ntoken to use the session it's associated with for up to\n30 minutes. A refresh token lets you generate a\nnew access token for a session even if the previous token has already\nexpired. The Realm SDKs automatically manage user access and refresh tokens for\nyou when they connect to App Services. You may want to manage sessions\nyourself if you're connecting to an API service like the Data API. You can authenticate a user and receive a user access token directly\nover HTTPS or through a session managed by a Realm SDK. You can get an access token by sending an authentication request\ndirectly to App Services over HTTPS. The authentication endpoint accepts\nan HTTPS POST request with a JSON body that contains the user's login\ncredentials. If the credentials are valid, the response contains a user\naccess token. Your App's authentication endpoint URL depends on the App's\ndeployment model. Select your App's deployment model to see how to construct the\nbase URL. You can find the base URL programmatically with the App\nlocation endpoint. The response body includes the base URL as\nthe  hostname  field. Replace  <App ID>  with your client App ID. For example,\n myapp-abcde . Replace  <Region>  with the region where your app is hosted. For example,  us-east-1 . Replace  <Cloud>  with the cloud where your app is hosted. For example,  aws ,  azure , or  gcp . Replace  <App ID>  with your client app ID. For example,  myapp-abcde . To authenticate a user, call the login endpoint, specifying the\nauthentication provider. The login endpoint is\n <Base URL>/auth/providers/<ProviderType>/login , where\n <ProviderType>  is one of the following types: Append the login endpoint to the base URL, and send an HTTPS POST request\nwith a JSON body that contains the user's login credentials for the chosen\nprovider type: If the authentication request succeeds, the response body includes\n access_token  and  refresh_token  values for the user. The\n access_token  is a JSON Web Token (JWT) that you can use to\nauthenticate requests. anon-user local-userpass api-key custom-token custom-function In the Realm SDKs, you can access a logged in user's access token from\ntheir  User  object. The SDKs automatically refresh expired access\ntokens for SDK operations and you can manually refresh it by calling a\nmethod. To learn more, refer to the documentation for your Realm SDK: Realm C++ SDK Realm Flutter SDK Realm Java SDK Realm Kotlin SDK Realm .NET SDK Realm Node.js SDK Realm React Native SDK Realm Swift SDK Web SDK Access tokens expire 30 minutes after they are\ngranted. When an access token expires, you must get a new access token\nto continue sending requests. You could acquire a new access token by logging the user in again, but\nthat would require the user to re-enter their credentials. Instead, you\ncan use the refresh token associated with the user's session to get a\nnew access token that's valid for another\n30 minutes. You get the refresh token in the  same response as the access token  after a successful login. The session refresh endpoint accepts a  POST  request that includes\nthe refresh token in the  Authorization  header. The endpoint URL\ndepends on your App's deployment model. Replace  <Region>  with the region where your app is hosted. For example,  us-east-1 . Replace  <Cloud>  with the cloud where your app is hosted. For example,  aws ,  azure , or  gcp . The Realm SDKs automatically refresh a logged in user's access token if\nthe token is expired at the time of a request. The SDKs also allow you to manually refresh a user's access token by\ncalling a method on the  User  object. To learn more, refer to the documentation for your Realm SDK: Realm C++ SDK Realm Flutter SDK Realm Java SDK Realm Kotlin SDK Realm .NET SDK Realm Node.js SDK Realm React Native SDK Realm Swift SDK Web SDK By default, refresh tokens expire\n60 days after they are issued.\nYou can configure the refresh token expiration interval for your App to be\nanywhere between 30 minutes\nand 5 years inclusive. Anonymous user refresh tokens have a long expiration time and\neffectively do not expire. Instead, Anonymous user accounts are\nautomatically deleted 90 days after they are created. You can configure the refresh token expiration time for all sessions in\nan App from the Admin UI or Admin API. To set the refresh token expiration time for all sessions in an App from\nthe Admin UI: Click  App Users  in the left navigation menu. Select the  User Settings  tab. Find the  Refresh Token Expiration  option and click the\n Edit  button. Enter a time value in the text input and choose the appropriate time\nunit (e.g. \"minutes\" or \"days\") from the dropdown menu. Click  Save . To set the refresh token expiration time for all sessions in an App from\nthe Admin API, call your App's  Set User Refresh\nToken Expiration Time \nendpoint with the  expiration_time_seconds  field of the request body\nset to your desired expiration time. If you successfully update the refresh token expiration time, the\nendpoint returns a  204  response. You can decode an access token to verify that it has the correct format\nand has a valid signature. The decoded response includes other\ninformation like when the access token expires. Call your App's  Verify and decode an access token  Admin API endpoint with the\ntoken in the  token  field of the request body. If the token is valid, you'll receive a  200  response. If the token is valid but expired, the response indicates that the token\nis expired rather than containing the decoded JWT. If the token is invalid, you'll receive a  401  response that contains\nan error message. Once a session has been established you cannot end it individually,\nthough you can  revoke all of a user's active sessions . You can effectively end a user session by deleting all copies of the\naccess token and refresh token. This prevents further access to the\nsession and requires the user to authenticate and start a new session\nto continue. Each Realm SDK has a  User.logOut()  method that deletes local copies\nof the access token and refresh token and invalidates the refresh token\nso that it can't be used to get a new access token. If a user account is  deleted , either by an\nadministrator or by the user, then all of the user's sessions are\nautomatically revoked. Even if you delete and invalidate all copies of a token that you\nhave, that does  not  end the active session on the server. If a\nmalicious user copied the access token before it was deleted, they\ncould use the token to make requests for up to\n30 minutes until it expires. These requests\nwould appear to come from the user who \"logged out\". You can revoke all of a user's current sessions. This invalidates the\nsession access and refresh tokens and prevents the user from making any\nrequests on any device until they log in again. If a user account is deleted, either by an administrator or by the user,\nthen all of the user's sessions are automatically revoked. Select  App Users  from the left navigation menu. Under the  Users  tab, find a user in the list and click on the ellipsis ( ... ). Click  Revoke all sessions . To revoke all sessions for a user, call  appservices users\nrevoke . The CLI will prompt you for your App ID and list users\nin that app for you to select. You can also specify the arguments when you run the command: You can revoke sessions for multiple users with a single command by\nspecifying their  id  values as a comma-separated list. Call the  Revoke user sessions  endpoint. Make sure to replace the following values in the endpoint URL: {groupId}  with your  Atlas Project ID . {appId}  with your App's  internal ObjectID . {userId}  with the user's account ID.",
            "code": [
                {
                    "lang": "text",
                    "value": "https://services.cloud.mongodb.com/api/client/v2.0/app/<App ID>"
                },
                {
                    "lang": "text",
                    "value": "https://<Region>.<Cloud>.services.cloud.mongodb.com/api/client/v2.0/app/<App ID>"
                },
                {
                    "lang": "shell",
                    "value": "curl 'https://services.cloud.mongodb.com/api/client/v2.0/app/<App ID>/location'"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"deployment_model\": \"LOCAL\",\n  \"location\": \"US-VA\",\n  \"hostname\": \"https://us-east-1.aws.services.cloud.mongodb.com\",\n  \"ws_hostname\": \"wss://ws.us-east-1.aws.services.cloud.mongodb.com\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJiYWFzX2RldmljZV9pZCI6IjAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMCIsImJhYXNfZG9tYWluX2lkIjoiNWVlYTg2NjdiY2I0YzgxMGI2NTFmYjU5IiwiZXhwIjoxNjY3OTQwNjE4LCJpYXQiOjE2Njc5Mzg4MTgsImlzcyI6IjYzNmFiYTAyMTcyOGI2YzFjMDNkYjgzZSIsInN0aXRjaF9kZXZJZCI6IjAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMCIsInN0aXRjaF9kb21haW5JZCI6IjVlZWE4NjY3YmNiNGM4MTBiNjUxZmI1OSIsInN1YiI6IjYzNmFiYTAyMTcyOGI2YzFjMDNkYjdmOSIsInR5cCI6ImFjY2VzcyJ9.pyq3nfzFUT-6r-umqGrEVIP8XHOw0WGnTZ3-EbvgbF0\",\n  \"refresh_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJiYWFzX2RhdGEiOm51bGwsImJhYXNfZGV2aWNlX2lkIjoiMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwIiwiYmFhc19kb21haW5faWQiOiI1ZWVhODY2N2JjYjRjODEwYjY1MWZiNTkiLCJiYWFzX2lkIjoiNjM2YWJhMDIxNzI4YjZjMWMwM2RiODNlIiwiYmFhc19pZGVudGl0eSI6eyJpZCI6IjYzNmFiYTAyMTcyOGI2YzFjMDNkYjdmOC1ud2hzd2F6ZHljbXZycGVuZHdkZHRjZHQiLCJwcm92aWRlcl90eXBlIjoiYW5vbi11c2VyIiwicHJvdmlkZXJfaWQiOiI2MjRkZTdiYjhlYzZjOTM5NjI2ZjU0MjUifSwiZXhwIjozMjQ0NzM4ODE4LCJpYXQiOjE2Njc5Mzg4MTgsInN0aXRjaF9kYXRhIjpudWxsLCJzdGl0Y2hfZGV2SWQiOiIwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAiLCJzdGl0Y2hfZG9tYWluSWQiOiI1ZWVhODY2N2JjYjRjODEwYjY1MWZiNTkiLCJzdGl0Y2hfaWQiOiI2MzZhYmEwMjE3MjhiNmMxYzAzZGI4M2UiLCJzdGl0Y2hfaWRlbnQiOnsiaWQiOiI2MzZhYmEwMjE3MjhiNmMxYzAzZGI3Zjgtbndoc3dhemR5Y212cnBlbmR3ZGR0Y2R0IiwicHJvdmlkZXJfdHlwZSI6ImFub24tdXNlciIsInByb3ZpZGVyX2lkIjoiNjI0ZGU3YmI4ZWM2YzkzOTYyNmY1NDI1In0sInN1YiI6IjYzNmFiYTAyMTcyOGI2YzFjMDNkYjdmOSIsInR5cCI6InJlZnJlc2gifQ.h9YskmSpSLK8DMwBpPGuk7g1s4OWZDifZ1fmOJgSygw\",\n  \"user_id\": \"636aba021728b6c1c03db7f9\"\n}"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST 'https://services.cloud.mongodb.com/api/client/v2.0/app/myapp-abcde/auth/providers/anon-user/login'"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST 'https://services.cloud.mongodb.com/api/client/v2.0/app/myapp-abcde/auth/providers/local-userpass/login' \\\n  --header 'Content-Type: application/json' \\\n  --data-raw '{\n    \"username\": \"test@example.com\",\n    \"password\": \"Pa55w0rd\"\n  }'"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST 'https://services.cloud.mongodb.com/api/client/v2.0/app/myapp-abcde/auth/providers/api-key/login' \\\n  --header 'Content-Type: application/json' \\\n  --data-raw '{\n    \"key\": \"hScMWZyOKnjQMbfDPMJ1qHgtdGT2raQXdVDDvlC2SzKEBKlHKV8FK9SPCSTnODPg\"\n  }'"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST 'https://services.cloud.mongodb.com/api/client/v2.0/app/myapp-abcde/auth/providers/custom-token/login' \\\n  --header 'Content-Type: application/json' \\\n  --data-raw '{\n    \"token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJ0ZXN0ZGV2LWJwcWVsIiwiZXhwIjoxNTE2MjM5MDIyLCJzdWIiOiIyNDYwMSIsInVzZXJfZGF0YSI6eyJuYW1lIjoiSmVhbiBWYWxqZWFuIiwiYWxpYXNlcyI6WyJNb25zaWV1ciBNYWRlbGVpbmUiLCJVbHRpbWUgRmF1Y2hlbGV2ZW50IiwiVXJiYWluIEZhYnJlIl19fQ.mVWr4yFf8nD1EhuhrJbgKfY7BEpMab38RflXzUxuaEI\"\n  }'"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST 'https://services.cloud.mongodb.com/api/client/v2.0/app/myapp-abcde/auth/providers/custom-function/login' \\\n  --header 'Content-Type: application/json' \\\n  --data-raw '{\n    \"someCustomFunctionArg\": \"<Login Info>\"\n  }'"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST 'https://services.cloud.mongodb.com/api/client/v2.0/auth/session' \\\n  --header 'Content-Type: application/json' \\\n  --header 'Authorization: Bearer <RefreshToken>'"
                },
                {
                    "lang": "json",
                    "value": "{\n    \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJiYWFzX2RldmljZV9pZCI6IjAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMCIsImJhYXNfZG9tYWluX2lkIjoiNWVlYTg2NjdiY2I0YzgxMGI2NTFmYjU5IiwiZXhwIjoxNjY3OTQzOTc5LCJpYXQiOjE2Njc5NDIxNzksImlzcyI6IjYzNmFjNzAyMDE5ZDJkYmY0NzUxMDRjMiIsInN0aXRjaF9kZXZJZCI6IjAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMCIsInN0aXRjaF9kb21haW5JZCI6IjVlZWE4NjY3YmNiNGM4MTBiNjUxZmI1OSIsInN1YiI6IjYzNmFjNzAyMDE5ZDJkYmY0NzUxMDQ5MyIsInR5cCI6ImFjY2VzcyJ9.pF3DR-096Ujt9-0KOWJTU25ZuryvwMfeCI7TiHJERNg\"\n}"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST 'https://<Region>.<Cloud>.services.cloud.mongodb.com/api/client/v2.0/auth/session' \\\n  --header 'Content-Type: application/json' \\\n  --header 'Authorization: Bearer <RefreshToken>'"
                },
                {
                    "lang": "json",
                    "value": "{\n    \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJiYWFzX2RldmljZV9pZCI6IjAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMCIsImJhYXNfZG9tYWluX2lkIjoiNWVlYTg2NjdiY2I0YzgxMGI2NTFmYjU5IiwiZXhwIjoxNjY3OTQzOTc5LCJpYXQiOjE2Njc5NDIxNzksImlzcyI6IjYzNmFjNzAyMDE5ZDJkYmY0NzUxMDRjMiIsInN0aXRjaF9kZXZJZCI6IjAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMCIsInN0aXRjaF9kb21haW5JZCI6IjVlZWE4NjY3YmNiNGM4MTBiNjUxZmI1OSIsInN1YiI6IjYzNmFjNzAyMDE5ZDJkYmY0NzUxMDQ5MyIsInR5cCI6ImFjY2VzcyJ9.pF3DR-096Ujt9-0KOWJTU25ZuryvwMfeCI7TiHJERNg\"\n}"
                },
                {
                    "lang": "bash",
                    "value": "curl -X PUT \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/security/refresh_token_expiration \\\n  --data-raw '{\n    \"expiration_time_seconds\": 864000\n  }'"
                },
                {
                    "lang": "json",
                    "value": "\"token expired\""
                },
                {
                    "lang": "json",
                    "value": "{\n  \"error\": \"signature is invalid\"\n}"
                },
                {
                    "lang": "bash",
                    "value": "curl -X POST \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/users/verify_token \\\n  --data-raw '{\n    \"token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJiYWFzX2RldmljZV9pZCI6IjAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMCIsImJhYXNfZG9tYWluX2lkIjoiNWNkYjEyNDA4ZTIzMmFjNGY5NTg3ZmU4IiwiZXhwIjoxNjc2NTExMjgyLCJpYXQiOjE2NzY1MDk0ODIsImlzcyI6IjYzZWQ4MTJhNDNiZTcyYzE3NmFhNWQyMyIsInN0aXRjaF9kZXZJZCI6IjAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMCIsInN0aXRjaF9kb21haW5JZCI6IjVjZGIxMjQwOGUyMzJhYzRmOTU4N2ZlOCIsInN1YiI6IjYzZWQ4MTJhNDNiZTcyYzE3NmFhNWQyMSIsInR5cCI6ImFjY2VzcyJ9.7kHO9wjWvIaD3VewDyPhLyb-oRc7wTYZdD9-hroF-H4\"\n  }'"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"sub\": \"63ed812a43be72c176aa5d21\",\n  \"exp\": 1676511282,\n  \"iat\": 1676509482,\n  \"iss\": \"63ed812a43be72c176aa5d23\",\n  \"domain_id\": \"5cdb12408e232ac4f9587fe8\",\n  \"device_id\": \"000000000000000000000000\"\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices users revoke"
                },
                {
                    "lang": "bash",
                    "value": "appservices users revoke \\\n  --app=<Your App ID> \\\n  --user=<User ID>"
                },
                {
                    "lang": "bash",
                    "value": "appservices users revoke --user=6099694d5debcbcc873ff413,60996996b78eca4a8d615d3a"
                },
                {
                    "lang": "sh",
                    "value": "curl -X PUT \\\n  -H 'Authorization: Bearer {access_token}' \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/users/{userId}/logout"
                }
            ],
            "preview": "Learn how to manage App Services user access tokens and user sessions.",
            "tags": "code example, curl",
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "users/read-metadata",
            "title": "Read User Metadata",
            "headings": [
                "Overview",
                "Schema",
                "Find Users",
                "Filter Users",
                "View User Data",
                "View a User's Devices",
                "View a User's Provider Data",
                "View a User's Activity Log",
                "Summary"
            ],
            "paragraphs": "Atlas App Services represents each application user internally with a\n User Object  that includes a unique ID and additional\nmetadata that describes the user. You can access user\nobjects: With the App Services UI, CLI, or Admin API to  find \nand  filter  users. In the App Services UI by  viewing user data . In a  Function  by accessing the  context.user  interface. In a  rule expression  with the  %%user \nexpansion. In  Android ,  iOS ,  Node ,\n React Native , and\n .NET  client applications using a Realm SDK. User objects have the following form: Field Type Description id string A string representation of the  ObjectId  that uniquely identifies the\nuser. type string The type of the user. The following types are possible: Type Description \"normal\" The user is an  application user  logged in\nthrough an authentication provider other than the\n API Key  provider. \"server\" The user is a server process logged in with any type of\n App Services API Key . \"system\" The user is the  system user  that\nbypasses all rules. data document A document that contains metadata that describes the\nuser. This field combines the data for all  identities \nassociated with the user, so the exact field names and values\ndepend on which  authentication providers \nthe user has authenticated with. In  system functions , the  user.data \nobject is empty. Use  context.runningAsSystem()  to test if\nthe function is running as a system user. custom_data document A document from your application's  custom user\ndata collection  that\nspecifies the user's ID. You can use the custom user data\ncollection to store arbitrary data about your application's\nusers. If you set the  name  field, App Services populates the\n username  metadata field with the return value of  name .\nApp Services automatically fetches a new copy of the data\nwhenever a user refreshes their access token, such as when they\nlog in. The underlying data is a regular MongoDB document, so you\ncan use standard CRUD operations through the  MongoDB Atlas\nservice  to define and modify the user's custom data. Custom user data is limited to  16MB , the maximum size of a\nMongoDB document. To avoid hitting this limit, consider\nstoring small and relatively static user data in each custom\nuser data document, such as the user's preferred language or\nthe URL of their avatar image. For data that is large,\nunbounded, or frequently updated, consider only storing a\nreference to the data in the custom user document or storing\nthe data with a reference to the user's ID rather than in the\ncustom user document. identities array A list of authentication provider identities associated with the\nuser. When a user first logs in with a specific provider, App Services\nassociates the user with an identity object that contains a\nunique identifier and additional metadata about the user from the\nprovider. For subsequent logins, App Services refreshes the existing\nidentity data but does not create a new identity. Identity\nobjects have the following form: Field Name Description id A provider-generated string that uniquely identifies this\nidentity provider_type The type of authentication provider associated with this\nidentity. data Additional metadata from the authentication provider that\ndescribes the user. The exact field names and values will\nvary depending on which authentication providers the user\nhas logged in with. For a provider-specific breakdown of\nuser identity data, see  User Metadata . In general, App Services creates a user object for a given user\nthe first time that they authenticate. If you create a\ntest  Email/Password \nuser through the App Services UI, App Services creates that user's user\nobject immediately. To find information about one or more users, click  App Users \nin the left navigation menu. The page opens to the  Users  tab which has\na list of users associated with your Atlas App Services App: If you already know the ID of the user you want to find, you can search\nfor their ID in the  Users  table search bar. To find information about one or more users, call  appservices users\nlist . The CLI will prompt you for your App ID and list users in that app\nfor you to select. You can also specify the arguments when you call the program: To find information about one or more users, create a  GET  request\nin the following format. You must specify the Group and App ID. If you already know the ID of the user you want to find, you can append\nthe ID to the request URL: Admin API Documentation You can specify filters that limit a list of users to a subset of users that\nsatisfy the filter conditions. Use the  filter bar  at the top of the  Users  table\nto define filters. You can filter by: Provider type Anonymous Email/Password API Key Facebook Google Apple Custom JWT Custom Function Status Confirmed Pending State Enabled Disabled You can filter user operations with the following flags and arguments: --pending : If included, lists only pending users. Otherwise, lists only confirmed users. --state : Lists only users in the specified state \"enabled\" \"disabled\" --provider : \"anon-user\" \"local-userpass\" \"api-key\" \"oauth2-facebook\" \"oauth2-google\" \"oauth2-apple\" \"custom-token\" \"custom-function\" The following command limits the output to only  pending  (unconfirmed) email/password users: The following command limits the output to only  disabled  users: You can filter user operations with the following query parameters: You can also limit results to only  pending  (unconfirmed)\nemail/password users. Create a  GET  request in the following format: Parameter Type Description after string The  id  of the last user returned by a previous paginated request. sort string The field name to sort results by. The only valid value is the\ndefault:  _id . desc boolean If  true , returns sorted results in descending order. If not specified or\nset to false, results return in ascending order. The following request returns users in descending order by their  _id : Admin API Documentation When a user connects to your application, Atlas App Services\n logs information  similar to the following: Logged information includes: To view this information, from the list of users: The device platform, which will be either an http client\n(\"chrome\", \"firefox\", \"phantomjs\", etc.) or a mobile OS (\"ios\",\n\"android\", etc.) The platform version. Your app version on the device. A unique device ID. Find the user whose devices you want to view. Click  ...  to open the options menu, and select\n View Devices . Whenever a user connects to your application and authenticates using one\nof the authentication providers you have enabled, App Services logs the\nprovider data. To view this information, from the list of users: Find the user whose provider data you want to view. Click  ...  to open the options menu, and select\n View Provider Data . App Services logs every request made by each of your App\nusers, including executed function calls.  Also, if a function writes to\nthe log using  console.log ,  console.warn , or  console.error ,\nthese entries will be included within the function's log output. To view this information, from the list of users: Find the user whose activity data you want to view. Click  View Activity . This redirects you to a log of requests\nmade by that specific user. You can expand an entry to view details for\nthat request. The user object contains relevant information about the user that you can use in your app logic. The exact information contained in the user object depends on the  authentication providers  used. You can search for users in the  Users  page of the App Services UI. App Services logs information such as user's connected devices,\nprovider data, and activity log.",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<Unique User ID>\",\n  \"type\": \"<User Type>\",\n  \"data\": {\n    \"<Metadata Field>\": <Value>,\n    ...\n  },\n  \"custom_data\": {\n    \"<Custom Data Field>\": <Value>,\n    ...\n  },\n  \"identities\": [\n    {\n      \"id\": <Unique Identity ID>,\n      \"provider_type\": \"<Authentication Provider>\",\n      \"data\": {\n        \"<Metadata Field>\": <Value>,\n        ...\n      }\n    }\n  ]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<Unique ID>\",\n  \"provider_type\": \"<Provider Name>\",\n  \"data\": {\n    \"<Metadata Field>\": <Value>,\n    ...\n  }\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices users list"
                },
                {
                    "lang": "bash",
                    "value": "appservices users list --app=<Your App ID>"
                },
                {
                    "lang": "sh",
                    "value": "curl --request GET \\\n  --header 'Authorization: Bearer <access_token>' \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/<groupId>/apps/<appId>/users"
                },
                {
                    "lang": "sh",
                    "value": "curl --request GET \\\n  --header 'Authorization: Bearer <access_token>' \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/<groupId>/apps/<appId>/users/<userId>"
                },
                {
                    "lang": "shell",
                    "value": "appservices users list --pending --provider=\"local-userpass\""
                },
                {
                    "lang": "shell",
                    "value": "appservices users list --state=\"disabled\""
                },
                {
                    "lang": "shell",
                    "value": "curl --request GET \\\n  --header 'Authorization: Bearer <access_token>' \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/<groupId>/apps/<appId>/user_registrations/pending_users"
                },
                {
                    "lang": "shell",
                    "value": "curl --request GET \\\n  --header 'Authorization: Bearer <access_token>' \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/<groupId>/apps/<appId>/users?desc=true"
                }
            ],
            "preview": "View metadata associated with a specific App Services user, such as device info, provider info, or user activity.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2",
            "title": "realm-cli v2 [Deprecated]",
            "headings": [
                "Overview",
                "Installation",
                "Authentication",
                "Generate an API Key",
                "Navigate to MongoDB Cloud Access Manager",
                "Create an API Key",
                "Configure Your API Access List",
                "Authenticate with an API Key",
                "Authenticate a CLI User",
                "Options",
                "Commands"
            ],
            "paragraphs": "realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: The MongoDB Realm Command Line Interface ( realm-cli )\nallows you to programmatically manage your Apps.\nWith  realm-cli , you can create or update\nApps from a local directory as well as export\nexisting applications to a local directory. This page is a quickstart for version 2 of  realm-cli . If you need\ndocumentation for version 1 of  realm-cli , see:  Realm CLI v1 .\nTo check your CLI version, use:  realm-cli --version .\nTo upgrade your global install to the latest version, use:  npm upgrade -g mongodb-realm-cli . realm-cli  is available on  npm . To install version 2 of the\n realm-cli  on your system, ensure that you have  Node.js  installed and then run the following\ncommand in your shell: To use  realm-cli , you must authenticate. To authenticate, you must generate an\nAPI Key. The  MongoDB Cloud Access Manager \nallows you to manage access to your project for users, teams, and API\nKeys. Use the Project Access Manager by clicking the\n Project Access  tab on the  access manager\ndropdown  on your screen's top left-hand side. Project Users can log in using  realm-cli  tool with a Project API\nKey. Create a project API Key by clicking the grey  Create\nAPI Key  button on the right-hand side of the Project Access Manager. Clicking this button navigates you to the \"Create API Key\" screen. Set a\ndescription for your key. For write access, the CLI requires an API key with \"Project Owner\"\npermissions. For read-only access, you can use \"Project Read Only\". Use the\n Project Permissions  dropdown to select the appropriate permissions\nfor your use case. Copy the public key to use later in order to log in. Click  next  to\ncontinue configuring your key details. Copy your Private Key to a safe location for later use. For security,\nthe Private Key will not be visible again after initialization.\nAnother security feature is the API Access List. Creating an API\nAccess List entry ensures that API calls originate from permitted IPs. The IP Address of the user who will be using the API Key is required\nto use the key. Click the  Add Access List Entry  button.\nType in the IP Address or click the  Use Current IP Address \nbuttton and click save.  Finally, click the done button on your screen's\nlower right-hand to finish setting up your API key. Using your newly created public and private key, log in by running the\ncommand below. You should see the following result: Use \"realm-cli [command] --help\" for information on a specific command Name Type Required Description --profile string no Specify your profile (Default value: \"default\") (default \"default\") --telemetry string no Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string no Write CLI output to the specified filepath -f, --output-format string no Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors no Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes no Automatically proceed through CLI commands by agreeing to any required user prompts -h, --help false help for realm-cli realm-cli accessList  - Manage allowed IP addresses and CIDR blocks realm-cli apps  - Manage the App Services apps associated with the current user (alias: app) realm-cli function  - Interact with the Functions of your App (alias: functions) realm-cli login  - Log the CLI into App Services using a MongoDB Cloud API key realm-cli logout  - Log the CLI out of App Services realm-cli logs  - Interact with the Logs of your App (alias: log) realm-cli pull  - Exports the latest version of your App into your local directory (alias: export) realm-cli push  - Imports and deploys changes from your local directory to your App (alias: import) realm-cli schema  - Manage the Schemas of your App (alias: schemas) realm-cli secrets  - Manage the Secrets of your App (alias: secret) realm-cli users  - Manage the Users of your App (alias: user) realm-cli whoami  - Display information about the current user",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": "shell",
                    "value": "npm install -g mongodb-realm-cli"
                },
                {
                    "lang": "shell",
                    "value": "realm-cli login --api-key=\"<my api key>\" --private-api-key=\"<my private api key>\""
                },
                {
                    "lang": "shell",
                    "value": "you have successfully logged in as <your public key>"
                }
            ],
            "preview": "The MongoDB Realm Command Line Interface (realm-cli)\nallows you to programmatically manage your Apps.\nWith realm-cli, you can create or update\nApps from a local directory as well as export\nexisting applications to a local directory.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "users/custom-metadata",
            "title": "Define Custom User Metadata",
            "headings": [
                "Overview",
                "Custom User Data",
                "Create and Manage Custom User Data",
                "Secure Custom User Data",
                "User Creation Function",
                "Enable Custom User Data",
                "Navigate to the Custom User Data Configuration Screen",
                "Enable Custom User Data",
                "Specify the Custom User Data Collection",
                "Specify the User ID Field",
                "Define a User Creation Function (Optional)",
                "Deploy the Updated Application",
                "Pull the Latest Version of Your App",
                "Configure Custom User Data",
                "Deploy the Custom User Data Configuration",
                "Authenticate a MongoDB Atlas User",
                "Configure Custom User Data",
                "Access Custom User Data from a Client Application",
                "Authentication Provider Metadata",
                "Configure Authentication Provider Metadata",
                "Navigate to the Authentication Provider Configuration Screen",
                "Configure User Metadata",
                "Deploy the Updated Application",
                "Pull the Latest Version of Your App",
                "Configure Metadata Fields",
                "Deploy the Custom User Data Configuration",
                "Authenticate a MongoDB Atlas User",
                "Configure Metadata Fields",
                "Access User Metadata from a Client Application"
            ],
            "paragraphs": "You can associate custom metadata with each user of your App. For\nexample, you might store a user's preferred language, date of birth, or\nany other information that you want to associate with the user. You can source the metadata for a user from two sources: A collection in MongoDB Atlas that stores custom user data. You can\nassociate each user with a document in the collection by their user\nID. You can store arbitrary data in each document. An authentication provider. If the provider uses JSON Web Tokens, such\nas Google, Facebook, or a custom provider, you can define metadata\nfields in the provider configuration that associate data from the\nuser's JWT with their user account. You can store arbitrary data about your application users in a MongoDB\ncollection. Your App maps each user to a document in the collection by\nquerying a specific field for the user's ID. When a user authenticates,\nyour App looks up the user's data and includes it in their access token. Consider a user with the ID  \"63ed2dbe5960df2af7fd216e\" . If your\ncustom user data collection was set up to store the user's ID in the\n userId  field, the user would map to the following document: When you use custom user data, keep the following things in mind: Store One Document Per User : Documents that contain user data must\ninclude the user's ID in a specific field. If multiple documents\nspecify the same user's ID, App Services only exposes the data from\nthe document that was inserted first. Keep Custom User Data Small : The user's full custom user document\nis included in their access token. In general, aim to keep custom user\ndata documents small (say, less than 16KB). Other services might limit\nthe HTTP header size, which means that larger custom user data objects\ncan cause integration issues. Custom Data May Be Stale : A user's custom data is  sourced  from a\nMongoDB collection but is stored in and read from a user's\nauthentication access token. If a user has a valid access token when\nthe underlying document changes, their custom data in that session\ndoes not update until they refresh their access token or\nre-authenticate. You are responsible for managing documents in the custom user data\ncollection. Depending on your use case, you may: Automatically create a document for each user when they register for\nyour application using a  user creation function . This function runs before the user is\nissued an access token, so the data you add will be in the access\ntoken on their first login. Use an  authentication trigger  to\nupdate a user's custom data when they register or log in and to delete\ntheir data if their account is deleted. Triggers run asynchronously\nand may finish after the user's access token is created. Use a  scheduled trigger  to periodically\nupdate or delete custom user data. Manually create, update, and delete documents in the collection using\nstandard CRUD operations from a Function, a Realm SDK, a MongoDB\ndriver or MongoDB Compass. If your App's custom user data includes personal or private user\ninformation, you should restrict access to the custom user data\ncollection. Consider using one of the following permission models to\nrestrict read and write access to privileged users only: A user may read or write their own custom user data document. Deny\nread and write access to all other documents. The following collection configuration has one role that grants a\nuser read and write access to a document if and only if their ID is\ncontained in the document's  user_id  field. No user may read or write any custom user data documents. Instead,\nuse a system function to manage custom user data on behalf of users. You can define a Function that runs every time a new user successfully\nregisters but before their new user account is created. If the function\nthrows or otherwise errors, the user account creation fails. This lets\nyou ensure that users always have custom data associated with them once\ncreated. The function receives a  user metadata object  as\nits only argument. You can use this create a new custom user data\ndocument for the user. Once you've configured a user creation function, App Services\nprevents you from deleting the function. If you want to delete the\nfunction, first change your custom user data configuration to use a\ndifferent user creation function. You can configure and enable custom user data in the App Services\nAdmin UI. Click  App Users  in the left navigation menu.\nThen, select the  User Settings  tab and find the\n Custom User Data  section. Click the  Enable Custom User Data  toggle to set\nit to  On . You must store the custom data for your application's users in a\nsingle collection of a linked MongoDB Atlas cluster. To configure your\napplication to read user data from this collection, you need to\nspecify the following values: Cluster Name : The name of a  linked MongoDB cluster  that contains the custom user data collection. Database Name : The name of the MongoDB database that\ncontains the custom user data collection. Collection Name : The name of the MongoDB collection that\ncontains custom user data. Every document in the custom user data collection has a field that\nmaps it to a specific application user. The field must be of type\n ObjectID  or a  string  type that represents that ObjectID.\nThis field must be present in every document that maps to a user. Specify the name of the field that contains each user's ID in the\n User ID Field  input. If two documents contain the same user ID, one stored as a string\nand the other as an ObjectID, App Services\nmaps the document with the string type to the user. If you want to use a  user creation Function , define it in the inline editor or\nreference and existing function by name. Once you have configured the custom user data collection, you can\nmake custom user data available to client applications by deploying\nyour application. To deploy a draft application from the App Services UI: Once the application successfully deploys, App Services begins to associate\ncustom data with users. When a user logs in, App Services automatically\nqueries the custom user data collection for a document where the\nspecified  User ID Field  contains the user's ID. If a\ndocument matches, App Services exposes the data in the document in the\n custom_data  field of that user's  user object . Click  Deploy  in the left navigation menu. Find the draft in the deployment history table and then click\n Review & Deploy Changes . Review the diff of changes and then click  Deploy . To define custom user data with appservices, you need a local copy of your\napplication's configuration files. To pull a local copy of the latest version of your app, run the following: You can also download a copy of your application's configuration files from\nthe  Deploy > Export App  screen in the App Services UI. You must store the custom data for your application's users in a single\ncollection of a  linked Atlas cluster . Every\ndocument in the collection should include a specific field that contains the\nuser ID of the App Services user that it describes. To configure your application to read user data from this collection, define a\n custom user data configuration document  in\nthe  /auth/custom_user_data.json : Once you've configured custom user data, you can push the updated config to\nyour remote app. App Services CLI immediately deploys the update on push. Call the admin user authentication endpoint with your MongoDB Atlas API\nkey pair: If authentication succeeds, the response body contains a JSON object\nwith an  access_token  value: The  access_token  grants access to the App Services Admin API. You\nmust include it as a Bearer token in the  Authorization  header for\nall Admin API requests. API Authentication Documentation Send a request to the  Configure Custom\nUser Data \nendpoint. Make sure to include your Admin API  access_token , the\n groupId  of the Atlas project containing your App, and\nthe App's internal  appId  hex string: If you successfully configure custom user data, App Services\nreturns a  204  response. For code examples that demonstrate how to access and update custom user\ndata from the client application, see the documentation for the Realm\nSDKs: C++ SDK Flutter SDK Java SDK Kotlin SDK .NET SDK Node SDK React Native SDK Swift SDK Web SDK Atlas App Services can read user metadata from authentication providers. Then,\nApp Services exposes each user's data in a field of their  user\nobject . For example, you might want to access a user's name,\nemail, birthday, or gender. You can configure App Services to request metadata with the access token\nwhen users log in. You can access that data from the logged-in user's\nobject with a client SDK. You can define the metadata to request when you configure authentication\nproviders. Specify optional metadata fields that you want to access through\nthe user's account. These metadata fields vary depending on provider. Provider Metadata fields Facebook name first_name last_name picture gender birthday min_age max_age email Google name first_name last_name picture email Custom JWT Any field in the JWT as specified by the Custom JWT provider's\n metadata fields \nconfiguration. You can configure and enable user metadata in the App Services UI. To\nget to the configuration page: Click  Authentication  in the left navigation menu. Select the  Authentication Providers  tab. Press the  EDIT  button for the provider whose metadata\nyou want to configure. Google or Facebook Select the checkboxes next to the metadata fields you want to enable. Custom JWT Authentication You can specify the metadata fields that your identity provider supports.\nAfter you press the  Add Field  button, define: For more details, see:  JWT metadata fields . After you configure the metadata you want to access, press\nthe  Save Draft  button. The path The field name Whether the field is optional or required After you update the metadata configuration, you must deploy your\napplication. To deploy a draft application from the App Services UI: Once the application successfully deploys, App Services begins to associate\nmetadata with users. When a user logs in, App Services requests user\npermission to access the requested metadata. If the user approves,\nApp Services exposes the data in that user's  user object . Click  Deploy  in the left navigation menu. Find the draft in the deployment history table and then click\n Review & Deploy Changes . Review the diff of changes and then click  Deploy . To update your app with appservices, you need a local copy of its\nconfiguration files. To pull a local copy of the latest version of your app, run the following: You can also download a copy of your application's configuration files from\nthe App Services UI. Go to the  Deploy > Export App  screen from\nthe App Dashboard. You can find authentication provider  metadata_fields  for your app in  /auth/providers.json .\nUpdate this array to request user metadata from the authentication provider. Google or Facebook This array resembles: Custom JWT authentication The metadata_fields array has an additional property,  field_name .\nIn custom JWT authentication,  name  represents the path to the field.\nThe  field_name  represents the name of the field. Once you've configured custom user data, you can push the updated config to\nyour remote app. App Services CLI immediately deploys the update on push. Call the admin user authentication endpoint with your MongoDB Atlas API\nkey pair: If authentication succeeds, the response body contains a JSON object\nwith an  access_token  value: The  access_token  grants access to the App Services Admin API. You\nmust include it as a Bearer token in the  Authorization  header for\nall Admin API requests. API Authentication Documentation Send a request to the  Update an\nauthentication provider \nendpoint. In the request body, define  metadata_fields \nfor the provider. Make sure to include your Admin API  access_token , the\n groupId  of the Atlas project containing your App, the\nApp's internal  appId  hex string, and the  _id  value\nof the authentication provider: If you successfully configure the provider's metadata\nfields, App Services returns a  204  response. For code examples that demonstrate how to access user metadata\ndata from the client application, see the documentation for the Realm\nSDKs: Flutter SDK .NET SDK Kotlin SDK Node SDK Swift SDK",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": \"63ed2e4fb7f367c92578e526\",\n  \"user_id\": \"63ed2dbe5960df2af7fd216e\",\n  \"preferences\": {\n    \"preferDarkMode\": true\n  },\n  \"dateOfBirth\": \"1989-03-11T00:00:00.000Z\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"database\": \"<Database Name>\",\n  \"collection\": \"<Collection Name>\",\n  \"roles\": [\n    {\n      \"name\": \"ThisUser\",\n      \"apply_when\": { \"user_id\": \"%%user.id%%\" },\n      \"insert\": false,\n      \"read\": true,\n      \"write\": true,\n      \"search\": false,\n      \"delete\": false\n    }\n  ],\n  \"filters\": []\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function onUserCreation(user) {\n  const customUserDataCollection = context.services\n    .get(\"mongodb-atlas\")\n    .db(\"myapp\")\n    .collection(\"users\");\n  try {\n    await customUserDataCollection.insertOne({\n      // Save the user's account ID to your configured user_id_field\n      user_account_id: user.id,\n      // Store any other user data you want\n      favorite_color: \"blue\",\n    });\n  } catch (e) {\n    console.error(`Failed to create custom user data document for user:${user.id}`);\n    throw e\n  }\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"enabled\": <Boolean>,\n  \"mongo_service_name\": \"<MongoDB Data Source Name>\",\n  \"database_name\": \"<Database Name>\",\n  \"collection_name\": \"<Collection Name>\",\n  \"user_id_field\": \"<User ID Field Name>\",\n  \"on_user_creation_function_name\": \"<Function Name>\"\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=\"<Your App ID>\""
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/auth/providers/mongodb-cloud/login \\\n  -H 'Content-Type: application/json' \\\n  -H 'Accept: application/json' \\\n  -d '{\n    \"username\": \"<Public API Key>\",\n    \"apiKey\": \"<Private API Key>\"\n  }'"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"access_token\": \"<access_token>\",\n  \"refresh_token\": \"<refresh_token>\",\n  \"user_id\": \"<user_id>\",\n  \"device_id\": \"<device_id>\"\n}"
                },
                {
                    "lang": "shell",
                    "value": "curl --request PATCH 'https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/custom_user_data' \\\n  --header 'Authorization:  Bearer <access_token>' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"mongo_service_id\": \"<MongoDB Data Source ID>\",\n    \"database_name\": \"<Database Name>\",\n    \"collection_name\": \"<Collection Name>\",\n    \"user_id_field\": \"<User ID Field Name>\",\n    \"on_user_creation_function_id\": \"<User Creation Function ID>\",\n    \"enabled\": true\n  }'"
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "javascript",
                    "value": "{ ...other config details...\n  \"metadata_fields\": [\n    {\n      \"required\": false,\n      \"name\": \"name\"\n    },\n    {\n      \"required\": false,\n      \"name\": \"gender\"\n    }\n  ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{ ...other config details...\n  \"metadata_fields\": [\n    {\n      \"required\": true,\n      \"name\": \"user.name\",\n      \"field_name\": \"name\"\n    },\n    {\n      \"required\": false,\n      \"name\": \"user.favoriteColor\",\n      \"field_name\": \"favoriteColor\"\n    }\n  ]\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=\"<Your App ID>\""
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/auth/providers/mongodb-cloud/login \\\n  -H 'Content-Type: application/json' \\\n  -H 'Accept: application/json' \\\n  -d '{\n    \"username\": \"<Public API Key>\",\n    \"apiKey\": \"<Private API Key>\"\n  }'"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"access_token\": \"<access_token>\",\n  \"refresh_token\": \"<refresh_token>\",\n  \"user_id\": \"<user_id>\",\n  \"device_id\": \"<device_id>\"\n}"
                },
                {
                    "lang": "shell",
                    "value": "curl --request PATCH 'https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/auth_providers/{providerId}' \\\n  --header 'Authorization:  Bearer <access_token>' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"_id\": \"<Provider ID>\",\n    \"name\": \"oauth2-facebook\",\n    \"type\": \"oauth2-facebook\",\n    \"redirect_uris\": [\"https://example.com/\"],\n    \"config\": {\n       \"clientId\": \"<Facebook Client ID>\"\n    },\n    \"secret_config\": {\n       \"clientSecret\": \"<Facebook Client Secret Name>\"\n    },\n    \"metadata_fields\": [\n      { \"required\": false, \"name\": \"name\" },\n      { \"required\": true, \"name\": \"first_name\" },\n      { \"required\": true, \"name\": \"last_name\" },\n      { \"required\": false, \"name\": \"picture\" },\n      { \"required\": false, \"name\": \"gender\" },\n      { \"required\": false, \"name\": \"birthday\" },\n      { \"required\": false, \"name\": \"min_age\" },\n      { \"required\": false, \"name\": \"max_age\" },\n      { \"required\": false, \"name\": \"email\" }\n    ],\n    \"disabled\": false\n  }'"
                },
                {
                    "lang": "shell",
                    "value": "curl --request PATCH 'https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/auth_providers/{providerId}' \\\n  --header 'Authorization:  Bearer <access_token>' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"_id\": \"<Provider ID>\",\n    \"name\": \"oauth2-google\",\n    \"type\": \"oauth2-google\",\n    \"redirect_uris\": [\"https://example.com/\"],\n    \"config\": {\n       \"clientId\": \"<Google Client ID>\"\n    },\n    \"secret_config\": {\n       \"clientSecret\": \"<Google Client Secret Name>\"\n    },\n    \"metadata_fields\": [\n      { \"required\": false, \"name\": \"name\" },\n      { \"required\": true, \"name\": \"first_name\" },\n      { \"required\": true, \"name\": \"last_name\" },\n      { \"required\": false, \"name\": \"picture\" },\n      { \"required\": false, \"name\": \"gender\" },\n      { \"required\": false, \"name\": \"birthday\" },\n      { \"required\": false, \"name\": \"min_age\" },\n      { \"required\": false, \"name\": \"max_age\" },\n      { \"required\": false, \"name\": \"email\" }\n    ],\n    \"disabled\": false\n  }'"
                },
                {
                    "lang": "shell",
                    "value": "curl --request PATCH 'https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/auth_providers/{providerId}' \\\n  --header 'Authorization:  Bearer <access_token>' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"_id\": \"<Provider ID>\",\n         \"name\": \"custom-token\",\n         \"type\": \"custom-token\",\n         \"metadata_fields\": [\n              {\n                      \"required\": true,\n                      \"name\": \"jwt.field.path\",\n                      \"field_name\": \"metadataFieldName\"\n              }\n         ],\n         \"config\": {\n              \"audience\": [],\n              \"requireAnyAudience\": false,\n              \"signingAlgorithm\": \"HS256\",\n              \"useJWKURI\": false\n         },\n         \"secret_config\": {\n              \"signingKeys\": [\n                      \"<JWT Signing Key>\"\n              ]\n         },\n         \"disabled\": true\n  }'"
                }
            ],
            "preview": "You can associate custom metadata with each user of your App. For\nexample, you might store a user's preferred language, date of birth, or\nany other information that you want to associate with the user.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-users-disable",
            "title": "realm-cli users disable",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Disable an application user of your App Deactivates a user on your App. A user that has been disabled will not be\nallowed to log in, even if they provide valid credentials. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -a, --app string false Specify the name or ID of an App to disable its users -u, --user strings false Specify the App's users' ID(s) to disable -h, --help false help for disable Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli users disable [options]"
                }
            ],
            "preview": "Disable an application user of your App",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-logout",
            "title": "realm-cli logout",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Log the CLI out of Atlas App Services Ends the authenticated session and deletes cached auth tokens. To\nre-authenticate, you must call Login with your Atlas programmatic API key. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -h, --help false help for logout Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli logout [options]"
                }
            ],
            "preview": "Log the CLI out of Atlas App Services",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-schema-datamodels",
            "title": "realm-cli schema datamodels",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Generate data models based on your Schema (alias: datamodel) Translates your Schema's objects into Atlas App Services data models. The data models define\nyour data as native objects, which can be easily integrated into your own repo\nto use with Atlas Device Sync. NOTE: You must have a valid JSON Schema before using this command. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Specify the language with a \"--language\" flag Filter which Schema objects you'd like to include in your output with \"--name\" flags Combine your Schema objects into a single output with a \"--flat\" flag Omit import groups from your model with a \"--no-imports\" flag Name Type Required Description -a, --app string false Specify the name or ID of an App to generate its data models -l, --language String false Specify the language to generate schema data models in (Default value: <none>) --flat false View generated data models (and associated imports) as a single code block --no-imports false View generated data models without imports --name strings false Filter generated data models by name(s) -h, --help false help for datamodels Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli schema datamodels [options]"
                }
            ],
            "preview": "Generate data models based on your Schema (alias: datamodel)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-users-list",
            "title": "realm-cli users list",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "List the application users of your App (alias: ls) Displays a list of your App's users' details. The list is grouped by Auth\nProvider type and sorted by Last Authentication Date. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -a, --app string false Specify the name or ID of an App to list its users' -u, --user strings false Filter the App's users by ID(s) --pending false View the App's pending users --state String false Filter the App's users by state (Default value: <none>; Allowed values: enabled, disabled) --provider Set false Filter the App's users by provider type (Default value: <none>; Allowed values: <none>, \"local-userpass\", \"api-key\", \"oauth2-facebook\", \"oauth2-google\", \"anon-user\", \"custom-token\", \"oauth2-apple\", \"custom-function\") (default []) -h, --help false help for list Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli users list [options]"
                }
            ],
            "preview": "List the application users of your App (alias: ls)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-users-create",
            "title": "realm-cli users create",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Create an application user for your App Adds a new user to your App. You can create a user for the following\nenabled authentication providers: \"Email/Password\" or \"API Key\". realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -a, --app string false Specify the name or ID of an App to create its users --type String false Select the type of user to create (Default value: <none>; Allowed values: api-key, email) --name string false Specify the name of the new API Key --email string false Specify the email of the new user --password string false Specify the password of the new user -h, --help false help for create Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli users create [options]"
                }
            ],
            "preview": "Create an application user for your App",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-users-revoke",
            "title": "realm-cli users revoke",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Revoke an application user's sessions from your App Logs a user out of your App. A revoked user can log in again if they\nprovide valid credentials. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -a, --app string false Specify the name or ID of an App to revoke its users' sessions -u, --user strings false Specify the App's users' ID(s) to revoke sessions for --pending false View the App's pending users --state String false Filter the App's users by state (Default value: <none>; Allowed values: enabled, disabled) --provider Set false Filter the App's users by provider type (Default value: <none>; Allowed values: <none>, \"local-userpass\", \"api-key\", \"oauth2-facebook\", \"oauth2-google\", \"anon-user\", \"custom-token\", \"oauth2-apple\", \"custom-function\") (default []) -h, --help false help for revoke Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli users revoke [options]"
                }
            ],
            "preview": "Revoke an application user's sessions from your App",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-pull",
            "title": "realm-cli pull",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Exports the latest version of your App into your local directory (alias: export) Exports the latest version of your App into your local directory Pulls changes from your remote App into your local directory. If\napplicable, Hosting Files and/or Dependencies associated with your App will be\nexported as well. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description --local string false Specify a local filepath to export an App to --remote string false Specify the name or ID of a remote App to export --include-node-modules false Export and include App dependencies from a node_modules archive (Note: The allowed formats are as a directory or compressed into a  .zip ,  .tar ,  .tar.gz , or  .tgz  file) --include-package-json false Export and include App dependencies from a package.json file -s, --include-hosting false Export and include App hosting files -x, --dry-run false Run without writing any changes to the local filepath -t, --template strings false Specify the frontend Template ID(s) to export. (Note: Specified templates must be compatible with the remote App) (Allowed values:  web.graphql.todo ,  web.mql.todo ,  triggers ,  ios.swift.todo ,  android.kotlin.todo ,  react-native.todo ,  xamarin.todo ) -h, --help false help for pull Name Type Required Description --profile string false Specify your profile (Default value: \"default\") This value defaults to \"default\". --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli pull [options]"
                }
            ],
            "preview": "Exports the latest version of your App into your local directory (alias: export)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-push",
            "title": "realm-cli push",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Imports and deploys changes from your local directory to your App (alias: import) Updates a remote App with your local directory. First, input an App\nthat you would like changes pushed to. This input can be either the application\nClient App ID of an existing App you would like to update, or the Name of\na new App you would like to create. Changes pushed are automatically\ndeployed. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description --local string false Specify the local filepath of an App to be imported --remote string false Specify the name or ID of a remote App to edit --include-node-modules false Import and include App dependencies from a node_modules archive (Note: The allowed formats are as a directory or compressed into a .zip, .tar, .tar.gz, or .tgz file) --include-package-json false Import and include App dependencies from a package.json file -s, --include-hosting false Import and include App hosting files -c, --reset-cdn-cache false Reset the hosting CDN cache of an App -x, --dry-run false Run without pushing any changes to the Atlas App Services server -h, --help false help for push Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli push [options]"
                }
            ],
            "preview": "Imports and deploys changes from your local directory to your App (alias: import)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-schema",
            "title": "realm-cli schema",
            "headings": [
                "Options",
                "Inherited Options",
                "Related Commands"
            ],
            "paragraphs": "Manage the Schemas of your App (alias: schemas) realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -h, --help false help for schema Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts realm-cli schema datamodels  - Generate data models based on your Schema (alias: datamodel)",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                }
            ],
            "preview": "Manage the Schemas of your App (alias: schemas)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-apps",
            "title": "realm-cli apps",
            "headings": [
                "Options",
                "Inherited Options",
                "Related Commands"
            ],
            "paragraphs": "Manage the Atlas App Services Apps associated with the current user (alias: app) realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -h, --help false help for apps Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts realm-cli apps create  - Create a new app from your current working directory and deploy it to the App Services server realm-cli apps delete  - Delete an App realm-cli apps describe  - Displays information about your App realm-cli apps diff  - Show differences between your local directory and your App realm-cli apps init  - Initialize an App in your current working directory (alias: initialize) realm-cli apps list  - List the App Services Apps you have access to (alias: ls)",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                }
            ],
            "preview": "Manage the Atlas App Services Apps associated with the current user (alias: app)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-accessList-list",
            "title": "realm-cli accessList list",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "List the allowed entries in the Access List of your App (alias: ls) This will display the IP addresses and/or CIDR blocks in the Access\nList of your App realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -a, --app string false Specify the name or ID of an App to list its allowed IP addresses and/or CIDR blocks -h, --help false help for list Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli accessList list [options]"
                }
            ],
            "preview": "List the allowed entries in the Access List of your App (alias: ls)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-function",
            "title": "realm-cli function",
            "headings": [
                "Options",
                "Inherited Options",
                "Related Commands"
            ],
            "paragraphs": "Interact with the Functions of your app (alias: functions) realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -h, --help false help for function Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts realm-cli function run  - Run a Function from your app",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                }
            ],
            "preview": "Interact with the Functions of your app (alias: functions)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-logs-list",
            "title": "realm-cli logs list",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Lists the Logs in your App (alias: ls) Displays a list of your App's Logs sorted by recentness, with most recent\nLogs appearing towards the bottom. You can specify a \"--tail\" flag to monitor\nyour Logs and follow any newly created Logs in real-time. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -a, --app string false Specify the name or ID of an App to list its logs --type Set false Specify the type(s) of logs to list (Default value: <none>; Allowed values: <none>, \"auth\", \"function\", \"push\", \"service\", \"trigger\", \"graphql\", \"sync\", \"schema\") (default []) --errors false View your App's error logs --start Date false Specify when to begin listing logs. Learn more at: :ref:` View Logs with realm-cli <cli-view-logs-for-date-range>`. --end Date false Specify when to finish listing logs Learn more at: :ref:` View Logs with realm-cli <cli-view-logs-for-date-range>`. --tail false View your App's logs in real-time (Note: \"--start\" and \"--end\" flags do not apply here) -h, --help false help for list Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli logs list [options]"
                }
            ],
            "preview": "Lists the Logs in your App (alias: ls)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-whoami",
            "title": "realm-cli whoami",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Display information about the current user Displays a table that includes your Public and redacted Private Atlas\nprogrammatic API Key (e.g. ........-....-....-....-3ba985aa367a). No session\ndata will be surfaced if you are not logged in. NOTE: To log in and authenticate your session, use \"realm-cli login\" realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -h, --help false help for whoami Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli whoami [options]"
                }
            ],
            "preview": "Display information about the current user",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-accessList-update",
            "title": "realm-cli accessList update",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Modify an IP address or CIDR block in the Access List of your App Changes an existing entry from the Access List of your App. You must\nspecify either the  --new-ip  or  --comment  option. You will\nbe prompted to select an IP address or CIDR block to update if neither is\nspecified. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description --new-ip string true if  --comment  is not specified Specify the new IP address or CIDR block that will replace the existing entry --comment string true if  --new-ip  is not specified Add or edit a comment to the IP address or CIDR block that is being modified -a, --app string false Specify the name or ID of an App to modify an entry in its Access List --ip string false Specify the existing IP address or CIDR block that you would like to modify -h, --help false help for update Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli accessList update [options]"
                }
            ],
            "preview": "Modify an IP address or CIDR block in the Access List of your App",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-apps-describe",
            "title": "realm-cli apps describe",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Displays information about your App View all of the aspects of your App to see what is configured and enabled\n(e.g. services, functions, etc.). If you have more than one App, you will\nbe prompted to select an App to view. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -a, --app string false Specify the name or ID of an App to describe -h, --help false help for describe Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli apps describe [options]"
                }
            ],
            "preview": "Displays information about your App",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-secrets-create",
            "title": "realm-cli secrets create",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Create a Secret for your App You will be prompted to name your Secret and define the value of your Secret. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -a, --app string false Specify the name or ID of an App to create its secrets -n, --name string false Name the secret -v, --value string false Specify the secret value -h, --help false help for create Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli secrets create [options]"
                }
            ],
            "preview": "Create a Secret for your App",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-function-run",
            "title": "realm-cli function run",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Run a Function from your App Atlas Functions allow you to define and execute server-side logic for your Atlas App Services\napp. Once you select and run a Function for your App, the following will\nbe displayed: realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: A list of logs, if present The function result as a document A list of error logs, if present Name Type Required Description -a, --app string false Specify the name or ID of an App to run its function --name string false Specify the name of the function to run --args stringArray false Specify the args to pass to your function. Learn more at:  Call a Function from the CLI . --user string false Specify which user to run the function as (Default value: <none>; Allowed values: <none>, <userID>) (Note: Using <none> will run as the System user) -h, --help false help for run Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli function run [options]"
                }
            ],
            "preview": "Run a Function from your App",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-secrets-update",
            "title": "realm-cli secrets update",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Update a Secret in your App NOTE: The Name of the Secret cannot be modified. In order to do so, you will\nneed to delete and re-create the Secret. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -a, --app string false Specify the name or ID of an App to update its secrets -s, --secret string false Specify the name or ID of the secret to update -n, --name string false Re-name the secret -v, --value string false Specify the new secret value -h, --help false help for update Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli secrets update [options]"
                }
            ],
            "preview": "Update a Secret in your App",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-accessList-create",
            "title": "realm-cli accessList create",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Create an IP address or CIDR block in the Access List for your App You will be prompted to input an IP address or CIDR block if none is provided in\nthe initial command. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -a, --app string false Specify the name or ID of an App to create an entry in its Access List --ip string false Specify the IP address or CIDR block that you would like to add --comment string false Add a comment to the IP address or CIDR block (Note: This action is optional) --use-current false Add your current IP address to your Access List --allow-all false Allows all IP addresses to access your App (Note: \u201c0.0.0.0/0\u201d will be added as an entry) -h, --help false help for create Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli accessList create [options]"
                }
            ],
            "preview": "Create an IP address or CIDR block in the Access List for your App",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-users",
            "title": "realm-cli users",
            "headings": [
                "Options",
                "Inherited Options",
                "Related Commands"
            ],
            "paragraphs": "Manage the users of your App (alias: user) realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -h, --help false help for users Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts realm-cli users create  - Create an application user for your App realm-cli users delete  - Delete an application user from your App realm-cli users disable  - Disable an application user of your App realm-cli users enable  - Enable an application user of your App realm-cli users list  - List the application users of your App (alias: ls) realm-cli users revoke  - Revoke an application user's sessions from your App",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                }
            ],
            "preview": "Manage the users of your App (alias: user)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-logs",
            "title": "realm-cli logs",
            "headings": [
                "Options",
                "Inherited Options",
                "Related Commands"
            ],
            "paragraphs": "Interact with the Logs of your App (alias: log) realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -h, --help false help for logs Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts realm-cli logs list  - Lists the Logs in your App (alias: ls)",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                }
            ],
            "preview": "Interact with the Logs of your App (alias: log)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-apps-create",
            "title": "realm-cli apps create",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Create a new app from your current working directory and deploy it to the Atlas App Services server Creates a new App by saving your configuration files in a local directory\nand deploying the new app to the App Services server. This command will create a new\ndirectory for your project. You can specify a \"--remote\" flag to create an App from an existing app;\nif you do not specify a \"--remote\" flag, the CLI will create a default App. NOTE: To create an App without deploying it, use \"app init\". realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description --remote string false Specify the name or ID of a remote App to clone --local string false Specify the local filepath of an App to be created -n, --name string false Name your new App -l, --location String false Select the App's location (Default value: <none>; Allowed values: US-VA, US-OR, DE-FF, IE, AU, IN-MB, SG) -d, --deployment-model String false Select the App's deployment model (Default value: <none>; Allowed values: GLOBAL, LOCAL) -e, --environment String false Select the App's environment (Default value: <none>; Allowed values: development, testing, qa, production) --cluster strings false Link Atlas clusters to your App --cluster-service-name strings false Specify the App Service name to reference your Atlas cluster --datalake strings false Link Federated database instances to your App --datalake-service-name strings false Specify the App Service name to reference your Federated database instance --template string false Create your App from an available template -x, --dry-run false Run without writing any changes to the local filepath or pushing any changes to the App Services server -h, --help false help for create Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli apps create [options]"
                }
            ],
            "preview": "Create a new app from your current working directory and deploy it to the Atlas App Services server",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-apps-init",
            "title": "realm-cli apps init",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Initialize an App in your current working directory (alias: initialize) Initializes a new App by saving your configuration files in your current\nworking directory. You can specify a \"--remote\" flag to initialize an App from an existing app;\nif you do not specify a \"--remote\" flag, the CLI will initialize a default\nApp. NOTE: To create a new App and have it deployed, use \"app create\". realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description --remote string false Specify the name or ID of a remote App to clone -n, --name string false Name your new App -l, --location String false Select the App's location (Default value: <none>; Allowed values: US-VA, US-OR, DE-FF, IE, AU, IN-MB, SG) -d, --deployment-model String false Select the App's deployment model (Default value: <none>; Allowed values: GLOBAL, LOCAL) -e, --environment String false Select the App's environment (Default value: <none>; Allowed values: development, testing, qa, production) -h, --help false help for init Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli apps init [options]"
                }
            ],
            "preview": "Initialize an App in your current working directory (alias: initialize)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-users-enable",
            "title": "realm-cli users enable",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Enable an application user of your App Activates a user on your App. A user that has been enabled will have no\nrestrictions with logging in. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -a, --app string false Specify the name or ID of an App to enable its users -u, --user strings false Specify the App's users' ID(s) to enable -h, --help false help for enable Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli users enable [options]"
                }
            ],
            "preview": "Enable an application user of your App",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-login",
            "title": "realm-cli login",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Log the CLI into Atlas App Services using a MongoDB Cloud API key Begins an authenticated session with App Services. To get a MongoDB Cloud API Key, open\nyour App in the App Services UI. Navigate to \"Deployment\" in the left navigation\nmenu, and select the \"Export App\" tab. From there, create a programmatic API key\nto authenticate your realm-cli session. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description --api-key string false Specify the public portion of your Atlas programmatic API Key --private-api-key string false Specify the private portion of your Atlas programmatic API Key -h, --help false help for login Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli login [options]"
                }
            ],
            "preview": "Log the CLI into Atlas App Services using a MongoDB Cloud API key",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-secrets",
            "title": "realm-cli secrets",
            "headings": [
                "Options",
                "Inherited Options",
                "Related Commands"
            ],
            "paragraphs": "Manage the Secrets of your App (alias: secret) realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -h, --help false help for secrets Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts realm-cli secrets create  - Create a Secret for your App realm-cli secrets delete  - Delete a Secret from your App realm-cli secrets list  - List the Secrets in your App (alias: ls) realm-cli secrets update  - Update a Secret in your App",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                }
            ],
            "preview": "Manage the Secrets of your App (alias: secret)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-users-delete",
            "title": "realm-cli users delete",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Delete an application user from your App You can remove multiple users at once with the \"--user\" flag. You can only\nspecify these users using their ID values. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -a, --app string false Specify the name or ID of an App to delete its users -u, --user strings false Specify the App's users' ID(s) to delete --pending false View the App's pending users --state String false Filter the App's users by state (Default value: <none>; Allowed values: enabled, disabled) --provider Set false Filter the App's users by provider type (Default value: <none>; Allowed values: <none>, \"local-userpass\", \"api-key\", \"oauth2-facebook\", \"oauth2-google\", \"anon-user\", \"custom-token\", \"oauth2-apple\", \"custom-function\") (default []) -h, --help false help for delete Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli users delete [options]"
                }
            ],
            "preview": "Delete an application user from your App",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-apps-diff",
            "title": "realm-cli apps diff",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Show differences between your local directory and your App Displays file-by-file differences between your local directory and the latest\nversion of your App. If you have more than one App, you will be\nprompted to select an App to view. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description --local string false Specify the local filepath of an App to diff --remote string false Specify the name or ID of an App to diff -d, --include-dependencies false Include App dependencies in the diff -s, --include-hosting false Include App hosting files in the diff -h, --help false help for diff Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli apps diff [options]"
                }
            ],
            "preview": "Show differences between your local directory and your App",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-secrets-list",
            "title": "realm-cli secrets list",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "List the Secrets in your App (alias: ls) This will display the IDs and Names of the Secrets in your App. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -a, --app string false Specify the name or ID of an App to list its secrets -h, --help false help for list Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli secrets list [options]"
                }
            ],
            "preview": "List the Secrets in your App (alias: ls)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-accessList-delete",
            "title": "realm-cli accessList delete",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Delete an IP address or CIDR block from the Access List of your App Removes an existing entry from the Access List of your App. You will be\nprompted to select an IP address or CIDR block if none are provided in the\ninitial command. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -a, --app string false Specify the name or ID of an App to remove entries from its Access List --ip strings false Specify the IP address(es) or CIDR block(s) to delete -h, --help false help for delete Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli accessList delete [options]"
                }
            ],
            "preview": "Delete an IP address or CIDR block from the Access List of your App",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-secrets-delete",
            "title": "realm-cli secrets delete",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Delete a Secret from your App realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Remove multiple Secrets at once with \"--secret\" flags. You can specify these\nSecrets using their ID or Name values Name Type Required Description -a, --app string false Specify the name or ID of an App to delete its secrets -s, --secret strings false Specify the name or ID of the secret to delete -h, --help false help for delete Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli secrets delete [options]"
                }
            ],
            "preview": "Delete a Secret from your App",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-apps-delete",
            "title": "realm-cli apps delete",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Delete an App If you have more than one App, you will be prompted to select one or\nmultiple apps that you would like to delete from a list of all your Atlas App Services Apps.\nThe list includes App Services Apps from all projects associated with your user profile. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -a, --app strings false Specify the name(s) or ID(s) of App Services Apps to delete -h, --help false help for delete Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli apps delete [options]"
                }
            ],
            "preview": "Delete an App",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-accessList",
            "title": "realm-cli accessList",
            "headings": [
                "Options",
                "Inherited Options",
                "Related Commands"
            ],
            "paragraphs": "Manage the allowed IP addresses and CIDR blocks of your App (aliases: accesslist, access-list) realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -h, --help false help for accessList Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts realm-cli accessList create  - Create an IP address or CIDR block in the Access List for your App realm-cli accessList delete  - Delete an IP address or CIDR block from the Access List of your App realm-cli accessList list  - List the allowed entries in the Access List of your App (alias: ls) realm-cli accessList update  - Modify an IP address or CIDR block in the Access List of your App",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                }
            ],
            "preview": "Manage the allowed IP addresses and CIDR blocks of your App (aliases: accesslist, access-list)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "realm-cli/v2/realm-cli-apps-list",
            "title": "realm-cli apps list",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "List the Atlas App Services Apps you have access to (alias: ls) Lists and filters your App Services Apps. realm-cli  is deprecated and will not receive future features or\nbug fixes. Instead, use the  App Services CLI . App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Name Type Required Description -a, --app string false Filter the list of App Services Apps by name -h, --help false help for list Name Type Required Description --profile string false Specify your profile (Default value: \"default\") (default \"default\") --telemetry String false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format String false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": null,
                    "value": "realm-cli apps list [options]"
                }
            ],
            "preview": "List the Atlas App Services Apps you have access to (alias: ls)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "rules/filters",
            "title": "Filter Incoming Queries",
            "headings": [
                "Overview",
                "How App Services Applies Filters",
                "Define Filters",
                "Procedure",
                "Navigate to the Rules Window",
                "Create a New Filter",
                "Specify the Apply When Expression",
                "Specify the Filter Query Predicates",
                "Specify the Filter Projection",
                "Save the Filter",
                "Pull the Latest Version of Your App",
                "Add a Rule Configuration File",
                "Add One or More Filters",
                "Deploy the Updated Rules"
            ],
            "paragraphs": "A  filter  modifies an incoming MongoDB query to return only a subset of the\nresults matched by the query. Adding a filter to a collection allows you to control\nthe shape of queried documents and can improve query performance. Filters add additional query parameters and omit fields from query results\n before  Atlas App Services runs the query. Every filter has three components: An \"apply when\"  expression  that determines if the filter\napplies to an incoming request. You can use variables like\n %%user  and  %%request  in the \"apply when\"\nexpression. However, you cannot use expansions that refer to a document like\n %%root  because App Services evaluates the \"apply when\"\nexpression before reading any documents. An optional query  expression , which merges with the\nexisting query of any request the filter applies to. An optional projection document, which uses standard MongoDB projection syntax\nand merges with the existing projection of any request the filter applies to. App Services evaluates and applies filters for all MongoDB requests where rules\napply except Device Sync requests. Examples of filterable MongoDB requests include: A filter applies to a given request if its \"apply when\"  expression  evaluates to true given that request's context. If a filter\napplies to a request, App Services merges the filter's query or projection into\nthe requested operation's existing query and projection. Multiple filters may apply to a single request. App Services applies filters to the request  before  it sends the request to MongoDB. A query on a collection. A write to a document. A collection contains several million documents and has one role with\nthe following \"apply when\" expression: If no filter is applied, App Services will evaluate a role for each\ndocument that the query matches. We know that App Services will withhold\nany document that does not have the user's id as the value of the\n owner_id  field, so we save time and compute resources by applying\nan additional query predicate that excludes those documents before\nApp Services evaluates any roles: Apply When Query Projection { \"%%true\": true } { \"owner_id\": \"%%user.id\" } {} You can use filters to optimize queries, minimize compute overhead, and secure\nsensitive data. Filters are most useful for cross-cutting concerns that affect\nsome or all of your queries. Consider using filters if you want a centralized system to: Restrict queries to a subset of all documents Omit sensitive data or unused fields In a voting app where some users have agreed to anonymously share their vote,\nyou could use the following filter to constrain all queries to an anonymous\nsubset of the existing data: You can define filters for specific collections in your linked cluster\nfrom the App Services UI or by deploying configuration files with Realm CLI: This guide requires a linked  MongoDB Atlas data source . Under  Data Access  in the left navigation menu,\nclick  Rules . Select the collection that you want to configure a filter for from\nthe  Rules  menu then click the  Filters  tab.\nClick  Add a Filter  and enter a  Filter Name  for the new\nfilter. In the  Apply When  input box, enter a  rule expression  that defines when the filter applies to a query. If the\nexpression evaluates to  true  for an incoming query, App Services adds\nthe  Filter Query  parameters to the incoming query. Atlas App Services evaluates and applies filters before it reads any\ndocuments, so you cannot use  MongoDB document expansions  in a filter's Apply When expression.\nHowever, you can use other expansions like  %%user ,\n %%values , and  %function . In the  Query  input box, specify a document that contains\nadditional query predicates to merge into the incoming query when the\nfilter applies. For example, a filter that withholds documents that\nhave a  score  below  20  could use the following filter query: In the  Projection  input box, specify a document that\ncontains a  projection  document to merge into\nthe incoming query when the filter applies. For example, a filter that withholds the  career_stats  and\n personal  fields from documents could use the following filter\nprojection: After you have configured the filter, click  Save Draft . After\nsaving, App Services immediately begins evaluating and applying filters to\nincoming queries on the collection. To define filters for a collection with appservices, you need a local\ncopy of your application's configuration files. To pull a local copy of the latest version of your app, run the following: You can also download a copy of your application's configuration files from\nthe  Deploy > Export App  screen in the App Services UI. To define or modify roles for a collection, open the  rules.json \nconfiguration file within the collection's configuration directory. The configuration file should have the following general form: If you haven't already defined rules or a schema for the collection, you\nneed to manually create its configuration directory and  schema.json : This guide focuses on creating  filters  for the collection. Check out\nthe other configuration guides to learn how to  define roles and\npermissions  and  enforce\nschemas . Federated data sources  do not support rules or schemas . You can only access a Federated data source\nfrom a system function. Add a document to the  filters  array for each filter that you want\nto configure. Filter documents have the following form: Field Description Required. The name of the filter. Filter names are\nuseful for identifying and distinguishing between filters.\nLimited to 100 characters or fewer. An  expression  that determines when this filter\napplies to an incoming MongoDB operation. Atlas App Services evaluates and applies filters before it reads any\ndocuments, so you cannot use  MongoDB document expansions  in a filter's Apply When expression.\nHowever, you can use other expansions like  %%user ,\n %%values , and  %function . A  MongoDB query  that App Services merges\ninto a filtered operation's existing query. A filter withholds documents that have a  score  below  20  using\nthe following query: A  MongoDB projection \nthat App Services merges into a filtered operation's existing projection. MongoDB projections can be either inclusive or exclusive, i.e.\nthey can either return only specified fields or withhold\nfields that are not specified. If multiple filters apply to a\nquery, the filters must all specify the same type of\nprojection, or the query will fail. A filter withholds the  _internal  field from all documents using\nthe following projection: Once you've defined and saved  rules.json  you can push the updated config\nto your remote app. App Services CLI immediately deploys the filters on push. While  Role-based Permissions  and Filters can hide specific\ndocuments and fields within a collection there is a potential that\ndata can be exposed if the system allows arbitrary\nqueries to access the collection. For example, queries or functions that\nraise errors depending on the values stored in a collection (such\nas division-by-zero errors) may reveal information about documents, even if\na role or filter prevents the querying user from viewing\ndocuments directly. Users may also make inferences about the underlying data\nin other ways (such as by measuring query execution time, which can be affected\nby the data's distribution). Be aware that this is possible and audit your\ndata access patterns where neccessary.",
            "code": [
                {
                    "lang": "json",
                    "value": "{ \"owner_id\": \"%%user.id\" }"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"AnonymizeVotes\",\n  \"apply_when\": true,\n  \"query\": {\n    \"shareVoteAnonymous\": true\n  },\n  \"project\": {\n    \"_id\": 0,\n    \"age\": 1,\n    \"vote\": 1\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"_id\": ObjectId(...), \"name\": \"sarah\", age: 42, \"vote\": \"yes\", \"shareVoteAnonymous\": true }\n{ \"_id\": ObjectId(...), \"name\": \"andy\", age: 22, \"vote\": \"no\", \"shareVoteAnonymous\": true }\n{ \"_id\": ObjectId(...), \"name\": \"jennifer\", age: 37, \"vote\": \"yes\", \"shareVoteAnonymous\": false }\n{ \"_id\": ObjectId(...), \"name\": \"rick\", age: 43, \"vote\": \"no\", \"shareVoteAnonymous\": true }\n{ \"_id\": ObjectId(...), \"name\": \"tom\", age: 64, \"vote\": \"yes\", \"shareVoteAnonymous\": false }\n{ \"_id\": ObjectId(...), \"name\": \"bob\", age: 67, \"vote\": \"yes\", \"shareVoteAnonymous\": true }"
                },
                {
                    "lang": "json",
                    "value": "{ age: 42, \"vote\": \"yes\" }\n{ age: 22, \"vote\": \"no\" }\n{ age: 37, \"vote\": \"yes\" }\n{ age: 43, \"vote\": \"no\" }\n{ age: 64, \"vote\": \"yes\" }\n{ age: 67, \"vote\": \"yes\" }"
                },
                {
                    "lang": "shell",
                    "value": "{ \"score\": { \"$gt\": 20 } }"
                },
                {
                    "lang": "shell",
                    "value": "{\n  \"career_stats\": 0,\n  \"personal\": 0\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "json",
                    "value": "{\n  \"database\": \"<Database Name>\",\n  \"collection\": \"<Collection Name>\",\n  \"roles\": [],\n  \"filters\": []\n}"
                },
                {
                    "lang": "bash",
                    "value": "# Create the collection's configuration directory\nmkdir -p data_sources/<service>/<db>/<collection>\n# Create the collection's schema file\necho '{}' >> data_sources/<service>/<db>/<collection>/rules.json"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"name\": \"<Filter Name>\",\n  \"apply_when\": { Expression },\n  \"query\": { MongoDB Query },\n  \"projection\": { MongoDB Projection }\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"score\": { \"$gte\": 20 } }"
                },
                {
                    "lang": "json",
                    "value": "{ \"_internal\": 0 }"
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=\"<Your App ID>\""
                }
            ],
            "preview": "Learn how to filter a query to return only a subset of matching results.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "rules/roles",
            "title": "Role-based Permissions",
            "headings": [
                "Overview",
                "What are Permissions?",
                "Document-Level Permissions",
                "Field-Level Permissions",
                "Read Permissions Flowchart",
                "Write Permissions Flowchart",
                "Roles",
                "How App Services Assigns Roles",
                "Without Device Sync",
                "With Device Sync",
                "Apply When Expressions",
                "Document Filters",
                "Role Order",
                "Sync Compatibility",
                "Define Roles & Permissions"
            ],
            "paragraphs": "You secure your App's data by defining  roles  that are automatically assigned\nto incoming user requests and Device Sync sessions. Each role has fine-grained\ndata access permissions and dynamic conditions that determine when the role\napplies. For examples of how you might configure permissions for common scenarios with\nDevice Sync, see the  Device Sync Permissions Guide . A  permission  is a status that Atlas App Services assigns to individual users\nto control what they can and cannot do with your app's data. App Services uses\nboth document-level and field-level permissions: Document-level permissions \ncontrol whether a user can insert, delete, modify,\nand search for a specific document in a MongoDB collection. Field-level permissions \ncontrol whether a user can read or write the data in\nspecific fields of a document. A role's document-level permissions determine which actions that affect\nthe entire document can be performed. These permissions always apply to the\nentire document regardless of the content. A role can have the\nfollowing document-level permissions: Insert : You can insert new documents. Delete : You can delete existing documents. Search : You can search for the document using  Atlas Search . A role's field-level permissions determine whether a user can read or\nwrite fields within the document. These permissions only affect the\nfield they apply to, so a user can have read or write access to only\na subset of the entire document. You can define field-level permissions for specific fields and\ndefault read/write permissions for any additional fields that you\ndon't explicitly define. The following diagram shows how App Services determines whether a user can\nread a given document: The following diagram shows how App Services determines whether a user can\nwrite a given document: A  role  is a named set of permissions that a user may have for documents in a\nMongoDB collection. A role has an \"apply when\"  expression \nthat determines whether App Services should assign the role to a user. Roles\nalso have a set of document- and field-level permissions that a user has when\nassigned the role. App Services only commits operations that a user is authorized to do based on\ntheir assigned role. If a role does not have permission to read a document or\nsome of its fields, App Services omits the document or fields from the results. Consider a collection named  employees  where each employee has their own\ndocument with all of their employment data. This collection might have two\nroles:  Employee  and  Manager . We're not using Device Sync, so App\nServices selects a role on a per-document basis. If a user requests their own document, their role is  Employee . An\nemployee can read and write their own data but can't create or delete their\nown documents. If a user requests a document for someone whose name is listed in the\nuser's  manages  arrays, their role is  Manager . A manager can read\nand write their direct reports' data and can create and delete their\ndocuments. If a user is neither an  Employee  nor a  Manager  for a given\ndocument, then they have no role and cannot read, write, or search that\ndocument. App Services assigns roles at different times depending on whether you're using\n Device Sync (Flexible Mode)  or not. When using Device Sync, App Services assigns roles at the start of a Sync\nsession for each collection to be synced. A Sync session is the period of time\nbetween opening and closing a Sync connection. When not using Device Sync, App Services assigns roles on a per-document,\nper-request basis. Whether using Device Sync or not, you can define a set of roles that are\nspecific to collections and default roles that apply to any other unspecified\ncollection. To assign a role, App Services evaluates each role's \"apply when\"\nexpression in the order that you specified them. The first role whose \"apply\nwhen\" expression evaluates to true becomes the assigned role. If no role\nmatches, access is denied. The set of evaluated roles for a given request or Sync session depends on the\ncollection the user is accessing. If you defined any collection-level roles for\nthe collection, the collection-level roles are evaluated. Otherwise, the data\nsource's default roles, if any, are evaluated. App Services does not \"fall back\" to default roles if no collection-level role\napplies. If any collection-level role is defined, only collection-level roles\nare evaluated. Default roles are evaluated if and only if no collection-level\nroles were defined. When not using Device Sync, App Services dynamically assigns roles for every\ndocument. The user is assigned a separate role, or no role, for each document\nthat matches the incoming query. First, your App evaluates and applies  filters  and then runs the\nquery. For each document returned by the query, your App evaluates possible\nroles in  role order  and assigns the first applicable\nrole, if any. A role applies to a given document if its \"apply when\"\n expression  evaluates to  true  when run against\nthe document. The following request causes App Services to evaluate a role for every document in the\n restaurants  collection where the  city  field is set to  \"Chicago\" : An employee will always be on their own team, so both the  Employee  and\n Teammate  roles apply to them for their own document. However, they can\nuse only one role, so we want to use  Employee  because it's more\nspecific. To configure this, specify  Employee  earlier than  Teammate  in the\ncollection's role definitions: When using Device Sync, App Services assigns roles at the beginning of each\nFlexible Sync session for each synced collection. The role determines which\npermissions apply to each collection for the duration of a session. App Services assigns at most one role per collection. If you did not specify\nroles for a given synced collection, App Services uses the default roles\ninstead. If no role applies for a collection, the user cannot sync (or read or\nwrite) any entries in that collection. A role stays assigned for the duration of the session. If something relevant to\na user's session role changes in the middle of the user's session, the user is\n not  assigned an updated role until they start a new session. For example, if\nthe user's metadata or the role's \"apply when\" expression changes, the user\ncontinues to use the existing role for that collection until the next time they\nstart a session. There are some special considerations when using Device Sync with regard to the\npermissions system. See  Device Sync-Compatible Permissions . For a guide to setting up Flexible Sync with common permissions models, see the\n Device Sync Permissions Guide . A role's \"apply when\" expression is a  rule expression  that\ndetermines whether the role should be assigned. You can use expression variables to make roles dynamic. For example, you can use the\n %%user  expansion to refer to the specific user that issued the\nrequest. This lets you customize your data access permissions on a per-user\nbasis. When not using Device Sync, you can refer to the current document for which a\nrole is being assigned. For example, you can use  %%root . This\nlets you customize your data access permissions on a per-document basis. The role's  document_filters  expressions determine whether the role's\nsubsequent document- and field-level permissions may be evaluated at all. This\nis required for Device Sync. Note: App Services evaluates document filters on a per-document basis. These are\nnot to be confused with the top-level query  filters . The roles for a given collection each have a position that determines\nthe order in which they are evaluated and applied. Each role's apply\nwhen expression is evaluated in role order until a role applies or no\nroles remain. A user may only have one role per document in a given query. Role order\ndetermines which role applies in the event that multiple roles' \"apply when\"\nexpressions are true. Therefore, when defining roles, put the most specific\nroles first. If Device Sync (Flexible Mode) is enabled, an assigned role must be  sync\ncompatible . See  Sync-Compatible Roles  for details. You can configure your app's data access rules from the App Services UI or by\ndeploying configuration files with App Services CLI: Click  Rules  in the left navigation menu and then select a\ncollection from the data source's collection list. You can also select\n Default roles and filters  to configure default rules for\nthe entire cluster. If no roles are defined yet, you'll be prompted to create a new\none. Otherwise you'll see an ordered list of existing roles. Click  Add role  to define a new role. You can use a preset\nrole as a starting point or click  Skip (start from\nscratch) . Give the role a name. The name can be anything you want but\nmust be unique within a given collection. Consider using names\nthat describe the user and/or their relationship to the data. For\nexample,  Admin  or  Owner . Define an \"apply when\" expression that determines when a given user has\nthe role for a given document. Define  document-level permissions \nfor the role. Define  document filters  for the\nrole. This is required for Device Sync\u00a0(Flexible Mode). Use the drop-down to select  field-level permissions  for the role. If you select\n Specify field-level permissions , enter the field names you\nwish to define permissions for. For each field you name and for  Additional Fields , specify\npermissions by selecting either  None ,  Read , or\n Read & Write . Save the role. If a collection has more than one role assigned, you can modify\nthe  role order  by clicking the arrows on\neach role. Pull the latest version of your app. Define  roles and filters  for one or\nmore collections. You can also define  default roles and filters  that apply to any unconfigured collection. See\n Rule Configuration  for details. Deploy your app. While Role-based Permissions and  Filters  can hide specific\ndocuments and fields within a collection there is a potential that\ndata can be exposed if the system allows arbitrary\nqueries to access the collection. For example, queries or functions that\nraise errors depending on the values stored in a collection (such\nas division-by-zero errors) may reveal information about documents, even if\na role or filter prevents the querying user from viewing\ndocuments directly. Users may also make inferences about the underlying data\nin other ways (such as by measuring query execution time, which can be affected\nby the data's distribution). Be aware that this is possible and audit your\ndata access patterns where neccessary.",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"Manager\",\n  \"apply_when\": { \"email\": \"%%user.custom_data.manages\" },\n  \"insert\": true,\n  \"delete\": true,\n  \"read\": true,\n  \"write\": true,\n  \"search\": true,\n  \"fields\": {},\n  \"additional_fields\": {\n    \"read\": true,\n    \"write\": true\n  }\n}\n{\n  \"name\": \"Employee\",\n  \"apply_when\": { \"email\": \"%%user.data.email\" },\n  \"insert\": false,\n  \"delete\": false,\n  \"read\": true,\n  \"write\": true,\n  \"search\": true,\n  \"fields\": {},\n  \"additional_fields\": {\n    \"read\": true,\n    \"write\": true\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": ObjectId(...),\n  \"employeeId\": \"0528\",\n  \"name\": \"Phylis Lapin\",\n  \"team\": \"sales\",\n  \"email\": \"phylis.lapin@dundermifflin.com\",\n  \"manages\": []\n}\n{\n  \"_id\": ObjectId(...),\n  \"employeeId\": \"0713\",\n  \"name\": \"Stanley Hudson\",\n  \"team\": \"sales\",\n  \"email\": \"stanley.hudson@dundermifflin.com\",\n  \"manages\": []\n}\n{\n  \"_id\": ObjectId(...),\n  \"employeeId\": \"0865\",\n  \"name\": \"Andy Bernard\",\n  \"team\": \"sales\",\n  \"email\": \"andy.bernard@dundermifflin.com\",\n  \"manages\": [\n    \"phylis.lapin@dundermifflin.com\",\n    \"stanley.hudson@dundermifflin.com\"\n  ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "db.restaurants.updateMany(\n  { \"city\": \"Chicago\" },\n  { \"$set\": { \"city\": \"Chicago, IL\" } }\n);"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"database\": \"<Database Name>\",\n  \"collection\": \"<Collection Name>\",\n  \"roles\": [\n    { \"name\": \"Manager\", ... },\n    { \"name\": \"Employee\", ... },\n    { \"name\": \"Teammate\", ... }\n  ]\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "json",
                    "value": "{\n  \"database\": \"<Database Name>\",\n  \"collection\": \"<Collection Name>\",\n  \"roles\": [\n    {\n      \"name\": \"<Role Name>\",\n      \"apply_when\": {},\n      \"document_filters\": {\n        \"read\": { <Expression> },\n        \"write\": { <Expression> }\n      },\n      \"insert\": true,\n      \"delete\": true,\n      \"search\": true,\n      \"fields\": {\n        \"myField\": { \"read\": true, \"write\": true }\n      },\n      \"additional_fields\": { \"read\": true, \"write\": true }\n    }\n  ],\n  \"filters\": [\n    {\n      \"name\": \"<Filter Name>\",\n      \"apply_when\": {},\n      \"query\": {},\n      \"projection\": {}\n    }\n  ]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"roles\": [\n    {\n      \"name\": \"<Role Name>\",\n      \"apply_when\": {},\n      \"document_filters\": {\n        \"read\": { <Expression> },\n        \"write\": { <Expression> }\n      },\n      \"insert\": true,\n      \"delete\": true,\n      \"search\": true,\n      \"fields\": {\n        \"myField\": { \"read\": true, \"write\": true }\n      },\n      \"additional_fields\": { \"read\": true, \"write\": true }\n   }\n  ],\n  \"filters\": [\n    {\n      \"name\": \"<Filter Name>\",\n      \"apply_when\": {},\n      \"query\": {},\n      \"projection\": {}\n    }\n  ]\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices push"
                }
            ],
            "preview": "Learn how to define data access permissions using Role-based Permissions in App Services.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "rules/sync-compatibility",
            "title": "Device Sync-Compatible Permissions",
            "headings": [
                "Sync-Compatible Roles",
                "Sync-Compatible Expressions",
                "Sync-Compatible Expansions",
                "Permission Changes",
                "Unified Rules System"
            ],
            "paragraphs": "When using  Device Sync (Flexible Mode) , there are special\nconsiderations when using the permissions system. For a guide to setting up Flexible Sync with common permissions models, see the\n Device Sync Permissions Guide . Flexible Sync ignores any custom  Collation  that you may have configured on\na MongoDB collection in Atlas. Instead, Synced collections always use\n {locale: \"simple\"}  when evaluating Sync subscriptions or permissions. If Device Sync (Flexible Mode) is enabled, an assigned  role  must\nbe  Sync compatible . If the role is not Sync compatible, but its \"apply when\"\nevaluated to true, other roles are not considered; access is denied. A role is not Sync compatible if any of the following conditions are true: document_filters.read  or  document_filters.write  are undefined. A document filter, insert, or delete expression: References a field that is not a  queryable field Uses an expansion other than  %%true ,  %%false ,  %%values ,  %%environment , or  %%user Uses the  %function  operator Top-level  read , top-level  write , or field-level permissions are not boolean literals ( true  or  false ). Field-level permissions are specified for the  _id  field. When Device Sync is enabled, expressions may only refer to the  queryable\nfields  of your data model. If a role refers to any other\nfields, it becomes Sync incompatible and cannot be used with Device Sync. Because a Sync-enabled App assigns roles at the start of a Sync session before\nany specific documents are queried, you can't refer to a document or its field\nvalues in an \"apply when\" expression. When using Device Sync, some  expansions  are not supported.\nThe following table specifies which expansions are Sync compatible in either the\n\"apply when\" or rule expressions: Expansion Can Use in \"Apply When\"? Can Use in Rule Expressions? %%true ,  %%false Yes Yes %%values ,  %%environment Yes Yes with an  important consideration %%request No No %%user Yes Yes with an  important consideration %%this ,  %%prev ,  %%root ,   %%prevRoot No. These expansions refer to the document. App Services evaluates \"apply when\" expressions at session start, so there's no document to evaluate. No. These expansions might access non-queryable fields of the document, which is not possible. %%partition No No %stringToOid ,  %oidToString Yes Yes %function Yes No. App Services expands the expansions at the start of the session, so the function would not operate on a per-document basis. $exists Yes Yes $in ,  $nin Yes Yes. $eq ,  $ne ,  $gt ,  $gte ,  $lt ,  $lte Yes Yes App Services causes a  client reset  if anything about\nthe role changed since the previous session. At the start of a session, App Services expands all expansions in the \"apply\nwhen\",  document_filters.read , and  document_filters.write  expressions\nand stores the result. This has the following implications: If the value changes during a session, App Services continues\nto use the value as it was at the time of session start. On the next session, if the value is different from what it was at\nthe start of this session, App Services causes a client\nreset. You cannot use the  %function  operator in read and\nwrite rules. Functions would not operate on a per-document basis. You cannot store permissions information (such as \"which document\nIDs may this user access?\") in the user object. Changes would not be\nre-evaluated until the next user session, and updates would cause a\nclient reset. If a user's permissions have changed since the last Sync session, Sync triggers\na  client reset  and re-downloads all data with the new\npermissions applied. A user's permissions might change in the following situations: A client reset will  not  be triggered in the following cases: You  updated the Data Source configuration  to\nmodify the rules. Your rules reference  custom user data  to determine\npermissions dynamically, and the value of that custom user data has changed\nsince the last Sync session. Adding a new collection to an App Services Schema and defining permissions for\nthe new namespace or using default roles. This will not trigger a client reset\nbecause permissions have not been applied previously. Configuring custom permissions for the new collection in the same  draft  as the new schema. Conversely, deploying a draft with permission\nchanges after you deploy the schema will lead to a client reset, since default\npermissions were applied in the initial deployment. Prior to February 23, 2023, Device Sync (Flexible Mode) rules existed in the\n permissions  field of the  Sync configuration . These permissions now exist in the same\nconfiguration files as the non-Sync permissions. When importing an app configured for the older permissions system, App Services\nautomatically migrates the permissions to the new unified rules system. You\nshould not have to migrate apps manually. If you have an old app configuration,\nyou can import and then  re-export  the migrated\nconfiguration. For reference, the following changes take place in the migration: Move  permissions.defaultRoles  to the  default_rule.json  file in the\ndata source configuration directory at  data_sources/<data-source-name>/ . Move any collection-specific rules to the  rules.json  file in the\nrespective collection configuration directories at\n data_sources/<data-source-name>/<database-name>/<collection-name>/ . Rename  defaultRoles  to  roles . Rename  applyWhen  to  apply_when . Move sync  read  and  write  into  document_filters.read  and\n document_filters.write . Ensure both  document_filters.read  and\n document_filters.write  are defined. Add the following to the roles, adjusting for your use case. You can only use\n true  or  false . Generally, you probably want  true . The\n document_filters  will restrict read and write access on a per-document\nlevel.",
            "code": [
                {
                    "lang": "json",
                    "value": "\"read\": true,\n\"write\": true,\n\"insert\": true,\n\"delete\": true,\n\"search\": true"
                }
            ],
            "preview": "Device Sync has special considerations when using the permissions system.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/go-to-production",
            "title": "Go to Production with Atlas Device Sync",
            "headings": [],
            "paragraphs": "Production Checklist Production Load Testing Optimize Sync Storage in Atlas Compact an Atlas Volume",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/error-handling",
            "title": "Error Handling",
            "headings": [],
            "paragraphs": "Client Reset Sync Errors",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/details",
            "title": "Technical Details",
            "headings": [],
            "paragraphs": "Conflict Resolution Atlas Device Sync Protocol",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/get-started",
            "title": "Get Started with Atlas Device Sync",
            "headings": [
                "Before You Start",
                "Configure Your Data Model",
                "Choose an SDK",
                "Define Data Model",
                "Define Data Access Patterns",
                "Define Data Access Rules",
                "Enable Sync",
                "Optimize Sync Data Usage",
                "Create Queries in Your Client Application"
            ],
            "paragraphs": "To sync data across devices, you  enable Device Sync  for your\nApp and then use the sync-related methods and properties in the SDKs. If you prefer to learn by example, check out the App Services\n tutorial , which describes\nhow to build a synced to-do list application with clients for common\nplatforms that App Services supports. If you don't already have one,  sign up for a free MongoDB Atlas account . Flexible Sync requires MongoDB version 5.0 or later. You can use a free shared\nM0 cluster to explore and develop your app. We recommend that you use a\ndedicated tier cluster (M10 and above) for production applications. You cannot\nuse sync with a  serverless instance  or\n Federated database instance . If you don't already have one,  create a new App  linked to your Atlas cluster. Realm is an object database optimized for mobile use cases. The\n Realm SDKs , available for multiple languages and\nplatforms, include everything you need to work with synced \"realms\" in your\nclient code. You can use multiple SDKs to work with the same set of synced data\nacross devices and platforms. This guide focuses on configuring sync with an SDK. For in-depth documentation\nthat includes details on how to install and use the Realm SDKs more generally,\ncheck out the SDK docs: C++ SDK Flutter SDK Java SDK Kotlin SDK .NET SDK Node.js SDK React Native SDK Swift SDK The Realm Web SDK does not currently support Realm Database or Atlas Device Sync.\nHowever, you can query the same data in an Atlas cluster using\n MongoDB Data Access . A synced realm uses object models that you define to determine the type of\nobjects in the realm. Additionally, Sync requires a server-side document schema\nfor each synced object type so that App Services can translate and apply changes\nbetween synced realms and MongoDB Atlas. To define your synced object models, do one of the following for each object\ntype: Sync object models from an SDK:  In  Development Mode , App Services automatically generates a document schema\nfor each synced object type and assigns that schema to a collection in the\nlinked cluster with the same name as the object type. Development mode lets\nyou configure your synced data model from a client application using the same\nobject models and code that you use in your production app. This is useful if\nyou prefer a client-first approach that uses idiomatic object models in your\npreferred programming language. For a walkthrough of how to turn on Development Mode, see  Enable\nDevelopment Mode . Generate object models from a document schema:  App Services can automatically\ngenerate language-specific object models that you can use in your client\napplications. The generated models match the document schemas defined for your\napp's synced cluster. You can define these schemas manually or App Services can\nsample existing data in the cluster to create them automatically. This is\nuseful if you prefer a server-first approach and/or plan to build apps with\nmultiple SDKs. For a walkthrough of how to generate your object models based on server-side\ndocument schemas, see  Generate SDK Object Models . Once you have the generated object models, you can copy them directly into\nyour client application and use them the same way you would any regular object\nor struct in your preferred programming language. Once you have decided on your app's data model, you can define a data access\npattern and access rules for your app's data. Client applications can query the queryable fields of a document to determine\nwhich objects to sync. Then, App Services applies rules and default roles to\ndetermine whether users can read or write the objects that match the query. Data access rules determine which data to sync, as well as each user's\nability to read and write data. Rules are closely linked to your app's data\nmodel. With Flexible Sync, you specify which data to sync through queries for matching\nobjects in a client application. App Services then evaluates  roles and\nrules  to determine which of those matching objects a user\ncan read and write. You can define roles on specific collections. Default roles provide read and\nwrite permissions when more specific roles do not apply. Default roles apply to\nall collections an App can access, but you can restrict a role to a specific\ncollection by specifying the collection name. When you enable Sync, you specify how clients can access data in your App.\nFor a walkthrough of how to turn on sync, refer to  Configure and Enable Atlas Device Sync . Device Sync syncs all data from collections with a defined\n schema  in your Atlas cluster.\nIf you do not specify a field in your schema, Device Sync will not sync that\nfield to the clients. Sync stores some utility data in your synced cluster to help synchronize\ndata with clients. Device Sync has optimizations built-in to reduce\nthe amount of utility data stored in synced clusters. If you'd like to\nreduce the amount of utility data further to suit your use case, see\nour guide to  Optimize Device Sync Atlas Usage . With Flexible Sync enabled, you can start creating queries from your client\napplication. The Realm SDKs provide methods to create, update, and remove queries from the\nclient application. The SDKs use  subscriptions  to maintain those queries on\nthe client side. Through these subscriptions, your applications sync objects\nwith the backend app and can watch for and react to changes. To create queries from your client application, refer to the Realm SDK documentation: Flexible Sync - Flutter SDK Flexible Sync - Java SDK Flexible Sync - Kotlin SDK Flexible Sync - .NET SDK Flexible Sync - Node.js SDK Flexible Sync - React Native SDK Flexible Sync - Swift SDK Flexible Sync ignores any custom  Collation  that you may have configured on\na MongoDB collection in Atlas. Instead, Synced collections always use\n {locale: \"simple\"}  when evaluating Sync subscriptions or permissions.",
            "code": [],
            "preview": "Learn how to get started with Atlas Device Sync in this high-level overview.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/configure-your-data-model",
            "title": "Configure Your Data Model",
            "headings": [],
            "paragraphs": "Sync Data Model Overview Create a Data Model Generate SDK Object Models Update a Data Model Make Breaking Schema Changes Data Model Mapping",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/configure-device-sync",
            "title": "Configure Atlas Device Sync",
            "headings": [],
            "paragraphs": "Enable Atlas Device Sync Sync Settings Pause or Terminate Sync",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "rules/expressions",
            "title": "Rule Expressions",
            "headings": [
                "Overview",
                "Restrictions",
                "Expression Syntax",
                "Embedded Expressions",
                "Multi-Field Expressions",
                "Expression Evaluation",
                "Expansions",
                "Logical Expansions",
                "Value Expansions",
                "Environment Expansions",
                "Request Expansions",
                "User Expansions",
                "MongoDB Document Expansions",
                "Service Expansions",
                "Operators",
                "Convert EJSON Values",
                "Call a Function",
                "Check Existence",
                "Compare Values"
            ],
            "paragraphs": "A  rule expression  is a JSON object that you write to control data access\nwith  permissions . Each expression defines the conditions\nunder which a user can take some action. For example, you can write an\nexpression to control whether a user can read or write data in a MongoDB\ndocument or a synced realm. App Services evaluates a rule expression, usually with a document as input, to\nget a true or false result. You can define simple, static expressions that use hardcoded values: You can also write expressions that support arbitrary logic to express dynamic\nrequirements and complex or custom workflows. A dynamic expression can include\nvariables that reflect the current context, called  expansions , and can use built-in  operators  to transform\ndata. The following example evaluates to true if the input document's  owner \nfield equals the user's  id  and the remote IP address of the request can be\nfound in an array of allowed IP addresses that is stored as a  value : When using Device Sync, expressions have special restrictions. See\n Sync-Compatible Expressions . An expression is either a boolean value (i.e.  true  or  false ) or\na JSON object. Expression object field names can be a value, an  expansion , or an  operator . Each field must\ncontain one of a value, an expansion, or a nested expression. Expression objects have the following format: You can embed multiple expression documents in the fields of another expression\ndocument to handle complex evaluation logic. App Services evaluates expressions\n depth-first, post-order : that is, it starts at the bottom of the expression\ntree and works back to the root-level fields by evaluating each expression after\nall of its embedded expressions. This expression evaluates to  true  only if the number provided\nas the  someNumber  argument falls in a specific range. When you have more than one field in an expression, that expression evaluates to\n true  if and only if every field in the expression evaluates to true. In\nother words, App Services treats multiple fields in a single expression as an \"AND\"\noperation. This third-party service rule expression evaluates to  true  only if both\nthe  url  argument was provided  and  the  body.userId  argument matches\nthe id of the user that called the action. App Services evaluates expressions by first replacing expansions with their\nruntime values and then evaluating each field of the expanded expression\ndocument to a boolean expression. If all fields in an expression evaluate to\n true , the expression also evaluates to  true . An empty expression\n( {} ) evaluates to  true . Expression fields evaluate based on the following rules: If an expanded field name matches its value, it evaluates to  true . If a field's value is an embedded expression, it evaluates to the\nsame value as the embedded expression. See  embedded expressions . If a rule does not explicitly use the  %%args  or\n %%root  expansion, expression field names default to\nchecking for arguments or document fields of the same name. For\nexample, the expression  { \"url\": \"https://www.example.com\" } \ndefaults to evaluating the value against  %%args.url  in a service\nrule and  %%root.url  in a MongoDB rule. An expansion is a variable that represents a dynamic value in an\nexpression. Expansions are denoted by two percent signs followed by the\nexpansion name. They are: When your app evaluates an expression, it replaces each expansion in the\nexpression with a specific value determined by your app's configuration\nand the context at the time of evaluation. %%root , which represents the data in a MongoDB document. %%user , which represents a user interacting with your app. %%request , which represents an incoming request. %%values , which represents a static value. %%environment , which represents your app's environment. %%args , which represents the arguments that were passed to\na service action. The following example uses the  %%user  and  %%root  expansions in an\n\"apply when\" expression: Some expansions, like  %%user , are available in all\nexpressions. Others are limited to specific contexts, like\n %%root  which is not Sync compatible and is only available\nin expressions that operate on a document. When using Device Sync, expansions have special restrictions. See\n Sync-Compatible Expansions . Evaluates to  true . Use this to assert that a nested expression\nmust evaluate to  true . Evaluates to  false . Use this to assert that a nested expression\nmust evaluate to  false . Represents your application's  values .\nEach field of the object maps a value name to its corresponding JSON\nvalue or secret. The following expression evaluates to  true  if the value\n admin_ids  is a list that contains the user's account ID: Represents the current  App environment .\nYou can read the environment name ( tag ) and access environment\nvalues. Each property of the object maps the name of an environment value to\nits value in the current environment. The following is a rule expression that evaluates to  true  if the\ncurrent environment is  \"production\"  and the  \"baseUrl\" \nenvironment value is defined: Represents the incoming  request . Represents the user that initiated the request. The  user object  contains account information, metadata from\nauthentication providers, and custom data from your app. The authenticated user's id. The type of user that initiated the request. Evaluates to\n \"server\"  for  API key  users and\n \"normal\"  for all other users. The user's custom data. The exact contents vary depending on\nyour  custom user data  configuration. The user's metadata. The exact contents will vary depending on\nthe  authentication provider \nidentities associated with the user. A list of all  authentication provider  identities associated with the user.\nAn identity consists of a unique identifier given to a user by an\nauthorization provider along with the provider's type: The value of a particular field as it exists at the end of a database operation. The value of a particular field before it is changed by a write operation. The full document as it exists at the end of a database operation. The full document before it is changed by a write operation. The following is a MongoDB schema validation expression that\nevaluates to  true  if either the document previously existed (i.e.\nthe action is not an insert) or the document's  status  field has a\nvalue of  \"new\" : A document containing the values passed as arguments to a\n service action . You can access each\nargument by its parameter name. The following is a  Twilio service  rule that\nevaluates to  true  if the sender's phone number (the  from \nargument) matches a specific value: ( Partition-Based Sync  only.) The partition key value of the current partition being evaluated. An expression operator represents an action or operation within an\nexpression. Operators take in one or more arguments and evaluate to a\nresult value. The type and value of the result depends on the operator\nyou use and the arguments you pass to it. Expression operators are denoted by strings that begin with either a\nsingle percent sign ( % ) or a dollar sign ( $ ). You can use them\nin any expression. The following operators allow you to convert values between EJSON\nand JSON representations: Converts a 12-byte or 24-byte string to an EJSON  objectId  object. Converts an EJSON  objectId  object to a string. Converts a 36-byte string to an EJSON  UUID  object. Converts an EJSON  UUID  object to a string. %stringToUuid ,  %uuidToString ,\n %stringToOid , and  %oidToString  do not\nevaluate JSON operators. You must provide either a literal string/EJSON\nobject or an expansion that evaluates to one. The following operators allow you to call functions in your App Services\napplication: Calls a  function  with the specified name and\narguments. Evaluates to the value that the function returns. The following operators allow you to determine if a value exists in an\nobject or array: Checks if the field it is assigned to has any value. Evaluates to a\nboolean representing the result. Checks a specified array of values to see if the array contains the\nvalue of the field that this operator is assigned to. Evaluates to a\nboolean representing the result. Checks a specified array of values to see if the array does not\ncontain the value of the field that this operator is assigned to.\nEvaluates to a boolean representing the result. The following operators allow you to compare values, including\n expanded  values: Checks if the field it is assigned to is equal to the specified\nvalue. Evaluates to a boolean representing the result. Checks if the field it is assigned to is not equal to the specified\nvalue. Evaluates to a boolean representing the result. Checks if the field it is assigned to is strictly greater than\nthe specified value. Evaluates to a boolean representing the\nresult. Checks if the field it is assigned to is greater than or equal to\nthe specified value. Evaluates to a boolean representing the\nresult. Checks if the field it is assigned to is strictly less than the\nspecified value. Evaluates to a boolean representing the result. Checks if the field it is assigned to is less than or equal to the\nspecified value. Evaluates to a boolean representing the result.",
            "code": [
                {
                    "lang": "json",
                    "value": "{ \"id\": \"aaaabbbbccccddddeeeeffff\" }"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"owner\": \"%%user.id\",\n  \"%%request.remoteIPAddress\": {\n    \"$in\": \"%%values.allowedClientIPAddresses\"\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  <Value | Expansion | Operator>: <Value | Expression>,\n  ...\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.someNumber\": {\n     \"%and\": [\n        { \"$gt\": 0 },\n        { \"$lte\": 42 }\n     ]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%args.url\": { \"$exists\": true },\n  \"%%args.body.userId\": \"%%user.id\"\n}"
                },
                {
                    "lang": "json",
                    "value": "\"applyWhen\": {\n   \"%%user.custom_data.status\": \"ACTIVE\",\n   \"%%root.owners\": \"%%user.id\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%user.id\": { \"$in\": \"%%values.admin_ids\" }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"tag\": \"<Environment Name>\"\n  \"values\": {\n    \"<ValueName>\": <Value>\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%environment.tag\": \"production\",\n  \"%%environment.values.baseUrl\": { \"%exists\": true }\n}"
                },
                {
                    "lang": "typescript",
                    "value": "{\n  \"httpMethod\": \"<HTTP Method>\",\n  \"httpReferrer\": \"<HTTP Referer Header>\",\n  \"httpUserAgent\": \"<HTTP User Agent>\",\n  \"rawQueryString\": \"<URL Query String>\",\n  \"remoteIPAddress\": \"<IP Address>\",\n  \"requestHeaders\": {\n    \"<Header Name>\": [\"<Header Value>\", ...]\n  },\n  \"service\": \"<Service Name>\",\n  \"action\": \"<Endpoint Function or Service Action Name>\",\n  \"webhookUrl\": \"<HTTPS Endpoint Route>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<User Account ID>\",\n  \"type\": \"<normal | server>\",\n  \"data\": {\n    \"<Field Name>\": <Value>,\n    ...\n  }\n  \"custom_data\": {\n    \"<Field Name>\": <Value>,\n    ...\n  }\n  \"identities\": [\n    {\n      \"providerType\": \"<Auth Provider Name>\",\n      \"id\": \"<Provider User ID\"\n    }\n    ...\n  ]\n}"
                },
                {
                    "lang": "json",
                    "value": "\"custom_data\": {\n  \"primaryLanguage\": \"English\",\n}"
                },
                {
                    "lang": "json",
                    "value": "\"data\": {\n  \"name\": \"Joe Mango\",\n  \"email\": \"joe.mango@example.com\"\n}"
                },
                {
                    "lang": "json",
                    "value": "\"identities\": [\n  {\n    \"id\": \"5bce299457c70db9bd73b8-aajddbkuvomsvcrjjfoxs\",\n    \"providerType\": \"local-userpass\"\n  }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"%or\": [\n    { \"%%prevRoot\": { \"%exists\": %%true } },\n    { \"%%root.status\": \"new\" }\n  ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"%%args.from\": \"+15558675309\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": {\n    \"%stringToOid\": \"%%user.id\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"string_id\": {\n    \"%oidToString\": \"%%root._id\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": {\n    \"%stringToUuid\": \"%%user.id\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"string_id\": {\n    \"%uuidToString\": \"%%root._id\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"%%true\": {\n    \"%function\": {\n      \"name\": \"isEven\",\n      \"arguments\": [42]\n    }\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": { \"$exists\": true }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": {\n    \"$in\": [\n      \"https://www.example.com\",\n      \"https://www.mongodb.com\"\n    ]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"url\": {\n    \"$nin\": [\n      \"https://www.example.com\",\n      \"https://www.mongodb.com\"\n    ]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"score\": { \"$eq\": 42 } }"
                },
                {
                    "lang": "json",
                    "value": "{ \"numPosts\": { \"$ne\": 0 } }"
                },
                {
                    "lang": "json",
                    "value": "{ \"score\": { \"$gt\": 0 } }"
                },
                {
                    "lang": "json",
                    "value": "{ \"score\": { \"$gte\": 0 } }"
                },
                {
                    "lang": "json",
                    "value": "{ \"score\": { \"$lt\": 0 } }"
                },
                {
                    "lang": "json",
                    "value": "{ \"score\": { \"$lte\": 0 } }"
                }
            ],
            "preview": "Evaluates to true. Use this to assert that a nested expression\nmust evaluate to true.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/app-builder",
            "title": "App Builder's Guide",
            "headings": [],
            "paragraphs": "Device Sync Permissions Guide Sync Atlas Data with Client Sync Client Data with Atlas Stream Data from Client to Atlas Add Sync to a Local-Only App Event Library",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "rules/examples",
            "title": "Data Access Role Examples",
            "headings": [
                "\"Apply When\" Expressions",
                "The User Is the Document Owner",
                "An Array Field Contains the User's ID",
                "The User Has An Email Address",
                "The User Has A Specific Email Address",
                "A Field Contains the User's Email Address",
                "An Array Field Contains the User's Email Address",
                "A Field Satisfies a Complex Condition",
                "CRUD Permissions",
                "The Role Can Read All Fields but Cannot Write",
                "The Role Can Read & Write All Fields",
                "The Role Can Read All Fields & Write to Specific Fields",
                "The Role Can Read & Write All Fields but Cannot Insert New Documents",
                "The Role Cannot Write to Specific Fields",
                "Advanced Role Patterns",
                "Insert-Only Roles",
                "Field-level Permissions for Embedded Documents"
            ],
            "paragraphs": "Atlas App Services determines if a role applies by evaluating the \"apply when\"\n expression  that you define for each role. This section contains examples of \"apply when\" expressions for common scenarios\nwhere you're not using Device Sync. For guidance on common Device Sync\nscenarios, see the  Device Sync Permissions Guide . To add an expression to a role, find the scenario that most closely matches your\nuse case and then copy and paste the provided template into the role. You may\nneed to modify placeholder values (denoted by  <angle brackets> ) in the\ntemplate to match your collection or otherwise adapt the template to fit your\nneeds. You can also use the \"apply when\" expressions on this page for  external\nservices . This expression evaluates to  true  if the active user's unique  id  value\nmatches the value of the specified field. \"Owner ID field\" refers to whatever\nfield in your schema represents a relationship with a user object. This expression evaluates to  true  if the active user's unique  id \nvalue matches one or more values in the specified array field. This expression evaluates to  true  if the active user has any email\naddress listed in their internal  user object . This expression evaluates to  true  if the active user's email address\nmatches the specified email address. This expression evaluates to  true  if the active user's email address\nmatches the value of the specified field. This expression evaluates to  true  if the active user's email address\nmatches one or more string values in the specified array field. This expression evaluates to  true  if the  Function   isAuthorizedUser  returns  true  when passed the\nactive user's id value. You can call any Atlas Function from a JSON expression using the\n %function  operator. App Services uses a role's permissions configuration to determine if the\nactive user can insert or delete a document as well as which fields in\nthe document they can read and write. This section contains templates that define roles for common scenarios. To apply\na set of permissions to a role, find the scenario that most closely matches your\nuse case. Update the Field Permissions, Document Permissions, and/or the role's permissions table\nto match the provided screenshot or copy and paste the provided\ntemplate into the collection's  advanced\nmode  configuration. Make sure that you\nmodify any placeholder values (denoted by  <angle brackets> ) in the template\nto match your needs. To allow a role to read any field, set the document-level  read  field\nto  true  and  write  field to  false . To allow a role to read or write any field, set the document-level\n write  field to  true . Document-level writes require read\npermission, so the role will be able to read all fields. To allow a role to read all fields, set the document-level  read \nfield to  true  and the  write  field to  false . To specify a field that the role can write to, set\nthe  write  field to  true  in the field's configuration document,\nwhich is embedded in the  fields  document. To allow a role to read or write any field, set the document-level\n write  field to  true . Document-level writes require read\npermission, so the role will be able to read all fields. To prevent the role from inserting new documents, set the document-level\n insert  field to  false . To allow a role to write to any field except for those you specify,\nset the document-level  read  field to  true . Set\nthe corresponding field-level  write  fields to  false  and\n read  fields to  true  in the\n fields  document. Lastly, set the  additional_fields.write  field to\n true . The use cases described in this section require you to use advanced\nfunctionality that is not supported by the default collection rules\neditor in the App Services UI. To use this template,  convert to advanced\nmode  or import a collection rule\nconfiguration with  App Services CLI . To allow a role to insert new documents but otherwise prevent them from reading\nor modifying any data, set  insert  to  true  and set the value of\ndocument-level  write  to a  rule expression  that\nevaluates to  true  only if the document didn't exist prior to the operation. You must specify a JSON expression for  write  to prevent users\nfrom reading data. To insert a document a role must also have write\npermission for all fields in the document; however, setting  write \ndirectly to  true  would also give the role read permission. The\nJSON expression ensures that the role only has read permission for\nthe initial document insert. To allow a role to read or write some but not all fields of an embedded\ndocument, add embedded documents that match the path of the embedded\nfield to the  fields  document. App Services applies any  read  and  write  permissions defined for a\ngiven field to all embedded fields that the field contains regardless\nof any permissions defined for those fields.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "{\n  \"<Owner ID Field>\": \"%%user.id\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"<Array Field>\": \"%%user.id\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"%%user.data.email\": { \"%exists\": true }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"%%user.data.email\": \"<Email Address>\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"%%root.email\": \"%%user.data.email\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"<Array Field>\": \"%%user.data.email\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"%%true\": {\n    \"%function\": {\n      \"name\": \"isAuthorizedUser\",\n      \"arguments\": [\"%%user.id\"]\n    }\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Role Name>\",\n  \"apply_when\": {<JSON Expression>},\n  \"document_filters\": {<JSON Expression>},\n  \"insert\": <boolean>,\n  \"delete\": <boolean>,\n  \"read\": true,\n  \"write\": false\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Role Name>\",\n  \"apply_when\": {<JSON Expression>},\n  \"document_filters\": {<JSON Expression>},\n  \"insert\": <boolean>,\n  \"delete\": <boolean>,\n  \"write\": true,\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Role Name>\",\n  \"apply_when\": {<JSON Expression>},\n  \"document_filters\": {<JSON Expression>},\n  \"insert\": <boolean>,\n  \"delete\": <boolean>,\n  \"read\": true,\n  \"write\": false,\n  \"fields\": {\n    \"<Field Name>\": { \"write\": true },\n    ...\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Role Name>\",\n  \"apply_when\": {<JSON Expression>},\n  \"document_filters\": {<JSON Expression>},\n  \"insert\": false,\n  \"delete\": <boolean>,\n  \"write\": true,\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"<Role Name>\",\n  \"apply_when\": {<JSON Expression>},\n  \"document_filters\": {<JSON Expression>},\n  \"insert\": <boolean>,\n  \"delete\": <boolean>,\n  \"read\": true,\n  \"fields\": {\n    \"<Field Name>\": {\n      \"read\": true,\n      \"write\": false\n    },\n    ...\n  },\n  \"additional_fields\": { \"write\": true }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"insertOnly\",\n  \"apply_when\": <JSON Expression>,\n  \"delete\": false,\n  \"insert\": true,\n  \"write\": {\n    \"%%prevRoot\": { \"%exists\": false }\n  },\n  \"additional_fields\": {}\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"canReadEmbeddedField\",\n  \"apply_when\": {},\n  \"delete\": true,\n  \"insert\": true,\n  \"fields\": {\n    \"someEmbeddedDocument\": {\n      \"fields\": {\n        \"someEmbeddedField\": {\n          \"read\": true,\n          \"write\": true\n        }\n      }\n    }\n  },\n  \"additional_fields\": {}\n}"
                }
            ],
            "preview": "Atlas App Services determines if a role applies by evaluating the \"apply when\"\nexpression that you define for each role.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/error-handling/client-resets",
            "title": "Client Resets",
            "headings": [
                "Overview",
                "Handle a Client Reset",
                "Recover Unsynced Changes",
                "Client Reset Recovery Rules",
                "Discard Unsynced Changes",
                "Manual Client Reset",
                "Examples",
                "Enable or Disable Recovery Mode"
            ],
            "paragraphs": "A  client reset error  is a scenario where a client realm cannot sync\ndata with the application backend. Clients in this state may continue to\nrun and save data locally but cannot send or receive sync changesets\nuntil they perform a client reset. The Realm SDKs provide methods to\nautomatically handle client resets under most scenarios. Client reset scenarios occur when the server's history is incompatible\nwith the client's history. The most common causes of client resets are: By default, the client reset process attempts to recover unsynced changes\nthat were successfully saved locally. When your client application\nhas breaking schema changes, or if  Recovery Mode \nis disabled on the server, the client reset process cannot recover unsynced\ndata that may be persisted locally on the device. Atlas App Services server crashes : A server may restore from a backup that has an earlier\nversion. A client reset for this scenario would reset the client to\nthe earlier version and lose any changes that were saved on the client but not\nyet synced with the server. Disabling and re-enabling sync : Turning Sync off and then on again in\nthe App Services UI causes all clients to require a client reset. Client/Backend schema mismatch : If a client application attempts\nto synchronize with the backend while using a Realm Object schema that\ndoes not exist in the backend, that client must perform a client reset.\nThis only applies in one direction: if the backend schema contains an\nRealm Object class not used in a client, that client does  not \nneed to client reset. Client maximum offline time : When a client has not synchronized with\nthe backend in more than  client maximum offline time  days, that client can no longer synchronize\nunsynced local changes with the backend. The client must discard all\nlocal changes since the last sync and download the current status of\nthe realm from the backend. Breaking schema changes : A  breaking or destructive change , like changing a property type or a\nprimary key, requires you to terminate and re-enable Sync. This creates a new\nsynced realm file with a version unrelated to the client's file. In this\nscenario, the client reset process cannot complete automatically, and your\napp must provide a  manual client reset handler . Upgrading from a Shared to Dedicated cluster : When you  upgrade from\na shared to a dedicated cluster , you must\nterminate Sync on the old cluster. After you upgrade, you can re-enable Sync.\nTurning Sync off and then on again causes all clients to require a\nclient reset. Session role changes : When using Flexible Sync, changes to a user's\n Flexible Sync session role  result\nin a client reset. The following scenarios all result in a client reset: server-side modifications to the session role changes to the value of any expansions in the \"apply when\", read, or\nwrite expressions changes that qualify the user for a different session role After a breaking schema change: All clients must perform a  manual client reset . You must update client models affected by the breaking schema change. The SDKs automatically detect the need for client resets. They can\nautomatically perform a client reset in most cases, except in the\nevent of a breaking schema change. During an automatic client reset, the client: In a breaking schema change, or if automatic client reset fails, the client\nreset falls back to a  manual client reset handler \nthat your app must define. The Realm SDKs cannot automatically perform a\nclient reset when you make a breaking schema change. Automatic client reset mode has several advantages compared to manual recovery: Downloads a fresh copy of the realm from the backend. Performs a diff to figure out the steps required to bring the original\n(local) copy of the realm to the same state as the fresh copy of the\nbackend. Applies that set of steps to transform the local realm into a state\nwhere it can sync with the backend. If there are no schema changes, or only non-breaking schema changes,\nthe SDK attempts to recover all local changes that have not yet synced\nto the backend. It also applies any inserts, updates, and deletes\nfrom the backend that haven't yet synced to the client. If you have chosen to discard unsynced changes in the SDK, or if Recovery\nMode cannot recover unsynced changes, the SDK can discard local changes\nthat have not yet synced to the backend. Then it can apply any inserts,\nupdates, and deletes from the backend that haven't yet synced to the\nclient. To fall back to discard local changes, choose your preferred SDK's\nversion of the  recoverOrDiscard  client reset mode. Discards the fresh copy. The app continues to use\nthe original copy of the realm with the diff applied. Your application can perform a client reset without writing any custom\nlogic, other than specifying the mode. You don't have to manually\ninitiate the client reset or interact with the error object at all. Your application can perform a client reset without closing any\nrealms, disconnecting from the backend app, or manual restarts. This\nmeans you don't have to write any logic to handle these situations. Application users receive notifications for changes as the local realm\nupdates to match the state of the backend realm. When  Client Recovery  is enabled in your Device Sync configuration - as it\nis by default - client applications can automatically recover unsynced\nchanges. In most scenarios, the client application can detect that a\nclient reset error has occurred and start an automated process to handle\nthe client reset. After the client reset, the app can open and operate as usual. Client Recovery can recover unsynced data in client resets except where\nthere has been a breaking schema change. Client Recovery applies\n Client Reset Recovery Rules  when determining how to integrate\nunsynced data from the device. You can choose to fall back to  Discard Unsynced Changes  in the\nevent that the client cannot recover unsynced data. In this case, local\ndata is discarded, but the client can automatically perform\nthe client reset. To fall back to discard local changes, choose your\npreferred SDK's version of the  recoverOrDiscard  client reset mode. In a client reset that does not involve a breaking schema change, the\nRealm SDKs attempt to recover unsynced changes. The SDK integrates objects\ncreated locally that did not sync before the client reset. These rules\ndetermine how conflicts are resolved when both the backend and the client\nmake changes to the same object: If an object is deleted on the server, but is modified on the recovering\nclient, the delete takes precedence and the client discards the update. If an object is deleted on the recovering client, but not the server,\nthen the client applies the delete instruction. In the case of conflicting updates to the same field, the client update\nis applied. The  discard unsynced changes  client reset mode automatically handles\nclient resets without attempting to recover data from the client device.\nYou might choose this mode if the  Client Reset Recovery Rules  do\nnot work for your app, or if you don't need to save unsynced data. When\nthis mode uses a diff to bring the local realm to the same state as the\nbackend, unsynced changes are permanently deleted. Discard Unsynced Changes mode cannot perform an automated client reset in\nthe event of a breaking schema change. This client reset mode permanently deletes any changes made\nlocally that have not yet synchronized to the backend.\nDo not use this client reset mode if your\napplication needs to preserve unsynced changes. In the event of  a breaking schema change ,\nRealm SDKs cannot automatically handle a client reset. You must define\na manual client reset handler if you make breaking schema changes. In this scenario, a manual client reset handler should do something like\ntell the user to update the app. In Realm SDK versions that automatically\nhandle client resets, a manual client reset only occurs in error scenarios\nwhere no meaningful recovery can occur. For more information on performing a client reset, check out the client\nreset examples for your SDK: C++ SDK: Handle Sync Errors - Client Reset Flutter SDK: Handle Sync Errors - Client Reset Java SDK: Reset a Client Realm Kotlin SDK: Handle Sync Errors - Client Reset .NET SDK: Client Resets Node.js SDK: Reset a Client Realm React Native SDK: Reset a Client Realm Swift SDK: Handle Sync Errors - Client Reset Recovery Mode is enabled by default in every Device Sync configuration.\nYou can disable Recovery Mode, or re-enable it if you have previously\ndisabled it. Select the  Device Sync  menu in the sidebar. Click the  Advanced Configuration  pane to display\nadditional configuration options. Click the  Enable Client Recovery  toggle. Press the  Save  button to confirm your\nchanges. If your app uses deployment drafts, you must deploy your application\nafter making changes. Pull a local copy of the latest version of your app with the\nfollowing  pull command : You can configure the number of days for your application's\nclient maximum offline time with the  is_recovery_mode_disabled \nproperty in your app's  sync/config.json  file: Deploy the updated app configuration with the following\n push command :",
            "code": [
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "json",
                    "value": "{\n  ...\n  \"is_recovery_mode_disabled\": false,\n  ...\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=\"<Your App ID>\""
                }
            ],
            "preview": "When device databases can no longer sync with Atlas, you must perform a client reset. Learn what can cause this condition, and how to resolve it.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/configure/enable-sync",
            "title": "Configure and Enable Atlas Device Sync",
            "headings": [
                "Overview",
                "Prerequisites",
                "Procedure",
                "Navigate to the Device Sync Configuration Screen",
                "Specify Your Sync Settings",
                "Turn On Sync",
                "Authenticate a MongoDB Atlas User",
                "Pull the Latest Version of Your App",
                "Add a Sync Configuration",
                "Deploy the Sync Configuration",
                "Select a Cluster to Sync",
                "Deploy the Sync Configuration"
            ],
            "paragraphs": "You can configure and enable Atlas Device Sync via the App Services UI, App Services CLI, or\nthe Atlas App Services Admin API. You can update an existing configuration in\nthe same way. If it is your first time enabling Device Sync, the UI is a great\nchoice because it walks you through the required steps. You might want to alter your Atlas Device Sync configuration after you have\nalready enabled Device Sync. You can update a configuration using the same\nprocedure as enabling Device Sync in the first place. If you're using Flexible\nSync mode, you can update your configuration without needing to terminate and\nre-enable Sync. If you're re-enabling Device Sync after pausing or terminating it, refer to\n Resume  or  Re-Enable  Device\nSync. Device Sync pauses automatically after 30 days\nof inactivity. If you haven't already decided how you want to configure your data model, see\n Sync Data Model Overview . Unless you are using  Development Mode , you must\nspecify at least one valid  schema  for a collection in the\nsynced cluster before enabling Sync. At a minimum, the schema must define\n _id  as a primary key. You should also include the field(s) you intend to use\nas your  queryable fields . For more details on how to define a schema, see  Enforce a Schema . Enabling Flexible Sync in Your App Services Application requires a\nnon-sharded MongoDB Atlas cluster running  MongoDB 5.0 or greater To enable Device Sync for your application, navigate to the\n Device Sync  configuration screen through the left\nnavigation menu. Follow the prompts to configure Device Sync. For details on the\navailable configuration settings, refer to  Sync Settings . Click  Enable Sync , take note of any\nrecommendations that appear and then confirm your choice. Use your MongoDB Atlas Admin API Key to log in to the CLI: Get a local copy of your App's configuration files. To get the latest\nversion of your App, run the following command: You can also export a copy of your application's configuration files\nfrom the UI or with the Admin API. To learn how, see  Export an App . You can enable sync for a single linked cluster in your application.\nIf you have not yet linked the cluster to your application, follow the\n Link a Data Source  guide first. The App Services App has a  sync  directory where you can find\nthe  sync configuration file . If you have not\nyet enabled Sync, this directory is empty. Add a  config.json  similar to: For details, refer to the  Sync Configuration File Reference . Deploy your changes to start syncing data. Import your app\nconfiguration: You can enable Device Sync for a single linked cluster in your\napplication. If you have not yet linked the cluster to your\napplication, follow the  Link a Data Source  guide. You'll need the cluster's service configuration file to configure sync. You\ncan find the configuration file by  listing all services\nthrough the Admin API : Identify the service whose configuration you need to update to enable\nSync. If you have accepted the default names when configuring\nyour App, this should be a service whose  name  is  mongodb-atlas \nand  type  is  mongodb-atlas . You need this service's  _id . Now you can  get the configuration file for\nthis service : Once you have the configuration, add the  flexible_sync  object with the\nfollowing template configuration: For details, refer to the  Sync Configuration File Reference . To authenticate your request to the App Services Admin API, you need a valid and\ncurrent authorization token from the MongoDB Cloud API. Read the\n API Authentication \ndocumentation to learn how to acquire a valid access token. To deploy your changes and start syncing data, send an\n Admin API request  that updates the cluster\nconfiguration with your sync configuration: To authenticate your request to the App Services Admin API, you need a valid and\ncurrent authorization token from the MongoDB Cloud API. Read the\n API Authentication \ndocumentation to learn how to acquire a valid access token.",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices login --api-key=\"<my api key>\" --private-api-key=\"<my private api key>\""
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"flexible\",\n  \"development_mode_enabled\": <Boolean>,\n  \"service_name\": \"<Data Source Name>\",\n  \"database_name\": \"<Development Mode Database Name>\",\n  \"state\": <\"enabled\" | \"disabled\">,\n  \"client_max_offline_days\": <Number>,\n  \"is_recovery_mode_disabled\": <Boolean>,\n  \"indexed_queryable_fields_names\": <Array of String Field Names>,\n  \"queryable_fields_names\": <Array of String Field Names>,\n  \"collection_queryable_fields_names\": <Map[String][]String>\n}"
                },
                {
                    "lang": "shell",
                    "value": "appservices push --remote=\"<Your App ID>\""
                },
                {
                    "lang": "shell",
                    "value": "curl https://services.cloud.mongodb.com/api/admin/v3.0/groups/{GROUP_ID}/apps/{APP_ID}/services \\\n  -X GET \\\n  -h 'Authorization: Bearer <Valid Access Token>'"
                },
                {
                    "lang": "shell",
                    "value": "curl https://services.cloud.mongodb.com/api/admin/v3.0/groups/{GROUP_ID}/apps/{APP_ID}/services/{MongoDB_Service_ID}/config \\\n  -X GET \\\n  -h 'Authorization: Bearer <Valid Access Token>'"
                },
                {
                    "lang": "json",
                    "value": "{\n  ...\n  \"flexible_sync\": {\n    \"state\": \"enabled\",\n    \"database_name\": \"<Name of Database>\",\n    \"client_max_offline_days\": <Number>,\n    \"is_recovery_mode_disabled\": <Boolean>,\n    \"indexed_queryable_fields_names\": <Array of String Field Names>,\n    \"queryable_fields_names\": <Array of String Field Names>,\n    \"collection_queryable_fields_names\": <Map[String][]String>\n  }\n  ...\n}"
                },
                {
                    "lang": "shell",
                    "value": "curl https://services.cloud.mongodb.com/api/admin/v3.0/groups/{GROUP_ID}/apps/{APP_ID}/services/{MongoDB_Service_ID}/config \\\n  -X PATCH \\\n  -h 'Authorization: Bearer <Valid Access Token>' \\\n  -h \"Content-Type: application/json\"\n  -d @/sync/config.json"
                }
            ],
            "preview": "You can configure and enable Atlas Device Sync via the App Services UI, App Services CLI, or\nthe Atlas App Services Admin API. You can update an existing configuration in\nthe same way. If it is your first time enabling Device Sync, the UI is a great\nchoice because it walks you through the required steps.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/error-handling/errors",
            "title": "Sync Errors",
            "headings": [
                "Overview",
                "Sync Protocol Errors",
                "Flexible Sync Errors",
                "MongoDB Translator Errors",
                "MongoDB Connection Errors",
                "Sync Client Errors",
                "Handle Sync Errors",
                "Set the Client Log Level"
            ],
            "paragraphs": "While you develop applications using Atlas Device Sync, you may run into errors. This\nsection lists common Sync errors and describes how to handle them. If you encounter an error not listed on this page, you can file a  support\nticket . The following table describes  Device Sync protocol \nerrors and how to handle them. Atlas App Services reports errors in your\n Device Sync logs . Error Name Description ErrorBadClientFileIdent This error occurs when the client is using a realm file that the\nserver cannot access after  terminating and re-enabling\nDevice Sync . This error triggers a client reset. To recover from this error,\nperform a  client reset . ErrorClientFileUserMismatch This error indicates that the client attempted to synchronize a\nrealm file associated with an identity other than the specified user.\nThis may occur if Device Sync is  terminated and reenabled  while the user is\noffline, which invalidates their previous identity. To recover from this error, delete the local realm file and then re-open\nthe realm. ErrorDivergingHistories This error indicates that the client attempted to synchronize a\nrealm file that has a different sync history than the server\nrealm. This may occur if Device Sync is  terminated and\nreenabled  while the\nuser is offline, which invalidates their previous sync history. This error triggers a client reset. To recover from this error,\nperform a  client reset . ErrorPermissionDenied This error occurs when a user's data access permissions are not\nsufficient for a given request. This can occur if a user attempts to open\na realm without read permission or modify data without write permission. To troubleshoot this error, review your  rules  to make sure\nthat users have proper data access permissions. ErrorOtherError This error indicates an internal failure that is not covered by a more\nspecific error. For example, this might occur when you hit the storage\nlimit of a free tier Atlas cluster. The following errors may occur when your App uses  Flexible Sync . Error Name Description ErrorBadQuery This error indicates the client query is invalid or malformed. This\nerror includes a message that provides details about why the query\nis invalid. To recover from this error, you may need to verify the  query\nsyntax  is correct, and that you are using\n query operators that are supported on the server . Additionally, confirm you are querying\na  queryable field  in your Flexible Sync\nconfiguration. If your query uses an indexed queryable field, ensure\nthat it meets  the requirements for valid client-side queries  for indexed\nqueryable fields. ErrorServerPermissionsChanged This error indicates that server permissions for the file ident have changed\nsince the last time it was used. This error triggers a client reset. To recover from this error,\nperform a  client reset . ErrorInitialSyncNotCompleted This error indicates that the client tried to open a session before the\ninitial sync was complete. This may occur when the app has just\nenabled Sync, and is still in the process of building the Sync history. The client attempts to reconnect until this process is complete. Then,\nthis error resolves and Sync begins working normally. ErrorCompensatingWrite This non-fatal error occurs when a client attempts an \"illegal\" write.\nThe following are considered illegal writes: Because the local realm has no concept of \"illegal\" writes, the write\nwill always succeed locally. Upon sync, the server will notice the\nillegal write. The server then undoes the change. The undo operation,\ncalled a \"compensating write\", syncs back to the client so the client's\nrealm no longer has the illegal write. The server also sends this error\nto let the client know what has happened. Any local writes to a given object between an illegal write to that\nobject and the corresponding compensating write will be lost. The illegal write appears in the applog as a non-fatal error. Illegal\nwrites might indicate that your application code is doing something you\ndidn't intend. In the following cases, you'll see  ErrorWriteNotAllowed  instead of\n ErrorCompensatingWrite  after an \"illegal\" write: Creating an object before opening a subscription. Creating an object that would be outside of the client's query view.\n\"Query view\" includes both the client's subscriptions and the client's\nread permissions. Modifying an object that is outside of the client's query view. Creating, deleting, or modifying an object or field that the client\ndoes not have write permissions to. Modifying an object in such a way that client would no longer have\nwrite permissions on that object or field after the write. Updating an existing object's  indexed queryable field \nvalue. Consider the following pseudo-code example: Here, the user does not have permission to write to\n fieldA  but attempts to write to it anyway -- an  illegal  write.\nThe user then performs two  legal  writes to the same object and\nanother legal write to a different object. Upon receiving the\ncompensating write for the illegal write to  obj1.fieldA , the\nsubsequent two legal writes to that object are lost. The end result is\nthat obj1 still exists and its two field values are whatever they were\nbefore the attempted modifications. Meanwhile, the deletion of obj2 is\nunrelated to the illegal write that caused the compensating write, so\nit stands and obj2 stays deleted. When using older SDK versions that are linked to a realm-core version\nbefore realm-core 12.1.0. In this case, the server will not undo the\nillegal write, and you will need to perform a  manual client reset . When modifying an object in a collection with  Data Ingest  enabled. In this case, the error is\nnon-fatal and does not trigger a client reset. The server skips over\nthe illegal change and does not apply it to the synced MongoDB cluster. The following errors may occur in the translation process between Device Sync\nand MongoDB Atlas. Error Name Description MaxIntegrationAttempts When the MongoDB translator cannot integrate a changeset, it\nretries for a fixed number of times. This error occurs when the\ntranslator reaches the maximum number of retries and could not\ncommit changes. This is usually caused by insufficient cluster\nsize. This can be due to a very large transaction that exceeds\nthe cluster's available resources. For example, a device is\noffline for a long time and tries to sync an atypical quantity of\nbatched transactions. Or the cluster resources are generally\ninsufficient for the app's needs. You can resolve this error by upgrading the cluster tier. To avoid this error, ensure the linked MongoDB cluster meets the\nneeds of your app. Also, make sure your app uses best practices\nfor reading and writing data. Refer to\n Atlas Cluster Sizing and Tier Selection \nfor more information. MongoEncodingError This error occurs when a MongoDB Atlas write (i.e. not a Sync client)\nmodifies a document such that it no longer conforms to the app's schema.\nDocuments that do not match the schema cannot be synced and any local\nupdates to the object represented by such a document will not propagate. For more information, see  Unsynced Documents . TranslatorCorrectiveErasure This error occurs when a synced MongoDB cluster rejects the write\noperation for a propagated Device Sync change. This is usually caused by\na duplicate key exception, which means that two objects use the same\nprimary key. To avoid this error, use an  ObjectId  or  UUID  as the\nprimary key value. Alternatively, ensure that every synced object has a\nunique primary key, even across partitions. TranslatorFatalError - ChangeStreamHistoryLost This error occurs when old entries in the  oplog  have expired before the server-side\n\"translator\" process could read them. Without these entries, the\ntranslator cannot bring the MongoDB cluster and the Realm object\nserver to an equivalent state. This can happen when: Because the free tier has a shared oplog, it is more vulnerable\nto this error. To resolve this error,  terminate  and  re-enable Sync . Sync is paused for so long that entries fall off the oplog. You drop a collection that the translator was using. The MongoDB cluster is unreachable for too long. If you enable Device Sync on a newly-created cluster, the operation may\nfail to parse the cluster URI. This happens because an SRV record for\nthe cluster hasn't propagated yet. There are two possible workarounds,\neither: Wait five minutes, then enable Atlas Device Sync. Unlink and  re-link the cluster . The  sync protocol  returns an  ERROR \nmessage when an error appears to have been caused by a connected client. Each\nmessage contains a code number and a description of the error. To see the full list of sync errors, refer to the  error code list  in the\nRealm Database Core GitHub repository. Every application that uses Sync needs a sync error handler. To learn more\nabout sync error handling, see your preferred SDK: Handle Sync Errors - C++ SDK Handle Sync Errors - Flutter SDK Handle Sync Errors - Java SDK Handle Sync Errors - Kotlin SDK Handle Sync Errors - .NET SDK Handle Sync Errors - Node.js SDK Handle Sync Errors - React Native SDK Handle Sync Errors - Swift SDK You can specify the client log level. Setting the log level to  trace  or\n debug  can help diagnose issues while your application is is in development.\nYou can log general information or details about all sync events, or log\nonly warnings or errors. For more information about available log levels, including how to set the\nclient log level, see your preferred SDK. Verbose logging negatively impacts performance. For production deployment,\nreduce the log level. Set the Client Log Level - Flutter SDK Set the Client Log Level - Java SDK Set the Client Log Level - Kotlin SDK Set the Client Log Level - .NET SDK Set the Client Log Level - Node.js SDK Set the Client Log Level - React Native SDK Set the Client Log Level - Swift SDK",
            "code": [
                {
                    "lang": "javascript",
                    "value": "obj1.fieldA = 10 // illegal due to field-level permissions\nobj1.fieldB = 5 // legal\nDELETE obj1 // legal\nDELETE obj2 // legal"
                }
            ],
            "preview": "Common errors encountered when using Atlas Device Sync and how to handle them.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/migrate-sync-modes",
            "title": "Migrate Device Sync Modes",
            "headings": [
                "Requirements",
                "What to Know Before Migrating",
                "Migration Stages",
                "Migrate Partition-Based Sync to Flexible Sync",
                "Begin Migration",
                "Migrate Permissions and Rules",
                "Cancel Migration",
                "Commit Migration",
                "Revert Migration",
                "After Backend Migration",
                "Migrate Client App to Flexible Sync"
            ],
            "paragraphs": "Atlas App Services Apps can have one of two Device Sync modes:\n Partition-Based Sync  and  Flexible Sync .\nPartition-Based Sync is an older mode and you should consider migrating to\nFlexible Sync. Migrating an App Services App that uses Partition-Based Sync to Flexible Sync is\nan automatic process. Other than upgrading the SDK version, migrating Sync modes\ndoesn't require any changes to your client app code. Migration results in Partition-Based Sync clients communicating with a\nFlexible Sync backend. We recommend eventually  migrating your client app code  to also use Flexible Sync instead of\nPartition-Based Sync. App Services App: Linked Atlas Cluster: Client apps: Currently use Partition-Based Sync Use MongoDB 5.0 or later for Flexible Sync compatibility Enable  storage auto-scaling Be an M10+ Cluster Tier Have time-based  oplog  setting enabled Minimum SDK version: Realm Swift SDK v10.40.0 Realm Kotlin SDK v1.9.0 Realm Node.js SDK v11.10.0 Realm React Native SDK v11.10.0 Realm .NET SDK v11.1.0 Realm Java SDK v10.16.0 Have a  Client Reset Handler  configured. Migrating your App's Sync mode involves changes that affect both client apps and\nyour App's backend. You should be aware of the effects and plan ahead. Client apps that don't meet the minimum SDK version requirement will not be\nable to sync after migration. How long a migration takes is directly proportional to the amount of data that\nneeds to be migrated. The more data, the longer it will take to migrate. Migrations can be  canceled  during the\nmigration or  reverted  within a certain\ntimeframe after the migration is finished. Progress notifications will not work after migration because Flexible Sync does\nnot support them. Client apps that were connected to the Partition-Based Sync backend will\nexperience a client reset after the backend is migrated to Flexible Sync. You\nshould use a  client reset  handler with recovery\nenabled so you don't lose any pending changes from before the client reset. Creating new tables in a migrated Flexible Sync backend using\n Development Mode  from a Partition-Based Sync client\nmay lead to unexpected behavior. You can see your migration's stage in the App Services UI. There are three stages\nto a Sync mode migration: After the migration has finished, check out the  After Migration  section for more information about how\nPartition-Based Sync clients interact with the Flexible Sync backend. Syncing Partition-Based Sync metadata At this stage, client apps can connect to your App's backend, but local writes\nwill not sync. These writes will be lost if you don't have a\n client reset handler  to recover unsynced changes. Clients will receive updates to synced collections, but won't be able to send\nuploads or receive upload acknowledgements. Building Flexible Sync metadata Clients cannot connect at this stage. The duration of this stage is directly\nproportional to the amount of data in synced collections. Completed migration Manage Sync Migration  appears on the Device Sync page if the\nmigration hasn't been committed or reverted. After beginning migration from Partition-Based Sync to Flexible Sync, the process\nis mostly automatic. However, you can control important parts of the migration. During a migration, you can cancel it. After a migration, you can revert it\nor manually commit the migration. Before you begin a migration, make sure you're aware of the\n migration stages  and\n effects . Migration generally has similar effects as  terminating \nand re-enabling Device Sync. There are some additional effects of migration: To migrate from Partition-Based Sync to Flexible Sync: After a migration is completed, Partition-Based Sync client apps will go through\na client reset. Then, they can communicate with your App's Flexible Sync backend.\nYou can disable and re-enable Flexible Sync and client apps will still\ncommunicate with your backend. If you re-enable Partitition-Based Sync you will lose the \"migrated\" status.\nThis means you'll need to do another migration if you want to use Flexible Sync\non the backend with Partition-Based Sync client apps. In the App Services UI, all configuration pages will be read-only. For\nexample: the Rules, Schemas, Functions, Triggers, and more pages. Your App will experience a period of read-only Sync and then a\nperiod of downtime. The new Flexible Sync configuration will set\n client max offline time  to 30 days. Storage in your Atlas cluster used by Device Sync metadata is temporarily doubled.\nPartition-Based Sync metadata and Flexible Sync metadata must temporarily co-exist.\nDepending on the size of your synced data, this could affect billing every time you\nmigrate. The duplicated metadata is removed after the migration is rolled back\nor committed. In the App Services UI, go to your App and select the Device Sync page. At the top of the page, select  [Migrate to Flexible Sync] . Read the information about migrating, then select\n Next: Migration Requirements . Ensure you meet the migration requirements, then select\n Start Migration . Clients will not be able to sync their local writes until the migration has\ncompleted. Your App's permissions and rules will be automatically migrated if the Partition-Based\nSync rules have direct App Services Rules equivalents. This will override\npreviously-defined App Services Rules. In step 4 above, you will see if your permissions and rules can be automatically\nmigrated. On the  Review Migration Process  step in the App Services UI,\nlook for  Rules & Permisssions . Some Partition-Based Sync rule strategies can't translate directly to App Services\nRules. You may need to manually migrate permissions that include: See the list of  Flexible Sync-compatible expansions \nfor all supported expansions. You should also check out the\n Device Sync Permissions Guide  for more\ninformation about how to work with permissions. %function  operator. Function rules  are not compatible with Flexible Sync\nand cannot be migrated. %or ,  %not ,  %nor ,  %and  expansions. These permissions may work, but there is enough nuance that you should test them\nto ensure expected behavior. Testing new permissions won't work on the App\nyou are migrating. Instead, create a new Flexible Sync app to test your\nmanually-migrated permissions. Then apply the tested permissions to the app\nyou are migrating. You can cancel a migration at any time while it is in progress. This will return\nyour App the Partition-Based Sync, with all settings as they were before the\nmigration started. If you cancel a migration, you need to start over from the beginning with future\nmigration attempts. To cancel a migration: In the App Services UI, go to your App and select the Device Sync page. At the top of the page, in the notification banner, select\n Stop Migration . This will cancel your migration. You will need to migrate again if you want to\nenable Flexible Sync in the future. After your migration has completed, it is in an evaluation state. You can manually\ncommit the migration to make it permanent or revert the migration to return your\nApp to Partition-Based Sync. If you don't commit or revert the migration, it will automatically commit\naccording to your\n minimum oplog window .\nFor example, if your minimum oplog window is 48 hours, you will have 48 hours to\nrevert your migration. After a migration is committed, the old Partition-Based Sync metadata is deleted\nand you cannot revert the migration. To manually commit a migration: In the App Services UI, go to your App and select the Device Sync page. At the top of the page, select the three dots next to\n Manage Sync Migration . From the list of options, select  Commit migration to Flexible Sync . Confirm migration commit by typing \"Commit\", then select  Commit . This will commit your migration and you will not be able to recover your\nPartition-Based Sync Metadata. You can revert a migration after it has completed and before it has been committed. Completed migrations are automatically committed in accordance with your\n minimum oplog window .\nYou can increase or decrease the amount of time you have to\nrevert a migration by adjusting the minimum oplog window. You cannot revert a migration that has been committed. Similar to canceling a migration, if you revert a migration, you need to\nstart over from the beginning with future migration attempts. To revert a migration: In the App Services UI, go to your App and select the Device Sync page. At the top of the page, select the three dots next to\n Manage Sync Migration . From the list of options, select\n Revert back to Partition-Based Sync . This will revert your migration. You will need to migrate again if you\nwant to enable Flexible Sync in the future. Your migrated backend uses a partition key field in your Realm object models\nto map the Partition-Based Sync client objects to Flexible Sync equivalents on\nthe backend. If your object models don't have a partition key field, the\nbackend automatically injects one into every object the Partition-Based Sync\nclient creates. These are some other characteristics of the injected field you should be aware\nof: The Partition-Based Sync client automatically creates sync subscriptions for\neach table where  partitionKey == partitionValue . This will continue\nuntil you  migrate your client app  to\nFlexible Sync. After migrating your client app to Flexible Sync, the backend will stop injecting\nthe partition key field. If your client app relies on the partition key field for\nmanaging sync subscriptions, you should add the parition key field to your\nobject models. If you don't, objects created in new versions of your app will not be synced\nto clients using older versions of your client app code. The injected field follows this structure:  <yourPartitionKey> ==\n<yourPartitionValue> . The field key is derived from your old\nPartition-Based Sync   backend   configuration. The value is derived from\nyour current Partition-Based Sync   client   configuration. The injected field does not sync back to the client that created the object.\nThis means the originating client won't be able to read the injected\n partitionKey  field. After your App's backend has been migrated to Flexible Sync, we recommend\nthat you migrate your client app to Flexible Sync. When you migrate your\nclient app to Flexible Sync, the client will stop automatically creating\nsync subscriptions for each Realm object model in your app's data model. We also recommend removing all sync subscriptions and then creating new subscriptions\nfor your data. This is the clearest way to control what data is synced after a\nmigration. If you want to remove individual subscriptions, the automatically\ngenerated subscriptions use a specific naming format:  flx_migrated_  + your\nobject model's name. For example, a  Person  object model's subscription\nname would be  flx_migrated_Person . You can keep the automatically-generated subscriptions if you don't want to\nremove and recreate them. But because these subscriptions aren't created in\nyour client code, they may make it harder for you to maintain and extend\nclient code. For more information about updating client code after migrating from\nPartition-Based Sync to Flexible Sync, refer to: Migrate from Partition-Based Sync to Flexible Sync - Java SDK Migrate from Partition-Based Sync to Flexible Sync - Kotlin SDK Migrate from Partition-Based Sync to Flexible Sync - .NET SDK Migrate from Partition-Based Sync to Flexible Sync - Node.js SDK Migrate from Partition-Based Sync to Flexible Sync - React Native SDK Migrate from Partition-Based Sync to Flexible Sync - Swift SDK",
            "code": [],
            "preview": "Learn how to migrate from Partition-Based Sync to Flexible Sync.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/configure/pause-or-terminate-sync",
            "title": "Pause or Terminate Sync",
            "headings": [
                "Pause Sync",
                "Press the Pause Sync Button",
                "Get an Authentication Token",
                "Get Group, App, and Service IDs",
                "Get the Sync Service Configuration",
                "Edit the Sync Service Configuration",
                "Update the Configuration on Atlas",
                "Terminate Sync",
                "Press the Terminate Sync Button",
                "Confirm That You Want to Terminate Sync",
                "Get an Authentication Token",
                "Get Group, App, and Service IDs",
                "Get the Sync Service Configuration",
                "Edit the Sync Service Configuration",
                "Update the Configuration on Atlas",
                "Re-Enable Sync",
                "Get an Authentication Token",
                "Get Group and App IDs",
                "Edit the Sync Service Configuration",
                "Update the Configuration on Atlas",
                "Connect from the Client"
            ],
            "paragraphs": "If you want to disable Atlas Device Sync for your app, you can temporarily pause or\npermanently terminate it. You can also re-enable Device Sync after\ndisabling it. You can temporarily  pause  Device Sync if\nyou need to pause your cluster. With a temporary pause, you can\n re-enable  without resetting your\n Device Sync configuration  or losing Device Sync\nmetadata. You can permanently  terminate  and\n re-enable  Device Sync for troubleshooting or\nif you want to change your  Device Sync configuration . Device Sync pauses automatically after 30 days\nof inactivity. You can pause Device Sync for a period of time without disabling\nit entirely. When you pause Device Sync, you stop syncing changes between\nAtlas and your application. Pausing Device Sync rejects any incoming client connections. This\nmeans that pausing Device Sync for your App stops syncing changes for\nall clients. After pausing Device Sync, you can  re-enable it .\nPausing Device Sync maintains the configuration settings and all of the\nDevice Sync metadata, which contains the sync history. When you re-enable\npaused Device Sync, your clients can reconnect normally. If you need to pause your cluster, pause Device Sync first. Otherwise, you\nmust  terminate  and  re-enable  Device Sync and perform a client reset. In the  Device Sync  tab of the App Services UI, press the\n Pause Sync  button at the top of the screen. Then, confirm that you want to pause sync. You can use the App Services API to programmatically pause\nDevice Sync. Every request to the App Services Admin API must include a valid\nand current authorization token from the MongoDB Cloud API as a\nbearer token in the  Authorization  header. For details on how to get an authentication token, refer to\n Get Authentication Tokens . Alternately, you can extract Group, App, and Service IDs from\nthe URL when you view the UI. The URL in your browser's toolbar should now resemble: https://services.cloud.mongodb.com/groups/$GROUP_ID/apps/$APP_ID/services/$SERVICE_ID/config Get the  Group and App ID \nfor your app. Get the Service ID for your Sync service. You can programmatically\nget all services using the  adminListServices \nendpoint. Go to the App Services App where Device Sync is configured. Select  Linked Data Sources  in the sidebar under\n Manage . Select the  mongodb-atlas CLUSTER . Send a GET request to the\n adminGetServiceConfig \nendpoint to get the Device Sync service configuration. This returns\na Service configuration object similar to: For details about the Sync configuration returned by this endpoint,\nrefer to  Sync Configuration File Reference . Remove just the  flexible_sync  part of the configuration to\na new JSON object. This configuration contains a  state  field\nthat reflects the current state of the Sync protocol for the\napplication. Set this value to  disabled . This should\nnow resemble a JSON blob similar to: Send the updated Sync Service configuration as a payload in a\nPATCH request to the  adminUpdateServiceConfig \nendpoint. To pause Device Sync from the client side, using logic\nthat situationally pauses Device Sync during a session, see your\npreferred SDK: Pause or resume a Device Sync session - Flutter SDK Pause or resume a Device Sync session - Kotlin SDK Pause or resume a Device Sync session - Java SDK Pause or resume a Device Sync session - .Net SDK Pause or resume a Device Sync session - Node SDK Pause or resume a Device Sync session - React Native SDK Pause or resume a Device Sync session - Swift SDK If your  oplog  rolls past the\ntime that you paused Device Sync, you must terminate and re-enable\nDevice Sync. For example, if you only keep 12 hours of oplog for your\ncluster, and you pause Device Sync for longer than 12 hours, you must\nterminate and re-enable Device Sync. Terminating and re-enabling paused Atlas Device Sync for your App erases\nAtlas Device Sync metadata and requires you to specify configuration settings again.\nClients must perform a client reset when they reconnect after Atlas Device Sync has\nbeen terminated. For more information, see:  Terminate Sync . If you have  enabled Device Sync , you may have to\nterminate and re-enable Device Sync under a few different circumstances: After terminating Device Sync, you can  re-enable it .\nTerminating Device Sync destroys the configuration settings and all of the\nDevice Sync metadata, which contains the sync history. Re-enabling Sync\nrequires you to complete the Sync configuration steps again. Clients that\nhave previously connected cannot reconnect until they perform a client\nreset. An upgrade from a Shared Tier Atlas Cluster to a Shared or Dedicated instance A move to an Atlas cluster that is distributed across regions/cloud providers An upgrade to NVMe Atlas clusters An  oplog  rollover A paused Device Sync session on a shared tier cluster due to infrequent usage Troubleshooting, at the request of MongoDB Support Switching between Sync modes. For instance, if you are switching from Partition-Based Sync to Flexible Sync Dropping a collection you've used with Sync. For example, if you\nhave a  Team  collection that stores and syncs  Team  objects, and\nthen you drop that collection, you must terminate and re-enable Sync. When you terminate and re-enable Atlas Device Sync, clients can no longer Sync.\nYour client must implement a client reset handler to restore Sync. This\nhandler can discard or attempt to recover unsynchronized changes. Client Reset - Flutter SDK Client Reset - Java SDK Client Reset - Kotlin SDK Client Reset - .NET SDK Client Reset - Node SDK Client Reset - React Native SDK Client Reset - Swift SDK Terminate Device Sync in your App to stop syncing data across\ndevices. In the  Sync  tab of the App Services UI, click the red\n Terminate Sync  button at the top of the screen. In the modal that appears, type \"Terminate Sync\" in the text\nentry, and then click the   Terminate Sync  button. If you are using  UI deployment drafts \nthen you must deploy the draft to actually terminate Sync. You have now terminated Device Sync. You can use the App Services API to programmatically terminate\nDevice Sync. Every request to the App Services Admin API must include a valid\nand current authorization token from the MongoDB Cloud API as a\nbearer token in the  Authorization  header. For details on how to get an authentication token, refer to\n Get Authentication Tokens . Alternately, you can extract Group, App, and Service IDs from\nthe URL when you view the UI: The URL in your browser's toolbar should now resemble: https://services.cloud.mongodb.com/groups/$GROUP_ID/apps/$APP_ID/services/$SERVICE_ID/config Get the  Group and App ID \nfor your app. Get the Service ID for your Sync service. You can programmatically\nget all services using the  adminListServices \nendpoint. Go to the App Services App where Device Sync is configured. Select  Linked Data Sources  in the sidebar under\n Manage . Select the  mongodb-atlas CLUSTER . Send a GET request to the\n adminGetServiceConfig \nendpoint to get the Device Sync service configuration. This returns\na Service configuration object similar to: For details about the Sync configuration returned by this endpoint,\nrefer to  Sync Configuration File Reference . This configuration contains a  state  field that reflects the current\nstate of the Sync protocol for the application. Set this value to\nan empty string  \"\" . This should now resemble a JSON blob\nsimilar to: Send the updated Sync Service configuration as a payload in a\nPATCH request to the  adminUpdateServiceConfig \nendpoint. After you have paused or terminated Device Sync, you can re-enable it.\nRe-enabling Device Sync enables your App to begin syncing changes to Atlas\nagain. After you re-enable Device Sync, your App begins accepting incoming\nclient connections again. When you pause Device Sync, App Services preserves your configuration\nsettings, and you can skip the  Configure Sync  step.\nWhen you terminate Device Sync, or when the  oplog  rolls past the time that you paused Device Sync,\nyou must specify the configuration settings again. To re-enable Device Sync, follow the steps in the  Configure and Enable Atlas Device Sync  guide. You can use the App Services API to programmatically re-enable\nDevice Sync. This procedure assumes you already have a Sync\nconfiguration for your cluster that you can use to re-enable Sync\nwith the same settings. Every request to the App Services Admin API must include a valid\nand current authorization token from the MongoDB Cloud API as a\nbearer token in the  Authorization  header. For details on how to get an authentication token, refer to\n Get Authentication Tokens . Get the  Group and App ID \nfor your app. This configuration contains a  state  field that reflects the current\nstate of the Sync protocol for the application. Set this value to\n enabled . This should now resemble a JSON blob similar to: Send the updated Sync Service configuration as a payload in a\nPATCH request to the  adminUpdateServiceConfig \nendpoint. Re-enabling Sync enables incoming client connections. However, under some\ncircumstances, your client app code may need to perform a\n client reset  before clients can Sync again. Your code must handle a client reset when a client has previously\nconnected with Device Sync, and you terminate Device Sync. Terminating\nDevice Sync for your App erases Device Sync metadata that the client\nneeds to reconnect. This also applies to apps where Device Sync is paused longer than the\n oplog  you keep for your cluster.\nFor example, if you only keep 12 hours of oplog for your cluster, and you\npause Device Sync for longer than 12 hours, you must terminate and\nre-enable Device Sync. For information on how to perform a client reset in the client, refer to\nthe SDK documentation: Perform a client reset - Flutter SDK Perform a client reset - Java SDK Perform a client reset - Kotlin SDK Perform a client reset - .NET SDK Perform a client reset - Node SDK Perform a client reset - React Native SDK Perform a client reset - Swift SDK",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"clusterId\": \"<MY-CLUSTER-ID>\",\n  \"clusterName\": \"Cluster0\",\n  \"clusterType\": \"atlas\",\n  \"flexible_sync\": {\n    \"state\": \"enabled\",\n    \"database_name\": \"todo\",\n    \"queryable_fields_version\": 1,\n    \"permissions\": {\n      \"rules\": {},\n      \"defaultRoles\": []\n    },\n    \"client_max_offline_days\": 30,\n    \"is_recovery_mode_disabled\": false\n  },\n  \"groupName\": \"<MY-GROUP-NAME>\",\n  \"orgName\": \"<MY-ORG-NAME>\",\n  \"readPreference\": \"primary\",\n  \"wireProtocolEnabled\": false\n}"
                },
                {
                    "lang": "json",
                    "value": " {\n   \"flexible_sync\": {\n     \"state\": \"disabled\",\n     \"database_name\": \"todo\",\n     \"queryable_fields_version\": 1,\n     \"permissions\": {\n       \"rules\": {},\n       \"defaultRoles\": []\n     },\n     \"client_max_offline_days\": 30,\n     \"is_recovery_mode_disabled\": false\n   }\n }"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"clusterId\": \"<MY-CLUSTER-ID>\",\n  \"clusterName\": \"Cluster0\",\n  \"clusterType\": \"atlas\",\n  \"flexible_sync\": {\n    \"state\": \"enabled\",\n    \"database_name\": \"todo\",\n    \"queryable_fields_version\": 1,\n    \"permissions\": {\n      \"rules\": {},\n      \"defaultRoles\": []\n    },\n    \"client_max_offline_days\": 30,\n    \"is_recovery_mode_disabled\": false\n  },\n  \"groupName\": \"<MY-GROUP-NAME>\",\n  \"orgName\": \"<MY-ORG-NAME>\",\n  \"readPreference\": \"primary\",\n  \"wireProtocolEnabled\": false\n}"
                },
                {
                    "lang": "json",
                    "value": " {\n   \"flexible_sync\": {\n     \"state\": \"\",\n     \"database_name\": \"todo\",\n     \"queryable_fields_version\": 1,\n     \"permissions\": {\n       \"rules\": {},\n       \"defaultRoles\": []\n     },\n     \"client_max_offline_days\": 30,\n     \"is_recovery_mode_disabled\": false\n   }\n }"
                },
                {
                    "lang": "json",
                    "value": " {\n   \"flexible_sync\": {\n     \"state\": \"enabled\",\n     \"database_name\": \"todo\",\n     \"queryable_fields_version\": 1,\n     \"permissions\": {\n       \"rules\": {},\n       \"defaultRoles\": []\n     },\n     \"client_max_offline_days\": 30,\n     \"is_recovery_mode_disabled\": false\n   }\n }"
                }
            ],
            "preview": "If you want to disable Atlas Device Sync for your app, you can temporarily pause or\npermanently terminate it. You can also re-enable Device Sync after\ndisabling it.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/configure/sync-settings",
            "title": "Sync Settings",
            "headings": [
                "Available Settings",
                "Sync Type",
                "Development Mode",
                "Breaking Changes",
                "Side Effects of Enabling Development Mode",
                "Cluster to Sync",
                "Database Name (Development Mode Only)",
                "Queryable Fields",
                "Queryable Field Scopes",
                "Configure Queryable Fields",
                "Configure Indexed Queryable Fields",
                "Eligible Field Types",
                "Reserved Field Names",
                "Performance and Storage",
                "Indexed Queryable Fields",
                "Consequences of Adding or Removing Queryable Fields",
                "Permissions",
                "Data Ingest",
                "Client Max Offline Time",
                "Client Recovery",
                "Sync Configuration File Reference",
                "Sync Config Object"
            ],
            "paragraphs": "This page explains the settings available when you  enable or configure\nDevice Sync . Atlas Device Sync has two sync modes: Flexible Sync and the older\nPartition-Based Sync. Partition-Based Sync has\nbeen deprecated and is disallowed for new Sync configurations. If you have\nan existing app that uses Partition-Based Sync, you can migrate\nto Flexible Sync. For more information, refer to  Migrate Device Sync Modes . Flexible Sync lets you define a query in the client and sync only the objects\nthat match the query. With client-side subscriptions, client applications can: Maintain queries React to changes Add, change, or delete queries Development Mode  is a configuration setting that allows Device Sync\nto infer and update schemas based on client-side data models. This\nstreamlines development, but should not be used in production. Development Mode speeds up development by allowing you to design  schemas  directly in client application code. When you sync a realm, Atlas App Services maps every synced object type\nto its own collection in the database specified by\n Database Name (Development Mode Only) . If you update the object model on the client\nside, App Services updates the collection schema to match. This lets you update\nobjects in your client code as you develop your app. You can use  data access rules  with Development Mode. Note\nthat schema changes ignore data access rules. This means that any client can\nupdate the backend schema by changing the client model. To learn more about how the Realm Object Schemas map to the App Services Schemas\nwhen using Development Mode, refer to  Data Model Mapping . For more information about modifying synced object schemas, see:\n Update Your Data Model . Development mode is a development utility that is not suitable for\nproduction use. Make sure that you turn off Development Mode before you\nmake your app accessible in a production environment. App Services Apps in Development Mode that were created after September 13, 2023\ncan make  breaking changes  to synced\nobject schemas from client code. If your App was created before September 13, 2023, you can contact support to\nenable this feature. Prerequisites To make a breaking change from client code: To delete a Realm file use the Realm SDK-specific methods: App Services App created after September 13, 2023 MongoDB 5.0 or later for Flexible Sync compatibility Minimum SDK version: Realm C++ SDK v1.0.0 Realm Flutter SDK v1.6.0 Realm Java SDK v10.16.2 Realm Kotlin SDK v11.1.1 Realm .NET SDK v11.6.0 Realm Node.js SDK v12.2.0 Realm React Native SDK v12.2.0 Realm Swift SDK v10.42.2 For Apps created before September 13, 2023, you need to update your\nobject schema in the App Services UI. For details, see\n Update Your Data Model Delete your local realm and data.\nThis will not affect data synced to the backend. Local changes that have not\nbeen synced will be removed and are unrecoverable. Change your local object model. Open a realm with your updated object model. Run your client app to sync your changes to the backend. Realm Kotlin SDK Realm Flutter SDK Realm Java SDK Realm .NET SDK Realm Node.js SDK Realm React Native SDK Realm Swift SDK Enabling Development Mode has two side effects: If your app does not need anonymous authentication, you may want to\ndisable it after enabling Development Mode. You cannot re-enable deployment drafts in the UI until Development Mode\nis disabled. However, you can still manually create deployment drafts\nthrough the CLI or the Admin API. Enabling   anonymous authentication . Disabling   deployment drafts . The name of the Atlas cluster  data source  where\nyou want the synced data stored. Flexible Sync requires MongoDB 5.0. You cannot use Sync with a  serverless\ninstance . In the UI, clusters display as gray and\nnot-selectable when they do not meet the requirements for Flexible Sync. When you enable  Development Mode , you specify a\ndatabase to store synced objects. App Services creates new collections in this\nDevelopment Mode database for every type of synced object. Specify a Development Mode database of  myapp . Your iOS client has a\n Person  model. You sync a realm that contains an instance of the\n Person  object. Development Mode creates a server-side schema associated\nwith the model. The object syncs to the  myapp.Person  collection. App Services continues creating new server-side schemas and collections\nfor each new object type. If you later add a  Dog  object, that object\nwill sync to a new  myapp.Dog  collection that App Services will\ncreate. When you configure Flexible Sync, you specify field names that your client\napplication can query in a Flexible Sync subscription. Fields that can be\nused in a subscription query are called  queryable fields . In a to-do list app, you might set  assignee  or  owner \nas queryable fields. On the client side, you can then query for tasks\nwhose  assignee  or  owner  matches the logged-in user. Queryable fields apply to a scope you designate when you configure them. The\ntwo available scopes are: Scoping a queryable field to a specific collection reduces the amount of\nbacking Atlas storage required to store Sync metadata. You can use  rules and permissions \nto configure more granular access control on a per-collection basis. You\ncan define collection-level rules and permissions for both global and\ncollection queryable fields. Global queryable fields : scoped across all collections in an App's Schema. Collection queryable fields : scoped to a single collection in the App. You can automatically specify queryable fields by enabling  Development\nMode . Fields that appear in client queries while using\nDevelopment mode are automatically added as  collection queryable fields \nfor the collection being queried. The field names you provide are arbitrary strings. If an object type has a field\nwhose name matches a field name you provided (and meets other eligibility\ncriteria), that field becomes available to Device Sync to query. You may only add or remove an indexed queryable field when Device Sync is\nnot enabled. If Device Sync is already running in your App, you must\n terminate Sync , and configure the indexed\nqueryable field when you  re-enable it . This causes  client resets  for any client attempting\nto reconnect after re-enabling Sync. Flexible Sync only supports top-level primitive fields with a scalar type as\nqueryable fields. You can also include arrays of these primitives as queryable\nfields. Flexible Sync does not support embedded objects or arrays of\nobjects as queryable fields. Indexed queryable fields  support a subset of data types. Your indexed\nqueryable field can be one of:  int64 ,  string ,  ObjectId ,  UUID . For information on the queries you can perform on these fields, see:\n Flexible Sync RQL Limitations App Services reserves some keywords for the Realm Query Language and other purposes.\nYou cannot use reserved keywords as field names. App Services reserves the following keywords with any capitalization: App Services also reserves the following keywords with the given exact capitalization: and asc ascending beginswith between contains desc descending distinct endswith falsepredicate inf infinity like limit nan nil null or sort subquery truepredicate You cannot use  descending ,  Descending ,  DESCENDING , or\n DeScEnDiNG  as a field name. ALL ANY B64 FALSE IN NONE NOT SOME TRUE all any false in none not oid some true uuid You cannot use  true  or  TRUE , since both capitalizations are\nspecifically reserved, but you can use  True  or  tRUE  as a field name. Each queryable field adds additional metadata storage to your Atlas cluster and\nmay lead to degraded write performance. You should have as few queryable fields\nas needed by your application, and scope them to the minimum number of\ncollections required. Many apps find a good balance between storage usage and query flexibility\nwith at most 10 queryable fields applying to any single collection.\nFor example, if you have 3  global queryable fields  and 7\n collection queryable fields , you have 10 queryable fields that apply to the\ncollection. If you have a field that you only want to query in one collection, but it\nis configured as a  global queryable field , this unnecessarily consumes\nAtlas storage space. For example, if you have a  user  field in every\ncollection, but you only use it for Sync queries in one collection, scoping\nthat as a  collection queryable field  reduces storage requirements. Reducing\nthe scope means that Sync does not have to maintain metadata for that field\nfor the other collections where you are not querying on the  user  field. If you need to reduce storage usage or improve performance, you can remove\nunneeded queryable fields from your App. However, be aware of the consequences\nof adding or removing queryable fields. For more information, refer to\n Consequences of Adding or Removing Queryable Fields . For additional considerations, refer to  optimizing performance and\nstorage when using Flexible Sync . You can improve performance for certain types of workloads by adding an\nindexed queryable field. An  indexed queryable field  is a\n global queryable field  that can be queried on more efficiently, providing\nimproved Sync performance. You can designate  one  global queryable field\nas an indexed queryable field. Indexing a queryable field improves performance for simple queries on a\nsingle field, such as  {\u201cstore_id\u201d: 1}  or\n {\u201cuser_id\u201d: \u201c641374b03725038381d2e1fb\u201d} . The indexed queryable field  must  appear in the schemas of all of your\nSync collections, and it must use the same valid data type. For example,\nif your indexed queryable field is  store_id , it must appear in all of\nthe collections you sync, and it must be the same valid type in all the\ncollections. For more information about eligible field types, refer to\n Eligible Field Types . You Can't Change Indexed Queryable Field Values on the Client After you configure an indexed queryable field, client devices  cannot \nupdate an existing object's indexed queryable field value. For example,\nif your indexed queryable field is  store_id , the client cannot change\nthis value directly. Changing it from the client is not supported because\nit may conflict with other updates made to the object in the same timeframe. If you attempt to change an indexed queryable field's value on the device,\nthis triggers a compensating write error. For more information about this\nerror and the behavior it entails, refer to  ErrorCompensatingWrite  in\nthe  Flexible Sync Errors  documentation. You can still change this value directly in the Atlas database. Client-Side Queries on Indexed Queryable Fields When your app uses an indexed queryable field, client-side queries in a\nFlexible Sync subscription  must  include the indexed queryable field using\nan  ==  or  IN  comparison against a constant at least once. For\nexample,  user_id == 641374b03725038381d2e1fb  or  store_id IN {1,2,3} . You can optionally include an  AND  comparison as long as the indexed\nqueryable field is directly compared against a constant using  ==  or  IN \nat least once. For example,  store_id IN {1,2,3} AND region==\"Northeast\" \nor  store_id == 1 AND (active_promotions < 5 OR num_employees < 10) . Invalid  Flexible Sync queries on an indexed queryable field include queries\nwhere: Changing an object's indexed queryable field value through Atlas may\noverwrite concurrent client updates to the object. The indexed queryable field does not use  AND  with the rest of the query.\nFor example  store_id IN {1,2,3} OR region==\"Northeast\"  is invalid\nbecause it uses  OR  instead of  AND .  Similarly,\n store_id == 1 AND active_promotions < 5 OR num_employees < 10  is invalid\nbecause the  AND  only applies to the term next to it, not the entire\nquery. The indexed queryable field is not used in an equality operator. For example\n store_id > 2 AND region==\"Northeast\"  is invalid because it uses only\nthe  >  operator with the indexed queryable field and does not have an\nequality comparison. The query is missing the indexed queryable field entirely. For example,\n region==\"Northeast\"  or  truepredicate  are invalid because they do\nnot contain the indexed queryable field. You can  update your Sync configuration \nto add or remove queryable field names while Sync is enabled, but be aware\nof the following: When you add a queryable field, devices can only sync on that field once the\ndevice has caught up to the point in time in  Device Sync History  where the field was added. When you remove a queryable field, any devices still using that field will have\ntheir Device Sync session dropped and must perform a  client reset . Clients not using the removed field won't receive any errors.\nTo avoid triggering a client reset when you remove the queryable field, you\nshould first remove usage of that field on the client-side. If you  terminate Sync  before adding or\nremoving queryable fields, these considerations do not apply. However,\nterminating Sync does trigger a  client reset  for\nany client that has Synced with your App. Atlas Device Sync enforces role-based data access rules for all requests to a\nsynced cluster. Rules are dynamic  JSON expressions  that\ndetermine a user's ability to sync, view, and modify data. For details, see  Role-based Permissions . Data Ingest is a sync strategy for applications with heavy client-side\ninsert-only workloads. You can enable it for one or more collections. It\nsupports writing to any collection type, including an Atlas time-series\ncollection. For example, an IoT app that frequently logs sensor data has a significant write\nworkload and no read workload. The device may also be offline for extended\nperiods of time. Data Ingest bypasses some of the processing required for\nbi-directional sync, significantly improving write speed to an Atlas collection. Other use cases include writing immutable data, such as invoices from a retail\napp, or logging application events, neither of which requires conflict\nresolution. You can apply Data Ingest to individual collections. This means your app can\nuse Data Ingest to write some data, but bi-directional Flexible Sync on\nother collections. Data Ingest collections are only for writing data. You cannot use Flexible\nSync queries against these collections. Instead, use  Connect to MongoDB Data Sources . After you have enabled Data Ingest, you implement it in the client app via\nthe client SDKs. Currently, the following Realm SDKs support Data Ingest: Atlas Device Sync completely manages the lifecycle of this data.\nIt is maintained on the device until Data Ingest synchronization is\ncomplete, and then removed from the device. C++ SDK:  Stream Data to Atlas - C++ SDK Flutter SDK:  Stream Data to Atlas - Flutter SDK Kotlin SDK:  Stream Data to Atlas - Kotlin SDK .NET SDK:  Unidirectional Data Ingest - .NET SDK Node.js SDK:  Define an Asymmetric Object React Native SDK:  Define an Asymmetric Object Swift SDK:  Stream Data to Atlas - Swift SDK Client Maximum Offline Time determines how long the client can be offline\nbetween sync sessions. Changing this value enables you to balance offline access\nwith storage used in the synced Atlas cluster. For more information, refer to\n Client Maximum Offline Time . Client Recovery enables the client to attempt to automatically perform a client\nreset while recovering data on the device. For more information, refer to\n Recover Unsynced Changes . You can find the Sync configuration file for your application in the  sync \ndirectory of an  exported  app: For example, the following Sync configuration applies to apps using Flexible\nSync. The deprecated  permissions  field might still appear in your exported app's\nconfiguration. That might indicate your app has not automatically migrated to\nthe  unified rule system yet . Please avoid\ndeleting this field until your app has been migrated. Field Description The sync mode. There are two Sync modes: Flexible Sync and the older\nPartition-Based Sync. We recommend using Flexible Sync. For more\ninformation about Partition-Based Sync, refer to\n Partition-Based Sync . Valid Options for a Flexible Sync Configuration: \"flexible\" If  true ,  Development Mode  is enabled\nfor the application. While enabled, App Services automatically stores synced\nobjects in a specific database (specified in  database_name ) and\nmirrors objects types in that database's collection schemas. The name of the Atlas cluster  data source \nto sync. You cannot use sync with a  serverless instance . The name of a database in the synced cluster where App Services stores data in\n Development Mode . App Services automatically\ngenerates a schema for each synced type and maps each object type to a\ncollection within the database. The current state of the sync protocol for the application. Valid Options: \"enabled\" \"disabled\" The number of days that the  backend compaction \nprocess waits before aggressively pruning metadata that some clients\nrequire to synchronize from an old version of a realm. If  false ,  Recovery Mode  is enabled\nfor the application. While enabled, Realm SDKs that support this feature\nattempt to recover unsynced changes upon performing a client reset.\nRecovery mode is enabled by default. A list of field names to use as  global queryable fields . A list of field names to use as the  indexed queryable field . While this property is an array,\nSync currently supports only one one indexed queryable field.\nTherefore, this array may contain at most one element. The indexed queryable field must be present in the schema and be\nthe same  eligible field type  in every collection you\nsync. The indexed queryable field name must  also  appear in\n queryable_fields_names  since this is a global queryable\nfield. A map from collection names to a list of  collection-level\nqueryable fields  for each collection. The date and time that sync was last paused or disabled, represented by\nthe number of seconds since the Unix epoch (January 1, 1970, 00:00:00\nUTC).",
            "code": [
                {
                    "lang": "none",
                    "value": "app/\n\u2514\u2500\u2500 sync/\n    \u2514\u2500\u2500 config.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"flexible\",\n  \"development_mode_enabled\": <Boolean>,\n  \"service_name\": \"<Data Source Name>\",\n  \"database_name\": \"<Development Mode Database Name>\",\n  \"state\": <\"enabled\" | \"disabled\">,\n  \"client_max_offline_days\": <Number>,\n  \"is_recovery_mode_disabled\": <Boolean>,\n  \"queryable_fields_names\": [\n    <Array of String Field Names>\n  ],\n  \"indexed_queryable_fields_names\": [\n   <Array of String Field Names>\n  ],\n  \"collection_queryable_fields_names\": <Map[String][]String>\n  \"permissions\": \"<Deprecated, Do Not Use>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"flexible\",\n  \"development_mode_enabled\": <Boolean>,\n  \"service_name\": \"<Data Source Name>\",\n  \"database_name\": \"<Development Mode Database Name>\",\n  \"state\": <\"enabled\" | \"disabled\">,\n  \"client_max_offline_days\": <Number>,\n  \"is_recovery_mode_disabled\": <Boolean>,\n  \"queryable_fields_names\": [\"<Field Name>\", ...],\n  \"indexed_queryable_fields_names\": [\"<Field Name>\", ...],\n  \"collection_queryable_fields_names\": {\n    \"<Collection Name>\": [\"<Field Name>\", ...],\n    ...\n  }\n}"
                }
            ],
            "preview": "Learn about Atlas Device Sync settings and how to use them.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/app-builder/event-library",
            "title": "Event Library",
            "headings": [
                "Overview",
                "Use Case",
                "How It Works",
                "Access Event Library Data",
                "Event Library Schema",
                "Event Types",
                "Read Events",
                "Read Event Combining",
                "Read Event Format",
                "Embedded Object Read Events",
                "Write Events",
                "Write Event Format",
                "Custom Events"
            ],
            "paragraphs": "The Event Library enables developers to track what data the user\nsees and edits while using a Device Sync-enabled mobile application. The\nEvent Library can record three types of events: Developers can specify the read and write transactions to record.\nAdditionally, you can configure custom events to record things like\nbutton presses, or what the user is seeing in the frontend application. This level of detail enables auditors or other interested parties to\nassess exactly what happened, and when. Read events Write events Custom events The Event Library does not support recording  AuditEvents  using Flexible\nSync. This functionality requires a Partition-Based Sync App Services App\nto record  AuditEvent  data. The Event Library provides the ability to perform audits to meet compliance\nrequirements in heavily-regulated industries, such as healthcare or\nfinancial services. A nurse in a healthcare facility uses an app with the Event Library\nenabled. The app presents the nurse with real-time vital signs,\ninformation streamed in from the medical equipment, and the patient's\nhistorical treatment information. The app inherently enforces compliance,\nbecause its code blocks actions that the nurse should not take based\non the data being viewed. The Event Library captures all the information\nthat the nurse sees within the app interface, as well as the actions\nthat the nurse takes after viewing that information. At some point, this nurse provides treatment to a patient that later\nexposes the facility to a malpractice lawsuit. The legal department\nmust review the information that was available to the nurse during the\ntreatment. The Event Library captures the digital data that the nurse\nviewed during treatment, as well as the actions he undertook. By\nreviewing this data, the legal team can assess whether the treatment was\nreasonable. Without this information, the legal department can't know\nand prove whether the nurse's actions were reasonable. The Event Library opens a separate \"event\" realm on the user's device. This\nrealm has access to any user realm that the developer chooses to monitor\nwith the Event Library. When developers implement the Event Library, they designate the types of\nevents they want to record, as well as any custom metadata they want to\nappend to the event recordings. When the client application runs, it records\nthe designated user interactions to the \"event\" realm as read events,\nwrite events, or custom events. While the client device has a network connection, Atlas Device Sync synchronizes\nthis event realm data to an  AuditEvent  collection in the linked Atlas\ndata source. For information on how to implement the Event Library in a client\napplication, see:  Event Library - Swift SDK . The Event Library records event data in a collection called  AuditEvent \nin your linked Atlas database. When you configure the Event Library,\nuse  Development Mode  to automatically\ncreate this collection and derive a schema from synced events. Remember\nto turn Development Mode off before taking your application to production. Your  AuditEvent  collection must have a schema containing the following\nfields: Additionally, the schema must contain an optional string field\nfor each metadata key you use. For example: Field name Type Required _id ObjectId Required activity String Required _partition String Required timestamp Date Required event String Optional data String Optional For information on how to add a schema to your collection, see:\n Enforce a Schema . If you're not using any custom metadata, your schema might look like this: The Event Library records three types of events: Read events Write events Custom events The Event Library records data returned as the result of a query as read\nevents. Read events also record any time a Realm object is instantiated,\nsuch as when following a link or looking an object up by primary key. The Event Library records read events as a JSON object with two fields: Read events store values as follows: type : stores the class name value : stores an array of serialized objects Single-object read events : the value is an array that has a single element Objects matching a query : the value is an array of all objects matching\na query, even if the objects are never used Reads that occur during a write transaction : the value is the data that\nthe object has before the write transaction began; it does not reflect\nany changes that occur during the write event. Objects that do not exist when a write transaction begins : objects that\nare created in a write transaction do not produce a read event at all. The Event Library cannot tell if only a subset of the query displays in\nthe client application. For example, say the client application has a\nlist view. The Event Library's read event doesn't record scrolling\ninformation; it records the read event as the full query result.\nDevelopers must use custom events to record when a client application\ndisplays only a subset of a query result. A stream of every read event could produce a lot of \"duplicate\" events on\nthe same objects that don't add information. To reduce these \"duplicate\"\nevents, the Event Library discards and merges some events. The Event Library discards: The Event Library merges: Queries which match no objects Queries which match only newly-created objects Object reads where the object is matched by a previous query Multiple queries on the same table into a single merged query. A read event object has this format: The Event Library represents embedded objects by creating a link in the\nparent object to the primary key of the embedded object. When the user\ndoes not follow the link, the primary key is the only representation of\nthe embedded object. When the user does follow the link, the embedded\nobject resolves within the parent object. This also produces a top-level object read for the embedded object. A  Person  object has an embedded object  Office  which contains details\nabout the location where the person works. When we do not follow the link\nto view any of the  office  details, the parent  Person  object shows\nonly the embedded object's object ID. When we do follow the link to view details of the embedded  Office  object,\nthis resolves the embedded object within the parent object. It  also \nproduces a second top-level read of just the child object - in this case,\nour  Office  object. Read Event Combining may affect the objects you see when you query\nfor an object and then later follow an embedded object link. In the example above, if you previously queried for the  Person ,\nthat would produce an read event where the office object\nis unresolved within the person object; you'd see only the ObjectID\nin the initial read event. Then, if you later follow a link that\nresolves the embedded object, you'd see the separate top-level read\nfor the embedded object, but would  not  get the parent object read\nthat shows the resolved embedded object within the parent object. The Event Library records write events when: The write event records both the before and after state of the object. For\nnew objects created, the before state is  null . For deletes, the after\nstate of the object is  null . For each write transaction the client commits during an event recording\nscope, the Event Library records a single write event. This write event\nrecords all of the changes made during the write transaction. The payload is an object keyed on class names. Each object type which had\nany objects created, modified, or deleted has an entry. The Event Library records changes to an object type as an object with three\narrays: In a modification, the  newValue  object only includes properties which\nare different from the  oldValue  object. If a write transaction assigns\nto an object but does not actually change the value of any properties, New objects are created Existing objects are modified Objects are deleted insertions: contain the serialized objects which were inserted, using the\nsame serialization scheme as reads modifications: report both the old and new values of each property deletions: contain the serialized objects which were deleted, using the\nsame serialization scheme as reads A write event object has this format: Custom events can record types of events that do not read or write to the\ndatabase, such as: You can use custom events to give context to read and write events, such\nas recording a custom event when the client application displays a given\nscreen. Then, you can infer that read and write events after the custom\nevent recording the app screen loading all occurred on that app screen. Custom events can store whatever data a developer desires, or no data at all. When a specific screen displays When the user clicks a button",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"<Metadata Key>\": {\n    \"bsonType\": \"string\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"AuditEvent\",\n  \"bsonType\": \"object\",\n  \"required\": [\n    \"_id\",\n    \"_partition\",\n    \"timestamp\",\n    \"activity\"\n  ],\n  \"properties\": {\n    \"_id\": {\n      \"bsonType\": \"objectId\"\n    },\n    \"_partition\": {\n      \"bsonType\": \"string\"\n    },\n    \"timestamp\": {\n      \"bsonType\": \"date\"\n    },\n    \"activity\": {\n      \"bsonType\": \"string\"\n    },\n    \"event\": {\n      \"bsonType\": \"string\"\n    },\n    \"data\": {\n      \"bsonType\": \"string\"\n    }\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": \"62b396f4ebe94d2b871889bb\",\n  \"_partition\":\"events-62b396f4ebe94d2b871889ba\",\n  \"activity\":\"read object\",\n  \"data\": \"{\n      \"type\":\"Person\",\n      \"value\": [{\n        \"_id\": \"62b396f4ebe94d2b871889b9\",\n        \"_partition\":\"\",\n        \"employeeId\":1,\n        \"name\":\"Anthony\"\n      }]\n  }\",\n  \"event\":\"read\",\n  \"timestamp\": 2022-06-23T14:54:37.756+00:00\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"Person\",\n  \"value\": [{\n    \"_id\": \"62b47624265ff7b58e9b204e\",\n    \"_partition\": \"\",\n    \"employeeId\": 1,\n    \"name\": \"Michael Scott\",\n    \"office\": \"62b47624265ff7b58e9b204f\"\n  }]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\":\"Person\",\n  \"value\": [{\n    \"_id\": \"62b47975a33224558bdf8b4d\",\n    \"_partition\": \"\",\n    \"employeeId\": 1,\n    \"name\": \"Michael Scott\",\n    \"office\": {\n      \"_id\": \"62b47975a33224558bdf8b4e\",\n      \"_partition\": \"\",\n      \"city\": \"Scranton\",\n      \"locationNumber\": 123,\n      \"name\": \"Dunder Mifflin\"\n    }\n  }]\n}\n\n{\n  \"type\": \"Office\",\n  \"value\": [{\n    \"_id\": \"62b47975a33224558bdf8b4e\",\n    \"_partition\": \"\",\n    \"city\": \"Scranton\",\n    \"locationNumber\": 123,\n    \"name\": \"Dunder Mifflin\"\n  }]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"Person\": {\n    \"insertions\": [{\n      \"_id\": \"62b47ead6a178a314ae0eb52\",\n      \"_partition\": \"\",\n      \"employeeId\": 1,\n      \"name\": \"Anthony\"\n    }]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"Person\":{\n    \"modifications\": [{\n      \"newValue\": {\n        \"name\": \"Tony\"\n      },\n      \"oldValue\": {\n        \"_id\": \"62b47d83cdac49f904c5737b\",\n        \"_partition\": \"\",\n        \"employeeId\": 1,\n        \"name\": \"Anthony\"\n      }\n    }]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"Person\":{\n    \"deletions\":[{\n      \"_id\":\"62b47ead6a178a314ae0eb52\",\n      \"_partition\":\"\",\n      \"employeeId\":1,\n      \"name\":\"Tony\",\n      \"userId\":\"tony.stark@starkindustries.com\"\n    }]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": \"62b4804c15659310991e5e0a\",\n  \"_partition\": \"events-62b4804b15659310991e5e09\",\n  \"activity\": \"login\",\n  \"event\": \"custom event\",\n  \"timestamp\": 2022-06-23T15:01:31.941+00:00\n}"
                }
            ],
            "preview": "The Event Library enables developers to track what data the user\nsees and edits while using a Device Sync-enabled mobile application. The\nEvent Library can record three types of events:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/app-builder/stream-data-from-client-to-atlas",
            "title": "Stream Data Unidirectionally from a Client to Atlas",
            "headings": [
                "Set up Atlas Access",
                "Create an Atlas App Services App",
                "Configure Device Sync and Data Ingest",
                "Enable an Authentication Provider",
                "Add Sync to the Client Application",
                "Connect the client to an App Services backend",
                "Authenticate a User",
                "Open a Synced Realm",
                "Create Asymmetric Objects and Write Data"
            ],
            "paragraphs": "If you have a mobile or client application that produces a large volume of\ndata you'd like to stream to MongoDB Atlas, you can sync data unidirectionally\nusing Device Sync. We call the feature that enables this unidirectional sync\n Data Ingest . You might want to sync data unidirectionally in IoT applications, such as\na weather sensor sending data to the cloud. Data Ingest is also useful\nfor writing other types of immutable data where you do not require conflict\nresolution, such as creating invoices from a retail app or logging\napplication events. Data Ingest is optimized to provide performance improvements for heavy\nclient-side  insert-only  workloads. You cannot read this data from the\nrealm where you're streaming it. Currently Data Ingest is only available for the following Realm SDKs: Follow these high-level steps to get started: C++ SDK .NET SDK Kotlin SDK Node.js SDK React Native SDK Swift SDK The App Services application is the gateway that enables your client\ndevice to connect to MongoDB Atlas. When you create an App, you name\nit, link it to an Atlas cluster, and specify the deployment model and\ndeployment region that work best for your application. To learn how to create an App Services App, see  Create an\nApp . Device Sync is the service that synchronizes data to Atlas\nand across your client devices. Device Sync, together with the\nRealm SDKs, automatically manages network connectivity, user\npermissions, and conflict resolution. The App Services app provides\nthe built-in user authentication that Device Sync and the Realm SDKs\nleverage to secure your data. When you configure Device Sync, you specify the data source that the\nclient devices can access as well as the permissions that determine\nwhat data a user can write. You can configure Device Sync via the  Atlas UI, the Atlas App\nServices Command Line Interface, or the App Services Admin API . The first time you configure Device Sync,\nyou may find it helpful to use the Atlas UI as it provides links and\ninformation about various settings and options. To enable Data Ingest for one or more collections, you select\nthe collection or collections in the  Advanced Configuration \nsection of your Device Sync configuration. Only collections with\nan  App Services Schema  are available to select in\nthis drop-down. If you have not yet created a schema for the collection that you\nwant to sync unidirectionally, you can either: Generate an App Services schema from your Realm object model.\nRefer to:  Create an App Services Schema from a Realm Object Model . Manually create an App Services Schema. For more information, refer\nto:  Enforce a Schema . Your client devices must authenticate in order to access synced data.\nApp Services provides several authentication providers, such as\nemail/password or anonymous authentication, to enable your users\nto authenticate. Configure one or more of these authentication\nproviders to enable authentication in your client application. You can configure authentication providers within the App Services App\nUI by selecting  Authentication  in the left navigation menu,\nand then click a provider to configure it. You can also configure\nauthentication providers by editing the App Services configuration\nwith the  Atlas App Services Command Line Interface , or the  App Services Admin API . After this, you've got everything set up that you need on the Atlas side,\nand you can prepare your client application to sync data unidirectionally. In your client application code, initialize an App client to connect\nyour client to your App Services backend. This lets your client use\nApp Services features like authentication, and enables opening a\nsynced realm. Connect to an Atlas App Services Backend - C++ SDK Connect to an Atlas App Services Backend - .NET SDK Connect to an Atlas App Services - Kotlin SDK Connect to an Atlas App Services Backend - Node.js SDK Connect to an Atlas App Services Backend - React Native SDK Connect to an Atlas App Services backend - Swift SDK Your client application user must be authenticated with the App Services\nbackend in order to write synchronized data. Add logic to your\nclient app to register and log in users. Authenticate Users - C++ SDK Authenticate Users - .NET SDK Authenticate Users - Kotlin SDK Authenticate Users - Node.js SDK Authenticate Users - React Native SDK Authenticate Users - Swift SDK Once you have an authenticated user, you can open a synced\ninstance of Realm Database to use for that user. Because Asymmetric\nSync is write-only, you cannot define a Flexible Sync query to read\ndata on the device. This differs from bidirectional Flexible Sync,\nwhere you would create a query subscription to determine what data\nto sync to the device. Open a Synced Realm - C++ SDK Open a Synced Realm - .NET SDK Configure & Open a Synced Realm - Kotlin SDK Open a Synced Realm - Node.js SDK Configure & Open a Synced Realm - React Native SDK Configure & Open a Synced Realm - Swift SDK The Realm SDKs provide a special type of object to use with\nData Ingest: an Asymmetric Object. Refer to the Realm SDK\ndocumentation for details on how to define Asymmetric Objects. Create and write Asymmetric Objects to a synced realm, and\nthe Realm SDKs use Device Sync manage the process of streaming the\ndata automatically. When the device has a network connection, the\nSDKs stream data to the App Services backend and into Atlas. When\nthe device does not have a network connection, the data persists on\nthe device and automatically uploads when the network connection is\nrestored. Atlas Device Sync completely manages the lifecycle of this data.\nIt is maintained on the device until Data Ingest synchronization is\ncomplete, and then removed from the device. Stream Data to Atlas - C++ SDK Stream Data to Atlas - .NET SDK Stream Data to Atlas - Kotlin SDK Define an Asymmetric Object - Node.js SDK Define an Asymmetric Object - React Native SDK Stream Data to Atlas - Swift SDK",
            "code": [],
            "preview": "If you have a mobile or client application that produces a large volume of\ndata you'd like to stream to MongoDB Atlas, you can sync data unidirectionally\nusing Device Sync. We call the feature that enables this unidirectional sync\nData Ingest.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/app-builder/local-to-sync",
            "title": "Migrate a Local-Only App to a Sync-Enabled App",
            "headings": [
                "Create an App Services App",
                "Enable Authentication",
                "Enable Sync",
                "Update the Client App",
                "Copy Existing Data"
            ],
            "paragraphs": "If you have a mobile app that uses a local realm and you want to\nconvert the app to use Sync, there are three primary tasks: When it comes to reading and writing to a realm database, there is no difference\nbetween working with a local realm and a synced realm. Once you configure sync\nand open the realm, your existing code continues to work in the same way it did\nbefore migrating. Create and configure an  App Services App Modify your client code to point to this new backend app Copy the local data to the new synced realm An app can have multiple realms, and you can migrate any number of them to\nuse Sync. For example, your app can continue to use a local-only realm for\ndevice-specific information while also using a synced realm for other data. To sync your data between devices and MongoDB Atlas, you first create\nan App Services App. This app provides a gateway to the data with security\nin the form of user authentication, data encryption, and access control. To get started creating the backend app, follow the steps in\n Create an App Services App . Device Sync requires authenticated users. Atlas App Services provides\nmany different  Authentication Providers  such\nas email/password and OAuth. Once you have enabled and configured authentication,\nyou have full control over who has access to which data. To learn more and get started, see\n Authentication Providers . In your App Services App, configuring Sync is the final step. When you set up\nSync, you enable authenticated users to have online access to their current data.\nWhile offline, users can work with the most-recent data, but their changes\nwon't sync until they're back online. With Flexible Sync, clients synchronize subsets of data based on queries to\nqueryable fields. When configuring Flexible Sync, you decide which fields\nclients can query on. To enable Flexible Sync, follow the steps in\n Procedure . Now that you have the backend set up for Device Sync, you need to make a few\nchanges to your client app code. Follow the steps to set up Flexible Sync in the\nQuick Start for the language/platform you are using: Quick Start - .NET SDK Quick Start - Node.js SDK Quick Start - React Native SDK Quick Start - Swift SDK At this point, your data still only exists in the local database. You need to\nperform an initial copy before Realm will sync the data. To do so, follow these\nsteps and refer to the diagram: Check if a synced realm already exists. If not, create one. Open a connection to it and to the local realm. Read each record from the local realm and modify it to match the schema of the\nsynced realm. Copy the modified record to the new synced realm. Sync automatically\ncopies the record to MongoDB Atlas when connected to your App. Confirm that all records you want to preserve are in the new realm. Delete the local realm file. On each subsequent app load, check if the local realm has been\ndeleted. If you run into errors while configuring Sync or copying data, check the\nApp Services App logs. The logs provide details about sync\nerrors. In many cases, performing a  client reset  is\nhelpful in solving Sync migration issues.",
            "code": [],
            "preview": "If you have a mobile app that uses a local realm and you want to\nconvert the app to use Sync, there are three primary tasks:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/app-builder/sync-data-in-client-with-atlas",
            "title": "Sync Data in a Client Application with Atlas and Other Devices",
            "headings": [
                "Set up Atlas Access",
                "Create an Atlas App Services App",
                "Configure Device Sync",
                "Enable an Authentication Provider",
                "Generate an App Services Schema",
                "Add Sync to the Client Application",
                "Connect the client to an App Services backend",
                "Authenticate a User",
                "Open a Synced Realm",
                "Copy Existing Data Into a Synced Realm",
                "Use the Synced Realm"
            ],
            "paragraphs": "If you have a mobile or client application that uses Realm Database, and\nyou would like to synchronize its data across other devices and back it up\nto MongoDB Atlas, you can do so using Device Sync. Follow these high-level\nsteps to get started: The App Services application is the gateway that enables your client\ndevice to connect to MongoDB Atlas. When you create an App, you name\nit, link it to an Atlas cluster, and specify the deployment model and\ndeployment region that work best for your application. To learn how to create an App Services App, see  Create an\nApp . Device Sync is the service that manages synchronizing data to Atlas\nand across your client devices. Device Sync, together with the\nRealm SDKs, automatically manages network connectivity, conflict\nresolution, user authentication, and user permissions and data access. When you configure Device Sync, you specify the data source that the\nclient devices can access, as well as the permissions that determine\nwhat data a user can read and write. You can configure Device Sync via the  Atlas UI, the Atlas App\nServices Command Line Interface, or the App Services Admin API . The first time you configure Device Sync,\nyou may find it helpful to use the Atlas UI as it provides links and\ninformation about various settings and options. Your client users must authenticate in order to access synced data.\nApp Services provides several authentication providers to enable\nyour users to authenticate. Configure one or more of these authentication\nproviders to enable authentication in your client application. You can configure authentication providers within the App Services App\nUI by selecting  Authentication  in the left navigation menu,\nand then click a provider to configure it. You can also configure\nauthentication providers by editing the App Services configuration\nwith the  Atlas App Services Command Line Interface , or the  App Services Admin API . When you already have a client application that uses Realm Database,\nyou can use your Realm Object Model to generate the App Services Schema\nthat maps data between client devices and Atlas. You can do this\nby enabling Development Mode, a feature that reads your object model\ndata from synced realm data, and generates a schema from that data. For more information on how to generate an App Services schema from your\nRealm object model, refer to:  Create an App Services Schema from a Realm Object Model . After this, you've got everything set up that you need on the Atlas side,\nand you can prepare your client application to sync data. If you already\nhave a client application that persists data using Realm Database, you only\nneed to add a few elements to synchronize that data across devices and with\nAtlas. In your client application code, initialize an App client to connect\nyour client to your App Services backend. This lets your client use\nApp Services features like authentication, and enables opening a\nsynced realm. Connect to an Atlas App Services Backend - C++ SDK Connect to Atlas App Services - Flutter SDK Connect to Atlas App Services - Java SDK Connect to Atlas App Services - Kotlin SDK Connect to Atlas App Services - .NET SDK Connect to Atlas App Services - Node.js SDK Connect to Atlas App Services - React Native SDK Connect to Atlas App Services - Swift SDK Your client application user must be authenticated with the App Services\nbackend in order to access synchronized data. Add logic to your\nclient app to register and log in users. Authenticate Users - C++ SDK Authenticate Users - Flutter SDK Authenticate Users - Java SDK Authenticate Users - Kotlin SDK Authenticate Users - .NET SDK Authenticate Users - Node.js SDK Authenticate Users - React Native SDK Authenticate Users - Swift SDK Once you have an authenticated user, you can open a synchronized\ninstance of Realm Database to use for that user. You define a Flexible Sync query subscription in your client\ncode to determine what data to sync to the client application.\nDevice Sync looks for Atlas documents that match the query, which\nthe user has permission to read and possibly write, and synchronizes\nthose documents to the client device as Realm objects. You can add,\nremove, or update Flexible Sync query subscriptions to change the\ndocuments that sync to the device. C++ SDK Open a Synced Realm - C++ SDK Manage Sync Subscriptions - C++ SDK Flutter SDK Open a Synced Realm - Flutter SDK Manage Flexible Sync Subscriptions - Flutter SDK Java SDK Open a Synced Realm - Java SDK Flexible Sync - Java SDK Kotlin SDK Open a Synced Realm - Kotlin SDK Subscribe to Queryable Fields - Kotlin SDK .NET SDK Open a Synced Realm - .NET SDK Manage Flexible Sync Subscriptions - .NET SDK Node.js SDK Open a Synced Realm - Node.js SDK Flexible Sync - Node.js SDK React Native SDK Configure a Synced Realm Flexible Sync - React Native SDK Swift SDK Configure & Open a Synced Realm - Swift SDK Manage Flexible Sync Subscriptions - Swift SDK If you already have client data, you cannot add Device Sync directly\nto a non-synced realm. You must copy the data from the non-synced\nrealm into the synced realm. If you do not have any client data,\nyou can skip this step. Some of the SDKs provide methods that enable you to do this when you\nopen a realm. However, many of the Realm SDKs do not currently support\nusing these methods with Flexible Sync. If your SDK does not support copying a local realm to a Flexible Sync\nrealm, you must write logic to iterate over each object in the\nnon-synced realm and copy it into the synced realm. This is a\none-time process. After you copy the data over, you can discard the\nnon-synced Realm and open only the synced realm going forward. Convert Between Non-Synced Realms and Synced Realms - .NET SDK The syntax to read, write, and watch for changes on a synced realm\nis identical to the syntax for non-synced realms. While you work with\nlocal data, a background thread efficiently integrates, uploads, and\ndownloads changesets. When a user who has write permissions makes\nchanges on the device, the data persists locally. When the user has\na network connection, the data automatically syncs back to Atlas\nand other devices.",
            "code": [],
            "preview": "If you have a mobile or client application that uses Realm Database, and\nyou would like to synchronize its data across other devices and back it up\nto MongoDB Atlas, you can do so using Device Sync. Follow these high-level\nsteps to get started:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/app-builder/sync-data-in-atlas-with-client",
            "title": "Sync Data in Atlas with a Client Application",
            "headings": [
                "Set up Atlas Access",
                "Create an Atlas App Services App",
                "Configure Device Sync",
                "Enable an Authentication Provider",
                "Generate Client Object Models",
                "Add Sync to the Client Application",
                "Connect the client to an App Services backend",
                "Authenticate a User",
                "Open a Synced Realm",
                "Use the Synced Realm"
            ],
            "paragraphs": "If your organization already has data in MongoDB Atlas, you can sync that\ndata to client devices using Device Sync and Realm Database. Follow these\nhigh-level steps to get started: The App Services application is the gateway that enables your client\ndevice to connect to MongoDB Atlas. When you create an App, you name\nit, link it to an Atlas cluster, and specify the deployment model and\ndeployment region that work best for your application. To learn how to create an App Services App, see  Create an\nApp . Device Sync is the service that manages synchronizing your Atlas data\nwith your client devices. Device Sync, together with the Realm SDKs,\nautomatically manages network connectivity, conflict resolution, user\nauthentication, and user permissions and data access. When you configure Device Sync, you specify the data source that the\nclient devices can access, as well as the permissions that determine\nwhat data a user can read and write. You can configure Device Sync via the  Atlas UI, the Atlas App\nServices Command Line Interface, or the App Services Admin API . The first time you configure Device Sync,\nyou may find it helpful to use the Atlas UI as it provides links and\ninformation about various settings and options. Your client users must authenticate in order to access synced data.\nApp Services provides several authentication providers to enable\nyour users to authenticate. Configure one or more of these authentication\nproviders to enable authentication in your client application. You can configure authentication providers within the App Services App\nUI by selecting  Authentication  in the left navigation menu,\nand then click a provider to configure it. You can also configure\nauthentication providers by editing the App Services configuration\nwith the  Atlas App Services Command Line Interface , or the  App Services Admin API . When you already have data in Atlas, you can generate object models\nthat match that data to use in your client application. To generate an object model, you must have an App Services schema.\nOnce you have a schema, you can generate object models that match that\nschema. You can select a programming language to use to generate the\nschema, which you can then view and copy to use in your client\napplication code. You can generate a schema from existing Atlas data using the App\nServices UI. For more information on how to do this, refer to:\n Create a Realm Object Schema from an App Services Schema . Once you have a schema, you can generate a Realm object model using\nthe  App Services UI or the Atlas App Services Command Line\nInterface . After this, you've got everything set up that you need on the Atlas side,\nand you can prepare your client application to sync data. Device Sync\nsynchronizes data with Atlas using a Realm Database SDK. You can use the\nRealm SDK in your client application code to connect to Atlas and sync\ndata with a Realm Database on the device. In your client application code, initialize an App client to connect\nyour client to your App Services backend. This lets your client use\nApp Services features like authentication, and enables opening a\nsynced realm. Connect to an Atlas App Services Backend - C++ SDK Connect to App Services - Flutter SDK Connect to an Atlas App Services backend - Java SDK Connect to an Atlas App Services backend - Kotlin SDK Connect to an Atlas App Services Backend - .NET SDK Connect to an Atlas App Services Backend - Node.js SDK Connect to an Atlas App Services Backend - React Native SDK Connect to an Atlas App Services backend - Swift SDK Your client application user must be authenticated with the App Services\nbackend in order to access synchronized data. Add logic to your\nclient app to register and log in users. Authenticate Users - C++ SDK Authenticate Users - Flutter SDK Authenticate Users - Java SDK Authenticate Users - Kotlin SDK Authenticate Users - .NET SDK Authenticate Users - Node.js SDK Authenticate Users - React Native SDK Authenticate Users - Swift SDK Once you have an authenticated user, you can open a synchronized\ninstance of Realm Database to use for that user. You define a Flexible Sync query subscription in your client\ncode to determine what data to sync to the client application.\nDevice Sync looks for Atlas documents that match the query, which\nthe user has permission to read and possibly write, and synchronizes\nthose documents to the client device as Realm objects. You can add,\nremove, or update Flexible Sync query subscriptions to change the\ndocuments that sync to the device. C++ SDK Open a Synced Realm - C++ SDK Manage Sync Subscriptions - C++ SDK Flutter SDK Open a Synced Realm - Flutter SDK Manage Flexible Sync Subscriptions - Flutter SDK Java SDK Open a Synced Realm - Java SDK Flexible Sync - Java SDK Kotlin SDK Open a Synced Realm - Kotlin SDK Subscribe to Queryable Fields - Kotlin SDK .NET SDK Configure & Open a Synced Realm - .NET SDK Manage Flexible Sync Subscriptions - .NET SDK Node.js SDK Open a Synced Realm - Node.js SDK Flexible Sync - Node.js SDK React Native SDK Open & Close a Realm - React Native SDK Flexible Sync - React Native SDK Swift SDK Configure & Open a Synced Realm - Swift SDK Manage Flexible Sync Subscriptions - Swift SDK The syntax to read, write, and watch for changes on a synced realm\nis identical to the syntax for non-synced realms. While you work with\nlocal data, a background thread efficiently integrates, uploads, and\ndownloads changesets. When a user who has write permissions makes\nchanges on the device, the data persists locally. When the user has\na network connection, the data automatically syncs back to Atlas\nand other devices. For a quick guide to how to do common tasks with Realm Database, refer\nto the Realm SDK quick starts. If you prefer to study a working client app to learn, some of the SDKs\nprovide  a working Device Sync app , which we call\ntemplate apps. Optionally, you can go through SDK-specific tutorials\nthat build on the Device Sync template app. C++ SDK Quick Start - C++ SDK Flutter SDK Quick Start - Flutter SDK Device Sync Tutorial - Flutter SDK Java SDK Quick Start with Sync - Java SDK Kotlin SDK Quick Start - Kotlin SDK Device Sync Tutorial - Android with Kotlin SDK .NET SDK Quick Start - .NET SDK Device Sync Tutorial - Xamarin (Android & iOS) Node.js SDK Quick Start - Node.js SDK React Native SDK Quick Start  - React Native SDK Device Sync Tutorial - React Native Swift SDK Quick Start - Swift SDK Quick Start - SwiftUI Device Sync Tutorial - SwiftUI",
            "code": [],
            "preview": "If your organization already has data in MongoDB Atlas, you can sync that\ndata to client devices using Device Sync and Realm Database. Follow these\nhigh-level steps to get started:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/go-to-production/optimize-sync-atlas-usage",
            "title": "Optimize Sync Storage in Atlas",
            "headings": [
                "Overview",
                "History",
                "Trimming",
                "Client Maximum Offline Time",
                "Key Concepts",
                "Client Maximum Offline Time Does Not Immediately Influence Client Resets",
                "Set the Client Maximum Offline Time",
                "Optimizing Performance and Storage When Using Flexible Sync",
                "Summary"
            ],
            "paragraphs": "Atlas Device Sync uses space in your app's synced Atlas cluster to store\nmetadata for sync. This includes a history of changes to each realm.\nAtlas App Services minimizes this space usage in your Atlas cluster.\nMinimizing metadata is necessary to reduce the time and data needed for sync. The App Services backend keeps a history of  changes to\nunderlying data  for each realm, similar to the\n MongoDB oplog . App Services\nuses this history to synchronize data between the backend and clients.\nApp Services stores history in your synced Atlas cluster. When you set a client maximum offline time in an App that uses  Flexible\nSync ,  trimming  deletes changes older than the client\nmaximum offline time. Used in trimming, the  client maximum offline time  controls the age limit of history.\nThis indirectly changes how long a client can remain offline between\nsynchronization sessions with the backend. Clients that do not\nsynchronize for more than the specified number of days\nmay experience a  client reset  the next time they\nconnect with the backend. Setting the client maximum offline time to a lower value will decrease the amount\nof history required by sync. The resulting optimization lowers storage usage\nin the synced Atlas cluster. New Apps automatically enable client maximum offline time with a default\nvalue of 30 days. Client maximum offline time enables trimming for\nolder history. This permanently changes affected\nhistory, and can cause client resets in the future even after\ndisabling the feature. Sync should always converge at the same end state on all clients. In\norder to converge during a sync, clients require the full history of\nchanges beginning immediately after their last sync. When a client does\nnot sync for a long period of time, trimming can\nalter the history in ways that prevent the client from converging. Since\nsynchronization relies on all clients converging on a common result,\nsuch a client cannot synchronize. As a result, the client must complete a client reset before it can\nresume synchronization. In a client reset scenario, the client deletes\nthe client-local copy of a realm and downloads the current state of\nthat realm from the backend. Synchronization then resumes using the\nnew copy of the realm. The client maximum offline time controls how long your backend waits\nbefore applying trimming. After the specified number\nof days without syncing, clients may experience a client reset the next\ntime they connect with the backend. Applications that do not specify the client maximum offline time never\napply trimming. This means that clients can connect\nafter any period of time offline -- weeks, months, or even years --\nand synchronize changes. As time passes, frequently-edited realms\naccumulate many changes. With a large changeset, synchronization\nrequires more time and data usage. Trimming causes permanent, irreversible changes to\nhistory. As a result, increasing the client maximum offline time does\nnot  immediately  change the length of time before clients experience a\nclient reset. Existing history has already been changed by trimming,\nrequiring a client reset. New history needs time to accumulate up to\nthe new client maximum offline time. Disabling the client maximum offline time feature stops additional\ntrimming, but history that has already been changed by\ntrimming will permanently cause client resets in clients. Decreasing client maximum offline time also does not  immediately \nchange the length of time before clients experience a client reset.\nClient resets begin taking place earlier once the regularly scheduled\ntrimming job applies trimming to the newly\neligible history. Select the  Device Sync  menu in the sidebar. Under  Sync Type , make sure  Flexible  is selected. Scroll down to the  Advanced Configuration (optional)  dropdown.\nClick the dropdown to open. Under the  Client Max Offline Time  section, click\nthe  Enable Max Offline Time  toggle. Specify a number of days for your application's client maximum\noffline time. The default value is 30 days. The minimum value is\n1. Click the  Save Changes  button at the bottom of the screen\nonce you are ready to save. In the confirmation window, click the  Save Changes  button again\nto confirm changes. Pull a local copy of the latest version of your app with the\nfollowing  pull command : You can configure the number of days for your application's\nclient maximum offline time with the  client_max_offline_days \nproperty in your app's  sync/config.json  file: Deploy the updated app configuration with the following\n push command : For Flexible Sync configuration, the amount of Atlas storage space used is\ndirectly proportional to the number of queryable fields you have set up.\nQueryable fields use storage on the backing Atlas cluster. The more queryable\nfields you configure, the more storage you use on the backing cluster. If you have a large number of collections in an App, you may need to use the\nsame queryable field name across multiple collections. Combine this with\n permissions  for more granular control over who can access\nwhich collections. For best performance, open a synced realm with a broad query. Then, add\nmore refined queries to expose targeted sets of data in the client\napplication. Slicing off working sets from a broad query provides\nbetter performance than opening multiple synced realms using more\ngranular queries. When you configure queryable fields, consider the broad queries you use\nfor Sync, and select fewer fields that support those broad queries. Your app may contain 20 or 30 collections, but you want to minimize the\nnumber of queryable fields. You can re-use  global queryable fields \nacross collections in order to sync objects from every collection.\nFor example,  owner_id  might be a field you want to query in multiple\ncollections. Alternately, you may have  owner_id  in multiple collections, but only\nneed to query on it in one collection. In this case, you might make\n owner_id  a  collection queryable field . This means Sync only has to\nmaintain metadata about this field for one collection, instead of storing\nmetadata for all of the collections where you're not querying on this\nfield. Finally, for Apps where devices want to query one specific facet of the\ndata, such as  owner_id == user.id , you may want to designate the\nfield an  indexed queryable field . Indexed queryable fields provide more\nefficient performance for Apps where the client only needs to sync on a\nsmall subset of their data - a group of stores or a single user, for\nexample. You can have one indexed queryable field per App. An indexed queryable\nfield is a  global queryable field  that must be present and use the\nsame eligible data type in each collection you sync. For more information, refer to  Queryable Field Scopes  and\n Indexed Queryable Fields . In a to-do list app, prefer broad queries such as  assignee == currentUser \nor  projectName == selectedProject  for a Sync query. This gives you a\ncouple of broad fields against which to Sync documents. In the client, you\ncan further refine your query for things like tasks of a certain priority\nor completion status to slice off a working set. Device Sync uses space in your synced Atlas cluster to store change history. Trimming reduces the space usage for Flexible Sync apps, but can cause client resets\nfor clients who have not connected to the backend in more than\nthe client maximum offline time (in days). Flexible Sync apps that have disabled a client maximum offline time do not apply trimming,\nso clients of any age can sync without experiencing a client reset. Adding additional queryable fields to a flexible sync configuration\nwill increase storage consumed on an Atlas Cluster. Using broad queries\nand selecting fewer fields that support broad queries decreases storage consumed.",
            "code": [
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "json",
                    "value": "{\n   \"client_max_offline_days\": 42,\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=\"<Your App ID>\""
                }
            ],
            "preview": "Atlas Device Sync uses space in your app's synced Atlas cluster to store\nmetadata for sync. This includes a history of changes to each realm.\nAtlas App Services minimizes this space usage in your Atlas cluster.\nMinimizing metadata is necessary to reduce the time and data needed for sync.",
            "tags": "maxOfflineTime, client, maximum, offline, time, client maximum offline time, maximum offline time, max, max offline time, client max offline time",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/app-builder/device-sync-permissions-guide",
            "title": "Device Sync Permissions Guide",
            "headings": [
                "Using the Template Apps",
                "About These Examples",
                "Read & Write Own Data",
                "Write Own Data, Read All Data",
                "Administrator Privileges",
                "Enable & Configure Custom User Data",
                "Set Up Admin Privileges",
                "Restricted News Feed",
                "Enable & Configure Custom User Data",
                "Create Authentication Trigger Function",
                "Set Up Restricted Permissions",
                "Create subscribeToUser Function",
                "Dynamic Collaboration",
                "Set Up Permissions",
                "Adding a Collaborator",
                "Security",
                "User Search",
                "Tiered Privileges",
                "Enable & Configure Custom User Data",
                "Create Authentication Trigger Function",
                "Create joinTeam Function",
                "Define Permissions"
            ],
            "paragraphs": "This page shows how to set up your Device Sync app's permissions for the\nfollowing common use cases: Read & Write Own Data Write Own Data, Read All Data Administrator Privileges Restricted News Feed Dynamic Collaboration Tiered Privileges You can get some of these permissions models up and running quickly with our\nDevice Sync Permissions Guide template apps. Each template app comes with a\nbackend and a Node.js demo client. The backend has all functions, permissions,\nand triggers set up as described here. The demo client connects to the backend\nand runs through a simple script to demonstrate how the backend works. You need an authenticated App Services CLI to use these templates. See  the CLI reference page  for login\ninstructions. Once logged in, you can use the  apps create  command with the  --template \nflag to instantiate a template. TEMPLATE_NAME  can be one of the following: App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: flex-sync-guides.add-collaborators  (corresponding to  Dynamic Collaboration ) flex-sync-guides.restricted-feed  (corresponding to  Restricted News Feed ) flex-sync-guides.tiered  (corresponding to  Tiered Privileges ) The examples here use  default roles . This means the same permissions rules\napply to all collections in your app. As your app grows in complexity, you might\nneed collection-specific roles that only apply to some collections and not\nothers. In particular, if a rule expression in a default role uses a \"queryable\nfield\" that doesn't exist on objects in a certain collection, you can override\nthe rules for that collection by providing a collection-specific role. See\n Role-based Permissions  for more information. In this case, users may read or write their own data, but not other users' data.\nConsider a notes app where the user wants to persist and share notes across\ntheir own devices but keep them  private to their user account. This strategy permits a user to create and edit a document if and only if that\ndocument's  owner_id  field equals the user's ID. To set up the \"Read & Write Own Data\" strategy, follow the general steps: Now you need to configure the permissions. Navigate to the  Rules \npage. Click the  Default Roles and Filters  button to edit the default\nroles. Use the template dropdown to select the template called \"Users can only\nread and write their own data\". This populates the rule expression box with the following: Note that the  document_filters.read  and  document_filters.write \nexpressions use the  owner_id  field we marked as \"queryable\" above. It also\nuses the  %%user   expansion  to read the requesting user's\nid. App Services replaces (or \"expands\") the expansion at evaluation time with\nthe corresponding value -- in this case, the user object. When evaluating permissions on a given document, App Services checks\n document_filters.read  and  document_filters.write  before the\ncorresponding top-level  read  or  write  expressions. For example, if\n document_filters.write  evaluates to false, write access is denied; no\nsubsequent rules are checked. However, if  document_filters.write  evaluates\nto true, App Services checks the top-level  write  expression. In this case,\n write  is  true , so write access is granted on the whole document. For\nmore information, see  Device Sync-Compatible Permissions . Log in to the Realm UI, and then click  Sync  in the left hand\npanel. Under  Sync Type , choose  Flexible . Set the toggle to enable  Development Mode . Select the cluster you want to sync. Define a Database Name : select  +Add a new\ndatabase  and type a name for the database Realm will use to store your synced\nobjects. You can name it anything you want. A common strategy would be to name\nthe database after the app you're making. Select Queryable Fields : type in  owner_id . This allows your\npermissions expressions (which you'll set next) to use the any fields\ncalled  owner_id . Skip  Advanced Configuration , and then click  Save\nChanges  to enable Sync. In this case, users can read all data, but write only their own data. Consider a\nrecipe app where users can read all recipes and add new recipes. The recipes\nthey add can be viewed by everyone using the app. Users may update or delete\nonly the recipes they contributed. To set up the \"Write Own Data, Read All Data\" strategy, follow these general\nsteps: Now you need to configure the permissions. Navigate to the  Rules \npage. Click the  Default Roles and Filters  button to edit the default\nroles. Use the template dropdown to select the template called \"Users can read\nall data but only write their own data\". This populates the rule expression box with the following: Note that the  document_filters.read  expression is set to  true ,\nindicating that no matter which user is authenticated, they can read all of the\ndata. The  document_filters.write  expression uses the  owner_id  field we\nmarked as \"queryable\" above and uses the  %%user   expansion  to match against the requesting user's id. App Services replaces\n(\"expands\") the expansion at evaluation time with the corresponding value -- in\nthis case, the user object. If and only if  document_filters.write  evaluates to true will App Services\ncheck the top-level  write  expression. When  document_filters.write \nevaluates to false, write access is denied regardless of what the top-level\n write  expression says. For more information, see\n Device Sync-Compatible Permissions . Log in to the Realm UI, and then click  Sync  in the left hand\npanel. Under  Sync Type , choose  Flexible . Set the toggle to enable  Development Mode . Select the cluster you want to sync. Define a Database Name : select  +Add a new\ndatabase  and type a name for the database Realm will use to store your synced\nobjects. You can name it anything you want. A common strategy would be to name\nthe database after the app you're making. Select Queryable Fields : type in  owner_id . This allows your\npermissions expressions (which you'll set next) to use the any fields\ncalled  owner_id . Skip  Advanced Configuration , and then click  Save\nChanges  to enable Sync. In this permission strategy, users with a specific \"administrator\" role can read\nand write any document. Users who do not have the specified role can only read\nand write their own data. To make this strategy work, you first need to define\nwhich users have administrator permissions. Atlas Apps allow you to associate custom user data in your cluster with\napplication users. Using this feature, you can create a document with a\nfield that indicates whether the user has administrative privileges.\nWhile there are many ways to set this up, one approach is to add a\nboolean property called  isGlobalAdmin , which is set to  true  for\nthose users with the elevated permissions. Another is to create a string\nfield called  role , in which one of the expected values might be\n\"admin\". In the following example, the custom user object we'll create has an  _id \nfield, which corresponds to the user's ID, and 3 additional fields:\n firstName ,  lastName , and  isGlobalAdmin : When using custom user data for permissions, never allow the client to write\nthe custom user data object. Doing so would allow any user to grant\nthemselves any permission. Instead, use  system functions  on the server side to update the custom user data object. Atlas App Services stores MongoDB documents that correspond to custom user data\nin a linked MongoDB Atlas cluster. When you configure custom user data for your\napplication, you specify the cluster, database, collection, and finally a\nUser ID field, which maps a custom user data document to an\nauthenticated user's ID. To enable Custom User Data in the App Services UI, follow these steps: Click  App Users  in the left hand panel. Select the  User Settings  tab and find the\n Custom User Data  section. Set the  Enable Custom User Data  toggle to  On . Specify the following values: Cluster Name : The name of a linked MongoDB cluster\nthat will contain the custom user data database. Database Name : The name of the MongoDB database that\nwill contain the custom user data collection. Collection Name : The name of the MongoDB collection that\nwill contain custom user data. Specify the User ID field.\nEvery document in the custom user data collection must have a field that\nmaps to a specific user. The field must be present in every\ndocument that maps to a user, and must contain the user's ID  as a string .\nWe recommend that you use the standard  _id  field to store the\nuser ID. MongoDB automatically places a constraint on the  _id  field,\nensuring uniqueness. If two documents in this collection contain the same user ID value,\nApp Services uses the first document that matches, which\nleads to unexpected results. Save and deploy the changes. The sync server caches custom user data for the duration of the session.\nAs a result, you must restart the sync session after updating custom user data\nto avoid a  compensating write error .\nTo restart the sync session, pause and resume all open Sync Sessions in the client application.\nFor more information on pausing and resuming sync from the client SDKs, see your preferred SDK: Pause or resume a Device Sync session - Flutter SDK Pause or resume a Device Sync session - Kotlin SDK Pause or resume a Device Sync session - Java SDK Pause or resume a Device Sync session - .Net SDK Pause or resume a Device Sync session - Node SDK Pause or resume a Device Sync session - React Native SDK Pause or resume a Device Sync session - Swift SDK Custom User Data After you have custom user data enabled, you can implement the Admin Privileges\nstrategy. To do so, follow these general steps: Now you need to configure the permissions. Navigate to the  Rules \npage. Click the  Default Roles and Filters  button to edit the default\nroles. Use the template dropdown to select the template called \"Users can read\nand write their own data, admins can read and write all data\". This populates the rule expression box with the following: Log in to the Realm UI, and then click  Sync  in the left hand\npanel. Under  Sync Type , choose  Flexible . Set the toggle to enable  Development Mode . Select the cluster you want to sync. Define a Database Name : select  +Add a new\ndatabase  and type a name for the database Realm will use to store your synced\nobjects. You can name it anything you want. A common strategy would be to name\nthe database after the app you're making. Select Queryable Fields : type in  owner_id . This allows your\npermissions expressions (which you'll set next) to use the any fields\ncalled  owner_id . Skip  Advanced Configuration , and then click  Save\nChanges  to enable Sync. This configuration has two default roles. The first defines the permissions\nfor an administrator. Note that the auto-generated expression assumes there\nis a boolean field in the custom user data document named  isGlobalAdmin .\nDepending on how you defined your custom user data document, you might need\nto change this. The second default role specifies the rules for all other users. The default\nis to restrict user access to read and write only their own data. You can\nchange either or both of these fields to  true , enabling users to read\nand/or write all data. See the previous sections to learn more about these\nstrategies. In this permission strategy, users can create their own content and subscribe\nto other creators' content. As with the Admin Privileges scenario, we will\nmake use of a Custom User Data collection to define which authors' content\na user is subscribed to read. Flexible Device Sync supports querying arrays, so we will create an array\nwithin a user data object. This array contains IDs of the authors that this user\nis authorized to \"follow\". We then set up a subscription that says, in essence,\n\"Give me all documents where I am the author, or the author's ID is in the array\nof authors in my custom user data.\" To use the backend template and get the demo client, run the following command: Next, in your terminal, go into the client directory, install the\ndependencies, and run the demo: Read the output on your console to see what the demo is doing. Custom User Data Configuration Bug Workaround:  Sometimes, the template\ngenerator does not copy the custom user data configuration to the new app\ncorrectly. You can fix this as follows: the  appservices apps create  command\nshould have output some JSON about the app you just created. From this JSON,\ncopy the \"url\" value (something like  https://services.cloud.mongodb.com/groups/... )\nand visit that URL in your browser. Log in if prompted. From the app dashboard,\nin the left-hand panel, click  App Users . Click  Custom\nUser Data . Ensure  Enable Custom User Data  is  ON . If it was not\non, turn it on and enter \"mongodb-atlas\", \"Item\", and \"User\" for\n Cluster Name ,  Database Name , and  Collection\nName , respectively. For  User ID Field , enter  _id . Hit\n Save  (or  Save Draft , then  deploy ). When a user subscribes or unsubscribes from an author, we update the array\nin the custom user data, but the changes don't take effect until the current\nsession is closed and a new session is started. In this example, we are creating an array in the Custom User Data. The\nsize of this array is not limited by App Services, but because the data is\nincluded in each request, we recommend keeping the size under 16KB, which\nis enough space for 1000 128-bit GUID-style user IDs. When using custom user data for permissions, never allow the client to write\nthe custom user data object. Doing so would allow any user to grant\nthemselves any permission. Instead, use  system functions  on the server side to update the custom user data object. To enable Custom User Data in the App Services UI, follow these steps: Click  App Users  in the left hand panel. Select the  User Settings  tab and find the\n Custom User Data  section. Set the  Enable Custom User Data  toggle to  On . Specify the following values: Cluster Name : The name of a linked MongoDB cluster\nthat will contain the custom user data database. Database Name : The name of the MongoDB database that\nwill contain the custom user data collection. Collection Name : The name of the MongoDB collection that\nwill contain custom user data. Specify the User ID field.\nEvery document in the custom user data collection must have a field that\nmaps to a specific user. The field must be present in every\ndocument that maps to a user, and must contain the user's ID  as a string .\nWe recommend that you use the standard  _id  field to store the\nuser ID. MongoDB automatically places a constraint on the  _id  field,\nensuring uniqueness. If two documents in this collection contain the same user ID value,\nApp Services uses the first document that matches, which\nleads to unexpected results. Save and deploy the changes. The sync server caches custom user data for the duration of the session.\nAs a result, you must restart the sync session after updating custom user data\nto avoid a  compensating write error .\nTo restart the sync session, pause and resume all open Sync Sessions in the client application.\nFor more information on pausing and resuming sync from the client SDKs, see your preferred SDK: Pause or resume a Device Sync session - Flutter SDK Pause or resume a Device Sync session - Kotlin SDK Pause or resume a Device Sync session - Java SDK Pause or resume a Device Sync session - .Net SDK Pause or resume a Device Sync session - Node SDK Pause or resume a Device Sync session - React Native SDK Pause or resume a Device Sync session - Swift SDK Custom User Data We need to create a authentication trigger function that creates a custom user\nobject when a user authenticates for the first time. To do so, follow these steps: Log in to the Realm UI, and then click  Triggers  in the left\nhand panel. Click the  Add a Trigger  button. Set the  Trigger Type  toggle to  Authentication . Under Trigger Details, specify the following values: Name : \"onUserCreated\" Enbaled : Switch toggle to \"On\" Action Type : Select \"Create\" Provider(s) : Select \"Email/Password\", or whichever authentication\nprovider(s) you are using Select an Event Type : Select \"Function\" Function : Select \"+New Function\", and then: Function Name : \"onUserCreated\" Function : Replace the placeholder text with the following\nfunction: In the custom user data object, create an array that holds the _id values of\neach author the user is following. In this example, we'll call it \"subscriptions\".\nOur user data object looks like the following, where Lily Realmster\n( \"_id\": \"1234\" ) is subscribed to all documents written by users \"456\"\nand \"789\": You can now implement the Restricted Privileges strategy. To do so, follow these\ngeneral steps: Now you need to configure the permissions. Navigate to the  Rules \npage. Click the  Default Roles and Filters  button to edit the default\nroles. Use the template dropdown to select the template called \"Users can only\nread and write their own data\". This populates the rule expression box with the\nfollowing, which is not  exactly what we want , but provides most of the logic\nfor us: Note that a user can currently read only their own documents ( \"read\":\n{\"owner_id\": \"%%user.id\"} ). We can change this to include documents whose\nauthors have IDs in the user's \"subscribedTo\" array. To do so, we use the\n $in  operator. The expression looks like this: Update the rule expression box with this new logic and save the changes. Note: we changed the  document_filters.read  expression to only mention the\nauthors in the subscribedTo array, but users still have \"read\" access to their\nown documents because of the  document_filters.read  expression. Whenever\nwrite access is granted, read access is implicitly granted. Log in to the Realm UI, and then click  Sync  in the left hand\npanel. Under  Sync Type , choose  Flexible . Set the toggle to enable  Development Mode . Select the cluster you want to sync. Define a Database Name : select  +Add a new\ndatabase  and type a name for the database Realm will use to store your synced\nobjects. You can name it anything you want. A common strategy would be to name\nthe database after the app you're making. Select Queryable Fields : type in  owner_id . This allows your\npermissions expressions (which you'll set next) to use the any fields\ncalled  owner_id . Skip  Advanced Configuration , and then click  Save\nChanges  to enable Sync. We need a function that subscribes one user to another. Our permissions are set\nup to consider a subscriber \"subscribed\" to an author if the author's user ID is\nin the subscriber's custom user data \"subscribedTo\" array. The \"subscribeToUser\"\nfunction takes a user's email address, finds the corresponding user ID in the\ncustom user data collection, and adds the ID to the requesting user's\n\"subscribedTo\" array. Note that this function will run under System authentication and writes to the\ncustom user data. When using custom user data for permissions, clients must\nnever be allowed to directly edit custom user data. Otherwise, any user could\ngrant themselves any permissions. Instead, modify user data on the backend with\na System function. The function should do whatever checks necessary to ensure\nthe user's request for permissions is valid. To create the function, follow these steps: Log in to the Realm UI, and then click  Functions  in the\nleft hand panel. Click the  Create New Function  button. Specify the following values: Name : \"subscribeToUser\" Authentication : \"System\" Switch to the  Function Editor  tab and replace the\nplaceholder text with the following code: In the Dynamic Collaboration strategy, users can create documents and add other\nusers as editors of that document. Like the Read & Write Own Data strategy, this strategy permits a user to create\nand edit a document if that document's  owner_id  field equals the user's ID.\nAdditionally, a user may edit the document if the document's  collaborators \narray field contains their ID. To use the backend template and get the demo client, run the following command: Next, go into the client directory, install the dependencies, and run the demo: Read the output on your console to see what the demo is doing. To implement this strategy, follow these general steps: In the  Select Queryable Fields  field, type in  collaborators  as\nwell. This will be the field that stores the IDs of users who may also read and\nwrite the document. Now you need to configure the permissions. Under  Define Permissions ,\nuse the template dropdown to select the \"Custom (start from scratch)\". Paste the\nfollowing into the rule expression box: The \"write\" and \"read\" expressions are identical. Take a look at one of them.\n $or  takes an array of options. We have two possible conditions where a user\nmay write the document: Generally speaking, when a user is granted write permission, that user\nautomatically gets read permission. However, we can't omit the\n document_filters.read  expression because App Services requires both\n document_filters.read  and  document_filters.write  to be defined for the\nrole to be  Sync compatible . Log in to the Realm UI, and then click  Sync  in the left hand\npanel. Under  Sync Type , choose  Flexible . Set the toggle to enable  Development Mode . Select the cluster you want to sync. Define a Database Name : select  +Add a new\ndatabase  and type a name for the database Realm will use to store your synced\nobjects. You can name it anything you want. A common strategy would be to name\nthe database after the app you're making. Select Queryable Fields : type in  owner_id . This allows your\npermissions expressions (which you'll set next) to use the any fields\ncalled  owner_id . Skip  Advanced Configuration , and then click  Save\nChanges  to enable Sync. The  owner_id  field of the document equals the user's ID The  collaborators  array field of the document contains the user's ID. A user can grant write access on their document to another user by adding that\nuser's ID to the  collaborators  array field on their document. This can be\ndone on the client side. We don't recommend using this model for highly sensitive data. This model uses the permissions system to keep documents private between the\ndocument creator and the collaborators they add to the document. However, if a\nuser has write access to a document, they may write to any field of the\ndocument. Consequently, this strategy  allows collaborators to add other\ncollaborators . It would also allow a collaborator to edit the  owner_id \nfield. Field-level permissions have to be boolean literals for  sync\ncompatibility , so we can't limit writes to these fields\nto specific users. How exactly to get the other user's ID depends on the details of your app. For\nexample, when a user wants to add another user to a document, you might have a\nsearch box that accepts an email address. If the given email address corresponds\nto another user, the client can add that user's ID to the document's\n collaborators  array. Realm has no built-in way to search users. Generally, the flow for searching\nusers is as follows: To create an authentication trigger, follow these steps: Next, create a system function called  findUser : In the  Function Editor , paste the following code and save: From your client, you can now call this function. The function's only argument\nis an email address string. If the email corresponds to a user, the function\nreturns the user's ID. Otherwise, it returns null. Set up an authentication trigger to create a user document when a user\nregisters. The user document contains information you'll use to look up later,\nsuch as the user's email address. Create a function that queries the user data collection for a user. Call the function from the client side when the user wants to find another user. Log in to the Realm UI, and then click  Triggers  in the left\nhand panel. Click the  Add a Trigger  button. Set the  Trigger Type  toggle to  Authentication . Under Trigger Details, specify the following values: Name : \"onUserCreated\" Enbaled : Switch toggle to \"On\" Action Type : Select \"Create\" Provider(s) : Select \"Email/Password\", or whichever authentication\nprovider(s) you are using Select an Event Type : Select \"Function\" Function : Select \"+New Function\", and then: Function Name : \"onUserCreated\" Function : Replace the placeholder text with the following\nfunction: Log in to the Realm UI, go to your Realm app, and then click  Functions  in the left\nhand panel. Click the  Create New Function  button. Provide a descriptive  Name  for your function. Authentication : Select  System . This allows your function to bypass permissions on your collections. Log Function Arguments : Leave it  off . Authorization : Can Evaluate : Leave it blank. Private : Leave it  off . Click  Save . This brings you to the  Function Editor , where you can now enter some to run. This configuration allows  anyone  to call this function. As a system\nfunction, this function bypasses access rules. Assume any client calling this\nfunction has malicious intent. In this permission strategy, we'll add special  roles  as well as rules. There\nare two roles:  a team member  and  a team administrator . The rules are as\nfollows: To make this work, we need to do the following: To use the backend template and get the demo client, run the following command: Next, in your terminal, go into the client directory, install the\ndependencies, and run the demo: Read the output on your console to see what the demo is doing. Custom User Data Configuration Bug Workaround:  Sometimes, the template\ngenerator does not copy the custom user data configuration to the new app\ncorrectly. You can fix this as follows: the  appservices apps create  command\nshould have output some JSON about the app you just created. From this JSON,\ncopy the \"url\" value (something like  https://services.cloud.mongodb.com/groups/... )\nand visit that URL in your browser. Log in if prompted. From the app dashboard,\nin the left-hand panel, click  App Users . Click  Custom\nUser Data . Ensure  Enable Custom User Data  is  ON . If it was not\non, turn it on and enter \"mongodb-atlas\", \"Item\", and \"User\" for\n Cluster Name ,  Database Name , and  Collection\nName , respectively. For  User ID Field , enter  _id . Hit\n Save  (or  Save Draft , then  deploy ). Each user is a member of a team. A user can read and write their own documents. All members of the team can read all documents created by team members. Each team has a team administrator, who has read & write permissions on every\nteam document. Enable custom user data Create a trigger function to create a new custom user data object for each\nnew user Create a function to add a user to a team Define the permissions When using custom user data for permissions, never allow the client to write\nthe custom user data object. Doing so would allow any user to grant\nthemselves any permission. Instead, use  system functions  on the server side to update the custom user data object. To enable Custom User Data in the App Services UI, follow these steps: Click  App Users  in the left hand panel. Select the  User Settings  tab and find the\n Custom User Data  section. Set the  Enable Custom User Data  toggle to  On . Specify the following values: Cluster Name : The name of a linked MongoDB cluster\nthat will contain the custom user data database. Database Name : The name of the MongoDB database that\nwill contain the custom user data collection. Collection Name : The name of the MongoDB collection that\nwill contain custom user data. Specify the User ID field.\nEvery document in the custom user data collection must have a field that\nmaps to a specific user. The field must be present in every\ndocument that maps to a user, and must contain the user's ID  as a string .\nWe recommend that you use the standard  _id  field to store the\nuser ID. MongoDB automatically places a constraint on the  _id  field,\nensuring uniqueness. If two documents in this collection contain the same user ID value,\nApp Services uses the first document that matches, which\nleads to unexpected results. Save and deploy the changes. The sync server caches custom user data for the duration of the session.\nAs a result, you must restart the sync session after updating custom user data\nto avoid a  compensating write error .\nTo restart the sync session, pause and resume all open Sync Sessions in the client application.\nFor more information on pausing and resuming sync from the client SDKs, see your preferred SDK: Pause or resume a Device Sync session - Flutter SDK Pause or resume a Device Sync session - Kotlin SDK Pause or resume a Device Sync session - Java SDK Pause or resume a Device Sync session - .Net SDK Pause or resume a Device Sync session - Node SDK Pause or resume a Device Sync session - React Native SDK Pause or resume a Device Sync session - Swift SDK We need to create a authentication trigger function that creates a custom user\nobject when a user authenticates for the first time. To do so, follow these steps: Log in to the Realm UI, and then click  Triggers  in the left\nhand panel. Click the  Add a Trigger  button. Set the  Trigger Type  toggle to  Authentication . Under Trigger Details, specify the following values: Name : \"onUserCreated\" Enbaled : Switch toggle to \"On\" Action Type : Select \"Create\" Provider(s) : Select \"Email/Password\", or whichever authentication\nprovider(s) you are using Select an Event Type : Select \"Function\" Function : Select \"+New Function\", and then: Function Name : \"onUserCreated\" Function : Replace the placeholder text with the following\nfunction: We now need a function that adds a user to a team. Note that this function\nwill run under System authentication and writes to the custom user data. It\ndoes not perform an upsert because the user custom data was created when the user\nsuccessfully authenticated for the first time. To create the function, follow these steps: Log in to the Realm UI, and then click  Functions  in the\nleft hand panel. Click the  Create New Function  button. Specify the following values: Name : \"joinTeam\" Authentication : \"System\" Switch to the  Function Editor  tab and replace the\nplaceholder text with the following code: The following permissions specify two roles: teamAdmin  applies only when the user's custom data has\n isTeamAdmin: true . If so, the user can read and write all documents where\nthe document's  team  value matches the user's  team  value. teamMember  applies to every user. The user can write their own\ndocuments and read all documents where the document's  team  value matches\nthe user's  team  value. This strategy can be expanded to support a \"globalAdmin\" role. The global\nadmin would have read & read permissions on any doc created in any team. Authentication Triggers Custom User Data",
            "code": [
                {
                    "lang": "bash",
                    "value": "appservices apps create --template=TEMPLATE_NAME"
                },
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"owner-read-write\",\n  \"apply_when\": {},\n  \"document_filters\": {\n    \"read\": { \"owner_id\": \"%%user.id\" },\n    \"write\": { \"owner_id\": \"%%user.id\" }\n  },\n  \"read\": true,\n  \"write\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"owner-write\",\n  \"apply_when\": {},\n  \"document_filters\": {\n    \"read\": true,\n    \"write\": { \"owner_id\": \"%%user.id\" }\n  },\n  \"read\": true,\n  \"write\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"_id\" : \"1234\",\n   \"firstName\": \"Lily\",\n   \"lastName\": \"Realmster\",\n   \"isGlobalAdmin\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "[\n   {\n     \"name\": \"admin\",\n     \"apply_when\": {\n       \"%%user.custom_data.isGlobalAdmin\": true\n     },\n     \"document_filters\": {\n       \"read\": true,\n       \"write\": true\n     },\n     \"read\": true,\n     \"write\": true\n   },\n   {\n     \"name\": \"user\",\n     \"apply_when\": {},\n     \"document_filters\": {\n       \"read\": { \"owner_id\": \"%%user.id\" },\n       \"write\": { \"owner_id\": \"%%user.id\" }\n     },\n     \"read\": true,\n     \"write\": true\n   }\n]"
                },
                {
                    "lang": "bash",
                    "value": "appservices apps create --name=restricted-feed --template=flex-sync-guides.restricted-feed"
                },
                {
                    "lang": null,
                    "value": "cd restricted-feed/frontend/flex-sync-guides.restricted-feed/\nnpm install\nnpm run demo"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function(authEvent) {\n    const user = authEvent.user;\n    const collection = context.services.get(\"mongodb-atlas\").db(\"Item\").collection(\"User\");\n    const newDoc = {\n      _id: user.id,\n      email: user.data.email, // Useful for looking up user IDs by email later - assuming email/password auth is used\n      team: \"\", // Used for tiered privileges\n      isTeamAdmin: false, // Used for tiered privileges\n      isGlobalAdmin: false, // Used for admin privileges\n      subscribedTo: [], // Used for restricted feed\n    };\n    return collection.insertOne(newDoc);\n};"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"_id\" : \"1234\",\n   \"firstName\": \"Lily\",\n   \"lastName\": \"Realmster\",\n   \"user.custom_data.subscribedTo\": [\n     \"456\",\n     \"789\"\n   ]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"owner-read-write\",\n  \"apply_when\": {},\n  \"document_filters\": {\n    \"read\": { \"owner_id\": \"%%user.id\" },\n    \"write\": { \"owner_id\": \"%%user.id\" }\n  },\n  \"read\": true,\n  \"write\": true\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"owner-read-write\",\n  \"apply_when\": {},\n  \"document_filters\": {\n    \"read\": {\n      \"owner_id\": {\n        \"$in\": \"%%user.custom_data.subscribedTo\"\n      }\n    },\n    \"write\": { \"owner_id\": \"%%user.id\" }\n  },\n  \"read\": true,\n  \"write\": true\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function(email) {\n  const collection = context.services.get(\"mongodb-atlas\").db(\"Item\").collection(\"User\");\n\n  // Look up the author object to get the user id from an email\n  const author = await collection.findOne({email});\n  if (author == null) {\n    return {error: `Author ${email} not found`};\n  }\n\n  // Whoever called this function is the would-be subscriber\n  const subscriber = context.user;\n\n  try {\n    return await collection.updateOne(\n      {_id: subscriber.id},\n      {$addToSet: {\n          subscribedTo: author._id,\n        }\n      });\n  } catch (error) {\n    return {error: error.toString()};\n  }\n};"
                },
                {
                    "lang": "bash",
                    "value": "appservices apps create --name=add-collaborators --template=flex-sync-guides.add-collaborators"
                },
                {
                    "lang": null,
                    "value": "cd add-collaborators/frontend/flex-sync-guides.add-collaborators/\nnpm install\nnpm run demo"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"collaborator\",\n  \"apply_when\": {},\n  \"document_filters\": {\n    \"read\": {\n      \"$or\": [\n        {\n          \"owner_id\": \"%%user.id\"\n        },\n        {\n          \"collaborators\": \"%%user.id\"\n        }\n      ]\n    },\n    \"write\": {\n      \"$or\": [\n        {\n          \"owner_id\": \"%%user.id\"\n        },\n        {\n          \"collaborators\": \"%%user.id\"\n        }\n      ]\n    }\n  },\n  \"read\": true,\n  \"write\": true\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function(email) {\n  const collection = context.services.get(\"mongodb-atlas\")\n    .db(\"Item\").collection(\"User\");\n  const filter = {\n    email,\n  };\n  // Search for the user by email\n  const result = await collection.findOne(filter);\n  // Return corresponding user id or null\n  return result != null ? result._id : null;\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function(authEvent) {\n    const user = authEvent.user;\n    const collection = context.services.get(\"mongodb-atlas\").db(\"Item\").collection(\"User\");\n    const newDoc = {\n      _id: user.id,\n      email: user.data.email, // Useful for looking up user IDs by email later - assuming email/password auth is used\n      team: \"\", // Used for tiered privileges\n      isTeamAdmin: false, // Used for tiered privileges\n      isGlobalAdmin: false, // Used for admin privileges\n      subscribedTo: [], // Used for restricted feed\n    };\n    return collection.insertOne(newDoc);\n};"
                },
                {
                    "lang": "bash",
                    "value": "appservices apps create --name=tiered --template=flex-sync-guides.tiered"
                },
                {
                    "lang": null,
                    "value": "cd tiered/frontend/flex-sync-guides.tiered/\nnpm install\nnpm run demo"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function(authEvent) {\n    const user = authEvent.user;\n    const collection = context.services.get(\"mongodb-atlas\").db(\"Item\").collection(\"User\");\n    const newDoc = {\n      _id: user.id,\n      email: user.data.email, // Useful for looking up user IDs by email later - assuming email/password auth is used\n      team: \"\", // Used for tiered privileges\n      isTeamAdmin: false, // Used for tiered privileges\n      isGlobalAdmin: false, // Used for admin privileges\n      subscribedTo: [], // Used for restricted feed\n    };\n    return collection.insertOne(newDoc);\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function(userId, teamName) {\n  const collection = context.services.get(\"mongodb-atlas\")\n    .db(\"Item\").collection(\"User\");\n\n  const filter = { _id: userId };\n  const update = { $set: { team: teamName }};\n  const options = { upsert: false };\n\n  return collection.updateOne(filter, update, options);\n};"
                },
                {
                    "lang": "json",
                    "value": "[\n   {\n     \"name\": \"admin\",\n     \"apply_when\": {\n       \"%%user.custom_data.isTeamAdmin\": true\n     },\n     \"document_filter\": {\n       \"read\": {\n         \"team\": \"%%user.custom_data.team\"\n       },\n       \"write\": {\n         \"team\": \"%%user.custom_data.team\"\n       }\n     },\n     \"read\": true,\n     \"write\": true\n   },\n   {\n     \"name\": \"user\",\n     \"apply_when\": {},\n     \"document_filters\": {\n       \"read\": {\n         \"team\": \"%%user.custom_data.team\"\n       },\n       \"write\": {\n         \"owner_id\": \"%%user.id\"\n       }\n     },\n     \"read\": true,\n     \"write\": true\n   }\n]"
                }
            ],
            "preview": "This page shows how to set up your Device Sync app's permissions for the\nfollowing common use cases:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/go-to-production/production-checklist",
            "title": "Device Sync Production Checklist",
            "headings": [
                "Atlas Cluster Configuration",
                "App Services Configuration",
                "Client Application Code",
                "Data Model and Schema",
                "Performance Best Practices"
            ],
            "paragraphs": "We recommend following these best practices to ensure optimal\nperformance and a smooth user experience. For related information about service limitations, refer to\n Service Limitations . Production Apps should use, at minimum, an  M10  dedicated cluster.\nDepending on your use case, you may need to upgrade to a higher tier\nfor optimal performance. If you use a shared cluster, like an  M0 ,  M2 , or  M5 , you\nmay experience performance issues due to limited resources and\ncontention with other users. If you upgrade from a shared tier to a\ndedicated tier after you go to production, you will need to\n terminate Device Sync  and reset or\nre-install all client applications. Device Sync requires access to a  time-based oplog  for your synced cluster. All new Atlas clusters\nprovide this by default. For best results, keep 48 hours of oplog for\na cluster using Device Sync. Use the latest version of MongoDB, if possible. Some Device Sync\noptimizations use new MongoDB version features and enhancements. If your cluster runs on hardware that uses  NVMe storage ,\nit must use MongoDB version 6.0 or later for Device Sync production\napplications. A schema in App Services is  not the same  as  MongoDB's\nbuilt-in schema validation . Device Sync\nmay interact with your cluster in a way that is incompatible with a\nbuilt-in schema. If you use schema validation on your cluster, you should either\ndisable it in favor of App Services schemas or manage the two schema\nvalidation layers so that they're compatible. For more information,\nsee  App Services Schema vs Built-In Schema Validation . Use a  local deployment model  when building a\nDevice Sync application. Configure your App and MongoDB data source\nto run within the same geographic region and cloud provider. The App Services backend uses a history of changes to underlying\ndata to synchronize clients. Configure a  client maximum offline time  to control the number of days of\nhistory stored by your App. Clients that have not synchronized in\nmore than that number of days must perform a client reset the next\ntime they connect to the backend. To recover from a serious error conditions where the client and server\nhistories diverge, you should define a  client reset \nhandler when you open each synced realm with an SDK. Once you've enabled Sync, there are limitations on how you can modify your\ndata model. Specifically, Sync does not support  breaking changes  to your object types such as changing\na given property's type. If you want to make a breaking change you\nneed to terminate and re-enable Sync with the updated data models. If you create or modify Device Sync documents using another tool, such as the\n mongosh shell  or MongoDB Compass,\nbe sure the documents validate against the App Services Schema for the collection.\nFor more information, see  Unsynced Documents . Measure performance and identify issues in a scaled-up production\ndeployment with  Sync Production Load Testing . When writing large amounts of data, consider using multiple small\nwrite transactions instead of a single larger transaction. Depending\non your Atlas cluster version and configuration, write transactions\ngreater than 16MB may be rejected by MongoDB and cause the sync\noperation to fail.",
            "code": [],
            "preview": "We recommend following these best practices to ensure optimal\nperformance and a smooth user experience.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/go-to-production/compact-disk",
            "title": "Compact an Atlas Volume",
            "headings": [
                "Overview",
                "Steps to Compact and Reclaim Disk Space",
                "An Alternate Approach"
            ],
            "paragraphs": "While Realm  data compaction  defragments the storage\nspace the realm file uses on a  client , and also reduces the size of the\n __realm_sync  database on Atlas clusters, it does not reduce the overall size of\nthe Atlas cluster. Compacting the data on an Atlas volume involves several\nadditional steps. You should use the following guidance for reducing Atlas disk space usage: Enable Compaction in App Services Before reclaiming volume space, be sure that you have enabled Compaction\non your Atlas App Services app. You set this by setting the Max Offline Time\nsetting in the Atlas UI. For more information, see\n Optimize Sync Storage in Atlas . If you are enabling compaction for the first time, you will need to wait for\nthe compaction to happen before continuing. You can file a ticket with\nsupport if you want to ensure compaction has completed. Perform a Rolling Resync Perform a rolling resync of your Atlas cluster, a process explained in\n Resync a Member of a Replica Set . The commands to do this are not available with Atlas. Instead, you can\nfile a ticket with support to have this process done for you. A final approach you can take for reclaiming some of your Atlas volume space\nis to run the  compact()  command. You run it on one of the secondary nodes\nand then the other secondary node(s). Finally, you perform a failover on the\nprimary mode and then run  compact() . For more information on this approach,\nsupport-enabled customers can refer to  How to use the compact() command in\nAtlas . Contact\n Support  for access. The  compact()  command may not reduce the volume size as much as\nperforming a rolling resync.",
            "code": [],
            "preview": "While Realm data compaction defragments the storage\nspace the realm file uses on a client, and also reduces the size of the\n__realm_sync database on Atlas clusters, it does not reduce the overall size of\nthe Atlas cluster. Compacting the data on an Atlas volume involves several\nadditional steps.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/go-to-production/production-load-testing",
            "title": "Sync Production Load Testing",
            "headings": [
                "Overview",
                "Recommended Strategy"
            ],
            "paragraphs": "To  load test  is to simulate the requests that users will put on your\napp once your app is in production. The goal is to measure performance\nand to identify and resolve issues before they happen to a real user. Use the Node.js SDK to simulate a common single user's workload for your\nmobile app. For example, you can use the SDK to: The exact details of the simulation depend on your app's actual\nworkload. For instance, you might open realms that already have\ninitialized sync data on the server side to simulate read use cases. You\nmight open different realms to simulate write workloads. These\nrealms might be a mix of shared or private per-user realms\ndepending on your use case. You can use a shared realm to test the\nconflict resolution speed and discover any problems. So, consider how\nyou use Realm Database in your actual app to design your\nsimulation. You can then deploy the single-user simulation to hundreds or thousands\nof concurrent nodes in order to test a real multi-user workload. For example: There are a variety of cloud-based Kubernetes-as-a-service offerings\nthat you can use. Open a set number of synced realms. Make a certain number of writes to the synced realms. Simulate backend server calls (for example, by calling Atlas Functions). The Node.js SDK uses the same underlying C++ database and sync client\nas the mobile (iOS, Android, .NET, etc.) SDKs. Therefore, you can\nexercise the same code paths as in your production mobile app while\nbenefiting from the headless, server environment of Node.js. Package the app in a Docker container. Deploy the container as part of a Kubernetes job. Adjust the parameters of the job to match it to a real production workload. Measure the time it takes to complete the Kubernetes job to calculate overall performance.",
            "code": [],
            "preview": "To load test is to simulate the requests that users will put on your\napp once your app is in production. The goal is to measure performance\nand to identify and resolve issues before they happen to a real user.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/data-model/create-a-schema",
            "title": "Create a Data Model",
            "headings": [
                "Overview",
                "Create an App Services Schema from a Realm Object Model",
                "Enable Development Mode Sync",
                "Edit Your Realm Object Schema",
                "Update Your App Services Schema with the Realm Object Schema Changes",
                "Create a Realm Object Schema from an App Services Schema",
                "Define an App Services Schema",
                "View the Realm Object Schema",
                "Open a Realm with the Realm Object Schema",
                "Further Reading"
            ],
            "paragraphs": "You can create a schema for your App in one of two ways: Create an App Services Schema from a Realm Object Schema :\nAlternatively, if you are developing mobile-first and do not already have\ndata in your Atlas cluster, you can translate your Realm Object Schema into\na schema for use with Atlas. Regardless of the approach that you take,\nwhen you configure both your Atlas cluster and Mobile application to use\nthe respective data model, changes to the data model between the server\nand client are auto-updated. Create a Realm Object Schema from an Atlas App Services Schema :\nIf you have data in your\n MongoDB Atlas \ncluster already, MongoDB generates a schema by sampling your data.\nAtlas App Services can then translate that schema into a Realm Object Schema\nto use in your mobile application with the Realm SDK. Your App must have at least one  linked data source  in order to create a schema. You cannot use Device Sync with a  serverless instance  or  Federated database instance . First,   enable Development Mode . You can alter or define a Realm Object Schema through your mobile client SDK.\nChanges to your Realm Object Schema are only allowed when\n Development Mode  is on in the App Services UI. App Services will\nreflect these changes to your Realm Object Schema in your App Services Schema used for\nAtlas. To enable Development Mode, click the slider to the right of\n Development Mode . As you continue to develop your application, you will need to modify\nyour data model with it to enforce different data validation rules\nbased on those changes. While  Development Mode  is on, you\ncan edit your Realm Object Schema in your client code. Data Validation\noccurs when  Development Mode  is off, so App Services does\nnot accept changes to your Realm Object Schema while\n Development Mode  is not on. Refer to the Realm SDK-specific documentation for creating Realm Object Schemas. To work with Atlas Device Sync, your data model  must  have a primary key field\ncalled  _id .  _id  can be of type  string ,  int , or\n objectId . A group is developing a social media application. When the group\nfirst developed their application, a user's birthday was a required\nfield of the User's data model. However, due to privacy concerns\nover the amount of user data that is stored, management creates a\nnew requirement to make the user's birthday field an optional\nfield. Application developers turn on  Development Mode \nin the App Services UI and then edit their user model within\ntheir client code. Realm C++ SDK Realm Flutter SDK Realm Java SDK Realm Kotlin SDK Realm .NET SDK Realm Node.js SDK Realm React Native SDK Realm Swift SDK While  Development Mode  is on, App Services doesn't validate\nwrites against your data model, allowing you to freely update your Realm\nObject Model. When you turn off  Development Mode , MongoDB\nApp Services automatically updates your App Services Schema and starts to\nenforce data validation for your Atlas cluster based on it. In the  Sync  screen, turn off  Development Mode  by\nclicking the slider next to  Development Mode . The UI indicates\nthat Development Mode has been turned off. To make future data model updates from your mobile client code, you\ncan follow this procedure again. Refer to the Realm SDK-specific documentation to use the generated Realm Object Schema. Your app must have at least one  linked data source  in order to define an App Services data model. You cannot use sync with a  serverless instance  or  Federated database instance . To get started, ensure you have an App Services Schema defined. App Services\nwill translate this App Services Schema into a Realm Object Schema to be\nconfigured and utilized in your mobile application. To work with Atlas Device Sync, your data model  must  have a primary key field\ncalled  _id .  _id  can be of type  string ,  int ,  uuid ,  ObjectId , or\n objectId . To work with Atlas Device Sync, your schema object  type names \ncannot exceed 57 UTF-8 characters. To learn how to define a schema for a collection in the synced cluster, see\n Enforce a Schema . The Realm Object Schema defines and validates your data in your mobile\nclient application. To view your Realm Object Schema, navigate to the\n Realm SDKs  page and then click the  Realm Object Models \ntab. On this page, you can view your App Services Schema as a\ngenerated Realm Object Schema in your language of choice. You can use the generated Realm Object Schema in your\nclient application. In order to begin enforcing data validation with\nyour data model, you can open a realm with the Realm Object Schema.\nThis will prevent improper data from entering your database from your\nmobile client. Click  Copy  on the right-hand side of the\nRealm Object Schema for the Object Model you want to integrate into\nyour mobile application code. This will copy the Realm Object Schema\ncode for the SDK of your choice into your clipboard. Open your mobile\napplication code in your IDE and paste the Realm Object Schema code in. Realm Flutter SDK Realm Java SDK Realm Kotlin SDK Realm .NET SDK Realm Node.js SDK Realm React Native SDK Realm Swift SDK To learn more about how these schemas map to each other,\nrefer to  Data Model Mapping . To update an existing Sync data model, refer to  Update Your Data Model .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const realmObjectModel = {\n  name: 'User',\n  properties: {\n    _id: 'objectId',\n    _partition: 'string',\n    name: 'string',\n    // developers set optional: true to adhere to the new requirement\n    birthday: {type: 'date', optional: true},\n  },\n  primaryKey: '_id'\n};\n\nRealm.open({schema: realmObjectModel, sync: {/*...*/}})\n  .then(realm => {\n    // ... use the realm instance to read and modify data\n  })"
                },
                {
                    "lang": "javascript",
                    "value": "const UserSchema = { // your copied and pasted Realm Object Schema\n  name: 'User',\n  properties: {\n    _id: 'objectId',\n    _partition: 'string',\n    name: 'string',\n    birthday: 'date'\n  },\n  primaryKey: '_id'\n};\n// Initialize a realm with your Realm Object Schema\nRealm.open({schema: UserSchema, sync: { /* ... */ }})\n  .then(realm => {\n    // ... use the realm instance to read and modify data\n  })"
                }
            ],
            "preview": "You can create a schema for your App in one of two ways:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/data-model/sync-schema-overview",
            "title": "Sync Data Model Overview",
            "headings": [
                "Overview",
                "Create a Data Model",
                "Update a Data Model",
                "Data Model Mapping"
            ],
            "paragraphs": "To use Atlas Device Sync you must define your data model in two formats: The App Services Schema and Realm Object Schema must be consistent with each other\nto sync data using Device Sync. You can also sync data between clients using different Realm SDKs,\nas long as they implement the same Realm Object Schema. These Realm Object Schemas\nmust all be consistent with the App Services Schema as well. For example, you have an iOS app and an Android app with their Realm Object Schemas\ndefined using the Realm Swift SDK and the Realm Kotlin SDK, respectively.\nBoth these mobile applications sync data using the same App Services App with Device Sync.\nThe Swift SDK schema and the Kotlin SDK schema must both be consistent\nwith the same App Services Schema. App Services Schema : A server-side schema which defines your\ndata in BSON. Device Sync uses the App Services Schema to convert your data\nto MongoDB documents and enforce validation and synchronize data between client\ndevices and Atlas. Realm Object Schema : Client-side schema of data defined using the Realm SDKs.\nEach Realm SDK defines the Realm Object Schema in its own language-specific way.\nThe Realm SDKs use this schema to store data in Realm Database\nand synchronize data with Device Sync. You can create your Device Sync data model in a few ways. If you already have data in Atlas, you can generate an App Services Schema\nby sampling that data. You can then generate a Realm Object Schema for each Realm SDK. If you prefer to develop your Realm Object Schema using the Realm SDKs first,\nuse Development Mode. When you use Development Mode, Device Sync automatically\ngenerates an App Services Schema when you sync data from a client using a Realm SDK. For more information on these approaches to modeling data,\nrefer to  Create a Data Model . When developing an application using Atlas Device Sync, you may need to make changes\nto your data model. This means you also need to update your App Services Schema\nand Realm Object Schema. For more information on how to perform the different types of data model changes,\nrefer to  Update a Data Model . To learn more about how the Realm Object Schemas map to the App Services Schemas\nwhen using Development Mode, refer to  Data Model Mapping .",
            "code": [],
            "preview": "To use Atlas Device Sync you must define your data model in two formats:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/data-model/generate-sdk-object-models",
            "title": "Generate SDK Object Models",
            "headings": [
                "Overview",
                "Procedure",
                "Navigate to the SDKs Screen",
                "Select the Data Models Tab",
                "Copy the Data Models",
                "Generate Data Models Based On Your Schema",
                "Confirm Options for Generating Data Models",
                "Use Your Data Model"
            ],
            "paragraphs": "Atlas App Services can automatically translate your server-side document schemas to a Realm Object Schema that will\nwork with your SDK of choice. You can generate the object models using either the App Services UI or CLI. To begin, navigate to the\n SDKs  screen under the  Build  section in the\nleft navigation menu. Here you will find the tools to make your app\ncompatible with different SDKs. In the  Data Models  tab, you can view the generated\nobject models for your  SDK. Use the  Language  dropdown\nto choose the language you are using for development.\nThe page updated with a generated version of each of your object types\ntranslated to the language you choose. Click on the  Copy All Data Models  button to copy the code\nfor all object models. If prompted, choose whether you want to include\nthe import statements (the default is to include them) and then click\nthe  Copy  button. You are now ready to paste these\ngenerated object models into your own app code. To begin generating the data object models for your app, run the following: You will be prompted for the following details: Select from the list of available languages which language you would like\nyour object models to be in Choose whether you'd like to omit import statements from your data models Choose whether you'd like the data model for each schema to be\ngrouped together in the output Copy and paste the object models from your terminal into your code.",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices schema datamodels -a \"<Your App ID>\""
                },
                {
                    "lang": null,
                    "value": "? Select the language you would like to generate data models in\n? Would you like to omit imports?\n? Would you like group all generated data models together?"
                }
            ],
            "preview": "Atlas App Services can automatically translate your server-side document schemas to a Realm Object Schema that will\nwork with your SDK of choice. You can generate the object models using either the App Services UI or CLI.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/data-model/migrate-schema-partner-collection",
            "title": "Make Breaking Schema Changes",
            "headings": [
                "Overview",
                "Partner Collections",
                "Procedure",
                "Initialize Your Partner Collection with an Aggregation Pipeline",
                "Set up Database Triggers for Your Partner Collections",
                "Add Trigger Functions",
                "Development Mode and Breaking Changes"
            ],
            "paragraphs": "If you need to make changes to an object schema that is used in Atlas Device Sync, you\ncan make  non-breaking  without any\nadditional work.  Breaking changes ,\nhowever, require additional steps. A breaking or destructive change includes\nrenaming an existing field or changing a field's data type. For more information, see  Update Your Data Model . If you need to make a breaking schema change, you have two choices: Terminate sync  in the backend and then\n re-enable  it from the start. Create a  partner collection , copy the old\ndata to this new collection, and set up triggers to ensure data\nconsistency. The remainder of this guide leads you through creating a partner collection. When you terminate and re-enable Atlas Device Sync, clients can no longer Sync.\nYour client must implement a client reset handler to restore Sync. This\nhandler can discard or attempt to recover unsynchronized changes. Client Reset - Flutter SDK Client Reset - Java SDK Client Reset - Kotlin SDK Client Reset - .NET SDK Client Reset - Node SDK Client Reset - React Native SDK Client Reset - Swift SDK In the following procedure, the  initial  collection uses the JSON Schema below\nfor a  Task  collection. Note that the schema for the  Task  contains an\n _id  field of type  objectId : The  new  schema is the same, except we want the  _id  field to be a string: Since breaking changes cannot be performed directly on a synced object\nschema, you must create a partner collection with a schema containing the\nrequired changes. You must ensure that the partner collection has the same\ndata as the original collection so that newer clients can synchronize with\nolder clients. The recommended approach to copying the data from your original collection to\nthe new partner collection is to use the  Aggregation Framework . You can create and run an aggregation pipeline from the\n mongo shell ,\nby using the  /aggregation-pipeline-builder/ , or with the\n /data-explorer/cloud-agg-pipeline/ . The pipeline will have the following stages: Here the same pipeline as represented in the Atlas and Compass UIs. Note that\nboth of these tools provide a preview of the changes; in this case, the\nconversion the  _id  field from an ObjectId to a string: The following example shows the complete aggregation pipeline as it would\nlook if you used  mongosh  to do the conversion: Match all the documents in the initial collection by passing\nan empty filter to the  $match operator . Modify the fields of the initial collection by using an\n aggregation pipeline operator . In\nthe following example, the data is transformed using the  $addFields\noperator . The  _id  field is\ntransformed to a  string  type with the  $toString operator . Write the transformed data to the partner collection by using the\n $out operator  and specifying\nthe partner collection name. In this example, we wrote the data to a new\ncollection named  TaskV2 . Once your partner collection is set up, you can use it to read existing data.\nHowever, any new writes to the data of  either collection  will not be\nin the other collection. This causes the old clients to be out of sync with the\nnew clients. To ensure that data is reflected in both collections, you set up a\n database trigger  on each collection. When\ndata is written to one collection, the trigger's function performs the write\nto the partner collection. Follow the steps in the  database trigger \ndocumentation to create a trigger that copies data from the  Task  collection to\nthe  TaskV2  collection for all operation types. Repeat these steps to create\na second trigger that copies data from the  TaskV2  collection to the\n Task  collection. Triggers require backing functions that run when the trigger fires. In this\ncase, we need to create two functions: a forward-migration function and a\nreverse-migration function. The forward migration trigger listens for inserts, updates, and deletes in the\nTask collection, modifies them to reflect the TaskV2 collection's schema, and\nthen applies them to the TaskV2 collection. To listen for changes to the TaskV2 collection and apply them to the Task\ncollection, write a reverse-migration function for the TaskV2 collection's\ntrigger. The reverse migration follows the same idea as the previous step. In the forward-migration function, we check which operation has triggered the\nfunction: if the operation type is  Delete  (meaning a document\nhas been deleted in the Task collection), the document is also deleted in the\nTaskV2 collection. If the operation type is a  Write  (inserted or modified)\nevent, an aggregation pipeline is created. In the pipeline, the inserted or\nmodified document in the Task collection is extracted using the\n $match operator . The\nextracted document is then transformed to adhere to the\n TaskV2  collection's schema. Finally, the transformed data is written to the\n TaskV2  collection by using the\n $merge operator : The reverse-migration function goes through similar steps as the example in the\nprior step. If a document has been deleted in one collection, the document is\nalso deleted in the other collection. If the operation type is a write event,\nthe changed document from  TaskV2  is extracted, transformed to match the\nTask collection's schema, and written into the  Task  collection: Applies to App Services Apps created after September 13, 2023. App Services Apps in Development Mode that were created after September 13, 2023\ncan make  breaking changes  from their\nclient code to synced object schemas. Refer to  Development Mode  for details about\nmaking breaking changes in Development Mode. Development Mode is not suitable for production use. If you use Development\nMode, make sure to disable it before moving your app to production.",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n    \"title\": \"Task\",\n    \"bsonType\": \"object\",\n    \"required\": [\n        \"_id\",\n        \"name\"\n    ],\n    \"properties\": {\n        \"_id\": {\n            \"bsonType\": \"objectId\"\n        },\n        \"_partition\": {\n            \"bsonType\": \"string\"\n        },\n        \"name\": {\n            \"bsonType\": \"string\"\n        }\n    }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n    \"title\": \"Task\",\n    \"bsonType\": \"object\",\n    \"required\": [\n        \"_id\",\n        \"name\"\n    ],\n    \"properties\": {\n        \"_id\": {\n            \"bsonType\": \"string\"\n        },\n        \"_partition\": {\n            \"bsonType\": \"string\"\n        },\n        \"name\": {\n            \"bsonType\": \"string\"\n        }\n    }\n}"
                },
                {
                    "lang": "shell",
                    "value": "use \"<database-name>\" // switch the current db to the db that the Task collection is stored in\ncollection = db.Task;\ncollection.aggregate([\n  { $match: {} }, // match all documents in the Task collection\n  {\n    $addFields: { // transform the data\n      _id: { $toString: \"$_id\" }, // change the _id field of the data to a string type\n    },\n  },\n  { $out: \"TaskV2\" }, // output the data to a partner collection, TaskV2\n]);"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function (changeEvent) {\n  const db = context.services.get(\"mongodb-atlas\").db(\"ExampleDB\");\n  const collection = db.collection(\"Task\");\n  // If the event type is \"invalidate\", the next const throws an error.\n  // Return early to avoid this.\n  if (!changeEvent.documentKey) { return; }\n  // The changed document's _id as an integer:\n  const changedDocId = changeEvent.documentKey._id;\n  // If a document in the Task collection has been deleted, \n  // delete the equivalent object in the TaskV2 collection:\n  if (changeEvent.operationType === \"delete\") {\n    const tasksV2Collection = db.collection(\"TaskV2\");\n    // Convert the deleted document's _id to a string value \n    // to match TaskV2's schema:\n    const deletedDocumentID = changedDocId.toString();\n    return tasksV2Collection.deleteOne({ _id: deletedDocumentID })\n  }\n  // A document in the Task collection has been created, \n  // modified, or replaced, so create a pipeline to handle the change:\n  const pipeline = [\n    // Find the changed document data in the Task collection:\n    { $match: { _id: changeEvent.documentKey._id } },\n    {\n      // Transform the document by changing the _id field to a string:\n      $addFields: {\n        _id: { $toString: \"$_id\" },\n      },\n    },\n    // Insert the document into TaskV2, using the $merge operator \n    // to avoid overwriting the existing data in TaskV2:\n    { $merge: \"TaskV2\" }]\n  return collection.aggregate(pipeline);\n};\n"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function (changeEvent) {\n  const db = context.services.get(\"mongodb-atlas\").db(\"ExampleDB\");\n  const collection = db.collection(\"TaskV2\");\n  // If the event type is \"invalidate\", the next const throws an error.\n  // Return early to avoid this.\n  if (!changeEvent.documentKey) { return; }\n  // The changed document's _id as a string:\n  const changedDocId = changeEvent.documentKey._id;\n  // If a document in the TaskV2 collection has been deleted, \n  // delete the equivalent object in the Task collection\n  if (changeEvent.operationType === \"delete\") {\n    const taskCollection = db.collection(\"Task\");\n    // Convert the deleted document's _id to an integer value\n    // to match Task's schema:\n    const deletedDocumentID = parseInt(changedDocId);\n    return taskCollection.deleteOne({ _id: deletedDocumentID })\n  }\n  // A document in the Task collection has been created, \n  // modified, or replaced, so create a pipeline to handle the change:\n  const pipeline = [\n    // Find the changed document data in the Task collection\n    { $match: { _id: changedDocId } },\n    {\n      // Transform the document by changing the _id field\n      $addFields: {\n        _id: { $toInt: \"$_id\" },\n      },\n    },\n    { $merge: \"Task\" }\n  ]\n  return collection.aggregate(pipeline);\n};\n"
                }
            ],
            "preview": "If you need to make changes to an object schema that is used in Atlas Device Sync, you\ncan make non-breaking without any\nadditional work. Breaking changes,\nhowever, require additional steps. A breaking or destructive change includes\nrenaming an existing field or changing a field's data type.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/data-model/update-schema",
            "title": "Update Your Data Model",
            "headings": [
                "Breaking Changes",
                "Breaking vs. Non-Breaking Change Quick Reference",
                "Development Mode and Breaking Changes",
                "Add an Object Type",
                "Add a Default Value on a Property",
                "Add a Required Property",
                "Modify Existing Atlas Documents",
                "Add an Optional Property",
                "Remove an Object Type",
                "Remove a Property",
                "Change an Object's Name",
                "Change a Property Name",
                "Change a Property Type but Keep the Name",
                "Change a Property's Status Between Optional and Required",
                "Using the Partner Collection Strategy"
            ],
            "paragraphs": "When developing an application using Atlas Device Sync, you may need to make changes to\nyour schema at some point, such as when you need to: To make it easier to understand how schema changes affect your app, we\ncharacterize them as \"breaking\" versus \"non-breaking\" changes. Atlas App Services provides for non-breaking schema changes to synced\nrealms, allowing old clients to sync with newer ones. Breaking schema changes, however, take some planning and work, and should\nbe avoided whenever possible. Add a new property to an already-synced object Remove a property from an already-synced object Change the type stored in a property Update an optional property to  required Breaking schema changes are difficult because older clients (those that\nhave not been updated to your new code and schema) still need access to\nthe data via the old schema definition. Clients that are updated need to\nwork with the new schema changes. Because breaking or destructive schema changes require special handling,\nApp Services does not support making these changes via App Services CLI or automated\ndeploy with GitHub. Instead, you should make breaking changes in\nthe App Services UI. A breaking change is a change that you make in your server-side\nschema that requires additional action to handle.\nA breaking server-side schema change requires you to  terminate sync  in the backend, and then  re-enable  sync. Breaking schema changes result in clients being unable\nto open a realm, or the appearance of data loss when server-side documents\nare unable to sync to client-side applications. They prevent applications from\nautomatically recovering from a  client reset . A non-breaking change is a change that you can make in your server-side\nschema or your object model without requiring additional handling in your\napp. Also known as additive changes, they apply automatically to synced realms. The following diagram shows the types of schema changes you can make and the\nprocess you go through to perform the change. Each of these changes is explained\nin more detail in the table and sections below. This table summarizes each type of change and whether it is a breaking or\nnon-breaking change. When you  only  make non-breaking changes to the server-side schema,\nno additional action is required. However, if you then try to apply\nthese changes to your client object model, you may need to perform\na migration. If the client device has an existing realm file, you must\nperform a migration. For details, see the Modify an Object Schema\npage in your preferred SDK. Type of Change Server-Side Schema Client-Side Object Model Add an object type Non-breaking Non-breaking Add a default value on a property Non-breaking Non-breaking Add a required property Non-breaking Non-breaking Add an optional property Non-breaking Non-breaking Remove an object type Breaking Non-breaking Remove an optional or required property Breaking Non-breaking Change an object type name Breaking* Breaking* Change a property name Breaking* Breaking* Change property type but keep the name Breaking Breaking Change property's status between optional and required Breaking Breaking While renaming a property or object is a breaking change, some of the Realm\nSDKs offer a workaround to remap the property name. See\n Change a Property Name  for more details. Applies to App Services Apps created after September 13, 2023. App Services Apps in Development Mode that were created after September 13, 2023\ncan make  breaking changes  from their\nclient code to synced object schemas. Refer to  Development Mode  for details about\nmaking breaking changes in Development Mode. Development Mode is not suitable for production use. If you use Development\nMode, make sure to disable it before moving your app to production. You can add an object type to either the server-side schema or the client\nobject model without doing any additional handling. If you want to add an object type to both the server-side schema and the\nclient object model, you can add the object type to the object model, and\nuse  Development Mode  to let App Services handle the\nserver-side schema updates. Or you can manually add the object type to both\nthe model and the schema. When you add a new object type, we retrieve the documents for the\ncollection and re-insert them into App Services to get the new values. This\nis expected behavior, but it does cause a temporary halt to propagating\nchanges while this process is underway. You can add a default value on an object's required property. When you\ninsert an Atlas document missing this required property into the\ncollection, the Realm clients set the property to the default value. However,\nthe same property on the Atlas document remains empty until a client makes\nchanges to the property or updates the document directly in Atlas. For more information on how default values are helpful when modifying\nexisting Atlas documents, see  Add a Required Property . The default field does not have type validation. If the default field's type\nand the property's type are not the same, the error will indicate that\nthe document is missing a required field. You can add a required property to the client's object model, and\nuse  Development Mode  to let App Services infer the\nserver-side schema updates. Or you can manually add the\nrequired property to both the client model and the Atlas schema.\nHowever, you should consider making the property optional to avoid the need to\nmodify existing Atlas documents. Clients can open the Realm with a schema subset that doesn't\ninclude a required property. The server populates the\nmissing required value field with a zero or blank value\n(like 0, \"\", or 0.0 depending on the property type) when the document syncs. When you add a new required property, you must update existing documents with\nthe new property or they do not sync to the client. This may give client\nusers the impression that the data has been lost. Resolve this issue by\nadding the new property to each impacted document and populating it with a\nvalue. After you update the documents to match the client schema, they\nsync to the client application. When you add a new required property, App Services retrieves the documents for\nthe collection that have new values per the updated schema. App Services iterates\nthrough those documents and re-inserts them to get the new values. This is\nexpected behavior, but it does cause a temporary halt to propagating changes\nwhile this process is underway. App Services uses a  __realm_sync.unsynced_documents  collection to track\nunsynced documents. When you add a required property, the re-sync process can\npush this collection over the limit of 100,000 documents. In this case, you must\n terminate and re-enable sync , even though the\nchange you're making is a non-breaking change. If you want to add an optional property to both the server-side schema and\nthe client object model, you can add the optional property to the object\nmodel, and use  Development Mode  to let\nApp Services infer the server-side schema updates. Or you can manually add the\noptional property to both the model and the schema. When you add a new optional property, we retrieve the documents for the\ncollection that have new values per the updated schema. We iterate\nthrough those documents and re-insert them into App Services to get\nthe new values. This is expected behavior, but it does cause a temporary\nhalt to propagating changes while this process is underway. You can remove an object from the client's object model as a non-breaking change.\nIf you remove the object from the server-side schema, it is a breaking change.\nFor this reason, we recommend you remove the object type from the client-side\nobject model only and leave it in place on the server-side schema. You can remove an optional or required property from the client-side object model and\nleave it in place on the server-side schema. This is a\nnon-breaking change to the object model. If you remove a property from the server-side schema, it is a breaking change.\nFor this reason, we recommend that you remove the property from the client-side\nobject model only and leave it in place on the server-side schema. To maintain backward compatibility, removing a property from a client-side\nobject model does not delete the property from the database. Instead,\nnew objects retain the removed property, and App Services sets the value to an\nappropriate empty value, such as  null  for nullable properties, a 0 for\ninteger values, or an empty string for string values. Changing an object's name on both the server-side schema and the client-side\nobject model is a breaking change. However, some SDKs offer an API to\nmap a new object name to an existing name in the schema. This allows you to\nrename an object on the client but not change the object name on the server.\nIn this way, you avoid triggering a migration. Object name\nmapping is supported in the following SDKs: If name mapping is not an option, consider implementing a\n partner collection strategy , in which\nyou keep the existing collection and schema, and create a new collection\nwith the new schema. If you choose to change the object's name instead of using the partner collection\nstrategy, you must terminate sync, manually update the schema,\nand re-enable sync. In addition, your client\napplication must perform a  client reset  to restore Sync.\nIn the default client reset mode, the client attempts to recover any\nunsynced changes before resetting. Kotlin Java .NET Flutter Development Mode \ndoes not automatically update your schema for breaking changes. Changing a property's name on both the server-side schema and the client-side\nobject model is a breaking change. However, some SDKs offer an API to\nmap a new property name to an existing name in the schema. This allows you to\nrename a property on the client but not change the property name on the server.\nIn this way, you avoid triggering a migration. Property name\nmapping is supported in the following SDKs: If name mapping is not an option, consider implementing a\n partner collection strategy , in which\nyou keep the existing collection and schema, and create a new collection\nwith the new schema. If you choose to change the property's name instead of using the partner collection\nstrategy, you must terminate sync, manually update the schema,\nand re-enable sync. In addition, your client\napplication must perform a  client reset  to restore Sync.\nIn the default client reset mode, the client attempts to recover any\nunsynced changes before resetting. When you terminate and re-enable sync, you must also update existing Atlas\ndocuments to enable them to Sync with your client applications.\nWithout this additional handling, those documents do not Sync and it may\nappear in the client that the data has been lost. You could resolve this issue\nin two ways: After you've made these changes, the appropriate documents sync to the client\napplication. Flutter: Remap a Property Java: Rename a Field Kotlin: Remap a Property .NET: Rename a Property Node.js: Remap a Property React Native: Remap a Property Swift: Remap a Property If you change a property name in the server-side schema, you must\nupdate existing documents with that new property name or they do not sync\nto the client. This may give client users the impression that the data\nhas been lost. Change the old field name on each document to match the new schema Add a new field to each document with a name that matches the new schema,\nand copy the value from the old field into it Changing a property's type is a breaking change to both the server-side\nschema and the client-side object model. Instead of changing a property's type, consider implementing the\n partner collection strategy , in which\nyou keep the existing collection and schema, and create a new collection\nwith the new schema. If you choose to change the property's type instead of using the partner collection\nstrategy, you must terminate sync, manually update the schema,\nand re-enable sync. In addition, your client\napplication must perform a  client reset  to restore Sync.\nIn the default client reset mode, the client attempts to recover any\nunsynced changes before resetting. When you terminate and re-enable sync, you must also update existing Atlas\ndocuments to enable them to Sync with your client applications.\nWithout this additional handling, those documents do not Sync and it may\nappear in the client that the data has been lost. You could resolve this issue\nin two ways: After you've made these changes, the appropriate documents should once again\nsync to the client application. If you change a property's type in the server-side schema, you must\nupdate existing documents with that new property type or they do not sync\nto the client. This may give client users the impression that the data\nhas been lost. Development Mode \ndoes not automatically update your schema for breaking changes. Change the old field type on each document to match the new schema Add a new field to each document with the type that matches the new schema,\nand copy the value from the old field into it, transforming its type Changing a property's status between optional and required\nis a breaking change to both the server-side schema and the client-side object model. Instead of changing a property's status, consider implementing the\n partner collection strategy , in which\nyou keep the existing collection and schema, and create a new collection\nwith the new schema. If you choose to change the property's status instead of using the partner collection\nstrategy, you must terminate sync, manually update the schema,\nand re-enable sync. In addition, your client\napplication must perform a  client reset  to restore Sync.\nIn the default client reset mode, the client attempts to recover any\nunsynced changes before resetting. If you change a property's status in the server-side schema, you must\nupdate existing documents with that new property type or they do not sync\nto the client. This may give client users the impression that the data\nhas been lost. Development Mode \ndoes not automatically update your schema for breaking changes. A partner collection is a collection that contains the same data as\nthe original collection, but has the new schema definition in place. Partner\ncollections use database triggers to ensure that data flows in both directions,\nmeaning that when one collection is written to, the other is also written to\n(with the data modifications required for the new schema). To implement a breaking schema change using the partner collection strategy,\nsee  Make Breaking Schema Changes .",
            "code": [],
            "preview": "When developing an application using Atlas Device Sync, you may need to make changes to\nyour schema at some point, such as when you need to:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/details/conflict-resolution",
            "title": "Conflict Resolution",
            "headings": [
                "Overview",
                "Rules of Conflict Resolution",
                "Special Considerations",
                "Counters",
                "Strings",
                "Custom Conflict Resolution",
                "Summary"
            ],
            "paragraphs": "Conflicts arise when two or more users make changes to the same piece of\ndata independently. This can happen due to latency between device and\nserver or loss of connectivity. In this event, Atlas Device Sync automatically\nuses conflict resolution strategies to merge the changes. Specifically,\nDevice Sync handles conflict resolution using  operational transformation ,\na set of rules that guarantee strong eventual consistency, meaning all clients' versions\nwill eventually converge to identical states. This will be true even if changes were\nmade in a different order. You must be aware of the rules to ensure consistent results, but the\nupside is that by following those rules, you can have devices work\nentirely offline and still converge on meaningful results when they\nmeet. At a very high level, the rules are as follows: If one side deletes an object it will always stay deleted, even if the\nother side has made changes to it later on. If two sides update the same property, Device Sync will keep the value from\nthe most recent update. If two items are inserted at the same position, the item that was\ninserted first will end up before the other item. This means that if\nboth sides append items to the end of a list, will include both items\nin order of insertion time. If two sides both create objects of the same class with identical\nprimary keys, they will be treated as instances of the same object. Matt and Sarah are working on data for their dog walking business. Matt\ndeletes data on one of their client's dogs, Doug, as they no longer need to walk him.\nWhile Sarah is out without internet connection, she edits Doug's required walk time\ndata on her local, offline version, as she does not know about Matt's deletion of Doug's data. Once Sarah regains internet connection, her change will be sent to the server. The server will\nsend her Matt's deletion operation. As deletes always win according to Device Sync's\nconflict resolution rules, Matt's deletion is kept rather than Sarah's edit.\nThe server will not send Sarah's edits to Matt's device. The data is again in agreement across\nMatt and Sarah's devices. Using integers for counting is a special case. The way that most\nprogramming languages would implement an increment operation (like  v\n+= 1 ) is to read the value, increment the result, and then store it\nback. This will obviously not work if you have multiple parties doing\nincrementing simultaneously (they may both read 10, increment it to 11,\nand when it merged you would get a result of 11 rather than the intended\n12). To support this common case, we offer a way to express whether\nyou are incrementing (or decrementing) the value, giving enough hints so\nthe merge can reach the correct result. You have the choice to\nupdate the entire value or edit it in a way that conveys more meaning,\nallowing you to get more precise control of the conflict resolution. Device Sync interprets the value of a string as a whole and does not merge\nconflicts on a per-character basis. For example, this means that if a character\nor substring is inserted or deleted within a string, Device Sync will treat\nthis as a replacement of the entire value of the string. Generally speaking, the conflict resolution of Device Sync should\nwork for most purposes, and you should not need to customize it. That\nsaid, the typical way to do custom conflict resolution is to change a\nproperty type from string to list. Each side can then add its updates to\nthe list and apply any conflict resolution rules it wants directly in\nthe data model. You can use this technique to implement max, min, first\nwrite wins, last write wins, or any other kind of resolution you can\nthink of. Device Sync implements a conflict resolution system to allow multiple offline writers to write simultaneously and still eventually converge on the same result. The conflict resolution system follows four rules: deletes always win, the last update wins, inserts in lists are ordered by time, and primary keys designate object identity. Counters and strings are special cases to be aware of in your client code.",
            "code": [],
            "preview": "Conflicts arise when two or more users make changes to the same piece of\ndata independently. This can happen due to latency between device and\nserver or loss of connectivity. In this event, Atlas Device Sync automatically\nuses conflict resolution strategies to merge the changes. Specifically,\nDevice Sync handles conflict resolution using operational transformation,\na set of rules that guarantee strong eventual consistency, meaning all clients' versions\nwill eventually converge to identical states. This will be true even if changes were\nmade in a different order.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/details/protocol",
            "title": "Atlas Device Sync Protocol",
            "headings": [
                "Overview",
                "Key Concepts",
                "Changeset",
                "Operational Transformation",
                "Client File Identifier",
                "Network Security",
                "Sync Session Process",
                "Client Connects to the App Server",
                "Client Initiates a Sync Session",
                "App Services Allocates a New Client File Identifier",
                "Client Sends a Client Identifier",
                "Client Uploads & Downloads Sync Changesets",
                "Client Terminates the Sync Session",
                "Request Types",
                "Client -> Server Messages",
                "Server -> Client Messages"
            ],
            "paragraphs": "Atlas Device Sync uses a protocol to correctly and efficiently sync data changes in\nreal time across multiple clients that each maintain their own local\nRealm files. The protocol defines a set of pre-defined  request\ntypes  as well as a  process \nby which a client, like a Realm SDK, can connect to an\nAtlas App Services application server and sync data. The Realm SDKs internally implement and manage the sync protocol,\nso for most applications you don't need to understand the sync protocol to\nuse Device Sync. This page covers the protocol at a high level and is\nnot an implementation spec. A  changeset  is a list of instructions that describe granular modifications\nmade to a known object state or version by one or more write operations.\nChangesets are the base unit of the sync protocol. Synced realm clients send\nchangesets to the Device Sync server whenever they perform a write\noperation. The server sends each connected client the changesets for write\noperations executed by other clients. The Device Sync server accepts changesets from any connected sync\nclient (including changes in a synced MongoDB cluster) at any time and uses an\noperational transformation algorithm to serialize changes into a linear order\nand resolve conflicting changesets before sending them to connected clients. When you make a change to a synced object, App Services does not\nre-upload the entire object. Instead, App Services sends only\nthe difference (\"delta\") between before and after. The service\ncompresses the deltas with  zlib  compression. This\nreduces network load, which is especially useful in mobile network\nconditions. An  operational transformation  is a function that, given two changesets,\nproduces a third changeset that represents logically applying one of the given\nchangesets after the other. Device Sync uses operational transformation to\nresolve conflicts between changesets from different sync clients that apply to\nthe same base state. Realm is an offline-first local database even when sync is enabled, which means\nthat any device may perform offline writes and upload the corresponding\nchangesets later when network connectivity is re-established. The operational\ntransformation algorithm is designed to gracefully handle changesets that arrive\n\"out of order\" with respect to the logical server clock such that every synced\nRealm file converges to the same version of each changed object. An operational transformation on Realm changesets is analogous to\na  rebase operation  in Git. A  client file identifier  is a value that uniquely identifies a synced client\nRealm file and its corresponding server file. The server generates a\nclient file identifier whenever an SDK requests one during its initial sync of a\nRealm file. Each identifier is a 64-bit, non-zero, positive signed\ninteger strictly less than 2^63. The server guarantees that all identifiers generated on behalf of a\nparticular server file are unique with respect to each other. The server is\nfree to generate identical identifiers for two client files if they are\nassociated with different server files. The SDK synchronizes with the application server over a WebSocket connection secured by\nHTTPS using  TLS 1.3 . To initiate, execute, and terminate a Device Sync session, a\nRealm SDK and application server send and receive a set of\nprotocol-specific requests. The SDK negotiates a WebSocket connection over HTTP and then establishes a sync\nsession by sending  BIND  and  IDENT \nrequests to the server over the WebSocket connection.\nOnce the session is established, the SDK and server send synced changesets for a given\nRealm file to each other via  UPLOAD  and\n DOWNLOAD  messages. To end the session, the SDK sends an\n UNBIND  request. The sync protocol is primarily handled over a WebSocket connection between the\nSDK and the server. To establish a connection, the SDK sends a handshake HTTP\nrequest that includes the following: The server sends an  HTTP 101 Switching Protocols \nresponse that specifies a WebSocket connection for the SDK. The rest of the\nsync protocol occurs over this connection. a protocol version a WebSocket key a valid access token for an authenticated App Services application user To begin a sync session, a Realm SDK sends a  BIND \nrequest to a Device Sync server. The request identifies a\nspecific local Realm Database file to sync and includes a WebSocket connection key that\nthe server will use to open a bidirectional connection to the SDK. If the SDK is attempting to sync a particular Realm Database file for the first time,\nit does not yet possess a server-generated client identifier for the file. In\nthis case, the  BIND  request also indicates that the\nDevice Sync server should allocate one. If a  BIND  request indicates that the SDK needs a client\nfile identifier, the Device Sync server generates a unique value for the specified\nRealm Database file and sends it to the SDK in an  IDENT \nresponse. When the SDK receives the  IDENT , it stores\nthe new client identifier persistently in the local Realm Database file. An SDK only needs to request a client file identifier the first time it syncs\neach Realm Database file. For subsequent sync sessions, the SDK can use the persisted\nidentifier. Once an SDK has initiated a sync session with a  BIND \nrequest, it must identify the local Realm Database file that it wants to sync. To do\nthis, the SDK sends the application server an  IDENT \nmessage that contains the client file identifier. If the SDK has previously\nsynced the realm with the server, it can specify the most recently synced\nserver version to optimize the sync process. When it receives the  IDENT  message, the server\nestablishes the session. The SDK and server can can now freely send upload and\ndownload sync changesets at any time. Once a sync session is established, the SDK and server can freely send and\nreceive  UPLOAD  and  DOWNLOAD \nmessages to sync changes whenever they occur. The SDK sends an  UPLOAD  message for every changeset it\napplies except for those that it received in a  DOWNLOAD \nmessage from the server. When the server receives an  UPLOAD  message, it applies\n operational transformations  to resolve\nany conflicts with other changesets and then applies the transformed changeset\nto the server version of the realm. This triggers the server to send\n DOWNLOAD  messages to other connected clients, including\nthe synced Atlas cluster which mirrors the server realm. A\n DOWNLOAD  message groups one or more transformed\nchangesets in chronological order from oldest to most recent according to the\nserver's history. The SDK applies the changesets in the same order. Once a sync session is established, Device Sync servers will continue to accept\n UPLOAD  messages and send\n DOWNLOAD  messages until the SDK terminates the session.\nTo terminate a sync session, an SDK sends an  UNBIND \nrequest to the Device Sync server. The following table describes the request types that a sync client can send to a\nDevice Sync server: Request Description Starts a new sync session on the server and provides a signed\nauthorization token for the current application user. If the client does\nnot yet possess a  client file identifier \nfor the Realm file it wants to sync, this also indicates that\nthe server should generate one and send it back to the client. A client must send a BIND before it can send any other requests. Provides the  client file identifier  that\nindicates the following: This request is related to but distinct from the\n IDENT  message sent by the server when a client\nrequests a client file identifier. the Realm file to sync the client realm's current version the client realm's most recently synced server version Specifies one or more  changesets  for operations that\noccurred on the client. The changesets are listed by client version in\nincreasing order. Specifies a  changeset  that describes a serialized\ntransaction that occurred on the client. The client may not upload any\nother changesets until the server confirms or rejects the transaction. Ends a running sync session. A client may not send any other requests for an\n UNBOUND  session. Requests that the server notify the client when it has synced the latest\n changeset  in the server history (at the time of the\nrequest). Re-authorizes a current sync session with a new user token. Requests that the server send one or more  STATE \nmessages, which the client uses to download the current server version of\nthe Realm file. Clients issue state requests when they\nasynchronously open a synced realm. Requests that the server send the client version of the latest changeset\nthat was sent by the client and processed by the server. This is most\ncommonly used when an SDK executes a client reset. Indicates that the client is still connected and that the server should\nmaintain the sync session. A client must send at least one PING to the\nserver every 10 minutes. The server acknowledges to each PING with a\n PONG . If the server has not received a PING from a client in more than 10\nminutes, it considers the client to be disconnected and may automatically\nend the session. The following table describes the request types that the Device Sync\nserver can send to a sync client: Request Description Provides a  client file identifier  that\nthe server generated in response to a  BIND  that\nrequested the identifier. Specifies one or more  changesets  (up to 16MB total)\nfor operations that occurred on other clients. The changesets are listed\nby server version in increasing order. The changesets in a DOWNLOAD may not be the exact changesets uploaded by\nother clients. Instead, they may be equivalent changesets output by\nDevice Sync's  operational transformation  algorithm. Specifies that the server ended a sync session in response to an\n UNBIND . Indicates whether or not the server successfully processed a changeset\nspecified in a  TRANSACT  from the client. Indicates that the server has sent the client the latest\n changeset  that was in the server history when the\nserver received a  MARK  from the client. Contains one or more segments of encoded data that the client can\nconcatenate to construct the latest server version of the realm. Sent\nin response to a  STATE_REQUEST . Specifies the client version of the latest changeset that was sent by the\nclient and processed by the server. Sent in response to a\n CLIENT_VERSION_REQUEST . Indicates that the server encountered an issue that appears to have been\ncaused by the connected client. For details, see\n Sync Client Errors . Acknowledges a  PING . If a client does not receive\na PONG acknowledgement, it indicates that the client cannot currently\ncommunicate with the server over the network and that the server may not\nhave received the corresponding  PING .",
            "code": [
                {
                    "lang": "text",
                    "value": "Realm SDK                      App Server\n    |                                  |\n    |  <---- 1. HTTP Handshake ----->  |\n    |                                  |\n    |  --------- 2. BIND ----------->  |\n    |                                  |\n    |  <-- 3. IDENT (first time) ----  |\n    |                                  |\n    |  --------- 4. IDENT ---------->  |\n    |                                  |\n    |  <---- 5. UPLOAD/DOWNLOAD ---->  |\n    |                                  |\n    |  --------- 6. UNBIND --------->  |"
                }
            ],
            "preview": "Atlas Device Sync uses a protocol to correctly and efficiently sync data changes in\nreal time across multiple clients that each maintain their own local\nRealm files. The protocol defines a set of pre-defined request\ntypes as well as a process\nby which a client, like a Realm SDK, can connect to an\nAtlas App Services application server and sync data.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "sync/data-model/data-model-map",
            "title": "Data Model Mapping",
            "headings": [
                "Overview",
                "Databases, Collections, and Objects",
                "Mapping with Development Mode",
                "Mappings",
                "Type Name",
                "Property Types",
                "Array Properties",
                "Embedded Objects",
                "Sets",
                "Dictionaries",
                "Relationships",
                "Example",
                "App Services Schema",
                "Realm Object Schema",
                "Data in Atlas"
            ],
            "paragraphs": "This page contains information on how the App Services Schema used by\nAtlas Device Sync maps to the Realm Object Schema used by the Realm SDKs. To generate Realm SDK Schemas from your App Services Schemas,\nrefer to  Generate SDK Object Models .\nTo generate App Services Schemas from Realm SDK client code,\nrefer to  Development Mode . To learn more about how Device Sync uses these two schemas,\nrefer to the  Sync Data Model Overview . When you configure Device Sync, you  specify the data source  where you want to store data. This data source\nmay contain multiple databases, and each database may contain multiple\ncollections. The  App Services schema  maps your Realm Database object\nnames to collections in databases in your Device Sync data source. The\n title  field in an App Services schema maps to the object type name in\nRealm Database. Because the  title  name determines the mapping between\nclient objects and the appropriate Atlas collection, this name must be\nunique among all schemas in your synced data source. The  title  does not need to match the collection name. Consider an app with a database named  Pets . It may contain multiple\ncollections, such as  Canine  and  Feline . The App Services schema\nfor the  Canine  collection might resemble  the example below , where the  title  field of\nthe schema is  Dog . That would map a Realm Database object called\n Dog  to the  Canine  collection in the  Pets  database. You could not have another schema whose  title  was  Dog  in the\nsame cluster. For example, you could not sync an object with a  title \nof  Dog  to both a  Debug  and a  Test  database in the\nsame cluster. If you want to sync the same object to different\ncollections for app development purposes, you must use different\ndata sources - one cluster for development and a different cluster\nfor production. When you  enable Development Mode  in your\nDevice Sync configuration, App Services can automatically create collections\nand schemas for Realm Database objects that you sync. It creates those\ncollections in the  database you specify  when you enable Development Mode. With Development Mode enabled, Sync looks for a collection whose App Services\nschema has a  title  that matches the name of your Realm Database object\ntype. This collection could be in any database in your linked data source.\nIt doesn't have to be in the database you add when you configure\nDevelopment Mode. If there is no corresponding  title  in any App Services schema in your\nlinked data source, App Services creates a new collection for this object\ntype. This collection is created in the database you specify when you enable\nDevelopment Mode. The collection name matches the object's type, and the\ncorresponding App Services schema has a  title  field whose value is the\nname of the object type. This creates the mapping between your Realm\nDatabase object and the new collection. Consider an Atlas cluster with a database named  Pets . It contains a\n Feline  collection with existing data. In your application code, you define a new  Dog  object. You enable\nDevelopment Mode, and specify the  Pets  database as the database\nin which to create collections for new object types. When you sync your\nnew  Dog  object, App Services creates a  Dog  collection in the\n Pets  database and creates a schema for it. The  title  of the\nobject in the schema is  Dog . If you later add a new  Person  object to your application code and\nthen sync it, App Services creates a new collection for this\n Person  object in the  Pets  database. If you wanted to create a\ncollection for the  Person  object in a different database, you\ncould  define a schema  for the  Person  object\nin a different database. Or you could disable and re-enable Development\nMode, and select a different database in which to create new collections. The  title  field contains the name of the object type represented by\nthe schema. This is equivalent to a class or schema name in a Realm SDK.\nThe type name must be unique among all schemas in your synced cluster\nbut is otherwise arbitrary and does not need to match the collection\nname. A conventional approach is to name each object type with a singular\nnoun, like \"Dog\" or \"Person\". Schemas generated in development mode or\nby sampling existing documents use this convention. To work with Atlas Device Sync, type names cannot exceed 57 UTF-8 characters. You can configure the following constraints for a given property: Realm SDK data type documentation: Parameter Type Description Type String Every property in a Realm Object Schema has a strongly defined data\ntype. A property's type can be a primitive data type or an object\ntype defined in the same Realm Object Schema. The type also specifies whether\nthe property contains a single value or a list of values. Realm Database supports the following property types: For more information on supported data types, refer to  Schema Types . boolean integer double string date decimal128 objectId uuid mixed array object Optional Boolean Optional properties may contain a null value or be entirely\nomitted from an object. By default, all properties are optional\nunless explicitly marked as required. Default Boolean If a client application creates a new object that does not have a\nvalue for a defined property, the object uses the default value\ninstead. If you open a Realm in the client with a schema subset that doesn't\ninclude a required property, the server will automatically\npopulate the value of the required property with a zero or blank\ndefault value. When you attempt to create an object that is missing a value\nfor a required field, it fails validation and does not persist\nto the realm. Indexed Boolean A property index significantly increases the speed of certain\nread operations at the cost of additional overhead for write\noperations. Indexes are particularly useful for equality\ncomparison, such as querying for an object based on the value of\na property. However, indexes consume additional storage. Supported Types - C++ SDK Data Types - Flutter SDK Realm Data Types - Java SDK Supported Types - Kotlin SDK Field Types - .NET SDK Realm Data Types - Node.js SDK Property Types - React Native SDK Supported Property Types - Swift SDK Both Realm Object Schemas and App Services Schemas support array properties. You can find information on using array data types in the Realm SDK documentation\non defining a Realm Object Schema. For more information on modeling array properties in an App Services Schema,\nrefer to  BSON Types - Array .\nApp Services Schemas support certain constraints that Realm Object Schemas do not,\nsuch as specifying the minimum and maximum number of items. Supported Types - C++ SDK Dart Types - Flutter SDK Lists - Java SDK Supported Types - Kotlin SDK Lists - .NET SDK Supported Property Types - Node.js SDK Supported Property Types - React Native SDK Supported Types - Swift SDK Embedded objects are embedded as nested data inside of a parent object.\nAn embedded object inherits the lifecycle of its parent object.\nIt cannot exist as an independent Realm object. Realm SDK embedded object documentation: For more information on modeling to-one relationships in an App Services Schema,\nrefer to  Embedded Object Relationships . Define an Embedded Object - C++ SDK Embedded Objects - Flutter SDK Define an Embedded Object Field - Java SDK Embedded Objects - Kotlin SDK Embedded Objects - .NET SDK Embedded Objects - Node.js SDK Embedded Objects - React Native SDK Define an Embedded Object Property - Swift SDK Realm Object Schemas and App Services Schemas both support the Set data type.\nA set is a collection of unique values. Realm SDK Set documentation: For more information on modeling sets in an App Services Schema,\nrefer to  Set . Set - C++ SDK RealmSet - Flutter SDK RealmSet - Java SDK RealmSet - Kotlin SDK Sets - .NET SDK Sets - Node.js SDK Sets - React Native SDK Mutable Set - Swift SDK Realm Object Schemas and App Services Schemas both support the Dictionary data type.\nA set is a collection of unique values.\nA dictionary is a collection of dynamic and unique string keys paired with values of a given type.\nA dictionary is functionally an object or document without pre-defined field names. Realm SDK dictionary documentation: For more information on modeling dictionaries in an App Services Schema,\nrefer to  Dictionary . Map/Dictionary - C++ SDK The Realm Flutter SDK does not support Dictionaries yet RealmDictionary - Java SDK Realm Dictionary - Kotlin SDK Dictionaries - .NET SDK Dictionaries - Node.js SDK Dictionaries - React Native SDK Map/Dictionary - Swift SDK Realm Object Schemas support the following types of relationships: App Services Schemas support to-one and to-many relationships.\nApp Services Schemas  do not  support inverse relationships. Realm SDK relationship documentation: For more information on modeling relationships in an App Services Schema,\nrefer to  Relationships . To-one relationships: A to-one relationship means that an object is related\nin a specific way to no more than one other object. To-many relationships: A to-many relationship means that an object is related\nin a specific way to multiple objects. Inverse relationships: An inverse relationship links an object back\nto any other objects that refer to it in a defined to-one or to-many relationship. Relationships - C++ SDK Define Relationship Properties - Flutter SDK Relationships - Kotlin SDK Define a Relationship Field - Java SDK Relationships - .NET SDK Define Relationship Properties - Node.js SDK Define Relationship Properties - React Native SDK Model Relationships - Swift SDK This example shows how to model a  Dog  with Device Sync. This App Services Schema creates the  Dog  data model used by Device Sync. The following code examples create the  Dog  Realm Object Schema in each of the\nRealm SDKs. An application using Device Sync for the  Dog  data model creates MongoDB documents\nin Atlas that looks like the following example.",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"Dog\",\n  \"bsonType\": \"object\",\n  \"required\": [\n    \"_id\",\n    \"_partition\",\n    \"name\"\n  ],\n  \"properties\": {\n    \"_id\": {\n      \"bsonType\": \"objectId\"\n    },\n    \"_partition\": {\n      \"bsonType\": \"string\"\n    },\n    \"name\": {\n      \"bsonType\": \"string\"\n    },\n    \"age\": {\n      \"bsonType\": \"int\"\n    }\n    \"breed\": {\n      \"bsonType\": \"string\"\n    }\n  }\n}"
                },
                {
                    "lang": "swift",
                    "value": "import Foundation\nimport RealmSwift\n\nclass Dog: Object {\n    @Persisted(primaryKey: true) var _id: ObjectId\n\n    @Persisted var age: Int?\n\n    @Persisted var breed: String?\n\n    @Persisted var name: String = \"\"\n}"
                },
                {
                    "lang": "java",
                    "value": "import io.realm.RealmObject;\nimport org.bson.types.ObjectId;\n\npublic class Dog extends RealmObject {\n    @PrimaryKey\n    @Required\n    private ObjectId _id;\n\n    private Integer age;\n\n    private String breed;\n\n    @Required\n    private String name;\n\n    // Standard getters & setters\n    public ObjectId getId() { return _id; }\n    public void setId(ObjectId _id) { this._id = _id; }\n\n    public Integer getAge() { return age; }\n    public void setAge(Integer age) { this.age = age; }\n\n    public String getBreed() { return breed; }\n    public void setBreed(String breed) { this.breed = breed; }\n\n    public String getName() { return name; }\n    public void setName(String name) { this.name = name; }\n}"
                },
                {
                    "lang": "kotlin",
                    "value": "import io.realm.RealmObject;\nimport org.bson.types.ObjectId;\n\nopen class Dog(\n    @PrimaryKey\n    var _id: ObjectId = ObjectId(),\n\n    var age: Int? = null,\n\n    var breed: String? = null,\n\n    var name: String = \"\"\n): RealmObject() {}"
                },
                {
                    "lang": "dart",
                    "value": "import 'package:realm/realm.dart';\npart 'realm_models.g.dart';\n\n@RealmModel()\nclass _Dog {\n  @PrimaryKey()\n  @MapTo('_id')\n  late ObjectId id;\n\n  int? age;\n\n  String? breed;\n\n  late String name;\n}"
                },
                {
                    "lang": "csharp",
                    "value": "using System;\nusing System.Collections.Generic;\nusing Realms;\nusing MongoDB.Bson;\n\npublic class Dog : RealmObject\n{\n    [MapTo(\"_id\")]\n    [PrimaryKey]\n    public ObjectId Id { get; set; }\n\n    [MapTo(\"age\")]\n    public int? Age { get; set; }\n\n    [MapTo(\"breed\")]\n    public string Breed { get; set; }\n\n    [MapTo(\"name\")]\n    [Required]\n    public string Name { get; set; }\n}"
                },
                {
                    "lang": "js",
                    "value": "export const DogSchema = {\n  name: 'Dog',\n  properties: {\n    _id: 'objectId',\n    age: 'int?',\n    breed: 'string?',\n    name: 'string',\n  },\n  primaryKey: '_id',\n};"
                },
                {
                    "lang": "js",
                    "value": "export const DogSchema = {\n  name: 'Dog',\n  properties: {\n    _id: 'objectId',\n    age: 'int?',\n    breed: 'string?',\n    name: 'string',\n  },\n  primaryKey: '_id',\n};"
                },
                {
                    "lang": "js",
                    "value": "{\n  \"_id\": ObjectId('616f44305a205add93ff1081'),\n  \"age\": 8,\n  \"breed\": \"Golden Retriever\",\n  \"name\": \"Jasper\"\n}"
                }
            ],
            "preview": "Learn about how data types in the Atlas App Services Schema map to objects used by Atlas Device SDK.",
            "tags": "code example",
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "functions/test",
            "title": "Test Atlas Functions",
            "headings": [
                "Before You Begin",
                "Unit Tests for Functions",
                "Get a local copy of your App Services App",
                "Create a new Function",
                "Write Function code",
                "Export Function for use in unit tests",
                "Unit test exported Function code",
                "Mock Services",
                "Integration Tests for Functions",
                "Create a test App",
                "Test in live environment"
            ],
            "paragraphs": "This page describes some strategies you can use to test your Atlas Functions. Due to differences between the Functions\nJavaScript runtime and the standard Node.js runtime, you must take some unique considerations\ninto account when testing Functions. This page covers how to handle\nthe uniqueness of Functions. You will need the following to test an Atlas Function: An Atlas App Services App. To learn how to create one, see  Create\nan App . A code deploy method to configure your App. Choose one of: A copy of App Services CLI installed and added to your local system  PATH . To\nlearn how, see  Install App Services CLI . A GitHub repository configured to hold and deploy configuration files\nfor your App. To learn how to set one up, see  Deploy Automatically\nwith GitHub . You can validate the functionality of your Functions with unit tests.\nUse any Node.js-compatible testing framework to test Functions.\nThe examples on this page use the  Jest testing framework . You must use  CommonJS modules  to write unit tests for\nFunctions. Pull the latest configuration of your App from the server. Pull the latest configuration of your App from Github. Create a new Function. In the App's configuration files, create a new JavaScript\nfile in the  functions  directory for your function. You also need to add configuration information for the Function to  functions/config.json . For more information on creating a new Function,\nrefer to  Define a Function . To make your Function code easy to test, keep it modular\nby separating its concerns into distinct components.\nYou must keep all logic for the Function in the file you defined\nin the previous step. You cannot perform relative imports from other files\nin your project in a Function file.\nYou can also  import dependencies using npm . You must export your function by assigning it to  exports . To export your code to use in separate Node.js unit test files,\nyou must use CommonJS  module.exports  syntax. This syntax is not compatible with the Functions runtime.\nThe Atlas Functions environment does not provide the Node.js global  module .\nTo export modules to your unit tests while keeping the file compatible with\nFunctions, wrap the  the  module.exports  statement\nwith a check to see if the global  module  object exists. Now you can write unit tests for the modules that you exported from the\nFunction file. Create a test file for the Function file in\na separate  test  directory somewhere in your project. Import the modules you exported in the previous step and add unit tests. To write unit tests for Functions that use the  global context object \nor one of the  other global modules \nthat Functions expose, you must create mocks of their behavior. In this example, the Function references an  App Services Value \nvia  context.values.get()  and creates an ObjectId using the global module\n BSON . Attach these mocks to the Node.js global namespace. This lets you call the mocks in your\nunit tests the same way you do in the Functions runtime. You may also want to declare and remove these mocks in setup and teardown blocks\nso that they do not pollute the global namespace. The function in this example accesses an App Services Value and returns it. Now create a test file  helloWithValue.test.js .\nThe test file contains the following: Import the function exported from  helloWithValue.js . A mock of  context.values.get() . Wrap the mock in set up and tear down blocks\nso that it does not pollute the global namespace. A test of the imported function that uses the mock. You should perform integration tests on all Functions before deploying\nthem to production environments. This is especially important because\nthe Atlas Function JavaScript runtime differs from the standard Node.js runtime.\nUnexpected errors can occur if you do not test functions deployed to App Services. There is no single way to write integration tests for Functions.\nAs Functions can be used in a variety of different contexts for different purposes,\neach use case requires a different integration testing strategy. For example, the way you create an integration test for a Function that you\ninvoke from a Device SDK client is different from the way you would test a\nDatabase Trigger Function. However, there are some general steps that you can take to writing integration tests\nfor Functions. On a high level these steps are: The remainder of this section explains how to implement integration tests for your\nApp in more detail. Create a testing App with the same configuration as your production App. Write integration tests that interact with your Functions deployed\nto a live testing environment. For more information on the unique aspects of the Functions JavaScript runtime,\nrefer to: For more information on the different use cases for Functions,\nrefer to  When to Use Functions . Function Constraints JavaScript Support Create an App for testing purposes that has the same configuration\nas your production App, except using different data sources and backend\nconfiguration. For more information on how you can create multiple Apps with the\nsame configuration, see  Configure an App Environment . Once you have deployed your test App, test its functionality using\nyour preferred testing language and framework. The  Realm client SDKs  are useful for testing\nApps. These SDKs provide first-class access to App Services.\nIn your testing suite, you can connect to your testing App with a Realm SDK.\nTest the interaction with the App using the Realm SDK. This example uses the Realm Node.js SDK and the Jest testing framework\nto test a Database Trigger. The Trigger Function creates a  materialized view \nof total sales for a product whenever a new sale is made. The Trigger fires every time an entry is added to the  sales  table.\nIt increments the  total_sales  field on the  total_sales_materialized  table\nby one. The Database Trigger has the following configuration: The Trigger invokes the following Function: This example tests the Trigger using the  Node.js Realm SDK \nto interact with MongoDB Atlas. You can also use any  Realm SDK \nwith the MongoDB Query API or one of the  MongoDB drivers \nto query MongoDB Atlas to test a Database Trigger.",
            "code": [
                {
                    "lang": "sh",
                    "value": "appservices pull --remote <App ID>"
                },
                {
                    "lang": "sh",
                    "value": "git pull <Remote Name> <Branch name>"
                },
                {
                    "lang": "sh",
                    "value": "touch functions/hello.js"
                },
                {
                    "lang": "json",
                    "value": "{\n    \"name\": \"hello\",\n    \"private\": false,\n    \"run_as_system\": true\n},"
                },
                {
                    "lang": "js",
                    "value": "function greet(word) {\n  return \"hello \" + word;\n}\n\nfunction greetWithPunctuation(word, punctuation) {\n  return greet(word) + punctuation;\n}\n\n// Function exported to App Services\nexports = greetWithPunctuation;\n"
                },
                {
                    "lang": "js",
                    "value": "function greet(word) {\n  return \"hello \" + word;\n}\n\nfunction greetWithPunctuation(word, punctuation) {\n  return greet(word) + punctuation;\n}\n\n// Function exported to App Services\nexports = greetWithPunctuation;\n\n// export locally for use in unit test\nif (typeof module !== \"undefined\") {\n  module.exports = { greet, greetWithPunctuation };\n}\n"
                },
                {
                    "lang": "sh",
                    "value": "mkdir -p test/unit\ntouch test/unit/hello.test.js"
                },
                {
                    "lang": "js",
                    "value": "const { greet, greetWithPunctuation } = require(\"../../functions/hello\");\n\ntest(\"should greet\", () => {\n  const helloWorld = greet(\"world\");\n  expect(helloWorld).toBe(\"hello world\");\n});\n\ntest(\"should greet with punctuation\", () => {\n  const excitedHelloWorld = greetWithPunctuation(\"world\", \"!!!\");\n  expect(excitedHelloWorld).toBe(\"hello world!!!\");\n});\n"
                },
                {
                    "lang": "js",
                    "value": " function accessAppServicesGlobals() {\n   const mongodb = context.services.get(\"mongodb-atlas\");\n   const objectId = BSON.ObjectId()\n\n   // ... do stuff with these values\n }\n\n exports = accessAppServicesGlobals;\n\n if (typeof module !== \"undefined\") {\n   module.exports = accessAppServicesGlobals;\n }"
                },
                {
                    "lang": "js",
                    "value": "global.context = {\n  // whichever global context methods you want to mock.\n  // 'services', 'functions', values, etc.\n}\n\n// you can also mock other Functions global modules\nglobal.BSON = {\n  // mock methods\n}"
                },
                {
                    "lang": "js",
                    "value": " // adds context mock to global namespace before each test\n beforeEach(() => {\n   global.context = {\n     // your mocking services\n   };\n });\n\n // removes context from global namespace after each test\n afterEach(() => {\n   delete global.context;\n });\n\n test(\"should perform operation using App Services globals\", () => {\n   // test function that uses context\n });"
                },
                {
                    "lang": "js",
                    "value": "function greet() {\n  const greeting = context.values.get(\"greeting\"); // the greeting is 'beautiful world'\n  return \"hello \" + greeting;\n}\n\nexports = greet;\n\nif (typeof module !== \"undefined\") {\n  module.exports = greet;\n}\n"
                },
                {
                    "lang": "js",
                    "value": "// import the function\nconst greet = require(\"../../functions/helloWithValue\");\n\n// wrap the mock in beforeEach/afterEach blocks to avoid\n// pollution of the global namespace\nbeforeEach(() => {\n  // mock of context.values.get()\n  global.context = {\n    values: {\n      get: (val) => {\n        const valsMap = {\n          greeting: \"magnificent morning\",\n        };\n        return valsMap[val];\n      },\n    },\n  };\n});\n\nafterEach(() => {\n  // delete the mock to not pollute global namespace\n  delete global.context;\n});\n\n// test function using mock\ntest(\"should greet with value\", () => {\n  const greeting = greet();\n  expect(greeting).toBe(\"hello magnificent morning\");\n});\n"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"62bb0d9f852c6e062432c454\",\n  \"name\": \"materializeTotalSales\",\n  \"type\": \"DATABASE\",\n  \"config\": {\n    \"operation_types\": [\"INSERT\"],\n    \"database\": \"store\",\n    \"collection\": \"sales\",\n    \"service_name\": \"mongodb-atlas\",\n    \"match\": {},\n    \"project\": {},\n    \"full_document\": true,\n    \"full_document_before_change\": false,\n    \"unordered\": false,\n    \"skip_catchup_events\": false\n  },\n  \"disabled\": false,\n  \"event_processors\": {\n    \"FUNCTION\": {\n      \"config\": {\n        \"function_name\": \"materializeTotalSales\"\n      }\n    }\n  }\n}\n"
                },
                {
                    "lang": "js",
                    "value": "exports = function (changeEvent) {\n  const {\n    fullDocument: { productId },\n  } = changeEvent;\n  const totalSalesMaterialization = context.services\n    .get(\"mongodb-atlas\")\n    .db(\"store\")\n    .collection(\"total_sales_materialized\");\n  totalSalesMaterialization.updateOne(\n    { _id: productId },\n    { $inc: { total_sales: 1 } },\n    { upsert: true }\n  );\n};\n"
                },
                {
                    "lang": "js",
                    "value": "const { app_id } = require(\"../../realm_config.json\");\nconst Realm = require(\"realm\");\nconst { BSON } = require(\"realm\");\n\nlet user;\nconst app = new Realm.App(app_id);\nconst sandwichId = BSON.ObjectId();\nconst saladId = BSON.ObjectId();\n\n// utility function\nasync function sleep(ms) {\n  await new Promise((resolve) => setTimeout(resolve, ms));\n}\n// Set up. Creates and logs in a user, which you need to query MongoDB Atlas\n// with the Realm Node.js SDK\nbeforeEach(async () => {\n  const credentials = Realm.Credentials.anonymous();\n  user = await app.logIn(credentials);\n});\n// Clean up. Removes user and data created in the test.\nafterEach(async () => {\n  const db = user.mongoClient(\"mongodb-atlas\").db(\"store\");\n  await db.collection(\"sales\").deleteMany({});\n  await db.collection(\"total_sales_materialized\").deleteMany({});\n  await app.deleteUser(user);\n});\n\ntest(\"Trigger creates a new materialization\", async () => {\n  const sales = user\n    .mongoClient(\"mongodb-atlas\")\n    .db(\"store\")\n    .collection(\"sales\");\n\n  await sales.insertOne({\n    _id: BSON.ObjectId(),\n    productId: sandwichId,\n    price: 12.0,\n    timestamp: Date.now(),\n  });\n\n  // give time for the Trigger to execute on Atlas\n  await sleep(1000);\n\n  const totalSalesMaterialized = user\n    .mongoClient(\"mongodb-atlas\")\n    .db(\"store\")\n    .collection(\"total_sales_materialized\");\n  const allSandwichSales = await totalSalesMaterialized.findOne({\n    _id: sandwichId,\n  });\n  // checks that Trigger increments creates and increments total_sales\n  expect(allSandwichSales.total_sales).toBe(1);\n});\n\ntest(\"Trigger updates an existing materialization\", async () => {\n  const sales = user\n    .mongoClient(\"mongodb-atlas\")\n    .db(\"store\")\n    .collection(\"sales\");\n\n  await sales.insertOne({\n    _id: BSON.ObjectId(),\n    productId: saladId,\n    price: 15.0,\n    timestamp: Date.now(),\n  });\n  await sales.insertOne({\n    _id: BSON.ObjectId(),\n    productId: saladId,\n    price: 15.0,\n    timestamp: Date.now(),\n  });\n\n  // give time for Trigger to execute on Atlas\n  await sleep(1000);\n\n  const totalSalesMaterialized = user\n    .mongoClient(\"mongodb-atlas\")\n    .db(\"store\")\n    .collection(\"total_sales_materialized\");\n  const allSaladSales = await totalSalesMaterialized.findOne({\n    _id: saladId,\n  });\n  // checks that Trigger increments total_sales for each sale\n  expect(allSaladSales.total_sales).toBe(2);\n});\n"
                }
            ],
            "preview": "This page describes some strategies you can use to test your Atlas Functions.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "functions/mongodb",
            "title": "Query MongoDB Atlas",
            "headings": [],
            "paragraphs": "You can work with data in a linked  MongoDB Data Source  from a function by using a built-in data source client.\nThe client includes methods that allow you to read, write, and aggregate\ndata. For examples of how to work with a data source in a function, refer to\nthe following guides: For detailed reference information on available query and aggregation\nmethods, see  MongoDB API Reference . Read Data from MongoDB Atlas Write Data in MongoDB Atlas Aggregate Data in MongoDB Atlas",
            "code": [
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  // 1. Get a data source client\n  const mongodb = context.services.get(\"mongodb-atlas\");\n  // 2. Get a database & collection\n  const db = mongodb.db(\"myDatabase\")\n  const collection = db.collection(\"myCollection\")\n  // 3. Read and write data with MongoDB queries\n  await collection.insertOne({\n    text: \"Hello, world!\",\n    author: context.user.id\n  })\n  return await collection.findOne({\n    author: context.user.id\n  })\n}"
                }
            ],
            "preview": "You can work with data in a linked MongoDB Data Source from a function by using a built-in data source client.\nThe client includes methods that allow you to read, write, and aggregate\ndata.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "functions/handle-errors",
            "title": "Handle Errors in Functions",
            "headings": [
                "Basic Error Handling",
                "View Logs",
                "Retry Functions",
                "Recursively Call Function in Error Handling Blocks",
                "Use Database Triggers to Retry",
                "Create a Function to handle execution retry",
                "Create retry database trigger",
                "Write Function to retry"
            ],
            "paragraphs": "This page explains how to work with errors in Atlas Functions. You can create a custom error handler specifically for database\nTriggers using AWS EventBridge. For more information, refer to\n Custom Error Handling . You can handle Function errors using standard JavaScript error handling techniques\nlike  try...catch statements . You can view records of all Function executions including which an error prevented\nsuccessful execution in  App Service Logs . Depending on how a Function is invoked, it shows up differently in the logs.\nFor example,  logs for Functions called by Atlas Triggers \nshow up in the logs as \"Triggers\" while  logs for Functions called from a Realm client SDK \nshow up in the logs as \"Functions\".\nFor more information, refer to the  Log entry type documentation . Atlas Functions do not have built-in retry behavior.\nYou can add custom retry behavior. For example, you might want to add\nretry behavior if the third-party service that your Function calls has\nintermittent connectivity, and you want the Function to re-execute\neven if the third-party service is temporarily down. This section describes the following strategies to add retry behavior to your Functions: Recursively Call Functions in Error Handling Blocks Use Database Triggers to Retry Functions You can handle operations that might fail by calling a Function recursively. On a high-level, this process includes the following components: The following table describes some advantages and disadvantages\nof handling Function retries with the recursive call strategy. The following code example demonstrates an implementation of retrying a Function\nby using recursion in error-handling blocks. Execute operations that you want to retry in a  try  statement\nand have the Function call itself in a  catch  statement. To prevent indefinite execution, set a maximum number of retries.\nEvery time the Function fails and enters the  catch  statement,\nincrement a count of the current number of retries.\nStop the recursive execution when the Function's current number of retries\nreaches the max number of retries. You may also want to throttle retries to reduce the total number of executions\nin a time frame. Advantages Disadvantages All retry logic occurs within one function. Function can return a value after a retry. Minimal additional code. All retries must occur within a single Function's max execution time. You can also retry Functions by using a  Database Trigger  to execute retries and a MongoDB collection to track previously-failed executions. On a high-level, this process includes the following components: You can support multiple main functions with one set of a handler Function,\nexecution tracker collection, and Database Trigger Function. Main Function  that executes the logic you want to retry,\nwrapped in the handler function (see below bullet point). Failed execution tracker MongoDB collection \nthat tracks failed executions of the main Function. Handler Function  that invokes the main Function and logs when the function fails\nto the failed execution tracker collection. Database Trigger Function  that reruns the handler function whenever\nthe handler function adds an error to the failed execution tracker collection. Advantages Disadvantages Each retry is its own Function execution, with own max execution time\nand resources. If the Function is retried, it cannot return a value. Each Function call requires two Function invocations, one for the Function\nitself and one for the retry handler. More complex logic, which can be more difficult to write, debug, and monitor. First, create the handler Function  handleRetry  that invokes the main Function. handleRetry  accepts the following parameters: handleRetry  performs the following operations: The main function is passed as the argument  functionToRetry .\n handleRetry  attempts to execute the main Function.\nIf the execution fails, this function attempts to retry the main function. Parameter Type Description functionToRetry JavaScript Function Function to retry. functionName String Name of the function you want to retry. operationId ObjectId Unique identifier for the main function's execution, including retries. previousRetries Number How many times the main function has previously been tried. ...args Rest parameters Indefinite number of arguments passed to the main function. Attempts to execute  functionToRetry  in a  try  statement.\nIf the execution is successful,  handleRetry  returns the value\nreturned by  functionToRetry . If the execution of  functionToRetry  in the previous step throws an error,\nthe  catch  statement handles the error as follows: Checks if the number of previous retries equals the\nmaximum permitted number of retries.\nIf the two numbers are the same, then the function throws an error\nbecause the max retries has been reached. The function no longer attempts\nto retry. Build a function execution log entry object to insert into the database. Get a reference to the failed execution tracker collection. Insert the function log exection log entry into\nthe failed execution tracker collection. This insertion operation\ncauses the Database Trigger Function, which you will make in the next step,\nto fire. Navigate to  Functions . Click the button  Create New Function . In the field  Name , add  handleRetry . In the  Function Editor  add the following code,\nthen save the Function: Add the following to  functions/config.json : Create the file for the Function  functions/handleRetry.js : Push your changes to App Services: If you're using the CLI to update your App Services App,\nyou must first install and set up the  App Services CLI . Now add the code for the Function that the Trigger invokes. The function  retryOperation  takes as a parameter  logEntry , the document that the\nretry handler posted to the failed execution tracker collection.\nThen,  retryOperation  uses  context.functions.execute() \nto invoke the main function with information from  logEntry . Navigate to the  Triggers  in the UI of your App. Click the  Add a Trigger  button. Create the Trigger with the following configuration: Field Value Name Name of your choosing (ex:  retryOperation ) Enabled Yes Skip Events on Re-Enable Yes Event Ordering Yes Cluster Name Name of your choosing (ex:  mongodb-atlas ) Database Name Name of your choosing (ex:  logs ) Collection Name Name of your choosing (ex:  failed_execution_logs ) Operation Type Insert Full Document Yes Document Preimage No Select An Event Type Function Function Click  + New Function . Refer to the following information\nabout the contents of the function. Advanced Configuration No advanced configuration necessary. Add configuration for the Database Trigger. For more information,\nrefer to the  Trigger configuration reference . In the field  Function Name , add  retryOperationDbTrigger . For the field  Function , add the following code,\nand then save the Trigger: Add the Function metadata to  functions/config.json : Add the following code to the file  functions/retryOperationDbTrigger.js : Push your changes to App Services: Now that you have the function handler and the retry Database Trigger Function,\nyou can write the main function. In the following example, the Function randomly throws an error when performing addition.\nThe JavaScript functions that execute this logic are the following: The invocation of  additionOrFailure()  wrapped by the retry handler\noccurs in the exported function  additionWithRetryHandler() .\n All  functions that use the retry handler function should resemble this function. You must include the correct parameters to make this function work\nwith the rest of the retry logic. These parameters are: The body of  additionWithRetryHandler  is the retry handler  handleRetry \ninvoked by  context.functions.execute() ,\nwhich in turn invokes  additionOrFailure . The arguments you pass to\ncontext.functions.execute() are the following: Now when you invoke  additionWithRetryHandler ,\nthe Function will retry if it fails. getRandomOneTwoThree() : Helper function for generating errors\nfor the example. additionOrFailure() : Function with the main logic. Parameter Type Description ...args Rest parameters Zero or more parameters to pass to the function with main logic.\nIn the case of this example, the two numbers added\nin  additionOrFailure() ,  num1  and  num2 . operationId BSON.Object.Id Unique identifier for the Function\ncall and retries. Set default value to a  new BSON.ObjectId() . retries Number Set default value to 0. Argument Type Description \"handleRetry\" String Name of the Function you defined to invoke the main function\nand post to the retry logs if the main function doesn't properly execute. additionOrFailure JavaScript function The main function that  handleRetry()  invokes. operationId BSON.ObjectId Passed in as argument from the parameter  operationId  of  additionWithRetryHandler() . retries Number Passed in as argument from the parameter  retries  of  additionWithRetryHandler() . ...args Spread arguments Zero or more arguments to pass to the function with main logic.\nPassed in as argument from the parameter  ...args  of  additionWithRetryHandler() In the field  Function Name , add  additionWithRetryHandler . For the field  Function , add the following code\nand save the Function: Add the Function metadata to  functions/config.json : Add the following code to the file  functions/additionWithRetryHandler.js : Push your changes to App Services:",
            "code": [
                {
                    "lang": "js",
                    "value": "function willThrowAndHandleError() {\n  try {\n    throw new Error(\"This will always happen\");\n  } catch (err) {\n    console.error(\"An error occurred. Error message:\" + err.message);\n  }\n}\n\nexports = willThrowAndHandleError;\n"
                },
                {
                    "lang": "js",
                    "value": "// Utility function to suspend execution of current process\nasync function sleep(milliseconds) {\n  await new Promise((resolve) => setTimeout(resolve, milliseconds));\n}\n\n// Set variables to be used by all calls to `mightFail`\n// Tip: You could also store `MAX_RETRIES` and `THROTTLE_TIME_MS`\n// in App Services Values\nconst MAX_RETRIES = 5;\nconst THROTTLE_TIME_MS = 5000;\nlet currentRetries = 0;\nlet errorMessage = \"\";\n\n\nasync function mightFail(...inputVars) {\n  if (currentRetries === MAX_RETRIES) {\n    console.error(\n      `Reached maximum number of retries (${MAX_RETRIES}) without successful execution.`\n    );\n    console.error(\"Error Message:\", errorMessage);\n    return;\n  }\n  let res;\n  try {\n    // operation that might fail\n    res = await callFlakyExternalService(...inputVars);\n  } catch (err) {\n    errorMessage = err.message;\n    // throttle retries\n    await sleep(THROTTLE_TIME_MS);\n    currentRetries++;\n    res = await mightFail(...inputVars);\n  }\n  return res;\n}\n\nexports = mightFail;\n"
                },
                {
                    "lang": "js",
                    "value": "\n// Tip: You could also put this in an App Services Value\nconst MAX_FUNC_RETRIES = 5;\n\nasync function handleRetry(\n  functionToRetry,\n  functionName,\n  operationId,\n  previousRetries,\n  ...args\n) {\n  try {\n    // Try to execute the main function\n    const response = await functionToRetry(...args);\n    return response;\n  } catch (err) {\n    // Evaluates if should retry function again.\n    // If no retry, throws error and stops retrying.\n    if (previousRetries === MAX_FUNC_RETRIES) {\n      throw new Error(\n        `Maximum number of attempts reached (${MAX_FUNC_RETRIES}) for function '${functionName}': ${err.message}`\n      );\n    }\n\n    // Build function execution log entry for insertion into database.\n    const logEntry = {\n      operationId,\n      errorMessage: err.message,\n      timestamp: new Date(),\n      retries: previousRetries + 1,\n      args,\n      functionName,\n    };\n\n    // Get reference to database collection\n    const executionLog = context.services\n      .get(\"mongodb-atlas\")\n      .db(\"logs\")\n      .collection(\"failed_execution_logs\");\n\n    // Add execution log entry to database\n    await executionLog.insertOne(logEntry);\n    return;\n  }\n}\n\nexports = handleRetry;\n"
                },
                {
                    "lang": "js",
                    "value": "[\n  {\n    \"name\": \"handleRetry\",\n    \"private\": true,\n    \"run_as_system\": true\n  }\n  // ...other configuration\n]"
                },
                {
                    "lang": "sh",
                    "value": "appservices push"
                },
                {
                    "lang": "js",
                    "value": "\n// Tip: You could also put this in an App Services Value\nconst MAX_FUNC_RETRIES = 5;\n\nasync function handleRetry(\n  functionToRetry,\n  functionName,\n  operationId,\n  previousRetries,\n  ...args\n) {\n  try {\n    // Try to execute the main function\n    const response = await functionToRetry(...args);\n    return response;\n  } catch (err) {\n    // Evaluates if should retry function again.\n    // If no retry, throws error and stops retrying.\n    if (previousRetries === MAX_FUNC_RETRIES) {\n      throw new Error(\n        `Maximum number of attempts reached (${MAX_FUNC_RETRIES}) for function '${functionName}': ${err.message}`\n      );\n    }\n\n    // Build function execution log entry for insertion into database.\n    const logEntry = {\n      operationId,\n      errorMessage: err.message,\n      timestamp: new Date(),\n      retries: previousRetries + 1,\n      args,\n      functionName,\n    };\n\n    // Get reference to database collection\n    const executionLog = context.services\n      .get(\"mongodb-atlas\")\n      .db(\"logs\")\n      .collection(\"failed_execution_logs\");\n\n    // Add execution log entry to database\n    await executionLog.insertOne(logEntry);\n    return;\n  }\n}\n\nexports = handleRetry;\n"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"retry\",\n  \"type\": \"DATABASE\",\n  \"config\": {\n    \"operation_types\": [\"INSERT\"],\n    \"database\": \"logs\",\n    \"collection\": \"failed_execution_logs\",\n    \"service_name\": \"mongodb-atlas\",\n    \"project\": {},\n    \"full_document\": true,\n    \"full_document_before_change\": false,\n    \"unordered\": false,\n    \"skip_catchup_events\": false\n  },\n  \"disabled\": false,\n  \"event_processors\": {\n    \"FUNCTION\": {\n      \"config\": {\n        \"function_name\": \"retryOperationDbTrigger\"\n      }\n    }\n  }\n}\n"
                },
                {
                    "lang": "js",
                    "value": "async function retryOperation({ fullDocument: logEntry }) {\n  // parse values from log entry posted to database\n  const { args, retries, functionName, operationId } = logEntry;\n  // Re-execute the main function\n  await context.functions.execute(functionName, ...args, operationId, retries);\n}\n\nexports = retryOperation;\n"
                },
                {
                    "lang": "js",
                    "value": "[\n   // ...other configuration\n   {\n     \"name\": \"retryOperationDbTrigger\",\n     \"private\": true\n   }\n]"
                },
                {
                    "lang": "sh",
                    "value": "appservices push"
                },
                {
                    "lang": "js",
                    "value": "async function retryOperation({ fullDocument: logEntry }) {\n  // parse values from log entry posted to database\n  const { args, retries, functionName, operationId } = logEntry;\n  // Re-execute the main function\n  await context.functions.execute(functionName, ...args, operationId, retries);\n}\n\nexports = retryOperation;\n"
                },
                {
                    "lang": "js",
                    "value": "// randomly generates 1, 2, or 3\nfunction getRandomOneTwoThree() {\n  return Math.floor(Math.random() * 3) + 1;\n}\n\nfunction additionOrFailure(num1, num2) {\n  // Throw error if getRandomOneTwoThree returns 1\n  const rand = getRandomOneTwoThree();\n  if (rand === 1) throw new Error(\"Uh oh!!\");\n  const sum = num1 + num2;\n  console.log(`Successful addition of ${num1} + ${num2}. Result: ${sum}`);\n\n  // Otherwise return the sum\n  return sum;\n}\n\nasync function additionWithRetryHandler(\n  inputVar1,\n  inputVar2,\n  // create a new `operation_id` if one not provided\n  operationId = new BSON.ObjectId(),\n  // count number of attempts\n  retries = 0\n) {\n  const res = await context.functions.execute(\n    \"handleRetry\",\n    additionOrFailure,\n    \"additionWithRetryHandler\", // MUST BE NAME OF FUNCTION\n    operationId,\n    retries,\n    inputVar1,\n    inputVar2\n  );\n  return res;\n}\n\nexports = additionWithRetryHandler;\n"
                },
                {
                    "lang": "js",
                    "value": "[\n   // ...other configuration\n   {\n     \"name\": \"additionWithRetryHandler\",\n     \"private\": false\n   }\n]"
                },
                {
                    "lang": "sh",
                    "value": "appservices push"
                },
                {
                    "lang": "js",
                    "value": "// randomly generates 1, 2, or 3\nfunction getRandomOneTwoThree() {\n  return Math.floor(Math.random() * 3) + 1;\n}\n\nfunction additionOrFailure(num1, num2) {\n  // Throw error if getRandomOneTwoThree returns 1\n  const rand = getRandomOneTwoThree();\n  if (rand === 1) throw new Error(\"Uh oh!!\");\n  const sum = num1 + num2;\n  console.log(`Successful addition of ${num1} + ${num2}. Result: ${sum}`);\n\n  // Otherwise return the sum\n  return sum;\n}\n\nasync function additionWithRetryHandler(\n  inputVar1,\n  inputVar2,\n  // create a new `operation_id` if one not provided\n  operationId = new BSON.ObjectId(),\n  // count number of attempts\n  retries = 0\n) {\n  const res = await context.functions.execute(\n    \"handleRetry\",\n    additionOrFailure,\n    \"additionWithRetryHandler\", // MUST BE NAME OF FUNCTION\n    operationId,\n    retries,\n    inputVar1,\n    inputVar2\n  );\n  return res;\n}\n\nexports = additionWithRetryHandler;\n"
                }
            ],
            "preview": "Learn how to handle errors in Atlas Functions.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "programming_language": [
                    "javascript/typescript"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "functions/javascript-support",
            "title": "JavaScript Support",
            "headings": [
                "Syntax",
                "Built-In Objects",
                "Built-In Methods & Properties",
                "Built-In Modules",
                "Fully Supported Modules",
                "Partially Supported Modules",
                "dgram",
                "dns",
                "fs",
                "http, http/2 and https",
                "process",
                "util",
                "crypto",
                "Unsupported Modules"
            ],
            "paragraphs": "Atlas Functions  fully support JavaScript ES5\nsyntax as well as most modern JavaScript features included in EcmaScript\n2015 (ES6) and more recent releases. They can also access most Node.js\nbuilt-in modules. Feature Supported arrow function expressions Yes classes Yes super Yes generators Yes default function parameters Yes rest parameters Yes spread iterables Yes object literal extensions Yes for...of loops Yes for await...of loops Yes octal and binary literals Yes template literals Yes destructuring assignment Yes new.target Yes RegExp -y and -u flags No Exponentiation (**) Yes Feature Supported BigInt No Map Yes Promise Yes Proxy No Reflect No Set Yes Symbol Yes TypedArray Yes WeakMap Yes WeakSet No Feature Supported Object static methods Yes String static methods Yes String.prototype methods Yes RegExp.prototype properties No Function.name Yes Array static methods No Array.prototype methods Yes Number static methods No Math methods No You can  import and use  standard\nNode built-in modules in  functions . Atlas Functions\nsupport most built-ins with either full or partial support. Some\nbuilt-ins that are not suited for serverless workloads are not\nsupported. The supported modules and partially supported modules are compatible\nwith  Node API version 10.18.1 . Avoid\nusing APIs in these modules introduced after or deprecated since Node\n10.18.1. Atlas App Services fully supports the following built-in modules: assert buffer events net os path punycode The  built-in punycode module is deprecated . However, App Services\nprovide the  punycode  module from  npm \nautomatically. You can import the module with: querystring stream string_decoder timers tls tty url zlib App Services supports a subset of the functionality of the following modules. App Services supports the following  dgram  APIs: App Services does  not  support the following  dgram  APIs: socket.addMembership socket.address socket.bind socket.close createSocket socket.dropMembership socket.send socket.setBroadcast socket.setMulticastLoopback socket.setMulticastTTL socket.ref socket.setTTL socket.unref App Services supports the  dns  module with the following  exceptions : App Services does  not  support the  dns Promises API App Services does  not  support  resolver.cancel() App Services supports the following  fs  APIs: accessSync constants lstatSync readFileSync statSync App Services supports all of the  http \nand  https  APIs  except \nfor the  Server \nclass functionality. Similarly, App Services supports only the client-side APIs of\n http/2 . App Services supports v1.3.6 of the HTTP library,\n axios . You can replace\nHTTP requests sent through an  HTTP Service \nclient with calls to an HTTP library like axios. App Services supports the following  process  APIs: hrTime nextTick version versions App Services supports the  util  module with the following  exceptions : App Services does  not  support  util.TextEncoder App Services does  not  support  util.TextDecoder App Services supports the  crypto  module with\nthe following  exceptions : App Services does  not  support  crypto.createDiffieHellman() App Services does  not  support  crypto.createDiffieHellmanGroup() App Services does  not  support  crypto.createECDH() Atlas Functions  do not  support the following built-in\nmodules: child_process cluster domain readline v8 vm",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const punycode = require(\"punycode\");"
                }
            ],
            "preview": "Atlas Functions fully support JavaScript ES5\nsyntax as well as most modern JavaScript features included in EcmaScript\n2015 (ES6) and more recent releases. They can also access most Node.js\nbuilt-in modules.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "functions/dependencies",
            "title": "External Dependencies",
            "headings": [
                "Add an External Package",
                "Add Packages by Name and Version",
                "Navigate to Dependencies in the UI",
                "Add Dependency Information",
                "Check Operation Success",
                "Pull your App",
                "Create a package.json File with Dependencies",
                "Push the Dependencies to App Services",
                "Check Operation Success",
                "Enable Github Automatic Deployment",
                "Pull your App",
                "Create a package.json File with Dependencies",
                "Push the Dependencies to Github",
                "Check Operation Success",
                "Upload a Dependency Directory",
                "Locally Install External Dependencies",
                "Create a Dependency Archive",
                "Upload the Dependency Archive",
                "Import a Package in a Function",
                "Import a Full Module",
                "Import a Module Subfolder"
            ],
            "paragraphs": "An  external dependency  is a library that includes code that you\ncan't or don't want to implement yourself. For example, you might use an\nofficial library for an external service or a custom implementation of\na data structure or algorithm. Atlas App Services automatically transpiles dependencies and also\nsupports most built-in Node.js modules. Though most npm modules are written by third parties, you can also\ncreate and publish your own npm modules to house logic specific to\nyour application. You can make your modules available to the Node.js\ncommunity or reserve them for private use. For more information,\ncheck out npm's guide on  Contributing packages to the registry . To import and use an external dependency, you first need to add the\ndependency to your application. You can either  add packages by\nname  or  upload a directory of\ndependencies . You can only use one method at a time to specify the external\npackages your app can use. The most recent method that you used to\nspecify dependencies is the source of truth and overrides previous\nspecifications. For example, a package added by name through the UI overrides any\ncopies of the same package that you've added previously, including\nthose in an  uploaded dependency directory . You can add packages from the  npm registry  to your app by\nname. You can either add a specific version or use the latest version. Select  Functions  from the left navigation menu. Select the  Dependencies  tab. Click the  Add Dependency  button. In the  Add Dependency  modal window that pops up from your actions\nin the previous step, include the following information: Click the  Add  button to start adding the package. You can track the status of adding the dependency in the progress tracker\nat the bottom of the window. The progress tracker provides a message letting\nyou know if the package succeeded or failed. Failure messages contain additional\ninformation about why the package could not be added. If drafts are enabled, you will also need to click  Review & Deploy \nto apply these changes. Field Description Define a Package Name The name of the npm package. Package Version Optional. Specific version of the dependency to use.\nBy default, App Services uses the latest version available. If App Services successfully adds the dependency, you'll see it on the\nlist of dependencies in the  Dependencies  tab. Pull your App to your device with the command: Create a  package.json  in the  /functions  directory of\nyour App. You can do this by copying in an existing  package.json  file\nor running  npm init  and following the prompts. Add dependencies by running  npm install <dependency-name> . Go to the root directory of your App: Push the latest version of your app with all the dependencies in the  package.json : Follow the CLI prompts to confirm that you want to include the dependencies in your operation.\nThe CLI will start adding the dependencies to your App. Once it finishes adding the dependencies, the CLI tells you if all packages\nwere successfully added. If anything fails during the installation, none of the dependencies are installed.\nRemove the failing dependencies from the  package.json  file and try again. Enable Github Automatic Deployments  so you can redeploy\nyour App whenever you push configuration updates to a specified branch on Github. Pull your App to your device with the command: Create a  package.json  in the  /functions  directory of\nyour App. You can do this by copying in an existing  package.json  file\nor running  npm init  and following the prompts. Add dependencies by running  npm install <dependency-name> . Git Push the latest version of your app with all the dependencies in the  package.json \nto Github with a command such as: The App Services GitHub app automatically deploys your updated\napp configuration after you push the commit. You can check the status\nof the deployment and confirm that all packages were successfully\nadded from the  Deployment  screen of the App Services UI. If anything fails during deployment, none of the dependencies are\ninstalled. Remove the failing dependencies from the  package.json \nfile and try again. You can upload a zipped  node_modules  directory of packages to your\napp. Zipped dependency directories may not exceed 15MB. To upload external dependencies, you first need a local\n node_modules  folder containing at least one Node.js package. You\ncan use the following code snippet to install a dependency locally\nyou would like to upload: If the  node_modules  folder does not already exist, this command\nautomatically creates it. You can also configure a  package.json  and run the\n npm install  command to install all packages (and their\ndependencies) listed in your  package.json . To learn more about npm and  node_modules , consult the\n npm documentation . Now that you've downloaded all of your npm modules, you need to\npackage them up in an archive so you can upload them to App Services.\nCreate an archive containing the  node_modules  folder: App Services supports  .tar ,  .tar.gz ,  .tgz , and  .zip  archive\nformats. Once you've created an archive containing your dependencies, you can upload your\ndependency archive using the App Services UI or the App Services CLI: Select  Functions  from the left navigation menu. Select the  Dependencies  tab. Click the  Upload  button. In the file picker, select the  node_modules.tar.gz  archive you\njust created and click Open. App Services automatically uploads the\narchive file, which may take several minutes depending on the\nspeed of your internet connection and the size of your dependency\narchive. Whether the operation succeeded or failed, App Services displays a banner\nindicating the success or failure of the operation. If successful,\nthe  Dependencies  tab displays a list of the\ndependencies that you included in your dependency archive. If\ndrafts are enabled, you will also need to click\n Review & Deploy  to apply these changes. If drafts are\ndisabled, the change will take effect within 5 to 60 seconds\ndepending on the size of your dependency archive. Add the  node_modules  archive to your  /functions \ndirectory: Push your application with the  --include-node-modules \noption: You can import built-in modules and external packages that you've added\nto your app and then use them in your functions. To import a package,\ncall  require()  with the package name from within the function body. Node.js projects commonly place  require()  calls in the global\nscope of each file, but App Services does not support this\npattern. You  must  place App Services  require()  calls\nwithin a function scope.",
            "code": [
                {
                    "lang": null,
                    "value": "appservices pull"
                },
                {
                    "lang": null,
                    "value": "cd path/to/MyRealmApp"
                },
                {
                    "lang": null,
                    "value": "appservices push --include-package-json"
                },
                {
                    "lang": null,
                    "value": "appservices pull"
                },
                {
                    "lang": null,
                    "value": "git push origin main"
                },
                {
                    "lang": "shell",
                    "value": "npm install <package name>"
                },
                {
                    "lang": "shell",
                    "value": "tar -czf node_modules.tar.gz node_modules/"
                },
                {
                    "lang": "shell",
                    "value": "mv node_modules.tar.gz ./myapp/functions"
                },
                {
                    "lang": "shell",
                    "value": "appservices push --include-node-modules"
                },
                {
                    "lang": "javascript",
                    "value": "exports = () => {\n   const R = require(\"ramda\");\n   return R.map(x => x*2, [1,2,3]);\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function(arg){\n   const cloneDeep = require(\"lodash/cloneDeep\");\n\n   var original = { name: \"Deep\" };\n   var copy = cloneDeep(original);\n   copy.name = \"John\";\n\n   console.log(`original: ${original.name}`);\n   console.log(`copy: ${copy.name}`);\n   return (original != copy);\n};"
                }
            ],
            "preview": "An external dependency is a library that includes code that you\ncan't or don't want to implement yourself. For example, you might use an\nofficial library for an external service or a custom implementation of\na data structure or algorithm.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "functions/mongodb/aggregate",
            "title": "Aggregate Data in MongoDB Atlas - Functions",
            "headings": [
                "Overview",
                "Data Model",
                "Snippet Setup",
                "Run an Aggregation Pipeline",
                "Find Data with Atlas Search",
                "Aggregation Stages",
                "Filter Documents",
                "Group Documents",
                "Project Document Fields",
                "Add Fields to Documents",
                "Unwind Array Values"
            ],
            "paragraphs": "The examples on this page demonstrate how to use the MongoDB Query API\nin a function to aggregate documents in your Atlas cluster. MongoDB  aggregation pipelines  run\nall documents in a collection through a series of data  aggregation\nstages  that allow you to filter and shape\ndocuments as well as collect summary data about groups of related\ndocuments. Atlas App Services supports nearly all MongoDB aggregation pipeline stages and\noperators, but some stages and operators must be executed within a\n system function . See  Aggregation Framework\nLimitations  for more\ninformation. The examples on this page use a collection named  store.purchases \nthat contains information about historical item sales in an online\nstore. Each document contains a list of the purchased  items ,\nincluding the item  name  and the purchased  quantity , as well as a\nunique ID value for the customer that purchased the items. To use a code snippet in a  function , you\nmust first instantiate a MongoDB collection handle: You can execute an aggregation pipeline using the\n collection.aggregate()  method. The following  function  snippet groups all documents\nin the  purchases  collection by their  customerId  value and\naggregates a count of the number of items each customer purchases as\nwell as the total number of purchases that they made. After grouping the\ndocuments the pipeline adds a new field that calculates the average\nnumber of items each customer purchases at a time,\n averageNumItemsPurchased , to each customer's document: You can run  Atlas Search  queries on a collection\nwith  collection.aggregate()  and the  $search  aggregation\nstage. App Services performs  $search  operations as a system user and\nenforces field-level rules on the returned search results. This means that a\nuser may search on a field for which they do not have read access. In this\ncase, the search is based on the specified field but no returned documents\ninclude the field. The  $$SEARCH_META \naggregation variable is only available for functions that  run as system  or if the first role on the searched collection has its\n apply_when  and  read  expressions set to  true . If neither of these two scenarios apply,  $$SEARCH_META  is undefined and\nthe aggregation will fail. You can use the  $match  stage\nto filter incoming documents using standard MongoDB  query syntax . The following  $match  stage filters incoming documents to include\nonly those where the  graduation_year  field has a value between\n 2019  and  2024 , inclusive. You can use the  $group  stage to aggregate summary\ndata for groups of one or more documents. MongoDB groups documents based\non the  _id  expression. You can reference a specific document field by prefixing the field\nname with a  $ . The following  $group  stage groups documents by the value of their\n customerId  field and calculates the number of purchase documents\nthat each  customerId  appears in. You can use the  $project  stage to include or omit\nspecific fields from documents or to calculate new fields using\n aggregation operators .\nTo include a field, set its value to  1 . To omit a field, set its\nvalue to  0 . You cannot simultaneously omit and include fields other than  _id .\nIf you explicitly include a field other than  _id , any fields you\ndid not explicitly include are automatically omitted (and\nvice-versa). The following  $project  stage omits the  _id  field, includes\nthe  customerId  field, and creates a new field named  numItems \nwhere the value is the number of documents in the  items  array: You can use the  $addFields  stage to add new fields\nwith calculated values using  aggregation operators . $addFields  is similar to  $project  but does not allow you to\ninclude or omit fields. The following  $addFields  stages creates a new field named\n numItems  where the value is the number of documents in the\n items  array: You can use the  $unwind  stage to aggregate\nindividual elements of array fields. When you unwind an array field,\nMongoDB copies each document once for each element of the array field\nbut replaces the array value with the array element in each copy. The following  $unwind  stage creates a new document for each\nelement of the  items  array in each document. It also adds a field\ncalled  itemIndex  to each new document that specifies the\nelement's position index in the original array: Consider the following document from the  purchases  collection: If we apply the example  $unwind  stage to this document, the stage\noutputs the following three documents:",
            "code": [
                {
                    "lang": "json\n :caption: The JSON schema for store.purchases",
                    "value": "{\n  \"title\": \"Purchase\",\n  \"required\": [\"_id\", \"customerId\", \"items\"],\n  \"properties\": {\n    \"_id\": { \"bsonType\": \"objectId\" },\n    \"customerId\": { \"bsonType\": \"objectId\" },\n    \"items\": {\n      \"bsonType\": \"array\",\n      \"items\": {\n        \"bsonType\": \"object\",\n        \"required\": [\"name\", \"quantity\"],\n        \"properties\": {\n          \"name\": { \"bsonType\": \"string\" },\n          \"quantity\": { \"bsonType\": \"int\" }\n        }\n      }\n    }\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const mongodb = context.services.get(\"mongodb-atlas\");\n  const itemsCollection = mongodb.db(\"store\").collection(\"items\");\n  const purchasesCollection = mongodb.db(\"store\").collection(\"purchases\");\n  // ... paste snippet here ...\n}"
                },
                {
                    "lang": "javascript",
                    "value": "const pipeline = [\n  { \"$group\": {\n      \"_id\": \"$customerId\",\n      \"numPurchases\": { \"$sum\": 1 },\n      \"numItemsPurchased\": { \"$sum\": { \"$size\": \"$items\" } }\n  } },\n  { \"$addFields\": {\n      \"averageNumItemsPurchased\": {\n        \"$divide\": [\"$numItemsPurchased\", \"$numPurchases\"]\n      }\n  } }\n]\n\nreturn purchasesCollection.aggregate(pipeline).toArray()\n  .then(customers => {\n    console.log(`Successfully grouped purchases for ${customers.length} customers.`)\n    for(const customer of customers) {\n      console.log(`customer: ${customer._id}`)\n      console.log(`num purchases: ${customer.numPurchases}`)\n      console.log(`total items purchased: ${customer.numItemsPurchased}`)\n      console.log(`average items per purchase: ${customer.averageNumItemsPurchased}`)\n    }\n    return customers\n  })\n  .catch(err => console.error(`Failed to group purchases by customer: ${err}`))"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function searchMoviesAboutBaseball() {\n  // 1. Get a reference to the collection you want to search.\n  const movies = context.services\n    .get(\"mongodb-atlas\")\n    .db(\"sample_mflix\")\n    .collection(\"movies\");\n  // 2. Run an aggregation with $search as the first stage.\n  const baseballMovies = await movies\n    .aggregate([\n      {\n        $search: {\n          text: {\n            query: \"baseball\",\n            path: \"plot\",\n          },\n        },\n      },\n      {\n        $limit: 5,\n      },\n      {\n        $project: {\n          _id: 0,\n          title: 1,\n          plot: 1,\n        },\n      },\n    ])\n    .toArray();\n  return baseballMovies;\n};"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"plot\" : \"A trio of guys try and make up for missed\n  opportunities in childhood by forming a three-player\n  baseball team to compete against standard children\n  baseball squads.\",\n  \"title\" : \"The Benchwarmers\"\n}\n{\n  \"plot\" : \"A young boy is bequeathed the ownership of a\n  professional baseball team.\",\n  \"title\" : \"Little Big League\"\n}\n{\n  \"plot\" : \"A trained chimpanzee plays third base for a\n  minor-league baseball team.\",\n  \"title\" : \"Ed\"\n}\n{\n  \"plot\" : \"The story of the life and career of the famed\n  baseball player, Lou Gehrig.\",\n  \"title\" : \"The Pride of the Yankees\"\n}\n{\n  \"plot\" : \"Babe Ruth becomes a baseball legend but is\n  unheroic to those who know him.\",\n  \"title\" : \"The Babe\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"$match\": {\n    \"<Field Name>\": <Query Expression>,\n    ...\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"$match\": {\n    \"graduation_year\": {\n      \"$gte\": 2019,\n      \"$lte\": 2024\n    },\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"$group\": {\n    \"_id\": <Group By Expression>,\n    \"<Field Name>\": <Aggregation Expression>,\n    ...\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"$group\": {\n    \"_id\": \"$customerId\",\n    \"numPurchases\": { \"$sum\": 1 }\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"$project\": {\n    \"<Field Name>\": <0 | 1 | Expression>,\n    ...\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"$project\": {\n    \"_id\": 0,\n    \"customerId\": 1,\n    \"numItems\": { \"$sum\": { \"$size\": \"$items\" } }\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"$addFields\": {\n    \"numItems\": { \"$sum\": { \"$size\": \"$items\" } }\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  $unwind: {\n    path: <Array Field Path>,\n    includeArrayIndex: <string>,\n    preserveNullAndEmptyArrays: <boolean>\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"$unwind\": {\n    \"path\": \"$items\",\n    \"includeArrayIndex\": \"itemIndex\"\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  _id: 123,\n  customerId: 24601,\n  items: [\n    { name: \"Baseball\", quantity: 5 },\n    { name: \"Baseball Mitt\", quantity: 1 },\n    { name: \"Baseball Bat\", quantity: 1 },\n  ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  _id: 123,\n  customerId: 24601,\n  itemIndex: 0,\n  items: { name: \"Baseball\", quantity: 5 }\n}, {\n  _id: 123,\n  customerId: 24601,\n  itemIndex: 1,\n  items: { name: \"Baseball Mitt\", quantity: 1 }\n}, {\n  _id: 123,\n  customerId: 24601,\n  itemIndex: 2,\n  items: { name: \"Baseball Bat\", quantity: 1 }\n}"
                }
            ],
            "preview": "The examples on this page demonstrate how to use the MongoDB Query API\nin a function to aggregate documents in your Atlas cluster.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "functions/context",
            "title": "Context",
            "headings": [
                "Overview",
                "Get App Metadata (context.app)",
                "Call a Function (context.functions)",
                "Check the App Environment (context.environment)",
                "context.environment.values",
                "Connect to a MongoDB Data Source or Third-Party Service (context.services)",
                "Get Request Metadata (context.request)",
                "Get User Data (context.user)",
                "Reference a Value (context.values)",
                "Send an HTTP Request (context.http)"
            ],
            "paragraphs": "Atlas Functions  have access to a global  context \nobject that contains metadata for the incoming requests and provides\naccess to components and services that you've configured in your App\nServices App. The  context  object exposes the following interfaces: Property Description context.app Access metadata about the app running the function. context.environment Access  environment values  and the\ncurrent environment tag. context.functions A client object that calls your app's  functions . context.http A built-in HTTP client. context.request Describes the incoming request that triggered a function call. context.services Exposes client objects that can access  data sources  and  services . context.user Describes the authenticated  user  that initiated the request. context.values Contains static global  values . The  context.app  object contains metadata about the App that\ncontains the Function. The unique internal ID of the App that contains the Function. The unique Client App ID for the App that contains the Function. The name of the App that contains the Function. The ID of the Atlas Project that contains the App. An object that describes the App's  deployment model and region . The date and time that the Atlas App was last deployed, formatted as\nan ISODate string. If  static hosting  is enabled, this value is\nthe base URL for hosted assets. You can call any  function  in your application\nthrough the  context.functions  interface. Calls a specific function and returns the result. Parameter Type Description functionName string The name of the function. args ... mixed A variadic list of arguments to pass to the function. Each\nfunction parameter maps to a separate, comma-separated\nargument. You can access information about your App's current  environment  configuration and access environment-specific\nvalues through the  context.environment  interface. The name of the app's current environment as a string. Possible values: \"\" \"development\" \"testing\" \"qa\" \"production\" An object where each field maps the name of an  environment value  to its value in the current environment. You can access a client for a linked MongoDB Atlas cluster or\nFederated data source through the  context.services  interface. You can\nalso access third-party services, although this feature is deprecated. Gets a service client for the specified service or  undefined  if\nno such service exists. Parameter Type Description serviceName string The name of the linked cluster, Federated database instance, or service. Linked  data sources  created by your\napp use one of the following default names: Cluster:  mongodb-atlas Federated database instance:  mongodb-datafederation You can access information about the incoming request with the\n context.request  interface. The  context.request  interface does not include request body\npayloads. In HTTPS endpoint functions, you can access the request\nbody and other request details from the provided  request \nargument. An object that contains information about the HTTP request that\ncaused the function to execute. Field Type Description remoteIPAddress string The IP address of the client that issued the Function request. requestHeaders object An object where each field maps to a type of  HTTP Header  that was included in the request that caused\nthe function to execute. The value of each field is an array of\nstrings where each string maps to a header of the specified type\nthat was included in the request. webhookUrl string Optional. In  HTTPS endpoint  functions,\nthe route of the endpoint. httpMethod string Optional. In  HTTPS endpoint  functions,\nthe  HTTP method  of the request that\ncalled the endpoint. rawQueryString string The  query string  attached to the\nincoming HTTP request. All query parameters appear in the same\norder as they were specified. For security reasons, Atlas App Services automatically removes any query\nstring key/value pair where the key is  secret . For\nexample, if an incoming request has the query string\n ?secret=hello&someParam=42  then the  rawQueryString  for\nthat request is  \"someParam=42\" . httpReferrer string Optional. The URL of the page from which the request was sent. This value is derived from the  HTTP Referer header . If the request did not include a\n Referer  header then this is  undefined . httpUserAgent string Optional. Characteristic information that identifies the source\nof the request, such as the software vendor, operating system, or\napplication type. This value is derived from the  HTTP User-Agent header . If the request did not include a\n User-Agent  header then this is  undefined . The following  context.request  document reflects a function call\nissued from  https://myapp.example.com/  by a user browsing with\nChrome 73 on macOS High Sierra: You can access information about the application or system user that\ncalled a function with the  context.user  interface. The  user object  of the authenticated user that\ncalled the function. Field Type Description id string A string representation of the  ObjectId  that uniquely identifies the\nuser. type string The type of the user. The following types are possible: Type Description \"normal\" The user is an  application user  logged in\nthrough an authentication provider other than the\n API Key  provider. \"server\" The user is a server process logged in with any type of\n App Services API Key . \"system\" The user is the  system user  that\nbypasses all rules. data document A document that contains metadata that describes the\nuser. This field combines the data for all  identities \nassociated with the user, so the exact field names and values\ndepend on which  authentication providers \nthe user has authenticated with. In  system functions , the  user.data \nobject is empty. Use  context.runningAsSystem()  to test if\nthe function is running as a system user. custom_data document A document from your application's  custom user\ndata collection  that\nspecifies the user's ID. You can use the custom user data\ncollection to store arbitrary data about your application's\nusers. If you set the  name  field, App Services populates the\n username  metadata field with the return value of  name .\nApp Services automatically fetches a new copy of the data\nwhenever a user refreshes their access token, such as when they\nlog in. The underlying data is a regular MongoDB document, so you\ncan use standard CRUD operations through the  MongoDB Atlas\nservice  to define and modify the user's custom data. Custom user data is limited to  16MB , the maximum size of a\nMongoDB document. To avoid hitting this limit, consider\nstoring small and relatively static user data in each custom\nuser data document, such as the user's preferred language or\nthe URL of their avatar image. For data that is large,\nunbounded, or frequently updated, consider only storing a\nreference to the data in the custom user document or storing\nthe data with a reference to the user's ID rather than in the\ncustom user document. identities array A list of authentication provider identities associated with the\nuser. When a user first logs in with a specific provider, App Services\nassociates the user with an identity object that contains a\nunique identifier and additional metadata about the user from the\nprovider. For subsequent logins, App Services refreshes the existing\nidentity data but does not create a new identity. Identity\nobjects have the following form: Field Name Description id A provider-generated string that uniquely identifies this\nidentity provider_type The type of authentication provider associated with this\nidentity. data Additional metadata from the authentication provider that\ndescribes the user. The exact field names and values will\nvary depending on which authentication providers the user\nhas logged in with. For a provider-specific breakdown of\nuser identity data, see  User Metadata . The following  context.user  document reflects an\n email/password \nuser that is associated with a single  User API Key . Evaluates to a boolean that is  true  if the function is running as\na  system user  and  false  otherwise. You can access your app's static  values  in a\nfunction with the  context.values  interface. Gets the data associated with the provided value name or\n undefined  if no such value exists. This data is either a plain\ntext JSON value or a  secret  exposed through a\nvalue. Parameter Type Description valueName string The name of the  value . You can send HTTPS requests through a built-in client with the\n context.http  interface. Sends an  HTTP GET  request to the\nspecified URL. See  http.get()  for detailed reference\ninformation, including parameter definitions and return types. Sends an  HTTP POST  request to the\nspecified URL. See  http.post()  for detailed reference\ninformation, including parameter definitions and return types. Sends an  HTTP PUT  request to the\nspecified URL. See  http.put()  for detailed reference\ninformation, including parameter definitions and return types. Sends an  HTTP PATCH  request to the\nspecified URL. See  http.patch()  for detailed reference\ninformation, including parameter definitions and return types. Sends an  HTTP DELETE  request to the\nspecified URL. See  http.delete()  for detailed reference\ninformation, including parameter definitions and return types. Sends an  HTTP HEAD  request to the\nspecified URL. See  http.head()  for detailed reference\ninformation, including parameter definitions and return types.",
            "code": [
                {
                    "lang": "typescript",
                    "value": "{\n  \"id\": string,\n  \"clientAppId\": string,\n  \"name\": string,\n  \"projectId\": string,\n  \"deployment\": {\n    \"model\": string,\n    \"providerRegion\": string,\n  },\n  \"lastDeployed\": string,\n  \"hostingUri\": string,\n}"
                },
                {
                    "lang": "json",
                    "value": "\"60c8e59866b0c33d14ee634a\""
                },
                {
                    "lang": "json",
                    "value": "\"myapp-abcde\""
                },
                {
                    "lang": "json",
                    "value": "\"myapp\""
                },
                {
                    "lang": "json",
                    "value": "\"5e1ec444970199272441a214\""
                },
                {
                    "lang": "json",
                    "value": "{\n  \"model\": \"LOCAL\",\n  \"providerRegion\": \"aws-us-east-1\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "\"2022-10-31T12:00:00.000Z\""
                },
                {
                    "lang": "json",
                    "value": "\"myapp-abcde.mongodbstitch.com\""
                },
                {
                    "lang": "javascript",
                    "value": "context.functions.execute(functionName, ...args)"
                },
                {
                    "lang": "javascript",
                    "value": "// difference: subtracts b from a using the sum function\nexports = function(a, b) {\n    return context.functions.execute(\"sum\", a, -1 * b);\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  switch(context.environment.tag) {\n    case \"\": {\n      return \"There is no current environment\"\n    }\n    case \"development\": {\n      return \"The current environment is development\"\n    }\n    case \"testing\": {\n      return \"The current environment is testing\"\n    }\n    case \"qa\": {\n      return \"The current environment is qa\"\n    }\n    case \"production\": {\n      return \"The current environment is production\"\n    }\n  }\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const baseUrl = context.environment.values.baseUrl\n};"
                },
                {
                    "lang": "javascript",
                    "value": "context.services.get(serviceName)"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  // Get the cluster's data source client\n  const mdb = context.services.get(\"mongodb-atlas\");\n  // Reference a specific database/collection\n  const db = mdb.db(\"myApp\");\n  const collection = db.collection(\"myCollection\");\n  // Run a MongoDB query\n  return await collection.find({\n    name: \"Rupert\",\n    age: { $lt: 50 },\n  })\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  // Instantiate a service client for the HTTP Service named \"myHttpService\"\n  const http = context.services.get(\"myHttpService\");\n  // Call the HTTP service's get() action\n  try {\n    const response = await http.get({ url: \"https://www.mongodb.com\" });\n    return response.body.text()\n  } catch(err) {\n    // You might get an error if:\n    // - you passed invalid arguments\n    // - the service's rules prevent the action\n    console.error(err)\n  }\n};"
                },
                {
                    "lang": "javascript",
                    "value": "{\n   \"remoteIPAddress\": <string>,\n   \"requestHeaders\": <object>,\n   \"webhookUrl\": <string>,\n   \"httpMethod\": <string>,\n   \"rawQueryString\": <string>,\n   \"httpReferrer\": <string>,\n   \"httpUserAgent\": <string>,\n   \"service\": <string>,\n   \"action\": <string>\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"requestHeaders\": {\n    \"Content-Type\": [\"application/json\"],\n    \"Cookie\": [\n      \"someCookie=someValue\",\n      \"anotherCookie=anotherValue\"\n    ]\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  return context.request\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"remoteIPAddress\": \"54.173.82.137\",\n  \"httpReferrer\": \"https://myapp.example.com/\",\n  \"httpUserAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36\",\n  \"rawQueryString\": \"?someParam=foo&anotherParam=42\",\n  \"requestHeaders\": {\n    \"Content-Type\": [\"application/json\"],\n    \"Cookie\": [\n      \"someCookie=someValue\",\n      \"anotherCookie=anotherValue\"\n    ]\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n    \"id\": <string>,\n    \"type\": <string>,\n    \"data\": <document>,\n    \"identities\": <array>\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<Unique ID>\",\n  \"provider_type\": \"<Provider Name>\",\n  \"data\": {\n    \"<Metadata Field>\": <Value>,\n    ...\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  return context.user\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"5cbf68583025b12840664682\",\n  \"type\": \"normal\",\n  \"data\": {\n    \"email\": \"someone@example.com\",\n    \"name\": \"myApiKeyName\"\n  },\n  \"identities\": [\n    {\n      \"id\": \"5cbf68583025b12880667681\",\n      \"provider_type\": \"local-userpass\"\n    },\n    {\n      \"id\": \"5cbf6c6a922616045a388c71\",\n      \"provider_type\": \"api-key\"\n    }\n  ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const isSystemUser = context.runningAsSystem()\n  if(isSystemUser) {\n    // Do some work that bypasses rules\n  } else {\n    // Do some work in the context of the user that called the function.\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  // Get a global value (or `undefined` if no value has the specified name)\n  const theme = context.values.get(\"theme\");\n  console.log(theme.colors)     // Output: { red: \"#ee1111\", blue: \"#1111ee\" }\n  console.log(theme.colors.red) // Output: \"#ee1111\"\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const response = await context.http.get({ url: \"https://www.example.com/users\" })\n  // The response body is a BSON.Binary object. Parse it and return.\n  return EJSON.parse(response.body.text());\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const response = await context.http.post({\n    url: \"https://www.example.com/messages\",\n    body: { msg: \"This is in the body of a POST request!\" },\n    encodeBodyAsJSON: true\n  })\n  // The response body is a BSON.Binary object. Parse it and return.\n  return EJSON.parse(response.body.text());\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const response = await context.http.put({\n    url: \"https://www.example.com/messages\",\n    body: { msg: \"This is in the body of a PUT request!\" },\n    encodeBodyAsJSON: true\n  })\n  // The response body is a BSON.Binary object. Parse it and return.\n  return EJSON.parse(response.body.text());\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const response = await context.http.patch({\n    url: \"https://www.example.com/diff.txt\",\n    body: { msg: \"This is in the body of a PATCH request!\" },\n    encodeBodyAsJSON: true\n  })\n  // The response body is a BSON.Binary object. Parse it and return.\n  return EJSON.parse(response.body.text());\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const response = await context.http.delete({ url: \"https://www.example.com/user/8675309\" })\n};"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function() {\n  const response = await context.http.head({ url: \"https://www.example.com/users\" })\n  // The response body is a BSON.Binary object. Parse it and return.\n  EJSON.parse(response.body.text());\n};"
                }
            ],
            "preview": "The unique internal ID of the App that contains the Function.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "functions/mongodb/read",
            "title": "Read Data from MongoDB Atlas - Functions",
            "headings": [
                "Overview",
                "Data Model",
                "Snippet Setup",
                "Query Methods",
                "Find a Single Document (findOne())",
                "Find One or More Documents (find())",
                "Count Documents in the Collection (count())",
                "Query Patterns",
                "Find by Document ID",
                "Find by Date",
                "Match a Root-Level Field",
                "Match Multiple Fields",
                "Match an Embedded Document Field",
                "Match an Array of Values",
                "Match an Array Element",
                "Query Operators",
                "Compare Values",
                "Evaluate a Logical Expression",
                "Evaluate a Regular Expression"
            ],
            "paragraphs": "The examples on this page demonstrate how to use the MongoDB Query API\nin a function to read documents from your Atlas cluster. Learn about the  methods  that you\ncan call to query data, the  operators \nthat let you write expressive match filters, and some  patterns  for combining them to handle common use cases. The examples on this page use a collection named  store.items  that\nmodels various items available for purchase in an online store. Each\nitem has a  name , an inventory  quantity , and an array of customer\n reviews . To use a code snippet in a  function , you\nmust first instantiate a MongoDB collection handle: You can find a single document using the  collection.findOne() \nmethod. The following  function  snippet finds a single\ndocument from the  items  collection that has a  quantity  greater\nthan or equal to  25 : You can find multiple documents using the  collection.find() \nmethod. The following  function  snippet finds all documents in\nthe  items  collection that have at least one review and returns them\nsorted by  name  with the  _id  field omitted: You can count documents in a collection using the\n collection.count()  method. You can specify a query to\ncontrol which documents to count. If you don't specify a query, the\nmethod counts all documents in the collection. The following  function  snippet counts the number of\ndocuments in the  items  collection that have at least one review: You can query a collection to find a document that has a specified ID.\nMongoDB automatically stores each document's ID as an  ObjectId  value\nin the document's  _id  field. The following query matches a document in the collection that has an\n _id  value of  5ad84b81b8b998278f773c1b : You can query a collection to find documents that have a field with a\nspecific date value, or query for a documents within a range of dates. The following query matches documents in the collection that have a\n createdAt  date of January 23, 2019: The following query matches documents in the collection that have a\n createdAt  date some time in the year 2019: You can query a collection based on the value of a root-level field in\neach document. You can specify either a specific value or a nested\nexpression that MongoDB evaluates for each document. For more information, see the  Query Documents  tutorial in the MongoDB Server Manual. The following query matches documents where the  name  field has a\nvalue of  Basketball : You can specify multiple query conditions in a single query document.\nEach root-level field of a query document maps to a field in the\ncollection. MongoDB only returns documents that fulfill all query\nconditions. For more information, see the  Query on Embedded/Nested\nDocuments  tutorial in the MongoDB\nServer Manual. The following query matches documents where the  name  field has a\nvalue of  Basketball  and the  quantity  value is greater than\nzero: You can query a collection based on the value of embedded document\nfields. To specify an embedded document field, use multiple nested query\nexpressions or standard document  dot notation . For more information, see the  Query on Embedded/Nested\nDocuments  tutorial in the MongoDB\nServer Manual. The following query matches documents where the first review in the\n reviews  array was left by someone with the username\n JoeMango : You can query a collection based on all the elements contained in an\narray field. If you query an array field for a specific array of values, MongoDB\nreturns documents where the array field  exactly matches  the specified\narray of values.\nIf you want MongoDB to return documents where the array field  contains \nall elements in the specified array of values, use the  $all  operator. For more information, see the  Query an Array  tutorial in the MongoDB Server Manual. The following query matches documents where the  reviews  array\ncontains exactly one element and the element matches the specified\ndocument: The following query matches documents where the  reviews  array\ncontains one or more elements that match all of the the specified\ndocuments: You can query a collection based on the value of one or more elements in\nan array field. If you query an array field with a query expression that has multiple\nconditions, MongoDB returns documents where  any combination  of the\narray's elements satisfy the expression. If you want MongoDB to return\ndocuments where a  single  array element satisfies all of the expression\nconditions, use the  $elemMatch  operator. For more information, see the  Query an Array  tutorial in the MongoDB Server Manual. The following query matches documents where both conditions in the\nembedded expression are met by  any combination  of elements in the\n reviews  array. The specified  username  and  comment  values\ndo not need to be in the same document: The following query matches documents where both conditions in the\nembedded expression are met by a  single  element in the  reviews \narray. The specified  username  and  comment  must be in the same\ndocument: You can use a  comparison operator  to compare the value of a\ndocument field to another value. The following comparison operators are available: Comparison Operator Description $eq Matches documents where the value of a field equals a specified\nvalue. $ne Matches documents where the value of a field does not equal a\nspecified value. $gt Matches documents where the value of a field is strictly greater\nthan a specified value. $gte Matches documents where the value of a field is greater than or\nequal to a specified value. $lt Matches documents where the value of a field is strictly less\nthan a specified value. $lte Matches documents where the value of a field is less than or\nequal to a specified value. $in Matches documents where the value of a field is included in a\nspecified array of values. $nin Matches documents where the value of a field is not included in\na specified array of values. The following query matches documents where  quantity  is greater\nthan zero and less than or equal to ten. You can use a  logical operator  to evaluate multiple expressions\nfor a single field. The following logical operators are available: Logical Operator Description $and Matches documents where the value of a field matches  all  of the\nspecified expressions. $or Matches documents where the value of a field matches  any  of the\nspecified expressions. $nor Matches documents where the value of a field matches  none  of\nthe specified expressions. $not Inverts the boolean result of the specified logical expression. The following query matches documents where either  quantity  is\ngreater than zero or there are no more than five documents in the\n reviews  array. You can use the  $regex  query\noperator to return documents with fields that match a\n regular expression . To avoid ambiguity\nwith the  $regex  EJSON type, you must use a  BSON.BSONRegExp  object. The following query matches documents where the  name  value\ncontains the substring  ball  (case-insensitive).",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"Item\",\n  \"required\": [\"_id\", \"name\", \"quantity\", \"reviews\"],\n  \"properties\": {\n    \"_id\": { \"bsonType\": \"objectId\" },\n    \"name\": { \"bsonType\": \"string\" },\n    \"quantity\": { \"bsonType\": \"int\" },\n    \"reviews\": {\n      \"bsonType\": \"array\",\n      \"items\": {\n        \"bsonType\": \"object\",\n        \"required\": [\"username\", \"comment\"],\n        \"properties\": {\n          \"username\": { \"bsonType\": \"string\" },\n          \"comment\": { \"bsonType\": \"string\" }\n        }\n      }\n    }\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const mongodb = context.services.get(\"mongodb-atlas\");\n  const itemsCollection = mongodb.db(\"store\").collection(\"items\");\n  const purchasesCollection = mongodb.db(\"store\").collection(\"purchases\");\n  // ... paste snippet here ...\n}"
                },
                {
                    "lang": "javascript",
                    "value": "const query = { \"quantity\": { \"$gte\": 25 } };\nconst projection = {\n \"title\": 1,\n \"quantity\": 1,\n}\n\nreturn itemsCollection.findOne(query, projection)\n  .then(result => {\n    if(result) {\n      console.log(`Successfully found document: ${result}.`);\n    } else {\n      console.log(\"No document matches the provided query.\");\n    }\n    return result;\n  })\n  .catch(err => console.error(`Failed to find document: ${err}`));"
                },
                {
                    "lang": "javascript",
                    "value": "const query = { \"reviews.0\": { \"$exists\": true } };\nconst projection = { \"_id\": 0 };\n\nreturn itemsCollection.find(query, projection)\n  .sort({ name: 1 })\n  .toArray()\n  .then(items => {\n    console.log(`Successfully found ${items.length} documents.`)\n    items.forEach(console.log)\n    return items\n  })\n  .catch(err => console.error(`Failed to find documents: ${err}`))"
                },
                {
                    "lang": "javascript",
                    "value": "return itemsCollection.count({ \"reviews.0\": { \"$exists\": true } })\n  .then(numDocs => console.log(`${numDocs} items have a review.`))\n  .catch(err => console.error(\"Failed to count documents: \", err))"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"_id\": <ObjectId> }"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"_id\": BSON.ObjectId(\"5ad84b81b8b998278f773c1b\") }"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"<Date Field Name>\": <Date | Expression> }"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"createdAt\": new Date(\"2019-01-23T05:00:00.000Z\") }"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"createdAt\": {\n    \"$gte\": new Date(\"2019-01-01T00:00:00.000Z\"),\n    \"$lt\": new Date(\"2020-01-01T00:00:00.000Z\"),\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"<Field Name>\": <Value | Expression> }"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"name\": \"Basketball\" }"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"<Field Name 1>\": <Value | Expression>,\n  \"<Field Name 2>\": <Value | Expression>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"name\": \"Basketball\",\n  \"quantity\": { \"$gt\": 0 }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"<Field Name>\": { \"<Nested Field Name>\": <Value | Expression> } }"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"<Field Name>.<Nested Field Name>\": <Value | Expression> }"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"reviews.0.username\": \"JoeMango\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"<Array Field Name>\": [<Value>, ...] }"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"reviews\": [{ username: \"JoeMango\", comment: \"This rocks!\" }]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"reviews\": {\n    \"$all\": [{ username: \"JoeMango\", comment: \"This rocks!\" }]\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"<Array Field Name>\": <Value | Expression> }"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"reviews\": {\n    \"username\": \"JoeMango\",\n    \"comment\": \"This is a great product!\"\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"reviews\": {\n    \"$elemMatch\": {\n      \"username\": \"JoeMango\",\n      \"comment\": \"This is a great product!\"\n    }\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"<Field Name>\": { \"<Comparison Operator>\": <Comparison Value> } }"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"quantity\": { \"$gt\": 0, \"$lte\": 10 }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"<Field Name>\": {\n    \"<Logical Operator>\": [<Expression>, ...]\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"$or\": [\n    { \"quantity\": { \"$gt\": 0 } },\n    { \"reviews\": { \"$size\": { \"$lte\": 5 } } }\n  ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"<Field Name>\": {\n    \"$regex\": BSON.BSONRegExp(<RegEx String>, <RegEx Options>)\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"name\": { \"$regex\": BSON.BSONRegExp(\".+ball\", \"i\") }\n}"
                }
            ],
            "preview": "The examples on this page demonstrate how to use the MongoDB Query API\nin a function to read documents from your Atlas cluster.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "functions/mongodb/write",
            "title": "Write Data in MongoDB Atlas - Functions",
            "headings": [
                "Overview",
                "Data Model",
                "Snippet Setup",
                "Insert",
                "Insert a Single Document (insertOne())",
                "Insert One or More Documents (insertMany())",
                "Update",
                "Update a Single Document (updateOne())",
                "Update One or More Documents (updateMany())",
                "Upsert Documents",
                "Field Update Operators",
                "Set the Value of a Field",
                "Rename a Field",
                "Increment a Value",
                "Multiply a Value",
                "Array Update Operators",
                "Push an Element Into an Array",
                "Pop the Last Element out of an Array",
                "Add a Unique Element to an Array",
                "Remove Elements from an Array",
                "Update All Elements in an Array",
                "Update Specific Elements in an Array",
                "Delete",
                "Delete a Single Document (deleteOne())",
                "Delete One or More Documents (deleteMany())",
                "Bulk Writes",
                "Transactions"
            ],
            "paragraphs": "The examples on this page demonstrate how to use the MongoDB Query API\nin a function to insert, update, and delete data in your Atlas cluster. Federated data sources  do not support write operations . The examples on this page use a collection named  store.items  that\nmodels various items available for purchase in an online store. Each\nitem has a  name , an inventory  quantity , and an array of customer\n reviews . To use a code snippet in a  function , you\nmust first instantiate a MongoDB collection handle: Insert operations take one or more documents and add them to a MongoDB collection. They return documents that describe the results of the operation. You can insert a single document using the\n collection.insertOne()  method. The following  function  snippet inserts a single item\ndocument into the  items  collection: You can insert multiple documents at the same time using the\n collection.insertMany()  method. The following  function  snippet inserts multiple item\ndocuments into the  items  collection: Update operations find existing documents in a MongoDB collection and\nmodify their data. You use standard MongoDB query syntax to specify\nwhich documents to update and  update operators  to describe the changes to apply to\nmatching documents. While running update operations, Atlas App Services temporarily adds a\nreserved field,  _id__baas_transaction , to documents. If you modify\ndata used by your app outside of App Services, you may need to\nunset this field. For more information, see  Transactional Locks . You can update a single document using the\n collection.updateOne()  method. The following  function  snippet updates the  name \nof a single document in the  items  collection from  lego  to\n blocks  and adds a  price  of  20.99 : Alternatively, you can update a single document using\n collection.findOneAndUpdate()  or\n collection.findOneAndReplace() . Both methods allow you to\nfind, modify, and return the updated document in a single operation. You can update multiple documents in a collection using the\n collection.updateMany()  method. The following  function  snippet updates all documents\nin the  items  collection by multiplying their  quantity  values by\n 10 : If an update operation does not match any document in the collection,\nyou can automatically insert a single new document into the collection\nthat matches the update query by setting the  upsert  option to\n true . The following  function  snippet updates a document in\nthe  items  collection that has a  name  of  board game  by\nincrementing its  quantity  by  5 . The  upsert  option is\nenabled, so if no document has a  name  value of  \"board game\"  then\nMongoDB inserts a new document with the  name  field set to  \"board\ngame\"  and the  quantity  value set to  5 : Field operators let you modify the fields and values of a document. You can use the  $set \noperator to set the value of a single field without affecting other\nfields in a document. You can use the  $rename \noperator to change the name of a single field in a document. You can use the  $inc \noperator to add a specified number to the current value of a field. The\nnumber can be positive or negative. You can use the  $mul \noperator to multiply a specified number with the current value of a\nfield. The number can be positive or negative. Array operators let you work with values inside of arrays. You can use the  $push \noperator to add a value to the end of an array field. You can use the  $pop  operator\nto remove either the first or last element of an array field. Specify\n -1  to remove the first element and  1  to remove the last element. You can use the  $addToSet  operator to add a value to an\narray field if that value is not already included in the array. If the\nvalue is already present,  $addToSet  does nothing. You can use the  $pull \noperator to remove all instances of any values that match a specified\ncondition from an array field. You can use the  $[] (All Positional Update)  operator to update all elements in\nan array field: Consider a  students  collection that describes individual students in a\nclass. The documents each include a  grades  field that contains an array\nof numbers: The following update operation adds 10 to all values in the  grades  array\nof every student: After the update, every grade value has increased by 10: You can use the  $[element] (Filtered Positional Update)  operator to update specific\nelements in an array field based on an array filter: Consider a  students  collection that describes individual students in a\nclass. The documents each include a  grades  field that contains an array\nof numbers, some of which are greater than 100: The following update operation sets all grade values greater than 100 to\nexactly 100: After the update, all grade values greater than 100 are set to exactly 100\nand all other grades are unaffected: Delete operations find existing documents in a MongoDB collection and\nremove them. You use standard MongoDB query syntax to specify which\ndocuments to delete. You can delete a single document from a collection using the\n collection.deleteOne()  method. The following  function  snippet deletes one document\nin the  items  collection that has a  name  value of  lego : Alternatively, you can update a single document using\n collection.findOneAndDelete() . This method allows you to\nfind, remove, and return the deleted document in a single operation. You can delete multiple items from a collection using the\n collection.deleteMany()  method. The following snippet deletes all documents in the  items  collection\nthat do not have any  reviews : A bulk write combines multiple write operations into a single operation.\nYou can issue a bulk write command using the\n collection.bulkWrite()  method. MongoDB supports  multi-document transactions  that let you read and write multiple documents\natomically, even across collections. To perform a transaction: The following example creates two users, \"henry\" and \"michelle\", and\na uses a transaction to move \"browniePoints\" between those users\natomically: Obtain and start a client session with  client.startSession() . Call  session.withTransaction()  to define the transaction. The\nmethod takes an async callback function and, optionally, a\nconfiguration object that defines custom  read and write\nsettings  for\nthe transaction. In the transaction callback function, run the MongoDB queries that\nyou would like to include in the transaction. Be sure to pass the\n session  to each query to ensure that it is included in the\ntransaction. If the callback encounters an error, call\n session.abortTransaction()  to stop the transaction. An aborted\ntransaction does not modify any data. When the transaction is complete, call  session.endSession()  to\nend the session and free resources.",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"Item\",\n  \"required\": [\"_id\", \"name\", \"quantity\", \"reviews\"],\n  \"properties\": {\n    \"_id\": { \"bsonType\": \"objectId\" },\n    \"name\": { \"bsonType\": \"string\" },\n    \"quantity\": { \"bsonType\": \"int\" },\n    \"reviews\": {\n      \"bsonType\": \"array\",\n      \"items\": {\n        \"bsonType\": \"object\",\n        \"required\": [\"username\", \"comment\"],\n        \"properties\": {\n          \"username\": { \"bsonType\": \"string\" },\n          \"comment\": { \"bsonType\": \"string\" }\n        }\n      }\n    }\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const mongodb = context.services.get(\"mongodb-atlas\");\n  const itemsCollection = mongodb.db(\"store\").collection(\"items\");\n  const purchasesCollection = mongodb.db(\"store\").collection(\"purchases\");\n  // ... paste snippet here ...\n}"
                },
                {
                    "lang": "javascript",
                    "value": "const newItem = {\n  \"name\": \"Plastic Bricks\",\n  \"quantity\": 10,\n  \"category\": \"toys\",\n  \"reviews\": [{ \"username\": \"legolover\", \"comment\": \"These are awesome!\" }]\n};\n\nitemsCollection.insertOne(newItem)\n  .then(result => console.log(`Successfully inserted item with _id: ${result.insertedId}`))\n  .catch(err => console.error(`Failed to insert item: ${err}`))"
                },
                {
                    "lang": "javascript",
                    "value": "const doc1 = { \"name\": \"basketball\", \"category\": \"sports\", \"quantity\": 20, \"reviews\": [] };\nconst doc2 = { \"name\": \"football\",   \"category\": \"sports\", \"quantity\": 30, \"reviews\": [] };\n\nreturn itemsCollection.insertMany([doc1, doc2])\n  .then(result => {\n    console.log(`Successfully inserted ${result.insertedIds.length} items!`);\n    return result\n  })\n  .catch(err => console.error(`Failed to insert documents: ${err}`))"
                },
                {
                    "lang": "javascript",
                    "value": "const query = { \"name\": \"lego\" };\nconst update = {\n  \"$set\": {\n    \"name\": \"blocks\",\n    \"price\": 20.99,\n    \"category\": \"toys\"\n  }\n};\nconst options = { \"upsert\": false };\n\nitemsCollection.updateOne(query, update, options)\n  .then(result => {\n    const { matchedCount, modifiedCount } = result;\n    if(matchedCount && modifiedCount) {\n      console.log(`Successfully updated the item.`)\n    }\n  })\n  .catch(err => console.error(`Failed to update the item: ${err}`))"
                },
                {
                    "lang": "javascript",
                    "value": "const query = {};\nconst update = { \"$mul\": { \"quantity\": 10 } };\nconst options = { \"upsert\": false }\n\nreturn itemsCollection.updateMany(query, update, options)\n  .then(result => {\n    const { matchedCount, modifiedCount } = result;\n    console.log(`Successfully matched ${matchedCount} and modified ${modifiedCount} items.`)\n    return result\n  })\n  .catch(err => console.error(`Failed to update items: ${err}`))"
                },
                {
                    "lang": "javascript",
                    "value": "const query = { \"name\": \"board games\" };\nconst update = { \"$inc\": { \"quantity\": 5 } };\nconst options = { \"upsert\": true };\n\nitemsCollection.updateOne(query, update, options)\n  .then(result => {\n    const { matchedCount, modifiedCount, upsertedId } = result;\n    if(upsertedId) {\n      console.log(`Document not found. Inserted a new document with _id: ${upsertedId}`)\n    } else {\n      console.log(`Successfully increased ${query.name} quantity by ${update.$inc.quantity}`)\n    }\n  })\n  .catch(err => console.error(`Failed to upsert document: ${err}`))"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"$set\": { \"<Field Name>\": <Value>, ... } }"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"$rename\": { \"<Current Field Name>\": <New Field Name>, ... } }"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"$inc\": { \"<Field Name>\": <Increment Number>, ... } }"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"$mul\": { \"<Field Name>\": <Multiple Number>, ... } }"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"$push\": { \"<Array Field Name>\": <New Array Element>, ... } }"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"$pop\": { \"<Array Field Name>\": <-1 | 1>, ... } }"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"$addToSet\": { \"<Array Field Name>\": <Potentially Unique Value>, ... } }"
                },
                {
                    "lang": "javascript",
                    "value": "{ \"$pull\": { \"<Array Field Name>\": <Value | Expression>, ... } }"
                },
                {
                    "lang": "json",
                    "value": "{ \"_id\" : 1, \"grades\" : [ 85, 82, 80 ] }\n{ \"_id\" : 2, \"grades\" : [ 88, 90, 92 ] }\n{ \"_id\" : 3, \"grades\" : [ 85, 100, 90 ] }"
                },
                {
                    "lang": "javascript",
                    "value": "await students.updateMany(\n   {},\n   { $inc: { \"grades.$[]\": 10 } },\n)"
                },
                {
                    "lang": "json",
                    "value": "{ \"_id\" : 1, \"grades\" : [ 95, 92, 90 ] }\n{ \"_id\" : 2, \"grades\" : [ 98, 100, 102 ] }\n{ \"_id\" : 3, \"grades\" : [ 95, 110, 100 ] }"
                },
                {
                    "lang": "json",
                    "value": "{ \"_id\" : 1, \"grades\" : [ 15, 92, 90 ] }\n{ \"_id\" : 2, \"grades\" : [ 18, 100, 102 ] }\n{ \"_id\" : 3, \"grades\" : [ 15, 110, 100 ] }"
                },
                {
                    "lang": "javascript",
                    "value": "await students.updateMany(\n   { },\n   {\n     $set: {\n       \"grades.$[grade]\" : 100\n     }\n   },\n   {\n     arrayFilters: [{ \"grade\": { $gt: 100 } }]\n   }\n)"
                },
                {
                    "lang": "json",
                    "value": "{ \"_id\" : 1, \"grades\" : [ 15, 92, 90 ] }\n{ \"_id\" : 2, \"grades\" : [ 18, 100, 100 ] }\n{ \"_id\" : 3, \"grades\" : [ 15, 100, 100 ] }"
                },
                {
                    "lang": "javascript",
                    "value": "const query = { \"name\": \"lego\" };\n\nitemsCollection.deleteOne(query)\n  .then(result => console.log(`Deleted ${result.deletedCount} item.`))\n  .catch(err => console.error(`Delete failed with error: ${err}`))"
                },
                {
                    "lang": "javascript",
                    "value": "const query = { \"reviews\": { \"$size\": 0 } };\n\nitemsCollection.deleteMany(query)\n  .then(result => console.log(`Deleted ${result.deletedCount} item(s).`))\n  .catch(err => console.error(`Delete failed with error: ${err}`))"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function(arg){\n    const doc1 = { \"name\": \"velvet elvis\", \"quantity\": 20, \"reviews\": [] };\n    const doc2 = { \"name\": \"mock turtleneck\",  \"quantity\": 30, \"reviews\": [] };\n\n    var collection = context.services.get(\"mongodb-atlas\")\n        .db(\"store\")\n        .collection(\"purchases\");\n\n    return await collection.bulkWrite(\n        [{ insertOne: doc1}, { insertOne: doc2}],\n        {ordered:true});\n};"
                },
                {
                    "lang": "javascript",
                    "value": "session.withTransaction(async () => {\n   // ... Run MongoDB operations in this callback\n}, {\n    readPreference: \"primary\",\n    readConcern: { level: \"local\" },\n    writeConcern: { w: \"majority\" },\n})"
                },
                {
                    "lang": "javascript",
                    "value": "await accounts.updateOne(\n  { name: userSubtractPoints },\n  { $inc: { browniePoints: -1 * pointVolume } },\n  { session }\n);"
                },
                {
                    "lang": "javascript",
                    "value": "try {\n  // ...\n} catch (err) {\n  await session.abortTransaction();\n}"
                },
                {
                    "lang": "javascript",
                    "value": "try {\n  // ...\n} finally {\n  await session.endSession();\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function () {\n  const client = context.services.get(\"mongodb-atlas\");\n\n  db = client.db(\"exampleDatabase\");\n\n  accounts = db.collection(\"accounts\");\n  browniePointsTrades = db.collection(\"browniePointsTrades\");\n\n  // create user accounts with initial balances\n  accounts.insertOne({ name: \"henry\", browniePoints: 42 });\n  accounts.insertOne({ name: \"michelle\", browniePoints: 144 });\n\n  // trade points between user accounts in a transaction\n  tradeBrowniePoints(\n    client,\n    accounts,\n    browniePointsTrades,\n    \"michelle\",\n    \"henry\",\n    5\n  );\n\n  return \"Successfully traded brownie points.\";\n};\n\nasync function tradeBrowniePoints(\n  client,\n  accounts,\n  browniePointsTrades,\n  userAddPoints,\n  userSubtractPoints,\n  pointVolume\n) {\n  // Step 1: Start a Client Session\n  const session = client.startSession();\n\n  // Step 2: Optional. Define options to use for the transaction\n  const transactionOptions = {\n    readPreference: \"primary\",\n    readConcern: { level: \"local\" },\n    writeConcern: { w: \"majority\" },\n  };\n\n  // Step 3: Use withTransaction to start a transaction, execute the callback, and commit (or abort on error)\n  // Note: The callback for withTransaction MUST be async and/or return a Promise.\n  try {\n    await session.withTransaction(async () => {\n      // Step 4: Execute the queries you would like to include in one atomic transaction\n      // Important:: You must pass the session to the operations\n      await accounts.updateOne(\n        { name: userSubtractPoints },\n        { $inc: { browniePoints: -1 * pointVolume } },\n        { session }\n      );\n      await accounts.updateOne(\n        { name: userAddPoints },\n        { $inc: { browniePoints: pointVolume } },\n        { session }\n      );\n      await browniePointsTrades.insertOne(\n        {\n          userAddPoints: userAddPoints,\n          userSubtractPoints: userSubtractPoints,\n          pointVolume: pointVolume,\n        },\n        { session }\n      );\n    }, transactionOptions);\n  } catch (err) {\n    // Step 5: Handle errors with a transaction abort\n    await session.abortTransaction();\n  } finally {\n    // Step 6: End the session when you complete the transaction\n    await session.endSession();\n  }\n}"
                }
            ],
            "preview": "The examples on this page demonstrate how to use the MongoDB Query API\nin a function to insert, update, and delete data in your Atlas cluster.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "functions/mongodb/api/snippets/setup",
            "title": "Snippet Setup",
            "headings": [],
            "paragraphs": "To use a code snippet in a  function , you\nmust first instantiate a MongoDB collection handle:",
            "code": [
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const mongodb = context.services.get(\"mongodb-atlas\");\n  const itemsCollection = mongodb.db(\"store\").collection(\"items\");\n  const purchasesCollection = mongodb.db(\"store\").collection(\"purchases\");\n  // ... paste snippet here ...\n}"
                }
            ],
            "preview": "To use a code snippet in a function, you\nmust first instantiate a MongoDB collection handle:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "functions/mongodb/api/snippets/data-model",
            "title": "Data Model",
            "headings": [],
            "paragraphs": "The examples on this page use a collection named  store.items  that\nmodels various items available for purchase in an online store. Each\nitem has a  name , an inventory  quantity , and an array of customer\n reviews .",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"Item\",\n  \"required\": [\"_id\", \"name\", \"quantity\", \"reviews\"],\n  \"properties\": {\n    \"_id\": { \"bsonType\": \"objectId\" },\n    \"name\": { \"bsonType\": \"string\" },\n    \"quantity\": { \"bsonType\": \"int\" },\n    \"reviews\": {\n      \"bsonType\": \"array\",\n      \"items\": {\n        \"bsonType\": \"object\",\n        \"required\": [\"username\", \"comment\"],\n        \"properties\": {\n          \"username\": { \"bsonType\": \"string\" },\n          \"comment\": { \"bsonType\": \"string\" }\n        }\n      }\n    }\n  }\n}"
                }
            ],
            "preview": "The examples on this page use a collection named store.items that\nmodels various items available for purchase in an online store. Each\nitem has a name, an inventory quantity, and an array of customer\nreviews.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "functions/globals",
            "title": "Global Modules",
            "headings": [
                "Utilities",
                "JSON Web Tokens (utils.jwt)",
                "Cryptography (utils.crypto)",
                "JSON (JavaScript Object Notation)",
                "EJSON (Extended JSON)",
                "BSON (Binary JSON)",
                "BSON.ObjectId",
                "BSON.BSONRegExp",
                "BSON.Binary",
                "BSON.MaxKey",
                "BSON.MinKey",
                "BSON.Int32",
                "BSON.Long",
                "BSON.Double",
                "BSON.Decimal128"
            ],
            "paragraphs": "All  functions  have access to built-in global modules\nthat support common data transformation, encoding, and processing work.\nYou can access the modules in your function source code via global\nvariables specific to each module. These global modules are  not  the same as the Node.js built-in modules.\nFor more information on supported Node.js modules, including how to\nimport them, see  Built-in Module Support . Module Description JSON Web Tokens Methods to read and write JSON Web Tokens. Cryptography Methods that implement cryptographic algorithms like hashes and\nsignatures. JSON Methods that convert between string and object\nrepresentations of  JSON  data. EJSON Methods that convert between string and object\nrepresentations of  Extended JSON  data. BSON Methods that create  Binary JSON \nobjects and convert between various BSON data types and\nencodings. You can create and read  JSON Web Tokens  with\nthe  utils.jwt  interface. Method Description utils.jwt.encode() Generates an encoded JSON Web Token string for a given\n payload ,  signingMethod , and  secret . utils.jwt.decode() Decodes the  payload  of a JSON Web Token string. Generates an encoded JSON Web Token string for the  payload  based\non the specified  signingMethod  and  secret . Parameter Type Description signingMethod string The cryptographic algorithm to use when encoding the JWT.\nAtlas App Services supports the following JWT signing methods: Signing Method Description \"HS256\" HMAC using SHA-256 \"HS384\" HMAC using SHA-384 \"HS512\" HMAC using SHA-512 \"RS256\" RSASSA-PKCS1-v1_5 using SHA-256 \"RS384\" RSASSA-PKCS1-v1_5 using SHA-384 \"RS512\" RSASSA-PKCS1-v1_5 using SHA-512 \"ES256\" ECDSA using P-256 and SHA-256 \"ES384\" ECDSA using P-384 and SHA-384 \"ES512\" ECDSA using P-512 and SHA-512 \"PS256\" RSASSA-PSS using SHA-256 and MGF1 with SHA-256 \"PS384\" RSASSA-PSS using SHA-384 and MGF1 with SHA-384 \"PS512\" RSASSA-PSS using SHA-512 and MGF1 with SHA-512 payload object A JSON object that specifies the token's claims and any\nadditional related data. secret string A secret string that App Services uses to sign the token. The value\nof the string depends on the signing method that you use: Signing Methods Description A random string. An RSA-SHA private key in PKCS#8 format. An RSA-PSS private key in PKCS#8 format. An  ECDSA   private\nkey in PKCS#8 format. customHeaderFields object A JSON object that specifies additional fields to include in\nthe JWT's  JOSE header . A JSON Web Token string encoded for the provided  payload . Consider the following JWT claims object: We can encode the claims object as a JWT string by calling\n utils.jwt.encode() . The following function encodes the JWT\nusing the  HS512  signing method and the secret\n \"SuperSecret\" : Decodes the  payload  of the provided JSON Web Token string. The\nvalue of  key  must correspond to the secret value that was used to\nencode the JWT string. Parameter Type Description jwtString string A JSON Web Token string that encodes a set of claims signed\nwith a secret value. key string A string that App Services uses to verify the token signature. The\nvalue of the string depends on the signing method that you use: Signing Methods Description The random string that was used to sign the token. The RSA-SHA public key that corresponds to the private\nkey that was used to sign the token. The RSA-PSS public key that corresponds to the private\nkey that was used to sign the token. The  ECDSA  public\nkey that corresponds to the private key that was used\nto sign the token. returnHeader boolean If  true , return the JWT's  JOSE header  in addition\nto the decoded payload. acceptedSigningMethods string[] Optional. Array of accepted signing methods. For example,  [\"PS256\", \"HS256\"] .\nThis argument should be included to prevent against\n known JWT security vulnerabilities . If  returnHeader  is  false , returns the decoded EJSON\npayload. If  returnHeader  is  true , returns an object that contains\nthe  JOSE header  in the\n header  field and the decoded EJSON payload in the  payload \nfield. Consider the following signed JWT string: The JWT was signed using the  HS512  signing method with the\nsecret value  \"SuperSecret\" . We can decode the JWT's claims\nobject  utils.jwt.decode() . The following function decodes the\nJWT string into an EJSON object: You can encrypt, decrypt, sign, and verify data using cryptographic\nalgorithms with the  utils.crypto  interface. Method Description utils.crypto.encrypt() Generates an encrypted text string from a given text string using\na specific encryption method and key. utils.crypto.decrypt() Decrypts a provided text string using a specific encryption\nmethod and key. utils.crypto.sign() Generates a cryptographically unique signature for a given\nmessage using a private key. utils.crypto.verify() Verifies that a signature is valid for a given message and public\nkey. utils.crypto.hmac() Generates an  HMAC  signature from a given\ninput and secret. utils.crypto.hash() Generates a hash value for a given input and hash function. Generates an encrypted text string from the provided text using the\nspecified encryption method and key. Parameter Type Description encryptionType string The type of encryption with which to encrypt the message. The\nfollowing encryption types are supported: AES Encryption  ( \"aes\" ) message string The text string that you want to encrypt. key string A cryptographic key used to encrypt the text. The key\nyou should use depends on the encryption method: Encryption Type Encryption Key AES A 16-byte, 24-byte, or 32-byte random string A  BSON Binary  object that contains the text\nstring encrypted with the specified encryption type and key. Assume that we have defined a  value \nnamed  aesEncryptionKey  that contains the following 32-byte AES\nencryption key: With this AES key, we can encrypt a message into a base64 string\nusing the following function: Decrypts the provided text string using the specified encryption type\nand key. If both the encryption type and key are the same as those\nused to encrypt, this returns the original, unencrypted text. Parameter Type Description encryptionType string The type of encryption that was used to encrypt the provided\ntext. The following encryption types are supported: AES Encryption  ( \"aes\" ) encryptedMessage BSON.Binary A BSON Binary that encodes the encrypted text string that\nyou want to decrypt. key string A cryptographic key used to decrypt the text. The key\nyou should use depends on the encryption type: Encryption Type Encryption Key AES A 16-byte, 24-byte, or 32-byte random string A  BSON Binary  object that contains the\ndecrypted message. If the provided encrypted message was encrypted with the\nspecified method and key, then the decrypted message is identical\nto the original message. Assume that we have defined a  Value  named\n aesEncryptionKey  that contains the following 32-byte AES\nencryption key: We can use this AES key to decrypt any base64 string that was\nencrypted with the same key using the following function: Generates a cryptographically unique signature for a message using a\nprivate key. The signature can be verified with the corresponding\npublic key to ensure that the signer has access to the private key\nand that the message content has not been altered since it was\nsigned. Parameter Type Description encryptionType string The type of encryption that was used to generate the\nprivate/public key pair. The following encryption types are\nsupported: RSA Encryption  ( \"rsa\" ) message string The text string that you want to sign. privateKey string A private key generated with the specified encryption type. Not all RSA keys use the same format. App Services can only sign\nmessages with a private key that conforms to the standard\n PKCS#1  format. Private keys in this\nformat have the header  -----BEGIN RSA PRIVATE KEY----- . You can use the following shell script to generate a valid RSA\nprivate/public key pair and save each key to its own text file: signatureScheme string Optional. Default:  \"pss\" The  padding scheme  that\nthe signature should use. App Services supports signing messages\nwith the following schemes: Probabilistic signature scheme  ( \"pss\" ) PKCS1v1.5  ( \"pkcs1v15\" ) A  BSON.Binary   cryptographic\nsignature  for the message signed using the\nspecified private key. Assume that we have defined a  value \nnamed  rsaPrivateKey  that contains the following RSA private key: With this RSA key, we can sign a message using the following\nfunction: Checks that the provided signature is valid for the specified message\nand public key. If the signature is valid, it guarantees that the signer has access\nto the corresponding private key and that the message content has not\nbeen altered since it was signed. Parameter Type Description encryptionType string The type of encryption that was used to generate the\nprivate/public key pair. The following encryption types are\nsupported: RSA Encryption  ( \"rsa\" ) message string The text string for which you want to verify the signature. If\nthe signature is valid, this is the exact message that was\nsigned. publicKey string The public key for which you want to verify the signature. If\nthe signature is valid, this is the corresponding public key\nof the private key that was used to sign the message. Not all RSA keys use the same format. App Services can only\nverify signatures with RSA keys that conform to the\nstandard  PKCS#1  format. Public keys\nin this format have the header  -----BEGIN RSA PUBLIC\nKEY----- . You can use the following shell script to generate a valid RSA\nprivate/public key pair and save each key to its own text file: signature BSON.Binary The signature that you want to verify. signatureScheme string Optional. Default:  \"pss\" The  padding scheme  that\nthe signature uses. App Services supports verifying signatures that\nuse the following schemes: Probabilistic signature scheme  ( \"pss\" ) PKCS1v1.5  ( \"pkcs1v15\" ) A boolean that, if  true , indicates whether or not the\nsignature is valid for the provided message and public key. We received a message with a signature in  BSON.Binary  format and want to verify that the message was\nsigned with the private key that corresponds to the sender's RSA\npublic key: With this RSA key, we can verify a message signed with the\ncorrespdoning private key using the following function: Generates an  HMAC  signature from the provided\ninput and secret. This is useful when communicating with third-party\nHTTP services that require signed requests. Parameter Type Description input string The input for which you would like to generate a signature. secret string The secret key to use when generating the signature. hash_function string The name of the hashing function to use when generating the\nsignature. The following functions are supported:  \"sha1\" ,\n \"sha256\" ,  \"sha512\" . output_format string The format of the generated signature. Can be either\n \"hex\"  for a hex string, or  \"base64\"  for a Base64\nstring. The signature of the input, in the format specified by\n output_format . The following function generates a sha256 signature as a\nbase 64 string: Generates a hash value for the provided input using the specified\nhash function. Parameter Type Description hash_function string The name of the hashing function.\nThe following functions are supported:  \"sha1\" ,\n \"sha256\" ,  \"md5\" . input string or BSON.Binary Required.\nThe input for which you would like to generate a hash value. A  BSON.Binary  object that encodes the hash value for the\nprovided input generated by the specified hashing function. The following function hashes an input string with sha256: The  JSON  global module provides  JSON methods  to serialize and\ndeserialize standard JavaScript objects. Method Description JSON.parse() Parse a serialized JSON string into a JavaScript object. JSON.stringify() Serialize a JavaScript object into a JSON string. Parses the provided JSON string and converts it to a JavaScript\nobject. Parameter Type Description jsonString string A serialized string representation of a standard JSON object. A standard JavaScript object generated from the provided JSON\nstring. The following function converts a serialized JSON string into an\nequivalent JavaScript object: Serializes the provided JavaScript object to a JSON string. Parameter Type Description object object A standard  JavaScript Object . A string representation of the provided JavaScript object. The following function serializes a JavaScript object into an\nequivalent JSON string: The  EJSON  global module is similar to  JSON  but\npreserves additional  Extended JSON  type information. EJSON is a superset of standard JSON that adds additional support for\ntypes that are available in  BSON  but not included in the\n JSON specification . Method Description EJSON.parse() Parse a serialized Extended JSON string into a JavaScript object. EJSON.stringify() Serialize a JavaScript object into an ExtendedJSON string. Parses the provided  EJSON  string and converts it to a\nJavaScript object. Parameter Type Description ejsonString string A serialized string representation of an Extended JSON object. A JavaScript object representation of the provided EJSON string. The following function converts a serialized EJSON string into an\nequivalent JavaScript object: Serializes the provided JavaScript object to an  EJSON  string. Parameter Type Description object document An  EJSON  object. The object may\ncontain BSON types. A string representation of the provided EJSON object. The following function serializes a JavaScript object into an\nequivalent EJSON string: The  BSON  global module allows you to create typed BSON objects and\nconvert them between various data formats and encodings. BSON , or Binary JSON, is the data format used\ninternally by MongoDB databases. It encodes a binary representation of\ndocument data structures using a superset of the standard JSON types.\nFor more information, refer to the  BSON specification . Type Description BSON.ObjectId Represent an ObjectId value BSON.BSONRegExp Represent a regular expression BSON.Binary Represent a binary data structure BSON.MaxKey Represent a value that compares higher than all other values BSON.MinKey Represent a value that compares lower than all other values BSON.Int32 Represent a 32-bit signed integer BSON.Long Represent a 64-bit signed integer BSON.Double Represent a 64-bit floating point number BSON.Decimal128 Represent a 128-bit floating point number The  BSON.ObjectId  type represents a 12-byte MongoDB  ObjectId \nidentifier. Constructs a  BSON.ObjectId  object that encodes an\n ObjectId Parameter Type Description id string Optional. A 12-byte string or a string of 24 hex characters. A  BSON.ObjectId  object that encodes the specified\n ObjectId  string or a\ngenerated  ObjectId  string if none was specified. The  BSON.BSONRegExp  type represents a  regular expression . You can use a\n BSON.BSONRegExp  object with the  $regex  query operator to\n perform a regular expression query  on a\nMongoDB collection. Constructs a  BSON.BSONRegExp  object from a regular expression\nstring. You can optionally specify configuration flags. Parameter Type Description pattern string A  regular expression  pattern. flags string Optional. One or more  regular expression flags . A  BSON.BSONRegExp  object that encodes the provided regular\nexpression pattern and flags. The  BSON.Binary  type represents a binary-encoded data string. Constructs a  BSON.Binary  object from data represented as a\nhexadecimal string. Parameter Type Description hexString string A  byte aligned  string\nof hexadecimal characters (0-9 and A-F). subType integer Optional. The type of data encoded in the hexadecimal string.\nThe value must be in the range 0-255 where  0 , the default\nvalue, represents a generic binary. For a full list of\nsupported subtypes, refer to the  BSON specification . A  BSON.Binary  object that encodes the provided hexadecimal\nstring. Converts the  BSON.Binary  object into a hexadecimal string. A hexadecimal string representation of the provided\n BSON.Binary  object. Constructs a  BSON.Binary  object from data represented as a base64\nstring. Parameter Type Description base64String string A string of base64 encoded characters. The base64-encoded string must include either one or two equals\nsigns ( = ), referred to as \"padding\", at the end of the string.\n BSON.Binary.fromBase64()  does not support unpadded strings. subType integer Optional. The type of data encoded in the hexadecimal string.\nThe value must be in the range 0-255 where  0 , the default\nvalue, represents a generic binary. For a full list of\nsupported subtypes, refer to the  BSON specification . A  BSON.Binary  object that encodes the provided base64 string. Converts a  BSON.Binary  object into a base64 string. A base64 string representation of the  BSON.Binary  object. Converts the  BSON.Binary  object into a UTF-8 string. A UTF-8 string representation of the provided  BSON.Binary \nobject. The  BSON.MaxKey  type represents a value that compares higher than\nall other BSON values. The  BSON.MinKey  type represents a value that compares lower than all\nother BSON values. The  BSON.Int32  type represents a 32-bit signed integer. Constructs a  BSON.Int32  object from a 32-bit number. Parameter Type Description low32 number A 32-bit number. A  BSON.Int32  object that encodes the specified integer.\nReturns  0  if no arguments are supplied. The  BSON.Long  type represents a 64-bit signed integer. Constructs a  BSON.Long  object from two 32-bit integers that\nrepresent the low 32 bits and the high 32 bits in the 64-bit  Long \ninteger. Parameter Type Description low32 integer Optional. The long integer's 32 low bits. These bits represent\nthe least significant digits of the number. high32 integer Optional. The long integer's 32 high bits. These bits\nrepresent the most significant digits of the number. A  BSON.Long  object that encodes the specified integer.\nReturns  0  if no arguments are supplied. BSON.Long  encodes using the following formula: The  BSON.Double  type represents a 64-bit (8-byte) floating point\nnumber. BSON.Double  is subject to floating point rounding errors, so it\nis not recommended for use cases where decimal values must round\nexactly, e.g. financial data. For these cases, use\n BSON.Decimal128  instead. Constructs a  BSON.Double  object from a 64-bit decimal value. Parameter Type Description double number A 64-bit decimal value. A  BSON.Double  object that encodes the specified double.\nReturns  0  if no argument is supplied. The  BSON.Decimal128  type represents a 128-bit (16-byte) floating\npoint number. This type is intended for use cases where decimal values\nmust round exactly, e.g. financial data. Constructs a  BSON.Decimal128  from a string representation of a\ndecimal number. Parameter Type Description decimalString string A string representing a decimal number, e.g.  \"1234.5678910123456\" . A  BSON.Decimal128  that encodes the provided decimal value.",
            "code": [
                {
                    "lang": "typescript",
                    "value": "utils.jwt.encode(\n   signingMethod: string,\n   payload: object,\n   secret: string,\n   customHeaderFields: object\n): string"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"sub\": \"1234567890\",\n  \"name\": \"Joe Example\",\n  \"iat\": 1565721223187\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const signingMethod = \"HS512\";\n  const payload = {\n    \"sub\": \"1234567890\",\n    \"name\": \"Joe Example\",\n    \"iat\": 1565721223187\n  };\n  const secret = \"SuperSecret\";\n  return utils.jwt.encode(signingMethod, payload, secret);\n}"
                },
                {
                    "lang": "json",
                    "value": "\"eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvZSBTY2htb2UiLCJpYXQiOjE1NjU3MjEyMjMxODd9.-QL15ldu2BYuJokNWT6YRiwZQIiIpvq6Kyv-C6qslNdNiUVxo8zqLJZ1iEkNf2yZKMIrQuMCtIC1tzd2H31AxA\""
                },
                {
                    "lang": "typescript",
                    "value": "utils.jwt.decode(\n   jwtString: string,\n   key: string,\n   returnHeader: boolean\n): object"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"header\": {\n    \"<JOSE Header Field>\": <JOSE Header Value>,\n    ...\n  },\n  \"payload\": {\n    \"<JWT Claim Field>\": <JWT Claim Value>,\n    ...\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "\"eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvZSBTY2htb2UiLCJpYXQiOjE1NjU3MjEyMjMxODd9.-QL15ldu2BYuJokNWT6YRiwZQIiIpvq6Kyv-C6qslNdNiUVxo8zqLJZ1iEkNf2yZKMIrQuMCtIC1tzd2H31AxA\""
                },
                {
                    "lang": "javascript",
                    "value": "exports = function(jwtString) {\n  const key = \"SuperSecret\";\n  return utils.jwt.decode(jwtString, key, false, [\"HS512\"]);\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"sub\": \"1234567890\",\n  \"name\": \"Joe Example\",\n  \"iat\": { \"$numberDouble\": 1565721223187 }\n}"
                },
                {
                    "lang": "typescript",
                    "value": "utils.crypto.encrypt(\n  encryptionType: string,\n  message: string,\n  key: string\n): BSON.Binary"
                },
                {
                    "lang": "json",
                    "value": "\"603082712271C525E087BD999A4E0738\""
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const message = \"MongoDB is great!\"\n  const key = context.values.get(\"aesEncryptionKey\");\n  const encryptedMessage = utils.crypto.encrypt(\"aes\", message, key);\n  return encryptedMessage.toBase64();\n}"
                },
                {
                    "lang": "json",
                    "value": "\"WPBuIvJ6Bity43Uh822dW8QlVYVJaFUiDeUjlTiJXzptUuTYIKPlXekBQAJb\""
                },
                {
                    "lang": "typescript",
                    "value": "utils.crypto.decrypt(\n  encryptionType: string,\n  encryptedMessage: BSON.Binary,\n  key: string\n): BSON.Binary"
                },
                {
                    "lang": "json",
                    "value": "\"603082712271C525E087BD999A4E0738\""
                },
                {
                    "lang": "javascript",
                    "value": "exports = function(encryptedMessage) {\n  // The encrypted message must be a BSON.Binary\n  if(typeof encryptedMessage === \"string\") {\n    encryptedMessage = BSON.Binary.fromBase64(encryptedMessage)\n  }\n  const key = context.values.get(\"aesEncryptionKey\");\n  const decryptedMessage = utils.crypto.decrypt(\"aes\", encryptedMessage, key);\n  return decryptedMessage.text();\n}"
                },
                {
                    "lang": "json",
                    "value": "\"MongoDB is great!\""
                },
                {
                    "lang": "typescript",
                    "value": "utils.crypto.sign(\n  encryptionType: string,\n  message: string,\n  privateKey: string,\n  signatureScheme: string\n): BSON.Binary"
                },
                {
                    "lang": "shell",
                    "value": "# Generate an RSA SSH key pair\n# Save the key to a file called `rsa_key` when prompted\nssh-keygen -t rsa -m PEM -b 2048 -C \"2048-bit RSA Key\"\n\n# Private Key\ncat rsa_key > rsa.private.txt\n# Public Key\nssh-keygen -f rsa_key.pub -e -m pem > rsa.public.txt"
                },
                {
                    "lang": "none",
                    "value": "-----BEGIN RSA PRIVATE KEY-----\nMIICXAIBAAKBgQDVsEjse2qO4v3p8RM/q8Rqzloc1lee34yoYuKZ2cemuUu8Jpc7\nKFO1+aJpXdbSPZNhGLdANn8f2oMIZ1R9hgEJRn/Qm/YyC4RPTGg55nzHqSlziNZJ\nJAyEUyU7kx5+Kb6ktgojhk8ocZRkorM8FEylkrKzgSrfay0PcWHPsKlmeQIDAQAB\nAoGAHlVK1L7kLmpMbuP4voYMeLjYE9XdVEEZf2GiFwLSE3mkJY44033y/Bb2lgxr\nDScOf675fFUAEK59ATlhxfu6s6rgx+g9qQQ+mL74YZWPqiZHBPjyMRaBalDVC4QF\nYJ+DopFcB8hY2ElXnbK70ALmVYNjw3RdmC97h0YfOsQcWW0CQQD18aeuPNicVnse\nKu22vvhvQYlabaQh4xdkEIxz1TthZj48f61wZwEMipYqOAc5XEtDlNnxgeipv0yF\nRHstUjwXAkEA3m0Br/U/vC9evuXppWbONana08KLgfELyd3Uw9jG7VKJZTBH5mS8\n7CB68aEF8egrJpo8Ss8BkWrvCxYDb4Y77wJAUlbOMZozVtvpKidrIFR9Lho91uWA\nHsw9h4W20AzibXBig7SnJ0uE4WMAdS/+0yhgFkceVCmO8E2YW8Gaj4jJjwJASxtg\nAHy+ItuUEL4uIW4Pn8tVW0BMP3qX0niXyfI/ag/+2S5uePv3V3y4RzNqgH83Yved\n+FziWKpVQdcTHeuj/QJBAJl1G3WFruk0llIoKKbKljaEiCm1WCTcuEPbdOtkJYvO\n9ZYQg/fji70FERkq2KHtY7aLhCHzy0d4n9xgE/pjV+I=\n-----END RSA PRIVATE KEY-----"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const message = \"MongoDB is great!\"\n  const rsaPrivateKey = context.values.get(\"rsaPrivateKey\");\n  const signature = utils.crypto.sign(\"rsa\", message, rsaPrivateKey, \"pss\");\n  return signature.toBase64();\n}"
                },
                {
                    "lang": "json",
                    "value": "\"SpfXcJwPbypArt+XuYQyuZqU52YCAY/sZj2kiuin2b6/RzyM4Ek3n3spOtqZJqxn1tfQavxIX4V+prbs74/pOaQkCLekl9Hoa1xOzSKcGoRd8U+n1+UBY3d3cyODGMpyr8Tim2HZAeLPE/D3Q36/K+jpgjvrJFXsJoAy5/PV7iEGV1fkzogmZtXWDcUUBBTTNPY4qZTzjNhL4RAFOc7Mfci+ojE/lLsNaumUVU1/Eky4vasmyjqunm+ULCcRmgWtGDMGHaQV4OXC2LMUe9GOqd3Q9ghCe0Vlhn25oTh8cXoGpd1Fr8wolNa//9dUqSM+QYDpZJXGLShX/Oa8mPwJZKDKHtqrS+2vE6S4dDWR7zKDza+DeovOeCih71QyuSYMXSz+WWGgfLzv/syhmUXP/mjqgLmJU6Kwg5htajDoylpzLC0BLGT4zDZEwBrq/AjwRs/EPjYdFgGCt1WCbbVlDyXvvH1ekDrzACzumhiMSZNNa+ZH3JmMJxTCQWDfiTeAfkauaaZHKIj2q2/QE7vuAhNcVPJ2YgpXnvnQHJpEZBc/Y3Q6JLxom6+cGC4P//9d++r2cwzXIkqD+wSGZVSVtpm5CLtWMRSK5FX2dv16bM+LE8ozoRvtMUDTrQ8SSSDGxyuYbvN9b2fYYPcWpCMchqOBXV6eZGoMldaHX3Qy5h8=\""
                },
                {
                    "lang": "typescript",
                    "value": "utils.crypto.verify(\n  encryptionType: string,\n  message: string,\n  publicKey: string,\n  signature: BSON.Binary,\n  signatureScheme: string\n): boolean"
                },
                {
                    "lang": "shell",
                    "value": "# Generate an RSA SSH key pair\n# Save the key to a file called `rsa_key` when prompted\nssh-keygen -t rsa -m PEM -b 2048 -C \"2048-bit RSA Key\"\n\n# Private Key\ncat rsa_key > rsa.private.txt\n# Public Key\nssh-keygen -f rsa_key.pub -e -m pem > rsa.public.txt"
                },
                {
                    "lang": "none",
                    "value": "-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBANWwSOx7ao7i/enxEz+rxGrOWhzWV57fjKhi4pnZx6a5S7wmlzsoU7X5\nomld1tI9k2EYt0A2fx/agwhnVH2GAQlGf9Cb9jILhE9MaDnmfMepKXOI1kkxDIRT\nJTuTHn4pvqS2CiOGTyhxlGSiszwUTKWSsrOBKt9rLQ9xYc+wqWZ5AgMBAAE=\n-----END RSA PUBLIC KEY-----"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const rsaPublicKey = context.values.get(\"rsaPublicKey\");\n  const message = \"MongoDB is great!\"\n  const signature = BSON.Binary.fromBase64(\"SpfXcJwPbypArt+XuYQyuZqU52YCAY/sZj2kiuin2b6/RzyM4Ek3n3spOtqZJqxn1tfQavxIX4V+prbs74/pOaQkCLekl9Hoa1xOzSKcGoRd8U+n1+UBY3d3cyODGMpyr8Tim2HZAeLPE/D3Q36/K+jpgjvrJFXsJoAy5/PV7iEGV1fkzogmZtXWDcUUBBTTNPY4qZTzjNhL4RAFOc7Mfci+ojE/lLsNaumUVU1/Eky4vasmyjqunm+ULCcRmgWtGDMGHaQV4OXC2LMUe9GOqd3Q9ghCe0Vlhn25oTh8cXoGpd1Fr8wolNa//9dUqSM+QYDpZJXGLShX/Oa8mPwJZKDKHtqrS+2vE6S4dDWR7zKDza+DeovOeCih71QyuSYMXSz+WWGgfLzv/syhmUXP/mjqgLmJU6Kwg5htajDoylpzLC0BLGT4zDZEwBrq/AjwRs/EPjYdFgGCt1WCbbVlDyXvvH1ekDrzACzumhiMSZNNa+ZH3JmMJxTCQWDfiTeAfkauaaZHKIj2q2/QE7vuAhNcVPJ2YgpXnvnQHJpEZBc/Y3Q6JLxom6+cGC4P//9d++r2cwzXIkqD+wSGZVSVtpm5CLtWMRSK5FX2dv16bM+LE8ozoRvtMUDTrQ8SSSDGxyuYbvN9b2fYYPcWpCMchqOBXV6eZGoMldaHX3Qy5h8=\")\n  const isValid = utils.crypto.verify(\"rsa\", message, rsaPublicKey, signature, \"pss\");\n  return isValid; // true if the signature matches, else false\n}"
                },
                {
                    "lang": "json",
                    "value": "true"
                },
                {
                    "lang": "typescript",
                    "value": "utils.crypto.hmac(\n  input: string,\n  secret: string,\n  hash_function: string,\n  output_format: string\n): string"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const signature = utils.crypto.hmac(\n    \"hello!\",\n    \"super-secret\",\n    \"sha256\",\n    \"base64\"\n  )\n  return signature\n}"
                },
                {
                    "lang": "json",
                    "value": "\"SUXd6PRMaTXXgBGaGsIHzaDWgTSa6C3D44lRGrvRak0=\""
                },
                {
                    "lang": "typescript",
                    "value": "utils.crypto.hash(\n  hash_function: string,\n  input: string | BSON.Binary\n): BSON.Binary"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  return utils.crypto.hash(\n    \"sha256\",\n    \"hello!\"\n  )\n}"
                },
                {
                    "lang": "json",
                    "value": "\"zgYJL7lI2f+sfRo3bkBLJrdXW8wR7gWkYV/vT+w6MIs=\""
                },
                {
                    "lang": "typescript",
                    "value": "JSON.parse(jsonString: string): object"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const jsonString = `{\n    \"answer\": 42,\n    \"submittedAt\": \"2020-03-02T16:50:24.475Z\"\n  }`;\n  return JSON.parse(jsonString);\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"answer\": 42,\n  \"submittedAt\": \"2020-03-02T16:50:24.475Z\"\n}"
                },
                {
                    "lang": "typescript",
                    "value": "JSON.stringify(json: object): string"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const jsonObject = {\n    answer: 42,\n    submittedAt: new Date(\"2020-03-02T16:46:47.977Z\")\n  };\n  return JSON.stringify(jsonObject);\n}"
                },
                {
                    "lang": "json",
                    "value": "\"{\\\"answer\\\":42,\\\"submittedAt\\\":\\\"2020-03-02T16:46:47.977Z\\\"}\""
                },
                {
                    "lang": "typescript",
                    "value": "EJSON.parse(ejsonString: string): object"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const ejsonString = `{\n    \"answer\": {\n      \"$numberLong\": \"42\"\n    },\n    \"submittedAt\": {\n      \"$date\": {\n        \"$numberLong\": \"1583167607977\"\n      }\n    }\n  }`;\n  return EJSON.parse(ejsonString);\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"answer\": {\n    \"$numberLong\": \"42\"\n  },\n  \"submittedAt\": {\n    \"$date\": {\n      \"$numberLong\": \"1583167607977\"\n    }\n  }\n}"
                },
                {
                    "lang": "typescript",
                    "value": "EJSON.stringify(json: object): string"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  const jsonObject = {\n    answer: 42,\n    submittedAt: new Date(\"2020-03-02T16:46:47.977Z\")\n  };\n  return EJSON.stringify(jsonObject);\n}"
                },
                {
                    "lang": "json",
                    "value": "\"{\\\"answer\\\":{\\\"$numberLong\\\":\\\"42\\\"},\\\"submittedAt\\\":{\\\"$date\\\":{\\\"$numberLong\\\":\\\"1583167607977\\\"}}}\""
                },
                {
                    "lang": "typescript",
                    "value": "new BSON.ObjectId(id: string)"
                },
                {
                    "lang": "javascript",
                    "value": "const objectId = new BSON.ObjectId(\"5e58667d902d38559c802b13\");\nconst generatedObjectId = new BSON.ObjectId();"
                },
                {
                    "lang": "typescript",
                    "value": "BSON.BSONRegExp(pattern: string, flags: string)"
                },
                {
                    "lang": "javascript",
                    "value": "const regex = BSON.BSONRegExp(\"the great\", \"ig\");"
                },
                {
                    "lang": "typescript",
                    "value": "BSON.Binary.fromHex(\n  hexString: string,\n  subType?: number\n): BSON.Binary"
                },
                {
                    "lang": "javascript",
                    "value": "const binary = BSON.Binary.fromHex(\"54657374206d65737361676520706c656173652069676e6f7265=\");"
                },
                {
                    "lang": "typescript",
                    "value": "BSON.Binary.prototype.toHex(): string"
                },
                {
                    "lang": "javascript",
                    "value": "export = function() {\n  const binary = BSON.Binary.fromHex(\n     \"54657374206d65737361676520706c656173652069676e6f7265=\"\n   );\n  const hexString = binary.toHex();\n  return hexString\n}"
                },
                {
                    "lang": "json",
                    "value": "\"54657374206d65737361676520706c656173652069676e6f7265=\""
                },
                {
                    "lang": "typescript",
                    "value": "BSON.Binary.fromBase64(\n  base64String: string,\n  subType?: number\n): BSON.Binary"
                },
                {
                    "lang": "javascript",
                    "value": "const binary = BSON.Binary.fromBase64(\"VGVzdCBtZXNzYWdlIHBsZWFzZSBpZ25vcmU=\");"
                },
                {
                    "lang": "typescript",
                    "value": "BSON.Binary.prototype.toBase64(): string"
                },
                {
                    "lang": "javascript",
                    "value": "const binary = BSON.Binary.fromBase64(\"VGVzdCBtZXNzYWdlIHBsZWFzZSBpZ25vcmU=\");\nconst base64String = binary.toBase64();"
                },
                {
                    "lang": "typescript",
                    "value": "BSON.Binary.prototype.text(): string"
                },
                {
                    "lang": "javascript",
                    "value": "const binary = BSON.Binary.fromBase64(\"VGVzdCBtZXNzYWdlIHBsZWFzZSBpZ25vcmU=\");\nconst decodedString = binary.text();"
                },
                {
                    "lang": "javascript",
                    "value": "await collection.findOne({ date: { $lt: BSON.MaxKey } });"
                },
                {
                    "lang": "javascript",
                    "value": "await collection.findOne({ date: { $gt: BSON.MinKey } });"
                },
                {
                    "lang": "typescript",
                    "value": "BSON.Int32(low32: number): BSON.Int32"
                },
                {
                    "lang": "javascript",
                    "value": "const int32 = BSON.Int32(42);"
                },
                {
                    "lang": "typescript",
                    "value": "BSON.Long(low32: number, high32: number): BSON.Long"
                },
                {
                    "lang": "javascript",
                    "value": "(high32 * (2**32)) + low32"
                },
                {
                    "lang": "javascript",
                    "value": "const long = BSON.Long(600206158, 342);"
                },
                {
                    "lang": "typescript",
                    "value": "BSON.Double(double: number): BSON.Double"
                },
                {
                    "lang": "javascript",
                    "value": "const double = BSON.Double(1234.5678);"
                },
                {
                    "lang": "typescript",
                    "value": "BSON.Decimal128(decimalString: string): BSON.Decimal128"
                },
                {
                    "lang": "javascript",
                    "value": "const double = BSON.Decimal128.fromString(\"1234.5678910123456\");"
                }
            ],
            "preview": "Generates an encoded JSON Web Token string for the payload based\non the specified signingMethod and secret.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "tutorial/flutter",
            "title": "Flutter Tutorial",
            "headings": [
                "Learning Objectives",
                "Prerequisites",
                "Start with the Template",
                "Set up the Template App",
                "Open the App",
                "Explore the App Structure",
                "Run the App",
                "Check the Backend",
                "Modify the Application",
                "Add a New Property",
                "Add a New Property to the Model",
                "Set the Priority when Creating and Updating Items",
                "Run and Test",
                "Change the Subscription",
                "Update the subscription",
                "Run and Test",
                "Conclusion",
                "What's Next?"
            ],
            "paragraphs": "The Realm Flutter SDK allows you to create a multi-platform\napplications with Dart and Flutter. This tutorial is based on the\nFlutter Flexible Sync Template App, named  flutter.todo.flex , which illustrates\nthe creation of a Todo application. This application enables users to: The template app also provides a toggle that simulates the device being in\n\"Offline Mode.\" This toggle lets you quickly test Device Sync functionality,\nemulating the user having no internet connection.\nHowever, you would likely remove this toggle in a production application. This tutorial adds on to the template app. You will add a new  priority  field\nto the existing  Item  model and update the  Flexible Sync subscription  to only show items within a range of priorities. Depending on your experience with Flutter, this tutorial should take\naround 30 minutes. Register their email as a new user account. Sign in to their account with their email and password (and sign out later). View, create, modify, and delete task items. View all tasks, even where the user is not the owner. This tutorial illustrates how you might adapt the\ntemplate app for your own needs. You would not necessarily make\nthis change given the current structure of the template app. In this tutorial, you will learn how to: If you prefer to get started with your own application rather than follow a\nguided tutorial, check out the  Flutter Quick Start .\nIt includes copyable code examples and the essential information that you\nneed to set up a Realm Flutter SDK application. Update a Realm object model with a non-breaking change. Update a Device Sync subscription Add a queryable field to the Device Sync configuration on the server\nto change which data is synchronized. You should have previous experience deploying a Flutter app\nto an Android Emulator, iOS Simulator, and/or a physical device. This tutorial starts with a Template App. You need an  Atlas Account , an API key, and\nappservices to create a Template App. You can learn more about creating an Atlas account in the\n Atlas Getting Started  documentation. For this\ntutorial, you need an Atlas account with a free-tier cluster. You also need an Atlas  API key  for the MongoDB\nCloud account you wish to log in with. You must be a  Project\nOwner  to create a Template\nApp using appservices. To learn more about installing appservices, see  Install App\nServices CLI . After installing, run the\n login  command using the API key for\nyour Atlas project. You can build this tutorial app on the following platforms: The Realm Flutter SDK does  not  support building web applications. iOS Android macOS Windows Linux This tutorial is based on the Flutter Flexible Sync Template App named\n flutter.todo.flex . We start with the default app and build new features\non it. To learn more about the Template Apps, see  Template Apps . If you don't already have an Atlas account,  sign-up  to deploy a Template App. Follow the procedure described in the\n Create an App Services App  guide, and select\n Create App from Template . Select the\n Real-time Sync  template. This creates an App Services App\npre-configured to use with one of the Device Sync template app clients. After you create a template app, the UI displays a modal labeled\n Get the Front-end Code for your Template . This modal\nprovides instructions for downloading the template app client code\nas a  .zip  file or using App Services CLI to get the client. After selecting the  .zip  or App Services CLI method, follow the on-screen\ninstructions to get the client code. For this tutorial, select the\n Dart (Flutter)  client code. Unzip the downloaded app, and you will see the Flutter app. The  appservices apps create \ncommand sets up the backend and creates a Flutter template\napp for you to use as a base for this tutorial. Run the following command in a terminal window to create an app\nnamed \"MyTutorialApp\" that is deployed in the  US-VA  region\nwith its environment set to \"development\" (instead of production\nor QA). The command creates a new directory in your current path with the\nsame name as the value of the  --name  flag. You can fork and clone a GitHub repository that contains the Device\nSync client code. The Flutter client code is available at\n https://github.com/mongodb/template-app-dart-flutter-todo . If you use this process to get the client code, you must create a\ntemplate app to use with the client. Follow the instructions at\n Create a Template App  to use the Atlas App Services UI, App Services CLI,\nor Admin API to create a Device Sync backend template app. Clone the repo and follow the instructions in the  README  to add\nthe backend App ID to the Flutter app. Open the Flutter app with your code editor. If you downloaded the client as a  .zip  file or cloned the client\nGitHub repository, you must manually insert the App Services App ID\nin the appropriate place in your client. Follow the\n Configuration  instructions in the client  README.md \nto learn where to insert your App ID. In your code editor, take a few minutes to explore how the project is\norganized. This is a standard multi-platform Flutter application\nthat has been modified for our specific use. Specifically, the following\nfiles contain important uses of the Realm Flutter SDK: File Purpose lib/main.dart Entry point into the app. Contains routing and state management. lib/realm/schemas.dart Defines Realm Database schema. lib/realm/schemas.g.dart Generated Realm object class. lib/realm/app_services.dart Handles interaction with Atlas App Services. lib/realm/realm_services.dart Handles interaction with Realm Database and Atlas Device Sync. lib/components/ Component parts of app featuring Flutter widgets. lib/screens/ Pages of the app. Without making any changes to the code, you should be able to run the app\nin either the Android emulator, iOS Simulator, physical mobile device,\nor desktop emulator. First, install all dependencies by running the following in your terminal: Then, attach to a device and run the Flutter application. Once the app is running, register a new user account,\nand then add a new Item to your todo list. For more information on running a Flutter app with development tools,\nrefer to the  Flutter Test Drive documentation . Log in to  MongoDB Atlas . In the\n Data Services  tab, click on  Browse Collections . In the list\nof databases, find and expand the  todo  database, and then the\n Item  collection. You should see the document you created\nin this collection. Now that you have confirmed everything is working as expected, we can\nadd changes. In this tutorial, we have decided that we want to add a\n\"priority\" property to each Item so that we can filter Items by their\npriorities. To do this, follow these steps: In the  flutter.todo.flex  project, open the file\n lib/realm/schemas.dart . Add the following property to the  _Item  class: Regenerate the  Item  Realm object class: In  lib/realm/realm_services.dart , add logic to set and update\n priority . Also add a  PriorityLevel  abstract class below the\n RealmServices  class to constrain the possible values. Add a new file to contain a widget to set the priority for an Item.\nCreate the file  lib/components/select_priority.dart . Now add the  SelectPriority  widget to the\n CreateItem  and  ModifyItem  widgets. You also need to add\nsome additional logic to handle setting the priority. The code you must\nadd is shown below. Some sections of the files you are adding to are\nreplaced with comments in the below code examples to focus on the relevant\nsections of code that are changed. Edit the  CreateItemForm  widget in  lib/components/create_item.dart : Edit the  ModifyItemForm  widget in  lib/components/modify_item.dart : Now add a visual indicator for priority in the  ItemCard  widget\nin  lib/components/todo_item.dart . Create a new widget  _PriorityIndicator  that gives\na visual indicator of the Item's priority. Add a  _PriorityIndicator  widget you just created to the  TodoItem \nwidget. Before you run the application again, perform a  hot restart .\nThis makes sure that the sync session restarts with the new schema and prevents\nsync errors. Then, Log in using the account you created earlier in this tutorial.\nYou will see the one Item you previously created.\nAdd a new Item, and you will see that you can now set the priority.\nChoose  High  for the priority and save the Item. Now switch back to the Atlas data page in your browser, and refresh the\n Item  collection. You should now see the new Item with the  priority \nfield added and set to  1 . You will also notice that the existing Item\nnow also has a  priority  field, and it is set to  null , as shown in\nthe following screenshot: Adding a property to a Realm object is not a breaking change and therefore\ndoes not require a  client reset . The template\napp has Development Mode enabled, so changes to the client Realm object\nare reflected in the server-side schema. For more information, see\n Development Mode  and  Update Your Data Model . Now that we added the priority field, we want to update the Device Sync subscription\nto only sync Items marked as a High or Severe priority. In the  lib/realm/realm_services.dart  file, we define the Flexible Sync\nsubscription that defines which documents we sync with the user's device\nand account. Currently, we are syncing all all documents where\nthe  owner  property matches the authenticated user. The current subscription: Now we're going to change the subscription to only sync High and Severe\npriority Items.\nAs you may recall, the priority field is of type  int , where the highest\npriority (\"Severe\") has a value of 0, and the lowest priority (\"Low\") has\na value of 3. We can make direct comparisons between an int and the priority property.\nTo do so, we're going to refactor the subscription query\nto include Items where the priority is less than or equal to\n PriorityLevel.high  (or 1). We will also give the subscription the new name\n \"getMyHighPriorityItemsSubscription\" . Update the subscription to delete the old subscription and add\na new one that uses priority: Run the application again. Log in using the account you created earlier\nin this tutorial. After an initial moment when Realm resyncs the document collection, you\n might  see an error message resembling the following: This error can occur with the  StreamBuilder  widget as the subscription updates.\nIn a production app, you could add error handling. But for the sake of\nthis tutorial, just perform a hot refresh and the error will go away. Now you should see the new Item of High priority that you created. If you want to further test the functionality, you can create Items of various\npriorities. You will see that a new Item with a lower priority briefly\nappears in the list of Items and then disappears.\nRealm creates the Item locally, syncs it with the backend, and then deletes\nthe Item because it doesn't meet the subscription rules. This is called\na  compensating write . You'll note, too, that the document you initially created is not synced,\nbecause it has a priority of  null .\nIf you want this Item to be synced,\nyou can edit the document in the Atlas UI and add a value for the priority\nfield, or you can change your subscription to include documents with null\nvalues. We will also give the subscription the new name\n \"getUserItemsWithHighOrNoPriority\" . Again, when a  StreamBuilder  error occurs the first time you open the\napp with the new subscription, perform a hot refresh to see the expected data. Adding a property to an existing Realm object is a non-breaking change, and\nDevelopment Mode ensures that the schema change is reflected server-side. Read our  Flutter SDK  documentation. Find developer-oriented blog posts and integration tutorials on the\n MongoDB Developer Hub . Join the  MongoDB Community forum \nto learn from other MongoDB developers and technical experts. Explore engineering and expert-provided  example projects . How did it go? Use the Share Feedback tab at the bottom right of the page to let us know\nif this tutorial was helpful or if you had any issues.",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices app create \\\n  --name MyTutorialApp \\\n  --template flutter.todo.flex \\\n  --deployment-model global \\\n  --environment development"
                },
                {
                    "lang": "sh",
                    "value": "flutter pub get"
                },
                {
                    "lang": "dart",
                    "value": "late int? priority;"
                },
                {
                    "lang": "sh",
                    "value": "dart run realm generate"
                },
                {
                    "lang": "dart",
                    "value": "// ... imports\n\nclass RealmServices with ChangeNotifier {\n  static const String queryAllName = \"getAllItemsSubscription\";\n  static const String queryMyItemsName = \"getMyItemsSubscription\";\n\n  bool showAll = false;\n  bool offlineModeOn = false;\n  bool isWaiting = false;\n  late Realm realm;\n  User? currentUser;\n  App app;\n\n  // ... RealmServices initializer and updateSubscriptions(),\n  //     sessionSwitch() and switchSubscription() methods\n\n\n  void createItem(String summary, bool isComplete, int? priority) {\n    final newItem = Item(ObjectId(), summary, currentUser!.id,\n        isComplete: isComplete, priority: priority);\n    realm.write<Item>(() => realm.add<Item>(newItem));\n    notifyListeners();\n  }\n\n  void deleteItem(Item item) {\n    realm.write(() => realm.delete(item));\n    notifyListeners();\n  }\n\n  Future<void> updateItem(Item item,\n      {String? summary,\n      bool? isComplete,\n      int? priority}) async {\n    realm.write(() {\n      if (summary != null) {\n        item.summary = summary;\n      }\n      if (isComplete != null) {\n        item.isComplete = isComplete;\n      }\n      if (priority != null) {\n        item.priority = priority;\n      }\n    });\n    notifyListeners();\n  }\n\n  Future<void> close() async {\n    if (currentUser != null) {\n      await currentUser?.logOut();\n      currentUser = null;\n    }\n    realm.close();\n  }\n\n  @override\n  void dispose() {\n    realm.close();\n    super.dispose();\n  }\n}\n\nabstract class PriorityLevel {\n  static int severe = 0;\n  static int high = 1;\n  static int medium = 2;\n  static int low = 3;\n}"
                },
                {
                    "lang": "dart",
                    "value": "import 'package:flutter/material.dart';\nimport 'package:flutter_todo/realm/realm_services.dart';\n\nclass SelectPriority extends StatefulWidget {\n  int priority;\n  void Function(int priority) setFormPriority;\n\n  SelectPriority(this.priority, this.setFormPriority, {Key? key})\n      : super(key: key);\n\n  @override\n  State<SelectPriority> createState() => _SelectPriorityState();\n}\n\nclass _SelectPriorityState extends State<SelectPriority> {\n  @override\n  Widget build(BuildContext context) {\n    return Padding(\n      padding: const EdgeInsets.only(top: 15),\n      child: Column(\n        crossAxisAlignment: CrossAxisAlignment.start,\n        children: [\n          const Text('Priority'),\n          DropdownButtonFormField<int>(\n            onChanged: ((int? value) {\n              final valueOrDefault = value ?? PriorityLevel.low;\n              widget.setFormPriority(valueOrDefault);\n              setState(() {\n                widget.priority = valueOrDefault;\n              });\n            }),\n            value: widget.priority,\n            items: [\n              DropdownMenuItem(\n                  value: PriorityLevel.low, child: const Text(\"Low\")),\n              DropdownMenuItem(\n                  value: PriorityLevel.medium, child: const Text(\"Medium\")),\n              DropdownMenuItem(\n                  value: PriorityLevel.high, child: const Text(\"High\")),\n              DropdownMenuItem(\n                  value: PriorityLevel.severe, child: const Text(\"Severe\")),\n            ],\n          ),\n        ],\n      ),\n    );\n  }\n}"
                },
                {
                    "lang": "dart",
                    "value": "// ... other imports\nimport 'package:flutter_todo/components/select_priority.dart';\n\n// ... CreateItemAction widget\n\n\nclass CreateItemForm extends StatefulWidget {\n  const CreateItemForm({Key? key}) : super(key: key);\n\n  @override\n  createState() => _CreateItemFormState();\n}\n\nclass _CreateItemFormState extends State<CreateItemForm> {\n  final _formKey = GlobalKey<FormState>();\n  late TextEditingController _itemEditingController;\n  int _priority = PriorityLevel.low;\n\n  void _setPriority(int priority) {\n    setState(() {\n      _priority = priority;\n    });\n  }\n\n  // ... initState() and dispose() @override functions\n\n  @override\n  Widget build(BuildContext context) {\n    TextTheme theme = Theme.of(context).textTheme;\n    return formLayout(\n        context,\n        Form(\n          key: _formKey,\n          child: Column(\n            mainAxisAlignment: MainAxisAlignment.center,\n            mainAxisSize: MainAxisSize.min,\n            children: <Widget>[\n              // ... Text and TextFormField widgets\n              SelectPriority(_priority, _setPriority),\n              // ... Padding widget\n            ],\n          ),\n        ));\n  }\n\n  void save(RealmServices realmServices, BuildContext context) {\n    if (_formKey.currentState!.validate()) {\n      final summary = _itemEditingController.text;\n      realmServices.createItem(summary, false, _priority);\n      Navigator.pop(context);\n    }\n  }\n}"
                },
                {
                    "lang": "dart",
                    "value": "// ... other imports\nimport 'package:flutter_todo/components/select_priority.dart';\n\nclass ModifyItemForm extends StatefulWidget {\n  final Item item;\n  const ModifyItemForm(this.item, {Key? key}) : super(key: key);\n\n  @override\n  _ModifyItemFormState createState() => _ModifyItemFormState(item);\n}\n\nclass _ModifyItemFormState extends State<ModifyItemForm> {\n  final _formKey = GlobalKey<FormState>();\n  final Item item;\n  late TextEditingController _summaryController;\n  late ValueNotifier<bool> _isCompleteController;\n\n  late int? _priority;\n  void _setPriority(int priority) {\n    setState(() {\n      _priority = priority;\n    });\n  }\n\n  _ModifyItemFormState(this.item);\n\n  @override\n  void initState() {\n    _summaryController = TextEditingController(text: item.summary);\n    _isCompleteController = ValueNotifier<bool>(item.isComplete)\n      ..addListener(() => setState(() {}));\n    _priority = widget.item.priority;\n    super.initState();\n  }\n\n  @override\n  void dispose() {\n    _summaryController.dispose();\n    _isCompleteController.dispose();\n    super.dispose();\n  }\n\n  @override\n  Widget build(BuildContext context) {\n    TextTheme myTextTheme = Theme.of(context).textTheme;\n    final realmServices = Provider.of<RealmServices>(context, listen: false);\n    return formLayout(\n        context,\n        Form(\n            key: _formKey,\n            child: Column(\n              mainAxisAlignment: MainAxisAlignment.center,\n              mainAxisSize: MainAxisSize.min,\n              children: <Widget>[\n                // ... Text and TextFormField widgets\n                SelectPriority(_priority ?? PriorityLevel.medium, _setPriority),\n                // ... StatefulBuilder widget\n                Padding(\n                  padding: const EdgeInsets.only(top: 15),\n                  child: Row(\n                    mainAxisAlignment: MainAxisAlignment.center,\n                    children: [\n                      cancelButton(context),\n                      deleteButton(context,\n                          onPressed: () =>\n                              delete(realmServices, item, context)),\n                      okButton(context, \"Update\",\n                          onPressed: () async => await update(\n                              context,\n                              realmServices,\n                              item,\n                              _summaryController.text,\n                              _isCompleteController.value,\n                              _priority)),\n                    ],\n                  ),\n                ),\n              ],\n            )));\n  }\n\n  Future<void> update(BuildContext context, RealmServices realmServices,\n      Item item, String summary, bool isComplete, int? priority) async {\n    if (_formKey.currentState!.validate()) {\n      await realmServices.updateItem(item,\n          summary: summary, isComplete: isComplete, priority: priority);\n      Navigator.pop(context);\n    }\n  }\n\n  void delete(RealmServices realmServices, Item item, BuildContext context) {\n    realmServices.deleteItem(item);\n    Navigator.pop(context);\n  }\n}"
                },
                {
                    "lang": "dart",
                    "value": "// ... imports\n\nenum MenuOption { edit, delete }\n\nclass TodoItem extends StatelessWidget {\n  final Item item;\n\n  const TodoItem(this.item, {Key? key}) : super(key: key);\n\n  @override\n  Widget build(BuildContext context) {\n    final realmServices = Provider.of<RealmServices>(context);\n    bool isMine = (item.ownerId == realmServices.currentUser?.id);\n    return item.isValid\n        ? ListTile(\n            // ... leading property and child content\n            title: Row(\n              children: [\n                Padding(\n                  padding: const EdgeInsets.only(right: 8.0),\n                  child: _PriorityIndicator(item.priority),\n                ),\n                SizedBox(width: 175, child: Text(item.summary)),\n              ],\n            ),\n            // ... subtitle, trailing, and shape properties with child content\n          )\n        : Container();\n  }\n\n  // ... handleMenuClick() function\n}\n\nclass _PriorityIndicator extends StatelessWidget {\n  final int? priority;\n  const _PriorityIndicator(this.priority, {Key? key}) : super(key: key);\n  Widget getIconForPriority(int? priority) {\n    if (priority == PriorityLevel.low) {\n      return const Icon(Icons.keyboard_arrow_down, color: Colors.blue);\n    } else if (priority == PriorityLevel.medium) {\n      return const Icon(Icons.circle, color: Colors.grey);\n    } else if (priority == PriorityLevel.high) {\n      return const Icon(Icons.keyboard_arrow_up, color: Colors.orange);\n    } else if (priority == PriorityLevel.severe) {\n      return const Icon(\n        Icons.block,\n        color: Colors.red,\n      );\n    } else {\n      return const SizedBox.shrink();\n    }\n  }\n\n  @override\n  Widget build(BuildContext context) {\n    return getIconForPriority(priority);\n  }\n}"
                },
                {
                    "lang": "dart",
                    "value": "  Future<void> updateSubscriptions() async {\n    realm.subscriptions.update((mutableSubscriptions) {\n      mutableSubscriptions.clear();\n      if (showAll) {\n        mutableSubscriptions.add(realm.all<Item>(), name: queryAllName);\n      } else {\n        mutableSubscriptions.add(\n            realm.query<Item>(r'owner_id == $0', [currentUser?.id]),\n            name: queryMyItemsName);\n      }\n    });\n    await realm.subscriptions.waitForSynchronization();\n  }"
                },
                {
                    "lang": "dart",
                    "value": "// ... imports\n\nclass RealmServices with ChangeNotifier {\n  static const String queryAllName = \"getAllItemsSubscription\";\n  static const String queryMyItemsName = \"getMyItemsSubscription\";\n  static const String queryMyHighPriorityItemsName =\n      \"getMyHighPriorityItemsSubscription\";\n\n  bool showAll = false;\n  bool offlineModeOn = false;\n  bool isWaiting = false;\n  late Realm realm;\n  User? currentUser;\n  App app;\n\n  RealmServices(this.app) {\n    if (app.currentUser != null || currentUser != app.currentUser) {\n      currentUser ??= app.currentUser;\n      realm = Realm(Configuration.flexibleSync(currentUser!, [Item.schema]));\n      showAll = (realm.subscriptions.findByName(queryAllName) != null);\n      // Check if subscription previously exists on the realm\n      final subscriptionDoesNotExists =\n          (realm.subscriptions.findByName(queryMyHighPriorityItemsName) ==\n              null);\n\n      if (realm.subscriptions.isEmpty || subscriptionDoesNotExists) {\n        updateSubscriptions();\n      }\n    }\n  }\n\n  Future<void> updateSubscriptions() async {\n    realm.subscriptions.update((mutableSubscriptions) {\n      mutableSubscriptions.clear();\n      if (showAll) {\n        mutableSubscriptions.add(realm.all<Item>(), name: queryAllName);\n      } else {\n        mutableSubscriptions.add(\n            realm.query<Item>(\n              r'owner_id == $0 AND priority <= $1',\n              [currentUser?.id, PriorityLevel.high],\n            ),\n            name: queryMyHighPriorityItemsName);\n      }\n    });\n    await realm.subscriptions.waitForSynchronization();\n  }\n\n  // ... other methods\n}"
                },
                {
                    "lang": null,
                    "value": "The following RangeError was thrown building StreamBuilder<RealmResultsChanges<Item>>(dirty, state:\n_StreamBuilderBaseState<RealmResultsChanges<Item>, AsyncSnapshot<RealmResultsChanges<Item>>>#387c4):\nRangeError (index): Invalid value: Only valid value is 0: 3"
                },
                {
                    "lang": "dart",
                    "value": "class RealmServices with ChangeNotifier {\n  static const String queryAllName = \"getAllItemsSubscription\";\n  static const String queryMyItemsName = \"getMyItemsSubscription\";\n  static const String queryMyHighPriorityItemsName =\n      \"getMyHighPriorityItemsSubscription\";\n  static const String queryMyHighOrNoPriorityItemsName =\n      \"getMyHighOrNoPriorityItemsSubscription\";\n\n  bool showAll = false;\n  bool offlineModeOn = false;\n  bool isWaiting = false;\n  late Realm realm;\n  User? currentUser;\n  App app;\n\n  RealmServices(this.app) {\n    if (app.currentUser != null || currentUser != app.currentUser) {\n      currentUser ??= app.currentUser;\n      realm = Realm(Configuration.flexibleSync(currentUser!, [Item.schema]));\n      // Check if subscription previously exists on the realm\n      final subscriptionDoesNotExists =\n          realm.subscriptions.findByName(queryMyHighOrNoPriorityItemsName) ==\n              null;\n\n      if (realm.subscriptions.isEmpty || subscriptionDoesNotExists) {\n        updateSubscriptions();\n      }\n    }\n  }\n\n  Future<void> updateSubscriptions() async {\n    realm.subscriptions.update((mutableSubscriptions) {\n      mutableSubscriptions.clear();\n      if (showAll) {\n        mutableSubscriptions.add(realm.all<Item>(), name: queryAllName);\n      } else {\n        mutableSubscriptions.add(\n            realm.query<Item>(\n              r'owner_id == $0 AND priority IN {$1, $2, $3}',\n              [currentUser?.id, PriorityLevel.high, PriorityLevel.severe, null],\n            ),\n            name: queryMyHighPriorityItemsName);\n      }\n    });\n    await realm.subscriptions.waitForSynchronization();\n  }\n\n  // ... other methods\n}"
                }
            ],
            "preview": "The Realm Flutter SDK allows you to create a multi-platform\napplications with Dart and Flutter. This tutorial is based on the\nFlutter Flexible Sync Template App, named flutter.todo.flex, which illustrates\nthe creation of a Todo application. This application enables users to:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "tutorial/triggers-fts",
            "title": "Build Reverse Search Into Your Application",
            "headings": [
                "Overview",
                "Prerequisites",
                "Create a New Template App",
                "Add an Alert Box",
                "Create the Alert Box Component",
                "Update the Main View",
                "Store Alert Terms in Atlas",
                "Create a New Collection",
                "Enable Access to the Collection",
                "Write to the Collection",
                "Run and Test the App",
                "Add a Trigger",
                "Create an Atlas Search Index",
                "Add a Database Trigger",
                "Define an Atlas function",
                "Run and Test the App",
                "What's Next?"
            ],
            "paragraphs": "You can use Atlas App Services with  Atlas Triggers  and\n Atlas Search  to build features, such as reverse search,\non top of your applications. Reverse search lets you store search parameters,\nand then match new documents to those parameters. In this tutorial, you start with a pre-built TypeScript mobile application\nthat includes a working React Native application (frontend) and its corresponding\nApp Services App configuration files (backend). This template app is a basic\nto-do list application that lets users do various things to manage their tasks.\nTo learn more about the template app, see the  React Native Tutorial . After you've got the template app running, you'll add a new feature that alerts\nyou when a user creates a task that contains a specific word or phrase. This\nfeature illustrates how you might implement reverse search into your production\napplication. You will use Atlas to store specific terms to be alerted on, and\nthen match new tasks against those terms by using Atlas Triggers and Atlas Search. The feature includes: For example, if you want to know when users submit a time-sensitive task,\nyou might enter a term such as  urgent . Then, when a user adds a task that\ncontains the term, such as  Urgent: complete this task , you'll be alerted\nright away. This tutorial should take around 30 minutes. An alert box using the  Realm React SDK  and\n @realm/react  that: Lets you enter the terms to be alerted on. Stores the specified terms in Atlas. A  Database Trigger  with a custom\n Atlas Function  that: Alerts you when a new task contains a term you've entered. Aggregates and returns related tasks by using an\n Atlas Search query . Before you begin: You must set up your local environment for React Native development.\nFor detailed instructions, see  Setting up the development environment  in the React Native\ndocs. This tutorial starts with a Template App. You need an  Atlas Account , an API key, and\nappservices to create a Template App. You can learn more about creating an Atlas account in the\n Atlas Getting Started  documentation. For this\ntutorial, you need an Atlas account with a free-tier cluster. You also need an Atlas  API key  for the MongoDB\nCloud account you wish to log in with. You must be a  Project\nOwner  to create a Template\nApp using appservices. To learn more about installing appservices, see  Install App\nServices CLI . After installing, run the\n login  command using the API key for\nyour Atlas project. To run Atlas Search queries, ensure that your Atlas cluster runs MongoDB\nversion 4.2 or higher. This tutorial is based on the React Native SDK Flexible Sync Template App named\n react-native.todo.flex . You start with the default app and build new features\non it. To get the template app up and running on your computer, follow the\nsteps described in the React Native tutorial: Start with the Template App Set Up the Template App Get Familiar With the Template App Once you've set up and explored the template app, it's time to write some code to\nimplement the new alerting feature. In this section, you add an alert box that lets you enter terms that\nmight appear in specific, important, or time-sensitive tasks. In the template app's  src/  directory, create a new  AlertBox.tsx  file\nand add the following code. This file contains the UI form that lets you\nenter the terms to be alerted on. In  src/ItemListView.tsx , import the alert box you just defined\nby adding the following line to the top of the file: Then, add the following code to render a button that displays the\nalert box when clicked: At the top of the  ItemListView  function block, add a  useState()  hook\nto keep track of the alert box view: After the  Add To-Do  button in the main view, add an  Alerts  button that\ntoggles the alert box overlay: To change the size and color of the button, add the following lines\nto the  styles  block at the bottom of the file: Now that you've created the frontend component for the alert box,\nconfigure the application's backend to store and keep track of your\nalert terms in Atlas. In the Atlas UI, create a collection to store the terms that users enter in the app: If you're not already at the  Database Deployments  page, click the\n Data Services  tab. For the deployment that's synced to the template app,\nclick  Browse Collections . In the left navigation, click the  +  icon next to the  todo \ndatabase to add a new collection. Name the collection  alerts , then click  Create  to save\nthe collection. After creating the collection, you must give your app the necessary permissions\nto write to the  todo.alerts  collection: Click the  App Services  tab. Click the tile for your app. In the left navigation under  Data Access , click  Rules . Under the  todo  database, click the  alerts  collection. In the right dialog box, select the  readAndWriteAll  preset role. Click  Add preset role  to confirm your selection. By default, your application enables deployment drafts. To\n manually deploy  your changes, click\n Review Draft & Deploy  and then  Deploy . Once you've configured write access to the  todo.alerts  collection, return\nto your application code. In  src/ItemListView.tsx , add the following lines to the top of the function block\nto create a helper function that writes to the collection: The  addAlert()  function takes a string input and uses the  React Native SDK  to connect to Atlas and insert the specified alert term\nas a document to your collection. Then, add the following line to the alert box submission handler to call  addAlert() \nwhen a user submits an alert term in the app: Your app should now allow users to enter alert terms one at a time\nto be stored in Atlas. Rebuild the app and open it. Submit a few terms to be alerted on such as\n important  or  urgent . Then,  view your documents  in the  todo.alerts  collection\nto confirm that the terms appear in Atlas. Now that you've created the alert box and set up its backing collection, create an\n Atlas Trigger  that alerts you when a new task contains one of your\nalert terms. Triggers can execute application and database logic in response to a\nchange event. Each trigger links to an  Atlas Function  that defines\nthe trigger's behavior. In this section, you create a  database trigger \nthat runs whenever a user creates a new task. In the trigger's\nfunction, you define: The message that displays in your application logs. The database logic, so that the trigger returns the message only when the\ndocument contains an alert term. An Atlas Search query that aggregates other tasks that contain\nthe same alert term. In order to run Atlas Search queries on your data, you must first create\nan  Atlas Search index  to map the fields\nin your collection. In the Atlas UI, create a search index on the  todo.Item \ncollection: Return to the  Database Deployments  page\nby clicking the  Data Services  tab. Click the name of the deployment that's synced to the template app,\nthen click the  Search  tab. To create your first Atlas Search index, click  Create Search Index . In the  Configuration Method  page, select  Visual Editor \nand click  Next . Leave the  Index Name  set to  default . In the  Database and Collection  section, find the\n todo  database and select the  Item  collection. Click  Next , then click  Create Search Index \nafter you've reviewed your index. Wait for the index to finish building. The index should take about one minute to build.\nWhen it's finished building, the\n Status  column reads  Active . To learn more about Atlas Search indexes, see\n Create an Atlas Search Index . To open the database trigger configuration page in the App Services UI: Click the  App Services  tab and select the tile for your\napp. In the left navigation menu, click  Triggers . Click  Add a Trigger  and leave the  Trigger type \nset to  Database . Name the trigger  sendAlerts . Configure the trigger to listen only for new task documents\ninserted into the  todo.Item  collection: For the  Cluster Name , select\nthe deployment that's synced to the template app. For the  Database Name  and  Collection Name ,\nselect the  todo  database and  Item  collection. For the  Operation Type , select  Insert . Enable  Full Document  to include each new report document\nin the change event passed to the trigger function. Navigate to the  Function \nsection of the trigger configuration page and select  + New Function \nfrom the drop-down menu. Then, define the trigger's function: Name the function  triggers/sendAlerts . Copy the following code into the function body: This JavaScript function returns a message in your application\nlogs when a user enters a task that contains a term stored in the\n todo.alerts  collection. The function also includes an Atlas Search query to find other task documents\nin the  todo.Item  collection that contain the same alert term. The query uses: The  $search  pipeline stage to query the\ncollection. The following  compound  operator clauses: The  must  clause and  phrase  operator\nto query for any tasks with a  summary  that contains the alert term. The  mustNot  clause and  equals  operator to\nexclude completed tasks. The  $limit  stage to limit\nthe output to 5 results. The  $project  stage to exclude\nall fields except  summary . To learn more, see  Create and Run Atlas Search Queries . When you're finished, click  Save  and deploy the trigger. Atlas is now set up to alert you when a user creates a task in the app\nthat contains an alert term. Rebuild and run the app to make sure everything works. Enter a few\ntasks that contain an alert term you've previously entered. Then,\n view your logs  to see the output from\nthe trigger. You can filter for your  trigger logs \nby selecting the  Triggers  type from the drop-down\nmenu. For example, if one of your alert terms is  important ,\nthe log output for a new task might resemble the following: Extend the alert function to call an external service that can send you a text\nor an email instead of simply logging. To learn more, see  External Dependencies . Read our  Atlas Search documentation  to learn more about\nindexing and querying your Atlas data. Learn about  authentication  and\n scheduled triggers . Define  custom HTTPS Endpoints  to create app-specific\nAPI routes or webhooks. How did it go? Use the Share Feedback tab at the bottom right of the page to let us know\nif this tutorial was helpful or if you had any issues.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "import React, {useState} from 'react';\nimport {StyleSheet, View} from 'react-native';\nimport {Text, Input, Button} from 'react-native-elements';\nimport {COLORS} from './Colors';\n\ntype Props = {\n  onSubmit: ({term}: {term: string}) => void;\n};\n\nexport function AlertBox(props: Props): React.ReactElement<Props> {\n  const {onSubmit} = props;\n  const [term, setTerm] = useState('');\n\n  return (\n    <View style={styles.modalWrapper}>\n      <Text h4 style={styles.addItemTitle}>\n        Add Alert Term\n      </Text>\n      <Input\n        placeholder=\"Enter term\"\n        onChangeText={(text: string) => setTerm(text)}\n        autoCompleteType={undefined}\n      />\n       <Button\n        title=\"Submit\"\n        buttonStyle={styles.saveButton}\n        onPress={() => onSubmit({term})}\n      />\n    </View>\n  );\n}\n\nconst styles = StyleSheet.create({\n  modalWrapper: {\n    width: 300,\n    minHeight: 200,\n    borderRadius: 4,\n    alignItems: 'center',\n  },\n  addItemTitle: {\n    margin: 20,\n  },\n  saveButton: {\n    width: 280,\n    backgroundColor: COLORS.primary,\n  },\n});\n"
                },
                {
                    "lang": "javascript",
                    "value": "import {AlertBox} from './AlertBox';"
                },
                {
                    "lang": "javascript",
                    "value": "export function ItemListView() {\n  const realm = useRealm();\n  const items = useQuery(Item).sorted('_id');\n  const user = useUser();\n\n  const [showNewItemOverlay, setShowNewItemOverlay] = useState(false);\t\n  const [showAlertBox, setShowAlertBox] = useState(false);\n"
                },
                {
                    "lang": "javascript",
                    "value": "return (\n  <SafeAreaProvider>\n    <View style={styles.viewWrapper}>\n      ...\n      <Button\n        title=\"Add To-Do\"\n        buttonStyle={styles.addToDoButton}\n        onPress={() => setShowNewItemOverlay(true)}\n        icon={\n          <Icon\n            type=\"material\"\n            name={'playlist-add'}\n            style={styles.showCompletedIcon}\n            color=\"#fff\"\n            tvParallaxProperties={undefined}\n          />\n        }\n      />\n      <Button\n        title=\"Alerts\"\n        buttonStyle={styles.alertButton}\n        onPress={() => setShowAlertBox(true)}\n        icon={\n          <Icon\n            type=\"material\"\n            name={'add-alert'}\n            style={styles.showCompletedIcon}\n            color=\"#fff\"\n            tvParallaxProperties={undefined}\n          />\n        }\n      />\n      <Overlay\n        isVisible={showAlertBox}\n        onBackdropPress={() => setShowAlertBox(false)}>\n        <AlertBox\n          onSubmit={({term}) => {\n            setShowAlertBox(false);\n          }}\n        />\n      </Overlay>\n    </View>\n  </SafeAreaProvider>\n);\n"
                },
                {
                    "lang": "javascript",
                    "value": "const styles = StyleSheet.create({\n  ... \n  toggleText: {\n    flex: 1,\n    fontSize: 16,\n  },\n  alertButton: {\n    backgroundColor: '#808080',\n    borderRadius: 4,\n    margin: 5,\n  }\n});\n"
                },
                {
                    "lang": "javascript",
                    "value": "export function ItemListView() {\n  const realm = useRealm();\n  const items = useQuery(Item).sorted('_id');\n  const user = useUser();\n\n  // addAlert() takes a string input and inserts it as a document in the todo.alerts collection\n  const addAlert = async (text: string) => {\n    const mongodb = user?.mongoClient(\"mongodb-atlas\");\n    const alertTerms = mongodb?.db(\"todo\").collection(\"alerts\");\n    await alertTerms?.insertOne({ term: text.toLowerCase() });\n  };\n"
                },
                {
                    "lang": "javascript",
                    "value": "<Overlay\n  isVisible={showAlertBox}\n  onBackdropPress={() => setShowAlertBox(false)}>\n  <AlertBox\n    onSubmit={({term}) => {\n      setShowAlertBox(false);\n      addAlert(term);\n    }}\n  />\n</Overlay>\n"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function(changeEvent) {\n  \n  // Read the summary field from the latest inserted document\n  const fullDocument = changeEvent.fullDocument;\n  const summary = fullDocument.summary;\n\n  // Connect to your Atlas deployment    \n  const mongodb = context.services.get(\"mongodb-atlas\");\n\n  // Read task and alert data from collections in the todo database\n  const tasks = mongodb.db(\"todo\").collection(\"Item\");\n  const alerts = mongodb.db(\"todo\").collection(\"alerts\");\n  const terms = await alerts.distinct(\"term\");\n  \n  // Check if the task summary matches any of the terms in the alerts collection\n  for (let i = 0; i < terms.length ; i++) {\n    if (summary.toLowerCase().includes(terms[i])) {\n      console.log(\"The following task has been added to a to-do list: \" + summary +\n        \". You've been alerted because it contains the term, \" + terms[i] + \".\");\n\n      // Aggregates any tasks that also contain the term by using an Atlas Search query\n      const query = await tasks\n      .aggregate([\n        {\n          $search: {\n            compound: {\n              must: [{\n                phrase: {\n                  query: terms[i],\n                  path: \"summary\",\n                },\n              }], \n              mustNot: [{\n                equals: {\n                path: \"isComplete\",\n                value: true,\n                },\n              }],\n            },\n          },\n        },\n        {\n          $limit: 5,\n        },\n        {\n          $project: {\n            _id: 0,\n            summary: 1,\n          },\n        },\n      ])\n      .toArray();\n      \n      relatedTerms = JSON.stringify(query);\n\n      if (relatedTerms != '[]') {\n        console.log(\"Related incomplete tasks: \" + relatedTerms);\n      }\n    }\n  }\n};\n"
                },
                {
                    "lang": "none",
                    "value": "Logs:\n[\n   \"The following task has been added to a to-do list: Complete important tutorial.\n   You've been alerted because it contains the term, important.\",\n   \"Related incomplete tasks: [\n   {\"summary\": \"Important: Create template app\"},\n   {\"summary\": \"Add important tasks\"},\n   {\"summary\": \"Make sure to read the documentation. This is important.\"}]\"\n]"
                }
            ],
            "preview": "You can use Atlas App Services with Atlas Triggers and\nAtlas Search to build features, such as reverse search,\non top of your applications. Reverse search lets you store search parameters,\nand then match new documents to those parameters.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "tutorial/swiftui",
            "title": "SwiftUI iOS Tutorial",
            "headings": [
                "Overview",
                "Learning Objectives",
                "Prerequisites",
                "Start with the Template",
                "Explore the Template App",
                "Open the App",
                "Explore the App Structure",
                "Run the App",
                "Check the Backend",
                "Modify the Application",
                "Add a New Property",
                "Add a Property to the Model",
                "Set the Priority when Creating a New Item",
                "Run and Test",
                "Change the Subscription",
                "Update the subscription",
                "Run and Test",
                "Conclusion",
                "What's Next?"
            ],
            "paragraphs": "Realm provides a Swift SDK that allows you to create a native iOS\nmobile application with Swift or Objective-C. This tutorial is based on the\nSwiftUI Flexible Sync Template App, named  swiftui.todo.flex , which illustrates\nthe creation of a to-do list application. This application enables users to: The template app also provides a toggle that simulates the device being in\n\"Offline Mode.\" This toggle lets you quickly test Device Sync functionality\non the simulator, emulating the user having no internet connection. However,\nyou would likely remove this toggle in a production application. This tutorial builds on the Template App. You will add a new  priority  field\nto the existing  Item  model and update the\n Flexible Sync subscription  to only show items within\na range of priorities. Depending on your experience with SwiftUI, this tutorial should take\naround 30 minutes. Register their email as a new user account. Sign in to their account with their email and password (and sign out later). View, create, modify, and delete their own tasks. View all tasks, even where the user is not the owner. This tutorial illustrates how you might adapt the\ntemplate app for your own needs. You would not necessarily make\nthis change given the current structure of the template app. In this tutorial, you will learn how to: Update a Realm object model with a non-breaking change. Update a Device Sync subscription. Add a queryable field to the Device Sync configuration on the server\nto change which data is synchronized. If you prefer to get started with your own application rather than follow a\nguided tutorial, check out the  Swift Quick Start .\nIt includes copyable code examples and the essential information that you\nneed to set up an Atlas App Services backend. For a SwiftUI-specific get started experience, refer to the  Realm with SwiftUI QuickStart . Ensure that you have the necessary software installed. The Swift SDK requires\n Xcode  version 13.1 or newer. This tutorial starts with a Template App. You need an  Atlas Account , an API key, and\nappservices to create a Template App. You can learn more about creating an Atlas account in the\n Atlas Getting Started  documentation. For this\ntutorial, you need an Atlas account with a free-tier cluster. You also need an Atlas  API key  for the MongoDB\nCloud account you wish to log in with. You must be a  Project\nOwner  to create a Template\nApp using appservices. To learn more about installing appservices, see  Install App\nServices CLI . After installing, run the\n login  command using the API key for\nyour Atlas project. This tutorial is based on the SwiftUI Flexible Sync Template App named\n swiftui.todo.flex . We start with the default app and build new features\non it. To learn more about the Template Apps, see  Template Apps . If you don't already have an Atlas account,  sign-up  to deploy a Template App. Follow the procedure described in the\n Create an App Services App  guide, and select\n Create App from Template . Select the\n Real-time Sync  template. This creates an App Services App\npre-configured to use with one of the Device Sync template app clients. After you create a template app, the UI displays a modal labeled\n Get the Front-end Code for your Template . This modal\nprovides instructions for downloading the template app client code\nas a  .zip  file or using App Services CLI to get the client. After selecting the  .zip  or App Services CLI method, follow the on-screen\ninstructions to get the client code. For this tutorial, select the\n SwiftUI (iOS + SwiftUI)  client code. The  appservices apps create \ncommand sets up the backend and creates a SwiftUI template app for\nyou to use as a base for this tutorial. Run the following command in a terminal window to create an app\nnamed \"MyTutorialApp\" that is deployed in the  US-VA  region\nwith its environment set to \"development\" (instead of production\nor QA). The command creates a new directory in your current path with the\nsame name as the value of the  --name  flag. You can fork and clone a GitHub repository that contains the Device\nSync client code. The SwiftUI client code is available at\n https://github.com/mongodb/template-app-swiftui-todo . If you use this process to get the client code, you must create a\ntemplate app to use with the client. Follow the instructions at\n Create a Template App  to use the Atlas App Services UI, App Services CLI,\nor Admin API to create a Device Sync template app. Open the frontend client's  App.xcodeproj  in Xcode. If you downloaded the client as a  .zip  file or cloned the client\nGitHub repository, you must manually insert the App Services App ID\nin the appropriate place in your client. Follow the\n Configuration  instructions in the client  README.md \nto learn where to insert your App ID. Take a few minutes to explore how the project is organized while\nSwift Package Manager downloads the latest version of the Realm Swift\nSDK. Within the App directory, you can see a few files worth noting: In this tutorial, you'll be working in the following files: File Purpose AppConfig.swift This file contains the logic to read the  appId  and  baseUrl \nfrom the  Realm.plist . This is pre-populated with the\n appId  for your Template App. App.swift This file uses the values from  AppConfig.swift  to initialize\nthe  RealmSwift.App . The  App  is how your app communicates\nwith the App Services backend. This provides access to login and\nauthentication. This file also contains the error handler that\nlistens for Device Sync errors. To learn more about how you can customize your app configuration,\nsee:  Connect to an Atlas App Services Backend . This file is also the entrypoint to the SwiftUI app. We pass the\n App  to the  ContentView  that observes the app state\nfor user authentication state. File Purpose Item.Swift This file, located at the root of the project, defines the\nRealm object we store in the database. CreateItemView.swift This file, located in the  Views  directory, provides the\nfunctionality to add a new item to the list. ContentView.Swift This file, located in the  Views  directory, defines the\nFlexible Sync subscription. Without making any changes to the code, you should be able to run the app\nin the iOS Simulator or on a physical device. Run the app, register a new user account, and then add a new Item to your\ntodo list. Log in to  Atlas App Services . In the\n Data Services  tab, click on  Browse Collections . In the list\nof databases, find and expand the  todo  database, and then the\n Item  collection. You should see the document you created\nin this collection. Now that you have confirmed everything is working as expected, we can\nadd changes. In this tutorial, we have decided that we want to add a\n\"priority\" property to each Item so that we can filter Items by their\npriorities. The priority property uses a PriorityLevel enum\nto constrain the possible values. To do this, follow these steps: Open the  App.xcodeproj  in Xcode. Open the  Item.swift  class file. Add the following property to the  Item  class: Also add a PriorityLevel  PersistableEnum  below the  Item  class: PersistableEnum is the protocol  that marks\nenum types as persistable directly in Realm. We set the enum's type\nas  Int  here instead of  String  so we can query based on a\nnumeric priority level later. We use a  description  computed property\nto display a string representation of the priority in the UI. In the  Views  directory, go to  CreateItemView.swift .\nAdd a new  @State  property under the existing  itemSummary  property.\nFor now, set the default value to medium priority: Now, in the  Form  body, add a Picker that enables the user to\nchoose which priority level to set on the new Item. Locate the\n Section  that contains the buttons, and insert the following\ncode  above  it: Now, move down to the  Button(action:  that sets the values of the\n newItem  when the user presses the  Save  button. Add a line\nbelow  newItem.summary  to also set the  priority  property: At this point, you can run the application again. Log in using the account\nyou created earlier in this tutorial. You will see the one Item you\npreviously created. Add a new Item, and you will see that you can now\nset the priority. Choose  High  for the priority and save the Item. Now switch back to the Atlas data page in your browser, and refresh the\n Item  collection. You should now see the new Item with the  priority \nfield added and set to  1 . The existing Item does not have a  priority \nfield. Adding a property to a Realm object is not a breaking change and therefore\ndoes not require a  client reset . The template\napp has Development Mode enabled, so changes to the client Realm object\nare reflected in the server-side schema. For more information, see\n Development Mode  and  Update Your Data Model . In the  ContentView.swift  file, we create the Flexible Sync subscription\nthat defines which documents we sync with the user's device & account.\nLook for the  let config = user.flexibleSyncConfiguration(initialSubscriptions: \nvariable where we set the initial subscriptions. Within the  subscriptions.append() \nmethod, you can see that we are currently subscribing to all documents where\nthe  owner_id  property matches the authenticated user's id. We want to maintain\nthat, but  only  sync Items that are marked as High or Severe priority. This is why we set the  PriorityLevel  enum to type  Int , where the highest\npriority (severe) has a value of 0, and the lowest priority (low) has\na value of 3. We can make direct comparisons between an Int and the\npriority property. To do so, update the query statement to include documents\nwhere the priority is equal to or  less  than PriorityLevel.High (or 1), as\nshown here. We'll also add the  reRunOnOpen  bool, and set it to  true ,\nto force the subscription query to recalculate which documents to sync\nevery time we open the app. Run the application again. Log in using the account you created earlier\nin this tutorial. Because we added  reRunOnOpen , the app\nshould re-sync only the documents that match the Flexible Sync query.\nAfter an initial moment when Realm resyncs the document collection, you\nwill only see the new Item of High priority that you created. The Item document you initially created is not synced,\nbecause it does not have a  priority  field. If you want this Item to\nbe synced, you can edit the document in the Atlas UI and add a value for\nthe priority field. If you want to further test the functionality, you can create Items of various\npriorities. You will see that a new Item with a lower priority briefly\nappears in the list of Items and then disappears. The Sync error handler\nhelpfully provides a message describing this behavior: You can also see this message in the console log. In this scenario, Realm creates the Item locally, syncs it with the\nbackend, and then reverts the write because it doesn't meet the\nsubscription rules. Adding a property to an existing Realm object is a non-breaking change, and\nDevelopment Mode ensures that the schema change is reflected server-side. Consider adding the new  Priority  property to the  ItemList ,  ItemRow , and  ItemDetail  Views. Read our  Swift SDK  and  SwiftUI  documentation. Find developer-oriented blog posts and integration tutorials on the\n MongoDB Developer Hub . Join the  MongoDB Community forum \nto learn from other MongoDB developers and technical experts. Explore engineering and expert-provided  example projects . How did it go? Use the Share Feedback tab at the bottom right of the page to let us know\nif this tutorial was helpful or if you had any issues.",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices app create \\\n  --name MyTutorialApp \\\n  --template swiftui.todo.flex \\\n  --deployment-model global \\\n  --environment development"
                },
                {
                    "lang": "swift",
                    "value": "@Persisted var priority: PriorityLevel"
                },
                {
                    "lang": "swift",
                    "value": "class Item: Object, ObjectKeyIdentifiable {\n   @Persisted(primaryKey: true) var _id: ObjectId\n   @Persisted var isComplete = false\n   @Persisted var summary: String\n   @Persisted var owner_id: String\n   @Persisted var priority: PriorityLevel\n}\n\nenum PriorityLevel: Int, PersistableEnum, CaseIterable {\n   case severe = 0\n   case high = 1\n   case medium = 2\n   case low = 3\n\n   var description: String {\n      switch self {\n      case .severe: return \"Severe\"\n      case .high: return \"High\"\n      case .medium: return \"Medium\"\n      case .low: return \"Low\"\n      }\n   }\n}"
                },
                {
                    "lang": "swift",
                    "value": "@State var itemSummary = \"\"\n@State var priority = PriorityLevel.medium"
                },
                {
                    "lang": "swift",
                    "value": "Section(header: Text(\"Priority\")) {\n    Picker(selection: $priority, label: Text(\"Set priority\")) {\n        ForEach(PriorityLevel.allCases, id: \\.self) { priority in\n            Text(priority.description)\n        }\n    }\n}"
                },
                {
                    "lang": "swift",
                    "value": "newItem.summary = itemSummary\nnewItem.priority = priority"
                },
                {
                    "lang": "swift",
                    "value": "let config = user.flexibleSyncConfiguration(initialSubscriptions: { subs in\n   if let foundSubscription = subs.first(named: Constants.myItems) {\n      foundSubscription.updateQuery(toType: Item.self, where: {\n         $0.owner_id == user.id && $0.priority <= PriorityLevel.high\n      })\n   } else {\n      // No subscription - create it\n      subs.append(QuerySubscription<Item>(name: Constants.myItems) {\n         $0.owner_id == user.id && $0.priority <= PriorityLevel.high\n      })\n   }\n}, rerunOnOpen: true)"
                },
                {
                    "lang": "sh",
                    "value": "ERROR\n\"Client attempted a write that is outside\nof permissions or query filters; it has been reverted\""
                }
            ],
            "preview": "Realm provides a Swift SDK that allows you to create a native iOS\nmobile application with Swift or Objective-C. This tutorial is based on the\nSwiftUI Flexible Sync Template App, named swiftui.todo.flex, which illustrates\nthe creation of a to-do list application. This application enables users to:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "tutorial/backend",
            "title": "Write a Serverless GitHub Contribution Tracker",
            "headings": [
                "Overview",
                "Prerequisites",
                "Create a New App",
                "Welcome New GitHub Contributors",
                "Create an Endpoint",
                "Update Your Endpoint Function's Authentication Settings",
                "Connect the Endpoint to a GitHub Webhook",
                "Get a GitHub Access Token",
                "Store the Token as a Value",
                "Install the GitHub API Client",
                "Write the Endpoint Logic",
                "Test the Endpoint",
                "Generate a Weekly Community Report",
                "Specify Which Repos Should Generate Reports",
                "Create a Function that Generates Reports",
                "Generate And Save Reports Every Week",
                "Send Report Notifications",
                "Test the Report Triggers",
                "What's Next?",
                "Keep Building",
                "Explore the Documentation",
                "Give Us Feedback",
                "Join the Community"
            ],
            "paragraphs": "Estimated time to complete: 30 minutes In this tutorial, you will use Atlas App Services to build a serverless\napplication that monitors GitHub repositories and tracks contributions. The App Services App that you build will have several features: After completing this tutorial, you will know how to do the following tasks: Keep track of users that have contributed to your project in a MongoDB Atlas collection. Leave a comment that welcomes new contributors when they open a pull request or file an issue. Generate and send a weekly report that summarizes contributions to your repository. Write and run serverless  functions  that handle your App's logic. Automate scheduled tasks and respond to changing data with  Atlas Triggers . Store and access static data, like API keys, in  Values . Connect to an external service through an API You need the following before you can begin this tutorial: Also note that this tutorial doesn't use the  deployment draft \nworkflow. A deployment draft is a collection of App changes that you can deploy or\ndiscard as a single unit. Unless you disabled deployment drafts, you may need to\nsave your changes to a draft and then deploy them manually. A GitHub repository. The App you build will track contributions to\nthis repository. You can  create a new repository \nfor this tutorial or use an existing repo that you have admin access\nto. A MongoDB Atlas cluster. If you don't already have a cluster,  sign up\nfor MongoDB Atlas and create one for free . First, you need to create an application in App Services. To create the App: Open your Atlas project at  cloud.mongodb.com . In the top navigation, click  App Services . Click  Create a New App . Name the App  github-tracker-tutorial . Select your cluster to link it to the new App. Click  Create App Service . We'll connect the app to your GitHub repository using a  GitHub\nrepository webhook .\nWebhooks let your App know whenever some event, like a new commit or\npull request, happens in your repository. Each event runs a serverless\nfunction where you can do something in response. For this tutorial, we'll use the  GitHub REST API  to send a welcome message to\ncontributors whenever they open their first pull request or issue on a\nrepository. Webhooks work by sending a request about the event to a URL that your\nApp controls. In your App, you'll need to expose a custom endpoint with\na unique URL to receive and handle the webhook requests. To create the endpoint: In the left navigation menu, click  HTTPS Endpoints . Click  Add An Endpoint . Name the endpoint route  /greetNewContributors . Leave the HTTP method set to  POST . For authorization, choose  Require a Secret . Enter a new secret name and click  Create  to create a new secret. Then, enter\n tutorial  as the secret value. This requires all incoming requests\nto include the query parameter  secret=tutorial  in the request\nURL. Create a new  Atlas Function  for the endpoint and name it\n endpoints/greetNewContributors . For now, set up a basic handler that only responds to incoming calls\nwithout doing other work. Copy the following code into the Function\nbody: Click  Save  and deploy the endpoint. Under  Operation Type , copy the endpoint callback URL. You'll need it\nto set up the GitHub webhook later. Now that you've created the endpoint in your App, you need to change the authentication\nsettings for the endpoint's Function so that the GitHub webhook is accepted. To update your Function's authentication settings: In the left navigation menu, click  Functions . Find your endpoint Function and select it. Click  Settings . Under  Authentication , change the authentication method to  System . Click  Save  to deploy the Function. With your endpoint Function ready to go, you need to set up a webhook in your\nGitHub repository that sends events to the endpoint. To create the webhook in your repository: To confirm that the webhook succesfully calls your endpoint, check your  application logs \nin App Services for entries with the type  Endpoint . You can get there by clicking  Logs \nin the left navigation menu. You can also check the  webhook's request logs \nin GitHub under  Recent Deliveries  on the webhook's settings page. Each successful\nrequest has a green checkmark next to it. Open the repository's settings and select  Webhooks  in the\nleft navigation menu. Add a new webhook \nand set the  Payload URL  to the URL of the endpoint that you just created.\nThe URL will look like one of the following depending on your app's\n deployment model : Set the webhook content type to  application/json . Set the  Secret  to the same secret value you defined for\nthe endpoint:  tutorial . Choose to select individual events and configure the webhook to only\nsend events for  Issues  and  Pull requests . Click  Add webhook  to save the new webhook. The webhook is now set up to send events from GitHub to your endpoint.\nHowever, to respond to events in the endpoint with the GitHub API you'll\nneed an access token. This tutorial uses personal access tokens, but you\ncould also set up a GitHub app and use that token instead. To  create a personal access token : Open your GitHub user settings (not your repository settings) and\nselect  Developer settings  in the left navigation menu. In the left navigation menu, select  Personal access tokens  and then\nclick  Generate new token . Configure the token with a descriptive name and a reasonable\nexpiration time. Since this is a tutorial, consider expiring the\ntoken after 7 days. Select the  repo  scope. Click  Generate token . Copy the token somewhere secure where you can access it again. GitHub\nwill never show you the token again after this point. Back in your App, add a new Value to hold the personal access token you\njust generated. You'll be able to reference the Value from your endpoint\nwithout hardcoding the token into your Functions. To create the Value: In the left navigation menu, click  Values . Click  Create New Value . Name the Value  GitHubAccessToken . Leave the type set to  Value . Paste the personal access token into the Value's input. The Value must\nbe valid JSON, so make sure you have surrounding quotation marks. Click  Save . The endpoint will interact with GitHub's REST API to leave comments. You\ncould write and send HTTP requests directly to the API using the\nbuilt-in  context.http  client or an external library. However, in\nthis tutorial we use GitHub's official Node.js library called\n Octokit  that wraps the API. Once installed,\nyou'll be able to import the library from any Function in your App. To add the Octokit library to your App: In the left navigation menu, click  Functions . Select the  Dependencies  tab. Click  Add Dependency . Enter the package name:  @octokit/request . Click  Add . Wait for App Services to install the package. The installation should\ncomplete in a few seconds, but may take up to a minute. Now that you have an access token and have installed Octokit, you can\nupdate the endpoint Function to actually do something when it receives\nevents. Specifically, the Function should: To update the Function: Parse the incoming webhook event Log the contribution in MongoDB Add a comment through the GitHub API Send an informative response back to GitHub In the left navigation menu, click  Functions . Click  endpoints/greetNewContributors  to open the endpoint\nFunction editor. Replace the basic Function with the following code: Click  Save  and deploy the endpoint. The welcome message endpoint should now be fully set up. To test that it\nworks correctly, open a new issue or pull request on the repository. The\nendpoint will add a new comment to the thread the first time you do this\nbut won't add a welcome message on subsequent attempts. GitHub logs repository webhook requests, so you can also check the log\nentry in GitHub to confirm that everything is working properly. Each\nrequest log includes a response message from the endpoint. If you want to reset the test, delete the document with your GitHub\nusername from  community.contributions . This lets the App \"forget\"\nthat you've contributed before and it will welcome you on your next\ncontribution. Your App is connected to GitHub, stores information about contributions,\nand welcomes new contributors. Now we'll extend it to automatically\nanalyze and generate reports for your repository. Your App needs a way to know which repositories it should generate\nreports for every week. For this tutorial, we'll hardcode the list in a\nValue. Create a new Value called  GitHubProjects  that contains an array of\nobjects. Each object specifies the  owner  and  repo  name of a\nGitHub repository. Make sure to include an entry for your repository. A report is a document that summarizes the contributions to a repository\nfor some time period. We'll use a Function to create on-demand reports\nfor a repo. Create a new Function named  generateCommunityReport  and add the\nfollowing code: The Function you just created creates a report for a repository\non-demand. However, at this point nothing calls the Function and the\ngenerated reports are not saved anywhere. To actually use it, we'll\ncreate a scheduled Atlas Trigger that calls the Function once every week and\nsaves the generated reports in your linked cluster. To create the Trigger: In the left navigation menu, click  Triggers . Click  Add a Trigger . Choose  Scheduled  for the Trigger type. Name the Trigger  generateAndSaveCommunityReports Choose the  Advanced  schedule type Enter the following cron schedule to run once a week on Monday at 5\nAM UTC: Create a new Function for the Trigger and name it\n triggers/generateAndSaveCommunityReports . Click  Add Dependency  and install  moment , which we use\nto work with dates in the Function. Copy the following code into the Function body: Click  Save . Update your new Function's  authentication settings \nto match the new Endpoint's Function from earlier in this tutorial. Your App will now automatically generate and save reports every week.\nHowever, the reports won't be very useful if nobody sees them. We'll\ncreate a database Trigger that listens for new reports and creates a\nformatted message that you can send to end users. To set up the messages: In the left navigation menu, click  Triggers . Click  Add a Trigger . Leave the Trigger type set to  Database . Name the Trigger  sendCommunityReport . Add the Trigger to the  community.reports  collection and listen\nfor  Insert  events. Enable  Full Document  to include each new report document\nin the change event passed to the Trigger Function. Create a new Function for the Trigger and name it\n triggers/sendCommunityReport . Copy the following code into the Function body: Click  Save  and deploy the Trigger. Your App is set up to automatically generate, save, and send reports\nevery week. To make sure that everything works, you can run this report\nflow manually. Open the Function editor for your scheduled Trigger,\n triggers/generateAndSaveCommunityReports , and then click the\n Run  button. This should generate and save on-demand reports\nfor every repo that you listed in the  GitHubProjects  Value. To confirm: Check  community.reports  for the new report documents. Check your App's database Trigger logs to find the formatted\nmessage for each report Congratulations! You've succesfully set up a serverless GitHub\ncontribution tracker and reached the end of this tutorial. If you want to keep developing, you can try to add some new features to\nthe tracker. For example, you could: Update the endpoint to handle more webhook event types like\n issue_comment  or\n pull_request_review . Update the weekly reports to include more information from the GitHub\nAPI. Connect to an external service like  Twilio  or  SendGrid  to actually send the report via email or\nSMS instead of just logging it. App Services includes many services that can power your App. Check out\nthe rest of the documentation to learn more about these services and how\nyou can use them. Authenticate & Manage Users Data Access Rules Schemas & Relationships Functions HTTPS Endpoints Triggers We're always working to improve our docs and tutorials. If you have a\nsuggestion or had issues with this tutorial, click  Give\nFeedback  at the bottom of this page to rate the tutorial and send us a\ncomment. The official  MongoDB Community Forums  are a great\nplace to meet other developers, ask and answer questions, and stay\nup-to-date with the latest App Services features and releases.",
            "code": [
                {
                    "lang": "javascript",
                    "value": "exports = async function greetNewContributors(request, response) {\n  return response\n    .setStatusCode(200)\n    .setBody(\"Successfully received a GitHub webhook event\")\n}"
                },
                {
                    "lang": "text",
                    "value": "https://data.mongodb-api.com/app/<App ID>/endpoint/greetNewContributors"
                },
                {
                    "lang": "text",
                    "value": "https://<Region>.aws.data.mongodb-api.com/app/<App ID>/endpoint/greetNewContributors"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function greetNewContributors(request, response) {\n  // Parse the webhook event from the incoming request.\n  const event = JSON.parse(request.body.text());\n\n  // Don't do anything unless this is a new issue or pull request\n  if (event.action !== \"opened\") {\n    return response.setStatusCode(200);\n  }\n\n  // Get data from the GitHub webhook event.\n  // Based on the webhook configuration the event will be one of the following:\n  // - issues: https://docs.github.com/en/developers/webhooks-and-events/webhooks/webhook-events-and-payloads#issues\n  // - pull_request: https://docs.github.com/en/developers/webhooks-and-events/webhooks/webhook-events-and-payloads#pull_request\n  const sender = event.sender;\n  const repo = event.repository;\n  const contribution = event.issue || event.pull_request;\n  const contribution_url = event.issue\n    ? event.issue.url\n    : event.pull_request.issue_url;\n  const issue_number = contribution.number;\n\n  // Record this contribution in the user's contributor document.\n  // If this user hasn't contributed to the repo before, create a document for them.\n  const atlas = context.services.get(\"mongodb-atlas\");\n  const contributors = atlas.db(\"community\").collection(\"contributors\");\n  const contributor = await contributors.findOneAndUpdate(\n    // Look up the user by their GitHub login\n    { login: sender.login },\n    // Add this issue or pull request to their list of contributions\n    {\n      $push: {\n        contributions: {\n          date: new Date(),\n          type: event.issue ? \"issue\" : \"pull_request\",\n          url: contribution_url,\n        },\n      },\n    },\n    // If they haven't contributed before, add them to the database\n    { upsert: true, returnNewDocument: true }\n  );\n\n  // Send a welcome message to first time contributors on their issue or pull request\n  const isFirstTimeContributor = contributor.contributions.length === 1;\n  if (isFirstTimeContributor) {\n    const octokit = require(\"@octokit/request\");\n    await octokit.request(\n      \"POST /repos/{owner}/{repo}/issues/{issue_number}/comments\",\n      {\n        headers: {\n          authorization: `token ${context.values.get(\"GitHubAccessToken\")}`,\n        },\n        owner: repo.owner.login,\n        repo: repo.name,\n        issue_number: issue_number,\n        body: `Hi there ${sender.login} \ud83d\udc4b Thanks for your first contribution!`,\n      }\n    );\n  }\n\n  // Configure the HTTP response sent back to GitHub\n  return response\n    .setStatusCode(200)\n    .setHeader(\"Content-Type\", \"application/json\")\n    .setBody(\n      isFirstTimeContributor\n        ? `This is ${sender.login}'s first contribution!`\n        : `${sender.login} has contributed before.`\n    );\n};\n"
                },
                {
                    "lang": "json",
                    "value": "[\n  { \"owner\": \"<GitHub Username>\", \"repo\": \"<Repository Name>\" }\n]"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function generateCommunityReport({ owner, repo, startDate }) {\n  // Look up issues and pull requests that had activity\n  const octokit = require(\"@octokit/request\");\n  const { data: issuesWithActivity } = await octokit.request(\n    \"GET /repos/{owner}/{repo}/issues\",\n    {\n      headers: {\n        authorization: `token ${context.values.get(\"GitHubAccessToken\")}`,\n      },\n      owner: owner,\n      repo: repo,\n      since: startDate,\n    }\n  );\n\n  // Look up users that contributed to the repo\n  const atlas = context.services.get(\"mongodb-atlas\");\n  const contributors = atlas.db(\"community\").collection(\"contributors\");\n  const allContributors = await contributors\n    .find({\n      contributions: {\n        $elemMatch: {\n          date: { $gt: new Date(startDate) },\n          owner: owner,\n          repo: repo,\n        },\n      },\n    })\n    .toArray();\n\n  // Keep track of users who made their first contribution\n  const newContributors = allContributors.filter((c) => {\n    new Date(c.contributions[0].date) > new Date(startDate);\n  });\n\n  // Return a report with the data\n  return {\n    owner,\n    repo,\n    startDate,\n    issuesWithActivity,\n    allContributors,\n    newContributors,\n  };\n};\n"
                },
                {
                    "lang": "text",
                    "value": "0 5 * * 1"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function generateAndSaveCommunityReports() {\n  const projects = context.values.get(\"GitHubProjects\");\n  const lastMonday = getLastMonday(); // e.g. \"2022-02-21T05:00:00.000Z\"\n\n  // Generate a report for every tracked repo\n  const reportsForLastWeek = await Promise.all(\n    // Call the `generateCommunityReport` function for each project\n    projects.map(async (project) => {\n      return context.functions.execute(\"generateCommunityReport\", {\n        owner: project.owner,\n        repo: project.repo,\n        startDate: lastMonday,\n      });\n    })\n  );\n\n  // Save the generated reports in Atlas\n  const atlas = context.services.get(\"mongodb-atlas\");\n  const reports = atlas.db(\"community\").collection(\"reports\");\n  return await reports.insertMany(reportsForLastWeek);\n};\n\n// Get an ISO string for last Monday at 5AM UTC\nfunction getLastMonday() {\n  const moment = require(\"moment\");\n  return moment(new Date().setUTCHours(5, 0, 0, 0))\n    .utc()\n    .day(1 - 7)\n    .toISOString();\n}\n"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function sendCommunityReport(changeEvent) {\n  // Pull out the report document from the database change event\n  const report = changeEvent.fullDocument;\n  \n  // Format values from the report to include in the message\n  const projectName = `${report.owner}/${report.repo}`;\n  const moment = require(\"moment\");\n  const formattedDate = moment(report.startDate).utc().format(\"MMMM Do, YYYY\");\n  const numIssuesWithActivity = report.issuesWithActivity.length;\n  const numContributors = report.allContributors.length;\n  const numNewContributors = report.newContributors.length;\n  \n  // Create a message string that describes the data in the report\n  const message = [\n    `# Community contributions to ${projectName} since ${formattedDate}`,\n    `Last week, there was activity on ${numIssuesWithActivity} issues and pull requests.`,\n    `We had ${numContributors} people contribute, including ${numNewContributors} first time contributors.`,\n  ].join(\"\\n\");\n  \n  // For this tutorial we'll just log the message, but you could use a\n  // service to send it as an email or push notification instead.\n  console.log(message);\n};\n"
                }
            ],
            "preview": "Estimated time to complete: 30 minutes",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "tutorial/react-native",
            "title": "React Native Tutorial",
            "headings": [
                "Overview",
                "Prerequisites",
                "Start with the Template App",
                "Set Up the Template App",
                "Install Dependencies",
                "Build the App",
                "Test the App",
                "Get Familiar With the Template App",
                "Atlas App Services App",
                "React Native App",
                "Add a Priority Level Field",
                "Define the Priority Levels",
                "Update the Item Data Model",
                "Add a Priority Picker",
                "Run and Test the App",
                "Update the Sync Subscription",
                "Add a Mode Toggle to the UI",
                "Update the Sync Subscription",
                "Test the App",
                "What's Next?"
            ],
            "paragraphs": "You can use the Realm React Native SDK and  @realm/react \nto build a mobile application with React Native. This tutorial\nwalks you through how to build your own app that uses Flexible Sync. For this tutorial, we'll start with a pre-built TypeScript template application\nto see how everything fits together. The app is a pre-built template that includes a working React\nNative application (frontend) and its corresponding App Services App configuration\nfiles (backend). The template app is a basic to-do list application that lets users do various\nthings to manage their tasks: After you've got the template app running, you will add a new  priority \nfield to the existing  Item  model and update the Flexible Sync\nsubscription to only show items within a range of priorities. This example\nillustrates how you might adapt the template app for your own needs.\nYou would not necessarily make this change given the current structure of\nthe template app. The template app provides a toggle that simulates the device being in\nan offline mode. This toggle lets you quickly test Device Sync functionality,\nemulating the user having no internet connection. However,\nyou would likely remove this toggle in a production application. This tutorial should take around 30 minutes. Create email/password accounts and log in and out of the app. Create, read, update, and delete their own tasks. View all tasks, even if the user is not the owner. If you prefer to explore on your own rather than follow a guided\ntutorial, check out the  React Native Quick Start . It includes copyable code\nexamples and the essential information that you need to set up a\nReact Native app with Atlas Device Sync. You must set up your local environment for React Native development\nbefore you can begin this tutorial. For detailed instructions, see\n Setting up the development environment  in the React Native\ndocs. This tutorial starts with a Template App. You need an  Atlas Account , an API key, and\nappservices to create a Template App. You can learn more about creating an Atlas account in the\n Atlas Getting Started  documentation. For this\ntutorial, you need an Atlas account with a free-tier cluster. You also need an Atlas  API key  for the MongoDB\nCloud account you wish to log in with. You must be a  Project\nOwner  to create a Template\nApp using appservices. To learn more about installing appservices, see  Install App\nServices CLI . After installing, run the\n login  command using the API key for\nyour Atlas project. This tutorial is based on the React Native SDK Flexible Sync Template App named\n react-native.todo.flex . We start with the default app and build new features\non it. To learn more about the Template Apps, see  Template Apps . If you don't already have an Atlas account,  sign-up  to deploy a Template App. Follow the procedure described in the\n Create an App Services App  guide, and select\n Create App from Template . Select the\n Real-time Sync  template. This creates an App Services App\npre-configured to use with one of the Device Sync template app clients. After you create a template app, the UI displays a modal labeled\n Get the Front-end Code for your Template . This modal\nprovides instructions for downloading the template app client code\nas a  .zip  file or using App Services CLI to get the client. After selecting the  .zip  or App Services CLI method, follow the on-screen\ninstructions to get the client code. For this tutorial, select the\n JavaScript (React Native)  client code. The  appservices apps create \ncommand sets up the backend and creates a React Native template\napp for you to use as a base for this tutorial. Run the following command in a terminal window to create an app\nnamed \"MyTutorialApp\" that is deployed in the  US-VA  region\nwith its environment set to \"development\" (instead of production\nor QA). The command creates a new directory in your current path with the\nsame name as the value of the  --name  flag. You can fork and clone a GitHub repository that contains the Device\nSync client code. The React Native client code is available at\n https://github.com/mongodb/template-app-react-native-todo . If you use this process to get the client code, you must create a\ntemplate app to use with the client. Follow the instructions at\n Create a Template App  to use the Atlas App Services UI, App Services CLI,\nor Admin API to create a Device Sync template app. Use the following steps to get the template app up and running on your\ncomputer: In your terminal, go to the directory that contains the client code. If you\ncreated the app with the App Services CLI, go to\n MyTutorialApp/frontend/react-native.todo.flex .\nOtherwise, go to the root of your downloaded or cloned project. Then\nrun following commands to navigate to install the app's dependencies: To build and run the app on an iOS device or simulator, install\nthe additional iOS dependencies with  CocoaPods . At this point, you should have a fully functional React Native app\nthat can run on iOS, Android, or both. If you encounter an error or otherwise have issues, make sure that your\nReact Native environment is set up correctly. Refer to the official React\nNative  development environment setup  guide. Follow all the\nsteps for your Development OS and Target OS. To make sure that everything works on iOS, build the app and\nrun it in an iOS simulator: To make sure that everything works on Android: Start an Android emulator. See  Run apps on the\nAndroid Emulator  for details on\nhow to do this. Build the app on the emulator: One common error is  Error: spawn ./gradlew EACCES . This means\nthe project file permissions aren't what they need to be. On\nMacOS, you can fix this by entering  chmod 755 android/gradlew \nin your terminal. When the build completes, you should have a functional app running\non your simulator. In the app, register a new account and test the features: If you connect to your Atlas Cluster and query the\n todo.Item  collection you can see your App's data. As long as the React Native\napp is not in offline mode, new data and changes in the app automatically\nsync to the  todo.Item  collection. Similarly, any changes in the collection automatically sync down\nto the React Native app. Try changing an item's completion status\nin your cluster - the React Native app will automatically update\nwith the new value whenever a network connection is available. Add a few to-do items to the list. Press the checkbox on one or two items to mark them as complete. Press the X on an item to delete it from the list. Toggle internet connectivity in the app to simulate offline mode. To learn how to connect to your Atlas Cluster, see\n Connect to a Database Deployment . To learn more about updating data in your cluster, see\n Update Documents . Now that you have the template app running let's dive into the code to\nsee what we're working with. The template app includes a fully configured App Services App in the\n backend  directory. It has a unique  app_id  value in\n realm_config.json  that client applications use to connect. It also includes the following pre-defined configurations: A data source linked to your Atlas Cluster. A data model for the  todo.Item  collection that matches the  Item \nclass in the React Native app. An authentication provider that lets users register for and log in to\nyour app with an email and password. A flexible sync configuration with a single session role that allows\nusers to read and write their own items and view other users' items. The React Native app is a fully-configured mobile client that can run on\niOS and Android devices. The app uses the  @realm/react  library. The library includes React hooks\nand components that streamline working with your Atlas backend and Realm\ndatabase. The app contains some configuration files and directories, but you can\nignore those unless you want to customize the app. For this tutorial,\nyou should be familiar with the React components in the  source/  directory: File Name Description ItemSchema.tsx The  Item  class, including its object data model. We import this\nclass in  AppWrapper.tsx  to include it in the app's overall Realm\nschema. AppWrapper.tsx This is the root component for the app. It functions as a wrapper\ncomponent and contains all of the  @realm/react  providers. This is\nwhere you configure your realm and your connection to your Atlas\nbackend. App.tsx Most of the app's functionality is contained in this component and its\nchildren. Because the  @realm/react  providers are wrapped around this\ncomponent, it can access an instance of your Atlas backend, user objects,\nand interact with the Realm database. WelcomeView.tsx The user registration and login form that users see when they\nfirst open the app. ItemListView.tsx The main to-do list app that users interact with after they log\nin. It queries for  Item  Realm objects and displays them\nin a list. It also includes the code to create new  Item \nobjects and store them in Realm. CreateToDoPrompt.tsx A UI form that lets us enter data for new  Item  objects. The\ncode that actually creates new objects is in  ItemListView.tsx . LogoutButton.tsx A reusable button that logs out an authenticated user. OfflineModeButton.tsx A reusable button that simulates an offline mode by pausing and resuming\nthe current Realm  syncSession . Now that you're more familiar with what's already provided in the\ntemplate app, let's write some code to implement a new feature. For this tutorial, we'll add a new  priority  property to the\n Item  objects. This will let us organize to-dos by how important they\nare and allow us to focus only on the most important ones. We want to allow a small number of named priority levels, and we\nwant to easily be able sort the levels. To do this, we'll use a\nhelper function to define an  enum  object that maps a set of\nordered level names to and from an integer that represents their\npriority. Add the following code directly under the import statements in\n source/ItemSchema.tsx : The priority levels in the  enum  are ordered from most important\nto least. The corresponding index value for each level increases\nfrom the most important,  Priority[0] , to the least important,\n Priority[3] . This means that a higher priority level (meaning more important)\nhas a lower index value. Now we have an  enum  that defines the possible values of the\n priority  field. However, we still have to define the\n priority  field in the  Item  class. Add the following lines to your code in  source/ItemSchema.tsx  to\nadd  priority  to the  Item  data model: At this point, your React Native  Item  model and its\ncorresponding schema in your App Services App no longer agree. That's\nokay! Adding a property to a Realm object is not a breaking change\nand therefore does not require a  client reset . The template app has Development Mode\nenabled, so changes to the client Realm object are reflected in\nthe server-side schema. For more information, see\n Development Mode  and\n Update Your Data Model . Your app's data model now includes a  priority  for each  Item \nobject. Let's update the app UI so that you can choose a priority\nvalue when you add a new to-do to the list. First, we'll install an external library to implement the priority\npicker component. Run the following in your terminal inside of\nyour project root: If you're building for iOS, make sure to link the associated\nCocoapods after you've installed the package: Now that the package is fully installed, let's update the new\nto-do creation prompt component to use the picker. Add the following imports to the top of  source/CreateToDoPrompt.tsx : Then, modify the  CreateToDoPrompt  component: In  source/ItemListView.tsx , modify the  createItem()  function\nto accept and use  priority : Then, modify the create to-do submission handler to accept the\n priority  level and pass it to  createItem() : Finally, modify the list item template to render the to-do's  priority \nbefore the  summary : You may need to rebuild your app after installing. To do so,\nstop the bundler for your project and then run the build\ncommand: Add  priority  to the  onSubmit()  props definition Keep track of  priority  in a state hook Connect the state to the  Picker  component that you imported Pass  priority  to the  onSubmit()  handler Your app should now allow users to set a priority level for new to-do items. Rebuild the app and open it. Add some new to-do items to confirm\nthat you can choose a priority level and that the list displays\neach to-do's priority. The Device Sync protocol uses a flexible model where each sync client\nuses a standard RQL query to choose a subset of application data and\nthen  subscribes  to the subset. This automatically pulls the latest\nversion of all data in the subset to the device and syncs changes to the\ndata between devices. For example, the template app you're using has the following\nbuilt-in subscription to items that the current user owns: You can customize the subscription during runtime to sync only the data that\nyour app needs. Let's add a feature to demonstrate how. For this tutorial, we'll add a button that lets us toggle between two\nmodes: one where the app syncs all to-do items and another where it\nonly syncs important ones with a  priority  of High or Severe. First, add a  useState()  hook to the  ItemListView  component\nto keep track of the current mode: Then, add a new button that toggles the mode to the bottom of the\nto-do list, after  <ListItem> : At this point, the app can switch modes in the UI, but we haven't\ndone anything else so the modes are functionally identical. Let's\nupdate the sync subscription to only sync data relevant to the\ncurrent mode. In the first  useEffect  of the  ItemListView  component,\nadd code that checks the current mode and appends an additional\n priority  filter to the query if the  showImportantOnly  mode is\nactive: Don't forget to add  showImportantOnly  to the list of\ndependencies in the second argument of  useEffect . Your app is now set up to modify its sync subscription based on\nthe current mode. Rebuild and run the app to make sure everything works. You should\nbe able to create, complete, and delete to-do items as well as toggle\nbetween viewing all items and only important items. Read our  React Native SDK  documentation. Find developer-oriented blog posts and integration tutorials on the  MongoDB Developer Hub . Join the  MongoDB Community forum  to learn from other MongoDB developers and technical experts. Add a feature to the template app by  using Triggers and Atlas Search . Explore engineering and expert-provided  example projects . How did it go? Use the Share Feedback tab at the bottom right of the page to let us know\nif this tutorial was helpful or if you had any issues.",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices app create \\\n  --name MyTutorialApp \\\n  --template react-native.todo.flex \\\n  --deployment-model global \\\n  --environment development"
                },
                {
                    "lang": "shell",
                    "value": "npm install"
                },
                {
                    "lang": "shell",
                    "value": "cd ios\nnpx pod-install"
                },
                {
                    "lang": "shell",
                    "value": "npm run ios"
                },
                {
                    "lang": "shell",
                    "value": "npm run android"
                },
                {
                    "lang": "javascript",
                    "value": "function createEnum(arr) {\n  arr.forEach((p, i) => arr[p] = i);\n  return arr;\n}\n// Priority.High === 1\n// Priority[Priority.High] === \"High\"\nexport const Priority = createEnum([\n  \"Severe\",\n  \"High\",\n  \"Medium\",\n  \"Low\",\n])\n"
                },
                {
                    "lang": "javascript",
                    "value": "export class Item extends Realm.Object<Item> {\n  _id!: BSON.ObjectId;\n  isComplete!: boolean;\n  summary!: string;\n  owner_id!: string;\n  priority!: string;\n\n  static schema: Realm.ObjectSchema = {\n    name: 'Item',\n    primaryKey: '_id',\n    properties: {\n      // This allows us to automatically generate a unique _id for each Item\n      _id: {type: 'objectId', default: () => new BSON.ObjectId()},\n      // All todo items will default to incomplete\n      isComplete: {type: 'bool', default: false},\n      summary: 'string',\n      owner_id: 'string',\n      priority: {\n        // Store the index value of the Priority enum rather than the name\n        type: 'int',\n        default: Priority.High\n      },\n    },\n  };\n}\n"
                },
                {
                    "lang": "shell",
                    "value": "npm install @react-native-picker/picker"
                },
                {
                    "lang": "shell",
                    "value": "npx pod-install"
                },
                {
                    "lang": "javascript",
                    "value": "import {Picker} from '@react-native-picker/picker';\nimport {Priority} from './ItemSchema';"
                },
                {
                    "lang": "shell",
                    "value": "npm run ios"
                },
                {
                    "lang": "shell",
                    "value": "npm run android"
                },
                {
                    "lang": "javascript",
                    "value": "type Props = {\n  onSubmit(args: {summary: string; priority: string;}): void;\n};\n\nexport function CreateToDoPrompt(props: Props): React.ReactElement<Props> {\n  const {onSubmit} = props;\n  const [summary, setSummary] = useState('');\n  const [priority, setPriority] = useState(Priority.High);\n\n  return (\n    <View style={styles.modalWrapper}>\n      <Text h4 style={styles.addItemTitle}>\n        Add To-Do Item\n      </Text>\n      <Input\n        placeholder=\"What do you want to do?\"\n        onChangeText={(text: string) => setSummary(text)}\n        autoCompleteType={undefined}\n      />\n      <Picker\n        style={{width: '80%'}}\n        selectedValue={priority}\n        onValueChange={value => setPriority(value)}>\n        {Priority.map(priority  => (\n          <Picker.Item\n            key={priority}\n            label={priority}\n            value={Priority[priority]}\n           />\n        ))}\n      </Picker>\n      <Button\n        title=\"Save\"\n        buttonStyle={styles.saveButton}\n        onPress={() => onSubmit({summary, priority})}\n      />\n    </View>\n  );\n}\n"
                },
                {
                    "lang": "javascript",
                    "value": "const createItem = useCallback(\n  ({summary, priority}: {summary: string, priority: string}) => {\n    realm.write(() => {\n      return new Item(realm, {\n        summary,\n        owner_id: user?.id,\n        priority\n      });\n    });\n  },\n  [realm, user],\n);\n"
                },
                {
                    "lang": "javascript",
                    "value": "<CreateToDoPrompt\n  onSubmit={({summary, priority}) => {\n    setShowNewItemOverlay(false);\n    createItem({summary, priority});\n  }}\n/>\n"
                },
                {
                    "lang": "javascript",
                    "value": "<ListItem\n  key={`${item._id}`}\n  bottomDivider\n  topDivider\n  hasTVPreferredFocus={undefined}\n  tvParallaxProperties={undefined}>\n  <Text>{item.priority}</Text>\n  <ListItem.Title style={styles.itemTitle}>\n    {item.summary}\n  </ListItem.Title>\n  <ListItem.Subtitle style={styles.itemSubtitle}>\n    {item.owner_id === user?.id ? '(mine)' : ''}\n  </ListItem.Subtitle>\n  <ListItem.CheckBox\n    checked={item.isComplete}\n    checkedColor={COLORS.primary}\n    iconType=\"material\"\n    checkedIcon=\"check-box\"\n    uncheckedIcon=\"check-box-outline-blank\"\n    onPress={() => toggleItemIsComplete(item._id)}\n  />\n  <Button\n    type=\"clear\"\n    onPress={() => deleteItem(item._id)}\n    icon={\n      <Icon\n        type=\"material\"\n        name=\"clear\"\n        size={12}\n        color=\"#979797\"\n        tvParallaxProperties={undefined}\n      />\n    }\n  />\n</ListItem>\n"
                },
                {
                    "lang": "javascript",
                    "value": "realm.subscriptions.update(mutableSubs => {\n  mutableSubs.removeByName(itemSubscriptionName);\n  mutableSubs.add(\n    realm.objects(Item).filtered(`owner_id == \"${user?.id}\"`),\n    {name: ownItemsSubscriptionName},\n  );\n});\n"
                },
                {
                    "lang": "javascript",
                    "value": "const [showImportantOnly, setShowImportantOnly] = useState(false);"
                },
                {
                    "lang": "javascript",
                    "value": "<Button\n  title={showImportantOnly ? 'Show All' : 'Show Important Only'}\n  buttonStyle={{\n    ...styles.addToDoButton,\n    backgroundColor: showImportantOnly ? '#00A35C' : '#FFC010',\n  }}\n  onPress={() => setShowImportantOnly(showImportantOnly => !showImportantOnly)}\n/>\n"
                },
                {
                    "lang": "javascript",
                    "value": "useEffect(() => {\n  if (showAllItems) {\n    realm.subscriptions.update(mutableSubs => {\n      mutableSubs.removeByName(ownItemsSubscriptionName);\n      mutableSubs.add(realm.objects(Item), {name: itemSubscriptionName});\n    });\n  } else if (showImportantOnly) {\n    realm.subscriptions.update(mutableSubs => {\n      mutableSubs.removeByName(itemSubscriptionName);\n      mutableSubs.add(\n        realm.objects(Item).filtered(`owner_id == \"${user?.id}\" && priority <= 1`),\n        {name: ownItemsSubscriptionName},\n      );\n    });\n  } else {\n    realm.subscriptions.update(mutableSubs => {\n      mutableSubs.removeByName(itemSubscriptionName);\n      mutableSubs.add(\n        realm.objects(Item).filtered(`owner_id == \"${user?.id}\"`),\n        {name: ownItemsSubscriptionName},\n      );\n    });\n  }\n}, [realm, user, showAllItems, showImportantOnly]);\n"
                }
            ],
            "preview": "You can use the Realm React Native SDK and @realm/react\nto build a mobile application with React Native. This tutorial\nwalks you through how to build your own app that uses Flexible Sync.",
            "tags": "realm/react",
            "facets": {
                "programming_language": [
                    "javascript/typescript"
                ],
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "functions/mongodb/api",
            "title": "MongoDB API Reference",
            "headings": [
                "mongodb.admin()",
                "Parameters",
                "Return Value",
                "admin.getDBNames()",
                "Parameters",
                "Return Value",
                "mongodb.db()",
                "Parameters",
                "Return Value",
                "database.getCollectionNames()",
                "Parameters",
                "Return Value",
                "database.collection()",
                "Parameters",
                "Return Value",
                "collection.find()",
                "Parameters",
                "Return Value",
                "collection.findOne()",
                "Parameters",
                "Return Value",
                "collection.findOneAndUpdate()",
                "Parameters",
                "Return Value",
                "collection.findOneAndReplace()",
                "Parameters",
                "Return Value",
                "collection.findOneAndDelete()",
                "Parameters",
                "Return Value",
                "collection.insertOne()",
                "Parameters",
                "Return Value",
                "collection.insertMany()",
                "Parameters",
                "Return Value",
                "collection.updateOne()",
                "Parameters",
                "Return Value",
                "collection.updateMany()",
                "Parameters",
                "Return Value",
                "collection.deleteOne()",
                "Parameters",
                "Return Value",
                "collection.deleteMany()",
                "Parameters",
                "Return Value",
                "collection.aggregate()",
                "Parameters",
                "Return Value",
                "collection.count()",
                "Parameters",
                "Return Value",
                "collection.distinct()",
                "Parameters",
                "Return Value",
                "collection.bulkWrite()",
                "Parameters",
                "Return Value"
            ],
            "paragraphs": "Gets a handle for the  admin  database in a linked MongoDB data\nsource. You can use this to run MongoDB admin commands like\n admin.getDBNames() . The  mongodb.admin()  method returns an  AdminDatabase  object. The\nobject contains helper methods that wrap a subset of MongoDB database\ncommands. See  admin.getDBNames() . Returns a list of database names in a MongoDB data source. This method is only available in  system functions . You cannot call this method from a function\nthat runs in the context of an application user. The  admin.getDBNames()  method returns an array of strings where each\nelement is the name of a database in the data source. Gets a handle for a database in a linked MongoDB data source. Parameter Type Description name string The name of the database. The  mongodb.db()  method returns a  Database  object that allows\nyou to access collections in the specified database. See  database.collection() Returns a list of collection names in the database. This method is only available in  system functions . You cannot call this method from a function\nthat runs in the context of an application user. The  database.getCollectionNames()  method returns an array of strings\nwhere each element is the name of a collection in the database. Gets a handle for a collection in a linked MongoDB data source from a\n database  handle. Parameter Type Description name string The name of the collection. The  database.collection()  method returns a collection object that\nallows you to query the specified collection. Finds all documents in a collection or view that match the provided\nquery filters. Returns a cursor object that allows you to access\nmatching documents. Parameter Type Description query object Optional. A  query filter  that specifies\nwhich documents to find. Specify an empty query ( {} ) or omit this\nparameter to match all documents in the collection. You can use most  query selectors  except for\n evaluation ,\n geospatial , or\n bitwise  selectors. You may\nonly use these selectors in  system functions . projection object Optional. A document that specifies which fields MongoDB should include or omit in\nmatching documents. To return all fields in the matching documents, omit this parameter or\nspecify an empty projection document ( {} ). To return specific fields and the document's  _id , specify the fields\nin the projection document with a value of  1 : To withhold specific fields, specify the fields in the projection\ndocument with a value of  0 : You may specify either fields to include or fields to withhold\nbut not both. For example, the following projection is\n invalid  because it simultaneously includes the  name \nfield and withholds the  address  field: The exception to this rule is the  _id  field, which you may\nwithhold from any query: options object An object that specifies additional configuration options. options.session ClientSession Optional. A session object that represents the transaction context in which the\noperation occurs. To learn more, see  Transactions . The  collection.find()  method returns a cursor object that points to\nany documents that match the specified query. You can manipulate and\naccess documents in the query result set with the following cursor\nmethods: Method Description cursor.next() Iterates the cursor and returns a  Promise  that resolves\nto the next document in the cursor. If the cursor is exhausted,\nthe promise resolves to  undefined . cursor.toArray() Iterates the cursor to exhaustion and returns a  Promise  that\nresolves to an array that contains all of the iterated\ndocuments. cursor.skip(amount) Specifies a number of matching documents to omit from the query\nresult set. MongoDB omits documents from the result set in sort\norder until it has skipped the specified number. If the query\nalso specifies a limit, skipped documents do not count towards\nthe limit threshold. You cannot call this method after retrieving one or more\ndocuments using  cursor.next()  or  cursor.toArray() . cursor.limit(limit) Specifies the maximum number of documents to include in the\nquery result set. If the result set contains more documents than\nthe specified  limit , the cursor will return documents in\norder up to the limit. You cannot call this method after retrieving one or more\ndocuments using  cursor.next()  or  cursor.toArray() . cursor.sort(sort) Sorts documents in the result set according to the  sort \nfilter. Sort documents specify one or more fields to sort on. The\nvalue of each field indicates whether MongoDB should sort it in\nascending ( 1 ) or descending ( -1 ) order. For more\ninformation, see  cursor.sort . You cannot call this method after retrieving one or more\ndocuments using  cursor.next()  or  cursor.toArray() . The following sort document specifies that documents should be\nsorted first by  age  from highest to lowest. Once sorted by\nage, the result set should further be sorted by  name  in\nalphabetical order for each distinct age value. You cannot return a cursor from a function. Instead, evaluate the\ncursor using  cursor.next()  or  cursor.toArray()  and return the\nresult. Finds a single document from a collection or view. If multiple documents\nmatch the query, this returns the first matching document in the collection. As a workaround, use  find()  with the  sort()  and  next()  cursor methods\nto return a single document from a sorted collection. Parameter Type Description query object Optional. A  query filter  that specifies\nwhich documents to find. Specify an empty query ( {} ) or omit this\nparameter to match all documents in the collection. You can use most  query selectors  except for\n evaluation ,\n geospatial , or\n bitwise  selectors. You may\nonly use these selectors in  system functions . projection object Optional. A document that specifies which fields MongoDB should include or omit in\nmatching documents. To return all fields in the matching documents, omit this parameter or\nspecify an empty projection document ( {} ). To return specific fields and the document's  _id , specify the fields\nin the projection document with a value of  1 : To withhold specific fields, specify the fields in the projection\ndocument with a value of  0 : You may specify either fields to include or fields to withhold\nbut not both. For example, the following projection is\n invalid  because it simultaneously includes the  name \nfield and withholds the  address  field: The exception to this rule is the  _id  field, which you may\nwithhold from any query: options object An object that specifies additional configuration options. options.session ClientSession Optional. A session object that represents the transaction context in which the\noperation occurs. To learn more, see  Transactions . The  collection.findOne()  method returns a  Promise  that resolves to the\nfirst document in the collection that matches the query. If no documents\nmatch the specified query, the promise resolves to  null . Updates a single document in a collection or view and returns the\ndocument in either its pre-update or post-update form. Unlike  collection.updateOne() , this action allows you to\natomically find, modify, and return a document with the same command.\nThis avoids the risk of other update operations changing the document\nbetween separate  find  and  update  operations. Parameter Type Description query object A  query filter  that specifies\nwhich documents to find. Specify an empty query ( {} ) or omit this\nparameter to match all documents in the collection. You can use most  query selectors  except for\n evaluation ,\n geospatial , or\n bitwise  selectors. You may\nonly use these selectors in  system functions . update object An  update document  that\nspecifies modifications to perform using MongoDB  update\noperators . options object An object that specifies additional configuration options. options.upsert boolean Optional. Default:  false . A boolean that, if  true , indicates that MongoDB should insert a new\ndocument that matches the query when the query does not match any\nexisting documents in the collection. options.sort boolean Optional. Specifies the query sort order. You can specify one or more fields to\nsort on where the value of each field indicates whether MongoDB should\nsort it in ascending ( 1 ) or descending ( -1 ) order. The following sort document specifies that documents should be\nsorted first by  age  from highest to lowest. Once sorted by\nage, the result set should further be sorted by  name  in\nalphabetical order for each distinct age value. options.projection boolean A document that specifies which fields MongoDB should include or omit in\nmatching documents. To return all fields in the matching documents, omit this parameter or\nspecify an empty projection document ( {} ). To return specific fields and the document's  _id , specify the fields\nin the projection document with a value of  1 : To withhold specific fields, specify the fields in the projection\ndocument with a value of  0 : You may specify either fields to include or fields to withhold\nbut not both. For example, the following projection is\n invalid  because it simultaneously includes the  name \nfield and withholds the  address  field: The exception to this rule is the  _id  field, which you may\nwithhold from any query: options.returnNewDocument boolean Optional. Default:  false . If  true , the method returns the modified document in its updated\nform instead of its original, pre-update form. options.session ClientSession Optional. A session object that represents the transaction context in which the\noperation occurs. To learn more, see  Transactions . The  collection.findOneAndUpdate()  method returns a  Promise  that resolves to a\nsingle document that the query overwrote. If no documents match the\nspecified query, the promise resolves to  null . You can specify whether to return the pre-replacement or\npost-replacement version of the document by setting the value of\n options.returnNewDocument . By default,  returnNewDocument  is\n false , which indicates that the promise should resolve to the\n pre-update  version of the document. Overwrites a single document in a collection or view and returns the\ndocument in either its pre-replacement or post-replacement form. Unlike  collection.updateOne() , this action allows you to\natomically find, modify, and return a document with the same command.\nThis avoids the risk of other update operations changing the document\nbetween separate  find  and  update  operations. Parameter Type Description query object A  query filter  that specifies\nwhich documents to find. Specify an empty query ( {} ) or omit this\nparameter to match all documents in the collection. You can use most  query selectors  except for\n evaluation ,\n geospatial , or\n bitwise  selectors. You may\nonly use these selectors in  system functions . replacement object A document that will replace the matching document. The replacement\ndocument cannot contain any MongoDB  update operators . options object An object that specifies additional configuration options. options.upsert boolean Optional. Default:  false . A boolean that, if  true , indicates that MongoDB should insert a new\ndocument that matches the query when the query does not match any\nexisting documents in the collection. options.sort boolean Optional. Specifies the query sort order. You can specify one or more fields to\nsort on where the value of each field indicates whether MongoDB should\nsort it in ascending ( 1 ) or descending ( -1 ) order. The following sort document specifies that documents should be\nsorted first by  age  from highest to lowest. Once sorted by\nage, the result set should further be sorted by  name  in\nalphabetical order for each distinct age value. options.projection boolean A document that specifies which fields MongoDB should include or omit in\nmatching documents. To return all fields in the matching documents, omit this parameter or\nspecify an empty projection document ( {} ). To return specific fields and the document's  _id , specify the fields\nin the projection document with a value of  1 : To withhold specific fields, specify the fields in the projection\ndocument with a value of  0 : You may specify either fields to include or fields to withhold\nbut not both. For example, the following projection is\n invalid  because it simultaneously includes the  name \nfield and withholds the  address  field: The exception to this rule is the  _id  field, which you may\nwithhold from any query: options.returnNewDocument boolean Optional. Default:  false . If  true , the method returns the modified document in its updated\nform instead of its original, pre-update form. options.session ClientSession Optional. A session object that represents the transaction context in which the\noperation occurs. To learn more, see  Transactions . The  collection.findOneAndReplace()  method returns a  Promise  that resolves to a\nsingle document that the query overwrote. If no documents match the\nspecified query, the promise resolves to  null . You can specify whether to return the pre-replacement or\npost-replacement version of the document by setting the value of\n options.returnNewDocument . By default,  returnNewDocument  is\n false , which indicates that the promise should resolve to the\n pre-update  version of the document. Removes a single document from a collection and returns the deleted\ndocument as it was immediately before it was deleted. Unlike  collection.updateOne() , this action allows you to\natomically find, modify, and return a document with the same command.\nThis avoids the risk of other update operations changing the document\nbetween separate  find  and  update  operations. Parameter Type Description query object A  query filter  that specifies\nwhich documents to find. Specify an empty query ( {} ) or omit this\nparameter to match all documents in the collection. You can use most  query selectors  except for\n evaluation ,\n geospatial , or\n bitwise  selectors. You may\nonly use these selectors in  system functions . options object An object that specifies additional configuration options. options.sort boolean Optional. Specifies the query sort order. You can specify one or more fields to\nsort on where the value of each field indicates whether MongoDB should\nsort it in ascending ( 1 ) or descending ( -1 ) order. The following sort document specifies that documents should be\nsorted first by  age  from highest to lowest. Once sorted by\nage, the result set should further be sorted by  name  in\nalphabetical order for each distinct age value. options.projection boolean A document that specifies which fields MongoDB should include or omit in\nmatching documents. To return all fields in the matching documents, omit this parameter or\nspecify an empty projection document ( {} ). To return specific fields and the document's  _id , specify the fields\nin the projection document with a value of  1 : To withhold specific fields, specify the fields in the projection\ndocument with a value of  0 : You may specify either fields to include or fields to withhold\nbut not both. For example, the following projection is\n invalid  because it simultaneously includes the  name \nfield and withholds the  address  field: The exception to this rule is the  _id  field, which you may\nwithhold from any query: options.session ClientSession Optional. A session object that represents the transaction context in which the\noperation occurs. To learn more, see  Transactions . The  collection.findOneAndDelete()  method returns a  Promise  that resolves to a\nsingle document that the query deleted. If no documents match the\nspecified query, the promise resolves to  null . Inserts a single document into a collection and returns the  _id  of\nthe inserted document. Parameter Type Description document object A document to insert into the collection. The  collection.insertOne()  method returns a  Promise  that\nresolves to a document that describes the insert operation. Value Type Description result.insertedId string The  _id  value of the document that the insert operation added\nto the collection. Inserts one or more documents into a collection and returns a list that\ncontains the  _id  value for each inserted document. Parameter Type Description documents object An array of documents to insert into the collection. options object An object that specifies additional configuration options. options.ordered boolean Optional. A boolean specifying whether the  mongod  instance should perform an ordered or unordered insert. Defaults to  true . The  collection.insertMany()  method returns a  Promise  that\nresolves to a document that describes the insert operation. Value Type Description result.insertedIds: Array<ObjectID> string An array that contains the  _id  values for all documents\nthat the insert operation added to the collection in the order\nthat they were passed to the method. Updates a single document in a collection and returns metadata about the\noperation. Parameter Type Description query object A  query filter  that specifies\nwhich documents to find. Specify an empty query ( {} ) or omit this\nparameter to match all documents in the collection. You can use most  query selectors  except for\n evaluation ,\n geospatial , or\n bitwise  selectors. You may\nonly use these selectors in  system functions . update object An  update document  that\nspecifies modifications to perform using MongoDB  update\noperators . options object An object that specifies additional configuration options. options.upsert boolean Optional. Default:  false . A boolean that, if  true , indicates that MongoDB should insert a new\ndocument that matches the query when the query does not match any\nexisting documents in the collection. options.session ClientSession Optional. A session object that represents the transaction context in which the\noperation occurs. To learn more, see  Transactions . The  collection.updateOne()  method returns a  Promise  that\nresolves to a document that describes the update operation. Value Type Description result.matchedCount number The number of documents in the collection that match the provided\nquery. result.modifiedCount number The number of documents in the collection that were modified by\nthe update operation. result.upsertedId string The  _id  value of the document inserted by an upsert\noperation. This value is only present when the  upsert  option\nis enabled and the update query does not match any documents. Updates one or more documents in a collection and returns metadata about\nthe operation. Parameter Type Description query object A  query filter  that specifies\nwhich documents to find. Specify an empty query ( {} ) or omit this\nparameter to match all documents in the collection. You can use most  query selectors  except for\n evaluation ,\n geospatial , or\n bitwise  selectors. You may\nonly use these selectors in  system functions . update object An  update document  that\nspecifies modifications to perform using MongoDB  update\noperators . options object An object that specifies additional configuration options. options.upsert boolean Optional. Default:  false . A boolean that, if  true , indicates that MongoDB should insert a new\ndocument that matches the query when the query does not match any\nexisting documents in the collection. options.session ClientSession Optional. A session object that represents the transaction context in which the\noperation occurs. To learn more, see  Transactions . The  collection.updateMany()  method returns a  Promise  that\nresolves to a document that describes the update operation. Value Type Description result.matchedCount number The number of documents in the collection that match the provided\nquery. result.modifiedCount number The number of documents in the collection that were modified by\nthe update operation. result.upsertedId string The  _id  value of the document inserted by an upsert\noperation. This value is only present when the  upsert  option\nis enabled and the update query does not match any documents. Removes a single document from a collection. Parameter Type Description query object A  query filter  that specifies\nwhich documents to find. Specify an empty query ( {} ) or omit this\nparameter to match all documents in the collection. You can use most  query selectors  except for\n evaluation ,\n geospatial , or\n bitwise  selectors. You may\nonly use these selectors in  system functions . options object An object that specifies additional configuration options. options.session ClientSession Optional. A session object that represents the transaction context in which the\noperation occurs. To learn more, see  Transactions . The  collection.deleteOne()  method returns a  Promise  that\nresolves to a document that describes the delete operation. Value Type Description result.deletedCount number The number of documents in the collection that were deleted by\nthe delete operation. Remove one or more documents from a collection. Parameter Type Description query object A  query filter  that specifies\nwhich documents to find. Specify an empty query ( {} ) or omit this\nparameter to match all documents in the collection. You can use most  query selectors  except for\n evaluation ,\n geospatial , or\n bitwise  selectors. You may\nonly use these selectors in  system functions . options object An object that specifies additional configuration options. options.session ClientSession Optional. A session object that represents the transaction context in which the\noperation occurs. To learn more, see  Transactions . The  collection.deleteMany()  method returns a  Promise  that\nresolves to a document that describes the delete operation. Value Type Description result.deletedCount number The number of documents in the collection that were deleted by\nthe delete operation. Executes an  aggregation  pipeline and returns a\ncursor that allows you to access the pipeline's output documents. Parameter Type Description pipeline object[] An array of one or more  aggregation pipeline stages . Atlas App Services supports nearly all MongoDB aggregation pipeline stages and\noperators, but some stages and operators must be executed within a\n system function . See  Aggregation Framework\nLimitations  for more\ninformation. options object An object that specifies additional configuration options. options.session ClientSession Optional. A session object that represents the transaction context in which the\noperation occurs. To learn more, see  Transactions . The  collection.aggregate()  method returns a cursor object\nthat points to any documents output from the final stage of the\naggregation pipeline. You can manipulate and access documents\nin the aggregation result set with the following methods: Method Description cursor.next() Iterates the cursor and returns a  Promise  that resolves\nto the next document in the cursor. If the cursor is exhausted,\nthe promise resolves to  undefined . cursor.toArray() Iterates the cursor to exhaustion and returns a  Promise  that\nresolves to an array that contains all of the iterated documents. cursor.skip(amount) Specifies a number of matching documents to omit from the\naggregation result set. MongoDB omits documents from the result\nset in sort order until it has skipped the specified number. You cannot call this method after retrieving one or more\ndocuments using  cursor.next()  or  cursor.toArray() . You cannot return a cursor from a function. Instead, evaluate the\ncursor using  cursor.next()  or  cursor.toArray()  and return the\nresult. Returns the number of documents in a collection or view that match a\ngiven query. Parameter Type Description query object Optional. A  query filter  that specifies\nwhich documents to find. Specify an empty query ( {} ) or omit this\nparameter to match all documents in the collection. You can use most  query selectors  except for\n evaluation ,\n geospatial , or\n bitwise  selectors. You may\nonly use these selectors in  system functions . options object An object that specifies additional configuration options. options.session ClientSession Optional. A session object that represents the transaction\ncontext in which the operation occurs. To learn more, see\n Transactions . The  collection.count()  method returns a  Promise  that\nresolves to the integer number of documents in the collection\nthat match the query. Value Description Count Result The number of documents in the collection that match the provided\nquery. Finds documents that match a given query filter and returns a list of\ndistinct values for a specific field across all matched documents. Parameter Type Description field string The name of the field in each document from which to find\ndistinct values. query object A  query filter  that specifies\nwhich documents to find. Specify an empty query ( {} ) or omit this\nparameter to match all documents in the collection. You can use most  query selectors  except for\n evaluation ,\n geospatial , or\n bitwise  selectors. You may\nonly use these selectors in  system functions . options object An object that specifies additional configuration options. options.session ClientSession Optional. A session object that represents the transaction context in which the\noperation occurs. To learn more, see  Transactions . The  collection.distinct()  method returns a  Promise  that resolves to an\narray of distinct values. Runs multiple insert, update, and delete operations on a collection with\na single call. Within the  bulkWrite()  function, you can specify one\nor more of the following write operations: insertOne updateOne updateMany deleteOne deleteMany replaceOne A bulk write can only operate on a single collection. Parameter Type Description operations object[] An array of bulkWrite operations to perform. Examples of\nsupported operations include the following: options object An object that specifies additional configuration options. options.ordered boolean Optional. Default:  true . If  true , the operations are executed one at a time in the specified\norder (i.e. serially). If an error occurs while processing an\n ordered  operation, the entire bulk operation returns without\nprocessing the remaining operations in the list. If  false , the operations are executed independently and may be\nprocessed in parallel. If an error occurs while processing an\n unordered  operation, MongoDB continues to process remaining write\noperations in the list. Unordered operations are theoretically faster since MongoDB can\nexecute them in parallel, but should only be used if the writes do\nnot depend on order. options.bypassDocumentValidation boolean Optional. Default:  false . If  true , the operation bypasses schema validation in\nApp Services. options.session ClientSession Optional. A session object that represents the transaction context in which the\noperation occurs. To learn more, see  Transactions . The  collection.bulkWrite()  function returns a  Promise  that resolves to\n null .",
            "code": [
                {
                    "lang": "javascript",
                    "value": "const mongodb = context.services.get(\"mongodb-atlas\");\nconst admin = mongodb.admin();"
                },
                {
                    "lang": "typescript",
                    "value": "admin(): AdminDatabase"
                },
                {
                    "lang": "javascript",
                    "value": "const mongodb = context.services.get(\"mongodb-atlas\");\nconst admin = mongodb.admin();\nconst dbNames = admin.getDBNames();"
                },
                {
                    "lang": "typescript",
                    "value": "getDBNames(): string[]"
                },
                {
                    "lang": "javascript",
                    "value": "const mongodb = context.services.get(\"mongodb-atlas\");\nconst db = mongodb.db(\"myDB\");"
                },
                {
                    "lang": "typescript",
                    "value": "db(name: string): Database"
                },
                {
                    "lang": "javascript",
                    "value": "const mongodb = context.services.get(\"mongodb-atlas\");\nconst db = mongodb.db(\"myDB\");\nconst collectionNames = db.getCollectionNames();"
                },
                {
                    "lang": "typescript",
                    "value": "getCollectionNames(): string[]"
                },
                {
                    "lang": "javascript",
                    "value": "const mongodb = context.services.get(\"mongodb-atlas\");\nconst db = mongodb.db(\"myDB\");\nconst collection = db.collection(\"myCollection\");"
                },
                {
                    "lang": "typescript",
                    "value": "collection(name: string): Collection"
                },
                {
                    "lang": "javascript",
                    "value": "const query = { \"reviews.0\": { \"$exists\": true } };\nconst projection = { \"_id\": 0 };\n\nreturn itemsCollection.find(query, projection)\n  .sort({ name: 1 })\n  .toArray()\n  .then(items => {\n    console.log(`Successfully found ${items.length} documents.`)\n    items.forEach(console.log)\n    return items\n  })\n  .catch(err => console.error(`Failed to find documents: ${err}`))"
                },
                {
                    "lang": "typescript",
                    "value": "find(\n   query?: object,\n   projection?: object,\n   options?: object\n): Cursor"
                },
                {
                    "lang": "javascript",
                    "value": "// Includes the field in returned documents\n{ <Field Name>: 1 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Withholds the field from returned documents\n{ <Field Name>: 0 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Invalid\n// Can't simultaneously include and withhold\n{ \"name\": 1, \"address\": 0 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Valid\n// Can exclude _id while including other fields\n{ \"_id\": 0, \"name\": 1 }"
                },
                {
                    "lang": "javascript",
                    "value": "collection.find().next()\n  .then(doc => console.log(\"next document\", doc))"
                },
                {
                    "lang": "javascript",
                    "value": "collection.find().toArray()\n  .then(docs => console.log(\"all documents\", docs))"
                },
                {
                    "lang": "javascript",
                    "value": "{ age: -1, name: 1 }"
                },
                {
                    "lang": "javascript",
                    "value": "collection.find({}).sort({\"<Field Name>\": 1}).next()\n  .then(result => console.log(\"Found Document: \", result))"
                },
                {
                    "lang": "javascript",
                    "value": "const query = { \"quantity\": { \"$gte\": 25 } };\nconst projection = {\n \"title\": 1,\n \"quantity\": 1,\n}\n\nreturn itemsCollection.findOne(query, projection)\n  .then(result => {\n    if(result) {\n      console.log(`Successfully found document: ${result}.`);\n    } else {\n      console.log(\"No document matches the provided query.\");\n    }\n    return result;\n  })\n  .catch(err => console.error(`Failed to find document: ${err}`));"
                },
                {
                    "lang": "typescript",
                    "value": "findOne(\n   query?: object,\n   projection?: object,\n   options?: object\n): Promise<object | null>"
                },
                {
                    "lang": "javascript",
                    "value": "// Includes the field in returned documents\n{ <Field Name>: 1 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Withholds the field from returned documents\n{ <Field Name>: 0 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Invalid\n// Can't simultaneously include and withhold\n{ \"name\": 1, \"address\": 0 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Valid\n// Can exclude _id while including other fields\n{ \"_id\": 0, \"name\": 1 }"
                },
                {
                    "lang": "typescript",
                    "value": "Promise<object | null>"
                },
                {
                    "lang": "javascript",
                    "value": "// Find the document that describes \"lego\"\nconst query = { \"name\": \"lego\" };\n// Set some fields in that document\nconst update = {\n  \"$set\": {\n    \"name\": \"blocks\",\n    \"price\": 20.99,\n    \"category\": \"toys\"\n  }\n};\n// Return the updated document instead of the original document\nconst options = { returnNewDocument: true };\n\nreturn itemsCollection.findOneAndUpdate(query, update, options)\n  .then(updatedDocument => {\n    if(updatedDocument) {\n      console.log(`Successfully updated document: ${updatedDocument}.`)\n    } else {\n      console.log(\"No document matches the provided query.\")\n    }\n    return updatedDocument\n  })\n  .catch(err => console.error(`Failed to find and update document: ${err}`))"
                },
                {
                    "lang": "typescript",
                    "value": "findOneAndUpdate(\n   query: object,\n   update: object,\n   options?: object\n): Promise<object | null>"
                },
                {
                    "lang": "javascript",
                    "value": "{ age: -1, name: 1 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Includes the field in returned documents\n{ <Field Name>: 1 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Withholds the field from returned documents\n{ <Field Name>: 0 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Invalid\n// Can't simultaneously include and withhold\n{ \"name\": 1, \"address\": 0 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Valid\n// Can exclude _id while including other fields\n{ \"_id\": 0, \"name\": 1 }"
                },
                {
                    "lang": "typescript",
                    "value": "Promise<object | null>"
                },
                {
                    "lang": "javascript",
                    "value": "// Find the document that describes \"lego\"\nconst query = { \"name\": \"lego\" };\n// Replace it with a new document\nconst replacement = {\n    \"name\": \"blocks\",\n    \"price\": 20.99,\n    \"category\": \"toys\"\n};\n// Return the original document as it was before being replaced\nconst options = { \"returnNewDocument\": false };\n\nreturn itemsCollection.findOneAndReplace(query, replacement, options)\n  .then(replacedDocument => {\n    if(replacedDocument) {\n      console.log(`Successfully replaced the following document: ${replacedDocument}.`)\n    } else {\n      console.log(\"No document matches the provided query.\")\n    }\n    return updatedDocument\n  })\n  .catch(err => console.error(`Failed to find and replace document: ${err}`))"
                },
                {
                    "lang": "typescript",
                    "value": "findOneAndReplace(\n   query: object,\n   replacement: object,\n   options?: object\n): Promise<object | null>"
                },
                {
                    "lang": "javascript",
                    "value": "{ age: -1, name: 1 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Includes the field in returned documents\n{ <Field Name>: 1 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Withholds the field from returned documents\n{ <Field Name>: 0 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Invalid\n// Can't simultaneously include and withhold\n{ \"name\": 1, \"address\": 0 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Valid\n// Can exclude _id while including other fields\n{ \"_id\": 0, \"name\": 1 }"
                },
                {
                    "lang": "typescript",
                    "value": "Promise<object | null>"
                },
                {
                    "lang": "javascript",
                    "value": "// Find the first document that has a quantity greater than 25\nconst query = { \"quantity\": { \"$gte\": 25 } };\n// Sort the documents in order of descending quantity before\n// deleting the first one.\nconst options = {\n  \"sort\": { \"quantity\": -1 }\n}\n\nreturn itemsCollection.findOneAndDelete(query, options)\n  .then(deletedDocument => {\n    if(deletedDocument) {\n      console.log(`Successfully deleted document that had the form: ${deletedDocument}.`)\n    } else {\n      console.log(\"No document matches the provided query.\")\n    }\n    return deletedDocument\n  })\n  .catch(err => console.error(`Failed to find and delete document: ${err}`))"
                },
                {
                    "lang": "typescript",
                    "value": "findOneAndDelete(\n   query: object,\n   options?: object\n): Promise<object | null>"
                },
                {
                    "lang": "javascript",
                    "value": "{ age: -1, name: 1 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Includes the field in returned documents\n{ <Field Name>: 1 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Withholds the field from returned documents\n{ <Field Name>: 0 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Invalid\n// Can't simultaneously include and withhold\n{ \"name\": 1, \"address\": 0 }"
                },
                {
                    "lang": "javascript",
                    "value": "// Valid\n// Can exclude _id while including other fields\n{ \"_id\": 0, \"name\": 1 }"
                },
                {
                    "lang": "typescript",
                    "value": "Promise<object | null>"
                },
                {
                    "lang": "javascript",
                    "value": "const newItem = {\n  \"name\": \"Plastic Bricks\",\n  \"quantity\": 10,\n  \"category\": \"toys\",\n  \"reviews\": [{ \"username\": \"legolover\", \"comment\": \"These are awesome!\" }]\n};\n\nitemsCollection.insertOne(newItem)\n  .then(result => console.log(`Successfully inserted item with _id: ${result.insertedId}`))\n  .catch(err => console.error(`Failed to insert item: ${err}`))"
                },
                {
                    "lang": "typescript",
                    "value": "insertOne(document: object): Promise<object>"
                },
                {
                    "lang": "typescript",
                    "value": "Promise<object>"
                },
                {
                    "lang": "javascript",
                    "value": "const doc1 = { \"name\": \"basketball\", \"category\": \"sports\", \"quantity\": 20, \"reviews\": [] };\nconst doc2 = { \"name\": \"football\",   \"category\": \"sports\", \"quantity\": 30, \"reviews\": [] };\n\nreturn itemsCollection.insertMany([doc1, doc2])\n  .then(result => {\n    console.log(`Successfully inserted ${result.insertedIds.length} items!`);\n    return result\n  })\n  .catch(err => console.error(`Failed to insert documents: ${err}`))"
                },
                {
                    "lang": "typescript",
                    "value": "insertMany(\n  document: object,\n  options?:  { ordered?: boolean },\n): Promise<object>"
                },
                {
                    "lang": "typescript",
                    "value": "Promise<object>"
                },
                {
                    "lang": "javascript",
                    "value": "const query = { \"name\": \"football\" };\nconst update = {\n  \"$push\": {\n    \"reviews\": {\n      \"username\": \"tombradyfan\",\n      \"comment\": \"I love football!!!\"\n    }\n  }\n};\nconst options = { \"upsert\": false };\n\nitemsCollection.updateOne(query, update, options)\n  .then(result => {\n    const { matchedCount, modifiedCount } = result;\n    if(matchedCount && modifiedCount) {\n      console.log(`Successfully added a new review.`)\n    }\n  })\n  .catch(err => console.error(`Failed to add review: ${err}`))"
                },
                {
                    "lang": "typescript",
                    "value": "updateOne(\n   query: object,\n   update: object,\n   options?: object\n): Promise<object>"
                },
                {
                    "lang": "typescript",
                    "value": "Promise<object>"
                },
                {
                    "lang": "javascript",
                    "value": "const query = {};\nconst update = { \"$mul\": { \"quantity\": 10 } };\nconst options = { \"upsert\": false }\n\nreturn itemsCollection.updateMany(query, update, options)\n  .then(result => {\n    const { matchedCount, modifiedCount } = result;\n    console.log(`Successfully matched ${matchedCount} and modified ${modifiedCount} items.`)\n    return result\n  })\n  .catch(err => console.error(`Failed to update items: ${err}`))"
                },
                {
                    "lang": "typescript",
                    "value": "updateMany(\n   query: object,\n   update: object,\n   options?: object\n): Promise<object>"
                },
                {
                    "lang": "typescript",
                    "value": "Promise<object>"
                },
                {
                    "lang": "javascript",
                    "value": "const query = { \"name\": \"lego\" };\n\nitemsCollection.deleteOne(query)\n  .then(result => console.log(`Deleted ${result.deletedCount} item.`))\n  .catch(err => console.error(`Delete failed with error: ${err}`))"
                },
                {
                    "lang": "typescript",
                    "value": "deleteOne(\n   query: object,\n   options?: object\n): Promise<object>"
                },
                {
                    "lang": "typescript",
                    "value": "Promise<object>"
                },
                {
                    "lang": "javascript",
                    "value": "const query = { \"reviews\": { \"$size\": 0 } };\n\nitemsCollection.deleteMany(query)\n  .then(result => console.log(`Deleted ${result.deletedCount} item(s).`))\n  .catch(err => console.error(`Delete failed with error: ${err}`))"
                },
                {
                    "lang": "typescript",
                    "value": "deleteMany(\n   query: object,\n   options?: object\n): Promise<object>"
                },
                {
                    "lang": "typescript",
                    "value": "Promise<object>"
                },
                {
                    "lang": "javascript",
                    "value": "const pipeline = [\n  { \"$group\": {\n      \"_id\": \"$customerId\",\n      \"numPurchases\": { \"$sum\": 1 },\n      \"numItemsPurchased\": { \"$sum\": { \"$size\": \"$items\" } }\n  } },\n  { \"$addFields\": {\n      \"averageNumItemsPurchased\": {\n        \"$divide\": [\"$numItemsPurchased\", \"$numPurchases\"]\n      }\n  } }\n]\n\nreturn purchasesCollection.aggregate(pipeline).toArray()\n  .then(customers => {\n    console.log(`Successfully grouped purchases for ${customers.length} customers.`)\n    for(const customer of customers) {\n      console.log(`customer: ${customer._id}`)\n      console.log(`num purchases: ${customer.numPurchases}`)\n      console.log(`total items purchased: ${customer.numItemsPurchased}`)\n      console.log(`average items per purchase: ${customer.averageNumItemsPurchased}`)\n    }\n    return customers\n  })\n  .catch(err => console.error(`Failed to group purchases by customer: ${err}`))"
                },
                {
                    "lang": "typescript",
                    "value": "aggregate(\n   pipeline: object[],\n   options?: object\n): Cursor"
                },
                {
                    "lang": "javascript",
                    "value": "collection.aggregate(pipeline).next()\n  .then(doc => console.log(\"next document\", doc))"
                },
                {
                    "lang": "javascript",
                    "value": "collection.aggregate(pipeline).toArray()\n  .then(docs => console.log(\"all documents\", docs))"
                },
                {
                    "lang": "javascript",
                    "value": "return itemsCollection.count({ \"reviews.0\": { \"$exists\": true } })\n  .then(numDocs => console.log(`${numDocs} items have a review.`))\n  .catch(err => console.error(\"Failed to count documents: \", err))"
                },
                {
                    "lang": "typescript",
                    "value": "count(\n   query?: object,\n   options?: object\n): Promise<number>"
                },
                {
                    "lang": "javascript",
                    "value": "Promise<number>"
                },
                {
                    "lang": "javascript",
                    "value": "const taskCollection = context.services.get(\"mongodb-atlas\")\n  .db(\"tracker\").collection(\"tasks\");\n\nreturn taskCollection.distinct(\"status\", {})\n  .then(results => {\n      console.log(JSON.stringify(results));\n      console.log(results.length);\n  })\n  .catch(err => console.error(err))"
                },
                {
                    "lang": "typescript",
                    "value": "distinct(\n   field: string,\n   query: object,\n   options?: object\n): Promise<any[]>"
                },
                {
                    "lang": "typescript",
                    "value": "Promise<any[]>"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function(arg){\n    const doc1 = { \"name\": \"velvet elvis\", \"quantity\": 20, \"reviews\": [] };\n    const doc2 = { \"name\": \"mock turtleneck\",  \"quantity\": 30, \"reviews\": [] };\n\n    var collection = context.services.get(\"mongodb-atlas\")\n        .db(\"store\")\n        .collection(\"purchases\");\n\n    return await collection.bulkWrite(\n        [{ insertOne: doc1}, { insertOne: doc2}],\n        {ordered:true});\n};"
                },
                {
                    "lang": "typescript",
                    "value": "bulkWrite(\n  operations: object[],\n  options?: object\n): Promise<null>"
                },
                {
                    "lang": "javascript",
                    "value": "{ insertOne: { document: { a: 1 } } }\n\n{ updateOne: { filter: {a:2}, update: {$set: {a:2}}, upsert:true } }\n\n{ updateMany: { filter: {a:2}, update: {$set: {a:2}}, upsert:true } }\n\n{ deleteOne: { filter: {c:1} } }\n\n{ deleteMany: { filter: {c:1} } }\n\n{ replaceOne: { filter: {c:3}, replacement: {c:4}, upsert:true}}"
                },
                {
                    "lang": "typescript",
                    "value": "Promise<null>"
                }
            ],
            "preview": "Learn how to use Functions to query a MongoDB Atlas data source.",
            "tags": "code example",
            "facets": {
                "programming_language": [
                    "javascript/typescript"
                ],
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "tutorial/dotnet",
            "title": "MAUI Tutorial",
            "headings": [
                "Overview",
                "Prerequisites",
                "Start with the Template",
                "Set up the Template App",
                "Open the App",
                "Explore the App Structure",
                "Run the App",
                "Check the Backend",
                "Modify the Application",
                "Add a New Property",
                "Add the Priority Property",
                "Set the Priority when Creating or Modifying an Item",
                "Update the UI Elements",
                "Run and Test",
                "Change the Subscription",
                "Update the subscription",
                "Run and Test",
                "Conclusion",
                "What's Next?"
            ],
            "paragraphs": "Realm provides a .NET SDK for creating multi-platform applications in C# with\nMAUI. This tutorial is based on the\n.NET Flexible Sync Template App, named  maui.todo.flex , which illustrates\nthe creation of a Todo application in MAUI. This application enables users to: In this tutorial, you will add a new  Priority  field\nto the existing  Item  model and update the\n Flexible Sync subscription  to only show items within\na range of priorities. Depending on your experience with MAUI, this tutorial should take\naround 30 minutes. Register their email as a new user account. Sign in to their account with their email and password (and sign out). View, create, modify, and delete tasks. If you prefer to get started with your own application rather than follow a\nguided tutorial, check out the  .NET Quick Start .\nIt includes copyable code examples and the essential information that you\nneed to set up an Atlas App Services backend. Ensure that you have the necessary software installed. Select the tab for\nyour development environment: 2019 or newer. Xcode  10.0 or newer. Note that\nXcode 10 requires macOS High Sierra (10.13) or newer. iOS 12 SDK . Android 6.0 / API level 23 . Windows 7 or newer. Windows 10 recommended. Visual Studio  2017\n(Visual Studio 2019 is recommended). Android 6.0 / API level 23 . To build iOS projects on Windows, you will also need a Mac computer,\nnetwork-accessible from the Windows computer, that\nconforms to the minimum requirements for running Xamarin on macOS. You need previous experience deploying a MAUI or Xamarin app to an Android\nEmulator, iOS Simulator, and/or a physical device. This tutorial starts with a Template App. You need an  Atlas Account , an API key, and\nappservices to create a Template App. You can learn more about creating an Atlas account in the\n Atlas Getting Started  documentation. For this\ntutorial, you need an Atlas account with a free-tier cluster. You also need an Atlas  API key  for the MongoDB\nCloud account you wish to log in with. You must be a  Project\nOwner  to create a Template\nApp using appservices. To learn more about installing appservices, see  Install App\nServices CLI . After installing, run the\n login  command using the API key for\nyour Atlas project. This tutorial is based on the MAUI Flexible Sync Template App named\n maui.todo.flex . We start with the default app and build new features\non it. To learn more about the Template Apps, see  Template Apps . If you don't already have an Atlas account,  sign-up  to deploy a Template App. Follow the procedure described in the\n Create an App Services App  guide, and select\n Create App from Template . Select the\n Real-time Sync  template. This creates an App Services App\npre-configured to use with one of the Device Sync template app clients. After you create a template app, the UI displays a modal labeled\n Get the Front-end Code for your Template . This modal\nprovides instructions for downloading the template app client code\nas a  .zip  file or using App Services CLI to get the client. After selecting the  .zip  or App Services CLI method, follow the on-screen\ninstructions to get the client code. For this tutorial, select the\n C# (.NET MAUI)  client code. The  appservices apps create \ncommand sets up the backend and creates a C# (MAUI) template\napp for you to use as a base for this tutorial. Run the following command in a terminal window to create an app\nnamed \"MyTutorialApp\" that is deployed in the  US-VA  region\nwith its environment set to \"development\" (instead of production\nor QA). The command creates a new directory in your current path with the\nsame name as the value of the  --name  flag. You can fork and clone a GitHub repository that contains the Device\nSync client code. The C# client code is available at\n https://github.com/mongodb/template-app-maui-todo . If you use this process to get the client code, you must create an\nApp Services app to use with the client. Follow the instructions at\n Create a Template App  to create a template app based on\nthe  sync.todo  template. Navigate to the directory where the Realm CLI created the template app\nand open the  realm-todo-app.sln  solution in Visual Studio. In Visual Studio, take a few minutes to explore how the solution is\norganized. This is a organized like a standard MAUI\n MVVM \nsolution, with a single project containing the views, models, and view\nmodels. The app uses a single model,  Item , which implements  IRealmObject .\nWe have three views, one for logging in ( LoginPage ), another for\nviewing Items ( ItemsPage ), and a third for editing and creating new\nitems. Each view has a corresponding view model. In addition to the standard MVVM structure, we have centralized all of the\nRealm logic into a RealmService class, which is found in the \"Services\" folder.\nThis architecture ensures we are sharing the same realm throughout. Without making any changes to the code, you should be able to run the app\nin either the Android emulator or the iOS Simulator, or on a physical device.\nYou don't need to make any changes because, when you set up the template in\nthe App Services UI or with the CLI, Atlas App Services also set up a new\nbackend. If you downloaded the template app, you will need to add your App\nServices app's ID. To do so, open the Services/RealmService.cs file and add\nyour ID to the  private const string appId = \"appId\";  line. Run the app, register a new user account, and then add a new Item to your\ntodo list. Log in to  Atlas App Services . In the\n Data Services  tab, click on  Browse Collections . In the list\nof databases, find and expand the  todo  database, and then the\n Item  collection. You should see the document you created\nin this collection. Now that you have confirmed everything is working as expected, we can\nadd changes. In this tutorial, we have decided that we want to add a\n\"Priority\" property to each Item so that we can filter Items by their\npriorities. The Priority property will be mapped to a PriorityLevel enum\nto constrain the possible values. To do this, follow these steps: In the  RealmTodo  project, expand the  Models  folder and open\nthe  Item  class file. Add the following public property: Note that we have set this property as nullable, which will ensure that\nexisting Items in our database (which do not have a Priority property)\nwill continue to be available. The  EditItemViewModel  ViewModel is used to both create new items and\nto modify existing items. When creating or modifying an item, the user needs\nthe UI (and code) to set the Priority of an item. Add an ObservableProperty to hold the priority: The  [ObservableProperty]  attribute is a feature provided by the\n MVVM Toolkit \nto simplify data binding. The  ApplyQueryAttributes  method acts as a bit of a \"constructor\" for\nthis view model, checking to see if an existing item is being passed to\nthis view for editing. In here, we capture any existing values to\ndisplay in the view. If we are editing an existing item, we want to set the Priority of the\nexisting item:\n Priority = InitialItem.Priority; . Likewise, if we're creating a new item, set the default priority to \"Medium\":\n Priority = 2; . When complete this method should now look like the following: Finally, in the  SaveItem()  method, we want to persist the Priority\nvalue. Since we are creating or modifying a managed object, the changes\nare wrapped in a  realm.WriteAsync  call. For the existing item, set the  InitialItem.Priority  on the existing\nobject, and for a new item, set the property in the  Add()  call. Your\ncompleted  WriteAsync  block should look like this: The final task is to add the UI elements needed to set and show the\npriority. First, in the  ItemsPage.xaml , we'll add a label to the ListView that\nshows the priority. Within the ViewCell, add a Label to display the\nitem's priority: In the  EditItemsPage.xaml , we will add two UI elements: a\nPicker that enables the user to choose which priority level to set on the\nnew Item and a label for the picker. Find the  Entry  element\nfor setting the Summary and add the following elements below it: At this point, you can run the application again. Log in using the account\nyou created earlier in this tutorial. You will see the one Item you\npreviously created. Add a new Item, and you will see that you can now\nset the priority. Choose  High  for the priority and save the Item. Now switch back to the Atlas data page in your browser, and refresh the\n Item  collection. You should now see the new Item with the  priority \nfield added and set to  1 . You will also notice that the existing Item\nnow also has a  priority  field, and it is set to  null , as shown in\nthe following screenshot: Adding a property to a Realm object is not a breaking change and therefore\ndoes not require a  client reset . The template\napp has Development Mode enabled, so changes to the client Realm object\nare reflected in the server-side schema. For more information, see\n Development Mode  and  Update Your Data Model . Change the Subscription In the  RealmService.cs  file, we define two Flexible Sync subscriptions.\nOne shows only the items created by the current user, while the other shows\nall items from all users. We're going to add a new subscription that shows the current user's\nitems that have a priority of 0 or 1. At the bottom of the  RealmService  class, add an entry to the\n SubscriptionType  enum called \"MyHighPriority\": Scroll up to find the  GetQueryForSubscriptionType  method. It is here\nthat we define the subscriptions. 4. Change the name of the new query to \"myHighPri\". Your code will look\nlike the following: Copy the first one, where  subType == SubscriptionType.Mine ,\nand paste it in an  else if  block immediately below. Set the new conditional to  subType == SubscriptionType.MyHighPriority . Modify this new subscription query to insert a LINQ query that still filters\nby OwnerId and also by Priority values less than 2. In the  GetCurrentSubscriptionType  method immediately above, add the\nnew subscription name to the switch statement, so it looks like this: Finally, open the  ItemsViewModel  class and find the  OnIsShowAllTasksChanged \nmethod. Rather than change the UI to enable 3 subscriptions, we'll just\nreplace the existing \"mine\" subscription with the new one. Change the\n SetSubscription  method so it looks like the following: Run the application again. If prompted to do so, log in in using the\naccount you created earlier in this tutorial. You should expect to see any Items you have created that have a priority\nof \"High\" (1) or \"Severe\" (0). If you toggle the \"Show all tasks\" switch,\nall tasks by all users should appear. Adding a property to an existing Realm object is a non-breaking change, and\nDevelopment Mode ensures that the schema change is reflected server-side. Read our  .NET SDK  documentation. Find developer-oriented blog posts and integration tutorials on the\n MongoDB Developer Hub . Join the  MongoDB Community forum \nto learn from other MongoDB developers and technical experts. Explore engineering and expert-provided  example projects . How did it go? Use the Share Feedback tab at the bottom right of the page to let us know\nif this tutorial was helpful or if you had any issues.",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices app create \\\n  --name MyTutorialApp \\\n  --template maui.todo.flex \\\n  --deployment-model global \\\n  --environment development"
                },
                {
                    "lang": "csharp",
                    "value": "[MapTo(\"priority\")]\npublic int? Priority { get; set; }"
                },
                {
                    "lang": "csharp",
                    "value": "[ObservableProperty]\nprivate int? priority;"
                },
                {
                    "lang": "csharp",
                    "value": "public void ApplyQueryAttributes(IDictionary<string, object> query)\n{\n      if (query.Count > 0 && query[\"item\"] != null) // we're editing an Item\n      {\n         InitialItem = query[\"item\"] as Item;\n         Summary = InitialItem.Summary;\n         Priority = InitialItem.Priority;\n         PageHeader = $\"Modify Item {InitialItem.Id}\";\n      }\n      else // we're creating a new item\n      {\n         Summary = \"\";\n         Priority = 2;\n         PageHeader = \"Create a New Item\";\n      }\n}"
                },
                {
                    "lang": "csharp",
                    "value": "await realm.WriteAsync(() =>\n{\n   if (InitialItem != null) // editing an item\n   {\n      InitialItem.Summary = Summary;\n      InitialItem.Priority = Priority;\n   }\n   else // creating a new item\n   {\n      realm.Add(new Item()\n      {\n            OwnerId = RealmService.CurrentUser.Id,\n            Summary = summary,\n            Priority = Priority\n      });\n   }\n});"
                },
                {
                    "lang": "csharp",
                    "value": "<Label Text=\"{Binding Priority}\"\n   HorizontalOptions=\"Center\"\n   VerticalOptions=\"Center\"/>"
                },
                {
                    "lang": "csharp",
                    "value": "<Label Text=\"Priority:\"/>\n<Picker x:Name=\"newItemPriority\" SelectedIndex=\"{Binding Priority}\">\n   <Picker.Items>\n         <x:String>Severe</x:String>\n         <x:String>High</x:String>\n         <x:String>Medium</x:String>\n         <x:String>Low</x:String>\n   </Picker.Items>\n</Picker>"
                },
                {
                    "lang": "csharp",
                    "value": "public enum SubscriptionType\n{\n   Mine,\n   MyHighPriority,\n   All,\n}"
                },
                {
                    "lang": "csharp",
                    "value": "if (subType == SubscriptionType.Mine)\n{\n      query = realm.All<Item>()\n         .Where(i => i.OwnerId == CurrentUser.Id)\n         .Where(i => i.Priority < 2);\n      queryName = \"mine\";\n}\nelse if (subType == SubscriptionType.MyHighPriority)\n{\n      query = realm.All<Item>()\n        .Where(i => i.OwnerId == CurrentUser.Id &&\n               i.Priority < 2);\n      queryName = \"myHighPri\";\n}\nelse if (subType == SubscriptionType.All)\n{\n      query = realm.All<Item>();\n      queryName = \"all\";\n}"
                },
                {
                    "lang": "csharp",
                    "value": "return activeSubscription.Name switch\n{\n    \"all\" => SubscriptionType.All,\n    \"mine\" => SubscriptionType.Mine,\n    \"myHighPri\" => SubscriptionType.MyHighPriority,\n    _ => throw new InvalidOperationException(\"Unknown subscription type\")\n};"
                },
                {
                    "lang": "csharp",
                    "value": "await RealmService.SetSubscription(realm, value\n   ? SubscriptionType.All\n   : SubscriptionType.MyHighPriority);"
                }
            ],
            "preview": "Realm provides a .NET SDK for creating multi-platform applications in C# with\nMAUI. This tutorial is based on the\n.NET Flexible Sync Template App, named maui.todo.flex, which illustrates\nthe creation of a Todo application in MAUI. This application enables users to:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "authentication/api-key",
            "title": "API Key Authentication",
            "headings": [
                "Overview",
                "Enable API Key Authentication",
                "API Key User Objects",
                "Server API Keys",
                "Create a Server API Key",
                "User API Keys",
                "Realm SDK Examples",
                "Log In With an API Key",
                "Create a User API Key"
            ],
            "paragraphs": "The API Key  authentication provider \nallows users to log in using generated keys. App Services supports the following two types of API keys: API keys do not expire automatically. Server API keys : API keys associated with server users that is created\nfrom the App Services CLI, API or UI. User API keys : API keys created from the Realm SDKs associated\nwith application users. To work with API key users, you must first enable the API key authentication provider.\nThe API Key authentication provider does not have any configuration options. You can enable and configure the API Key authentication\nprovider from the App Services UI by selecting  API Keys \nfrom the  Authentication  page. To enable and configure the API Key authentication provider with\n :ref:`App Services CLI <appservices-cli>` , define a  configuration\nobject  for it in  /auth/providers.json . API Key provider configurations have the following form: The  name  of an authentication provider is always the same as its  type . Every App Services user has a unique metadata object. The object is passed to Functions\ncalled by the user and rule expressions for requests made by the user.\nIn API key user objects, the  type  field has the value  \"server\" . For example: You can use this field to evaluate if requests come from API keys. For more information, refer to  User Objects . Server API keys are generated in a server-side context using one of\nthe App Services CLI, API or UI. When you create a server API key,\nyou also create an associated server user. You can provide a server key to external applications and services\nto allow them to authenticate directly with App Services. You can associate up to 100 server API keys with an app. You must enable the API key provider before you can create an API key. You must copy the server key's value as soon as you create it.\nOnce you create the API key, you can no longer retrieve it. Select  App Users  from the left navigation menu. Select the  Authentication Providers  button and then select\nthe  API Keys  provider. If the API Key provider is not currently enabled, you must enable it\nand then deploy your changes before you can create a key. Click  Create API Key . Enter a unique name for the key and then click  Save . To create a new server API key, call  appservices users create  and\nspecify  --type=api-key . The CLI will prompt you for your App ID as\nwell as a name for the new API key. You can also specify the arguments when you call the program: To create a server API key using the Admin API, make a request to the\n Create a new API key  endpoint. You can generate user API keys with the Realm SDKs. Each user API Key is associated\nwith a single non-anonymous user. Each user can associate up to 20 user API keys\nwith their account. Once the key is associated with a user account, the user\ncan use the key to authenticate. The following diagram shows how to create,\nand then use, a User API Key: To learn how to generate user API keys, refer to the Realm SDK documentation. For code examples that demonstrate how to register and log in using\nAPI Key authentication, see the documentation for the Realm SDKs. The Realm SDK can log in with an existing server or user API key. C++ SDK Flutter SDK Java SDK Kotlin SDK .NET SDK Node SDK React Native SDK Swift SDK Web SDK The Realm SDK can create a new user API key for an existing user account. Flutter SDK Java SDK Kotlin SDK .NET SDK Node SDK React Native SDK Swift SDK Web SDK Not yet available for the C++ SDK",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"api-key\": {\n    \"name\": \"api-key\",\n    \"type\": \"api-key\",\n    \"disabled\": false\n  }\n}"
                },
                {
                    "lang": "js",
                    "value": "{\n  id: \"<Unique User ID>\",\n  type: \"server\",\n  data: <user data object>,\n  custom_data: <custom user data object>,\n  identities: <array of user identities>,\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices users create --type=api-key"
                },
                {
                    "lang": "bash",
                    "value": "appservices users create --type=api-key \\\n  --app=<Your App ID> \\\n  --name=<API Key Name>"
                }
            ],
            "preview": "The API Key authentication provider\nallows users to log in using generated keys.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "tutorial/kotlin",
            "title": "Android With Kotlin Tutorial",
            "headings": [
                "Overview",
                "Learning Objectives",
                "Prerequisites",
                "Start with the Template",
                "Set up the Template App",
                "Open the App",
                "Explore the App Structure",
                "Run the App",
                "Check the Backend",
                "Modify the Application",
                "Add a New Property",
                "Add a New Property to the Model",
                "Set the Priority when Creating a New Item",
                "Run and Test",
                "Change the Subscription",
                "Update the subscription",
                "Run and Test",
                "Conclusion",
                "What's Next?"
            ],
            "paragraphs": "Realm provides a Kotlin SDK that allows you to create an Android mobile\napplication with Kotlin using  Jetpack Compose .\nThis tutorial is based on the Kotlin Flexible Sync\nTemplate App, named  kotlin.todo.flex , which illustrates the creation of a\nTo-do Item List management application. This application enables users to: The template app also provides a toggle that simulates the device being in\n\"Offline Mode.\" This toggle lets you quickly test Device Sync functionality\non the simulator, emulating the user having no internet connection. However,\nyou would likely remove this toggle in a production application. This tutorial adds functionality to the Template App. You will add a new  Priority  field\nto the existing  Item  model and update the\n Flexible Sync subscription  to only show items within\na range of priorities. This example illustrates how you might adapt the\ntemplate app for your own needs. Depending on your experience with Kotlin, this tutorial should take\naround 30 minutes. Register their email as a new user account. Sign in to their account with their email and password (and sign out later). View, create, modify, and delete their own tasks. View all tasks, even where the user is not the owner. This tutorial illustrates how you might adapt the\ntemplate app for your own needs. In this tutorial, you will learn how to: Update a Realm object model with a non-breaking change. Update a Device Sync subscription. Add a queryable field to the Device Sync configuration on the server\nto change which data is synchronized. If you prefer to get started with your own application rather than follow a\nguided tutorial, check out the  Kotlin Quick Start .\nIt includes copyable code examples and the essential information that you\nneed to set up an Atlas App Services backend. Android Studio  Bumblebee 2021.1.1 or higher. JDK 11 or higher. Kotlin Plugin for Android Studio, version 1.6.10 or higher. An Android Virtual Device (AVD) using a supported CPU architecture. This tutorial starts with a Template App. You need an  Atlas Account , an API key, and\nappservices to create a Template App. You can learn more about creating an Atlas account in the\n Atlas Getting Started  documentation. For this\ntutorial, you need an Atlas account with a free-tier cluster. You also need an Atlas  API key  for the MongoDB\nCloud account you wish to log in with. You must be a  Project\nOwner  to create a Template\nApp using appservices. To learn more about installing appservices, see  Install App\nServices CLI . After installing, run the\n login  command using the API key for\nyour Atlas project. This tutorial is based on the Kotlin SDK Flexible Sync Template App named\n kotlin.todo.flex . We start with the default app and build new features\non it. To learn more about the Template Apps, see  Template Apps . If you don't already have an Atlas account,  sign-up  to deploy a Template App. Follow the procedure described in the  Create an App \nguide, and select  Create App from Template . Select the\n Real-time Sync  template. This creates an App Services App\npre-configured to use with one of the Device Sync template app clients. After you create a template app, the UI displays a modal labeled\n Get the Front-end Code for your Template . This modal\nprovides instructions for downloading the template app client code\nas a  .zip  file or using App Services CLI to get the client. After selecting the  .zip  or App Services CLI method, follow the on-screen\ninstructions to get the client code. For this tutorial, select the\n Kotlin (Android)  client code. The  appservices apps create \ncommand sets up the backend and creates a Kotlin template\napp for you to use as a base for this tutorial. Run the following command in a terminal window to create an app\nnamed \"MyTutorialApp\" that is deployed in the  US-VA  region\nwith its environment set to \"development\" (instead of production\nor QA). The command creates a new directory in your current path with the\nsame name as the value of the  --name  flag. You can fork and clone a GitHub repository that contains the Device\nSync client code. The Kotlin client code is available at\n https://github.com/mongodb/template-app-kotlin-todo . If you use this process to get the client code, you must create a\ntemplate app to use with the client. Follow the instructions at\n Create a Template App  to use the Atlas App Services UI, App Services CLI,\nor Admin API to create a Device Sync template app. In Android Studio, open the  kotlin.todo.flex  folder located in the\n frontend  folder of the template app. If you downloaded the client as a  .zip  file or cloned the client\nGitHub repository, you must manually insert the App Services App ID\nin the appropriate place in your client. Follow the\n Configuration  instructions in the client  README.md \nto learn where to insert your App ID. Take a few minutes to explore the project organization while Android\nStudio indexes your project. Within the  app/java/com.mongodb.app \ndirectory, you can see a few files worth noting: In this tutorial, you'll be working in the following files: File Purpose ComposeItemActivity.kt Activity class that defines the layout and provides\nfunctionality for opening a realm, writing Items to the realm,\nlogging a user out, and closing a realm. ComposeLoginActivity.kt Activity class that defines the layout and provides\nfunctionality for registering a user and logging a user in. TemplateApp.kt Class that initializes the App Services App. File Purpose Item.kt Located in the  domain  directory. Defines the Realm object we store in the database. AddItem.kt Located in the  ui/tasks  directory. Contains the composable function that\ndefines the layout used when adding an item. AddItemViewModel.kt Located in the  presentation/tasks  directory. The view model that contains\nbusiness logic and manages state when adding an item. SyncRepository.kt Located in the  data  directory. Repository used to access Realm Sync and\ndefines the Flexible Sync subscription. Strings.xml Located in the  res/values  directory. Defines the  text string resources \nused in the app. Without making any changes to the code, you should be able to\n run the app \non an Android Emulator using Android Studio or on a physical device. Run the app, register a new user account, and then add a new Item to your\ntodo list. Log in to  Atlas App Services . In the\n Data Services  tab, click on  Browse Collections . In the list\nof databases, find and expand the  todo  database, and then the\n Item  collection. You should see the document you created\nin this collection. Now that you have confirmed everything is working as expected, we can add\nchanges. In this tutorial, we have decided that we want to add a\n\"priority\" property to each Item so that we can filter Items by their\npriority level. The priority property will be mapped to a  PriorityLevel  enum\nto constrain the possible values, and we will use the ordinal of each\nenum to correspond to the priority integer so we can query based on a\nnumeric priority level later. To do this, follow these steps: Within the  app/java/com.mongodb.app/domain  folder, open the  Item  class file. Add a  PriorityLevel  enum to constrain the possible\nvalues. Also add a  priority \nproperty to the  Item  class, which sets the default priority to 3,\nindicating that it is a low-priority todo item: From the  ui/tasks  folder, open the  AddItem.kt  file. This file defines\nthe composable functions for the UI that is displayed when a user clicks\nthe '+' button to add a new todo item. First, add the following imports below the  package com.mongodb.app : Now we can add a dropdown field to the  AddItemPrompt  composable function\nthat will enable the user to pick a priority level\nfrom a list using the PriorityLevel enums as available values: Android Studio will identify several errors. We'll correct these in the next steps\nby adding the related functions. Next, we'll define the dropdown field label as a string resource.\nOpen the  res/values/strings.xml  file, and add the following before\nthe closing of the 'resource' element: Now within the  presentation/tasks  folder, open the  AddItemViewModel.kt \nfile. Here we will add the business logic related to our new dropdown field. Add the  PriorityLevel  import below the  package com.mongodb.app , then\nadd the variables and functions to the  AddItemViewModel  class needed\nto handle the state changes within the dropdown: Now update the  addTask()  and  cleanUpAndClose()  functions to include the new\n taskPriority  parameter, update the message with the priority information, and\nreset the priority field to low once the Add Item view is closed: Finally, from the  data  folder, open the  SyncRepository.kt  file to\nreflect the same changes in the  addTask()  function, which writes the\nItem to the realm. First, add the  PriorityLevel  import below the  package com.mongodb.app ,\nthen update the  addTask()  functions to pass the  taskPriority \nas a parameter and write the  priority  field to the realm\nas an integer (using the enum ordinal): At this point, you can rerun the application. Log in using the account\nyou created earlier in this tutorial. You will see the one Item you\npreviously created. Add a new Item, and you will see that you can now\nset the priority. Choose  High  for the priority and save the Item. Now switch back to the Atlas data page in your browser, and refresh the\n Item  collection. You should now see the new Item with the  priority \nfield added and set to  1 . The existing Item does not have a  priority \nfield. Adding a property to a Realm object is not a breaking change and therefore\ndoes not require a  client reset . The template\napp has Development Mode enabled, so changes to the client Realm object\nare reflected in the server-side schema. For more information, see\n Development Mode  and  Update Your Data Model . Change the Subscription Within the  app/java/com.mongodb.app/data  folder, open the  SyncRepository.kt \nfile, where we define the Flexible Sync subscription. The subscription defines\nwhich documents we sync with the user's device and account. Find the\n getQuery()  function. You can see that\nwe are currently subscribing to two subscriptions: We want to update the  MINE  subscription to  only  sync Items that are marked as High or Severe priority. As you may recall, the  priority  field is of type  int , where the highest\npriority (\"Severe\") has a value of 0, and the lowest priority (\"Low\") has\na value of 3. We can make direct comparisons between an integer and the\npriority property. To do so, edit the RQL statement to include documents\nwhere the priority is equal to or  less  than PriorityLevel.High (or 1), as\nshown here: We'll also force the subscription query to recalculate which documents to\nsync every time we open the app. To do this, find the  SyncConfiguration.Builder().initialSubscriptions()  function\nthat our application calls on start. First add the  reRunOnOpen  parameter set to  true ,\nthen set  updateExisting  to  true , which allows the existing query to be updated. MINE : All documents where the  ownerId  property matches the authenticated user. ALL : All documents from all users. Run the application again.\nLog in using the account you created earlier in\nthis tutorial. After an initial moment when Realm re-syncs the\ndocument collection, you will see the new Item of High priority that you created. If you want to test the functionality further, you can create Items of various\npriorities. You'll note that if you try to add an Item with a priority lower than High, you will\nget a Toast message indicating you do not have permission. And if you check your\n logs using Logcat , you will see a message\nindicating the item was \"added successfully\", followed by a sync error: That's because, in this scenario, Realm creates the Item locally, syncs it with the\nbackend, and then reverts the write because it doesn't meet the\nsubscription rules. You'll note, too, that the document you initially created is not synced,\nbecause it has a priority of  null . If you want this Item to be synced,\nyou can edit the document in the Atlas UI and add a value for the priority\nfield. Adding a property to an existing Realm object is a non-breaking change, and\nDevelopment Mode ensures that the schema change is reflected server-side. Read our  Kotlin SDK  documentation. Find developer-oriented blog posts and integration tutorials on the\n MongoDB Developer Hub . Join the  MongoDB Community forum \nto learn from other MongoDB developers and technical experts. Explore engineering and expert-provided  example projects . How did it go? Use the Share Feedback tab at the bottom right of the page to let us know\nif this tutorial was helpful or if you had any issues.",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices app create \\\n  --name MyTutorialApp \\\n  --template kotlin.todo.flex \\\n  --deployment-model global \\\n  --environment development"
                },
                {
                    "lang": "kotlin",
                    "value": "// ... imports\n\n   enum class PriorityLevel() {\n      Severe, // priority 0\n      High,   // priority 1\n      Medium, // priority 2\n      Low     // priority 3\n   }\n\n   class Item() : RealmObject {\n      @PrimaryKey\n      var _id: ObjectId = ObjectId.create()\n      var isComplete: Boolean = false\n      var summary: String = \"\"\n      var owner_id: String = \"\"\n      var priority: Int = PriorityLevel.Low.ordinal\n\n      constructor(ownerId: String = \"\") : this() {\n         owner_id = ownerId\n      }\n\n// ... equals() and hashCode() functions\n\n}"
                },
                {
                    "lang": "kotlin",
                    "value": "import androidx.compose.foundation.layout.fillMaxWidth\nimport androidx.compose.foundation.layout.padding\nimport androidx.compose.material3.DropdownMenuItem\nimport androidx.compose.material3.ExposedDropdownMenuBox\nimport androidx.compose.ui.Modifier\nimport androidx.compose.ui.unit.dp\nimport com.mongodb.app.domain.PriorityLevel"
                },
                {
                    "lang": "kotlin",
                    "value": "// ... imports\n\n@OptIn(ExperimentalMaterial3Api::class)\n@Composable\nfun AddItemPrompt(viewModel: AddItemViewModel) {\n   AlertDialog(\n      containerColor = Color.White,\n      onDismissRequest = {\n            viewModel.closeAddTaskDialog()\n      },\n      title = { Text(stringResource(R.string.add_item)) },\n      text = {\n            Column {\n               Text(stringResource(R.string.enter_item_name))\n               TextField(\n                  colors = ExposedDropdownMenuDefaults.textFieldColors(containerColor = Color.White),\n                  value = viewModel.taskSummary.value,\n                  maxLines = 2,\n                  onValueChange = {\n                        viewModel.updateTaskSummary(it)\n                  },\n                  label = { Text(stringResource(R.string.item_summary)) }\n               )\n               val priorities = PriorityLevel.values()\n\n               ExposedDropdownMenuBox(\n                  modifier = Modifier.padding(16.dp),\n                  expanded = viewModel.expanded.value,\n                  onExpandedChange = { viewModel.open() },\n               ) {\n                  TextField(\n                        readOnly = true,\n                        value = viewModel.taskPriority.value.name,\n                        onValueChange = {},\n                        label = { Text(stringResource(R.string.item_priority)) },\n                        trailingIcon = { ExposedDropdownMenuDefaults.TrailingIcon(expanded = viewModel.expanded.value) },\n                        colors = ExposedDropdownMenuDefaults.textFieldColors(),\n                        modifier = Modifier\n                           .fillMaxWidth()\n                           .menuAnchor()\n                  )\n                  ExposedDropdownMenu(\n                        expanded = viewModel.expanded.value,\n                        onDismissRequest = { viewModel.close() }\n                  ) {\n                        priorities.forEach {\n                           DropdownMenuItem(\n                              text = { Text(it.name) },\n                              onClick = {\n                                    viewModel.updateTaskPriority(it)\n                                    viewModel.close()\n                              }\n                           )\n                        }\n                  }\n               }\n            }\n      },\n      // ... buttons\n   )\n}"
                },
                {
                    "lang": "xml",
                    "value": "<string name=\"item_priority\">Item Priority</string>"
                },
                {
                    "lang": "kotlin",
                    "value": "   // ... imports\n   import com.mongodb.app.domain.PriorityLevel\n\n   // ... events\n\n   class AddItemViewModel(\n      private val repository: SyncRepository\n   ) : ViewModel() {\n\n      private val _addItemPopupVisible: MutableState<Boolean> = mutableStateOf(false)\n      val addItemPopupVisible: State<Boolean>\n         get() = _addItemPopupVisible\n\n      private val _taskSummary: MutableState<String> = mutableStateOf(\"\")\n      val taskSummary: State<String>\n         get() = _taskSummary\n\n      private val _taskPriority: MutableState<PriorityLevel> = mutableStateOf(PriorityLevel.Low)\n      val taskPriority: State<PriorityLevel>\n         get() = _taskPriority\n\n      private val _expanded: MutableState<Boolean> = mutableStateOf(false)\n      val expanded: State<Boolean>\n         get() = _expanded\n\n      private val _addItemEvent: MutableSharedFlow<AddItemEvent> = MutableSharedFlow()\n      val addItemEvent: Flow<AddItemEvent>\n         get() = _addItemEvent\n\n      fun openAddTaskDialog() {\n         _addItemPopupVisible.value = true\n      }\n\n      fun closeAddTaskDialog() {\n         cleanUpAndClose()\n      }\n\n      fun updateTaskSummary(taskSummary: String) {\n         _taskSummary.value = taskSummary\n      }\n\n      fun updateTaskPriority(taskPriority: PriorityLevel) {\n         _taskPriority.value = taskPriority\n      }\n\n      fun open() {\n         _expanded.value = true\n      }\n\n      fun close() {\n         _expanded.value = false\n      }\n\n      // addTask() and cleanUpAndClose() functions\n   }"
                },
                {
                    "lang": "kotlin",
                    "value": "fun addTask() {\n   CoroutineScope(Dispatchers.IO).launch {\n         runCatching {\n            repository.addTask(taskSummary.value, taskPriority.value)\n         }.onSuccess {\n            withContext(Dispatchers.Main) {\n               _addItemEvent.emit(AddItemEvent.Info(\"Task '$taskSummary' with priority '$taskPriority' added successfully.\"))\n            }\n         }.onFailure {\n            withContext(Dispatchers.Main) {\n               _addItemEvent.emit(AddItemEvent.Error(\"There was an error while adding the task '$taskSummary'\", it))\n            }\n         }\n         cleanUpAndClose()\n   }\n}\n\nprivate fun cleanUpAndClose() {\n   _taskSummary.value = \"\"\n   _taskPriority.value = PriorityLevel.Low\n   _addItemPopupVisible.value = false\n}"
                },
                {
                    "lang": "kotlin",
                    "value": "// ... imports\nimport com.mongodb.app.domain.PriorityLevel\n\ninterface SyncRepository {\n\n   // ... Sync functions\n\n   suspend fun addTask(taskSummary: String, taskPriority: PriorityLevel)\n\n   // ... Sync functions\n}\n\nclass RealmSyncRepository(\n   onSyncError: (session: SyncSession, error: SyncException) -> Unit\n) : SyncRepository {\n\n   // ... variables and SyncConfiguration initializer\n\n   // ... Sync functions\n\n   override suspend fun addTask(taskSummary: String, taskPriority: PriorityLevel) {\n      val task = Item().apply {\n            owner_id = currentUser.id\n            summary = taskSummary\n            priority = taskPriority.ordinal\n      }\n      realm.write {\n            copyToRealm(task)\n      }\n   }\n\n   override suspend fun updateSubscriptions(subscriptionType: SubscriptionType) {\n      realm.subscriptions.update {\n            removeAll()\n            val query = when (subscriptionType) {\n               SubscriptionType.MINE -> getQuery(realm, SubscriptionType.MINE)\n               SubscriptionType.ALL -> getQuery(realm, SubscriptionType.ALL)\n            }\n            add(query, subscriptionType.name)\n      }\n   }\n\n   // ... additional Sync functions\n\n}\n\nclass MockRepository : SyncRepository {\n   override fun getTaskList(): Flow<ResultsChange<Item>> = flowOf()\n   override suspend fun toggleIsComplete(task: Item) = Unit\n   override suspend fun addTask(taskSummary: String, taskPriority: PriorityLevel) = Unit\n   override suspend fun updateSubscriptions(subscriptionType: SubscriptionType) = Unit\n   override suspend fun deleteTask(task: Item) = Unit\n   override fun getActiveSubscriptionType(realm: Realm?): SubscriptionType = SubscriptionType.ALL\n   override fun pauseSync() = Unit\n   override fun resumeSync() = Unit\n   override fun isTaskMine(task: Item): Boolean = task.owner_id == MOCK_OWNER_ID_MINE\n   override fun close() = Unit\n\n   // ... companion object\n\n}"
                },
                {
                    "lang": "kotlin",
                    "value": "private fun getQuery(realm: Realm, subscriptionType: SubscriptionType): RealmQuery<Item> =\n   when (subscriptionType) {\n         SubscriptionType.MINE -> realm.query(\"owner_id == $0 AND priority <= ${PriorityLevel.High.ordinal}\", currentUser.id)\n         SubscriptionType.ALL -> realm.query()\n   }"
                },
                {
                    "lang": "kotlin",
                    "value": "config = SyncConfiguration.Builder(currentUser, setOf(Item::class))\n   .initialSubscriptions(rerunOnOpen = true) { realm ->\n       // Subscribe to the active subscriptionType - first time defaults to MINE\n       val activeSubscriptionType = getActiveSubscriptionType(realm)\n       add(getQuery(realm, activeSubscriptionType), activeSubscriptionType.name, updateExisting = true)\n   }\n   .errorHandler { session: SyncSession, error: SyncException ->\n       onSyncError.invoke(session, error)\n   }\n   .waitForInitialRemoteData()\n   .build()"
                },
                {
                    "lang": "sh",
                    "value": "ERROR \"Client attempted a write that is outside of permissions or query\nfilters; it has been reverted\""
                }
            ],
            "preview": "Realm provides a Kotlin SDK that allows you to create an Android mobile\napplication with Kotlin using Jetpack Compose.\nThis tutorial is based on the Kotlin Flexible Sync\nTemplate App, named kotlin.todo.flex, which illustrates the creation of a\nTo-do Item List management application. This application enables users to:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "authentication/facebook",
            "title": "Facebook Authentication",
            "headings": [
                "Overview",
                "Configuration",
                "Set Up a Facebook App",
                "Create a Facebook App",
                "Enable Facebook Login",
                "Add App Services as a Valid OAuth Redirect URI",
                "Configure the Facebook Authentication Provider",
                "Examples"
            ],
            "paragraphs": "The Facebook authentication provider allows users to log in\nwith their existing Facebook account through a companion\nFacebook application. When a user logs in, Facebook provides\nAtlas App Services with an  OAuth 2.0 access token  for\nthe user. App Services uses the token to identify the user and\naccess approved data from the Facebook API on their behalf.\nFor more information on Facebook Login, see  Facebook Login\nfor Apps . The following diagram shows the OAuth logic flow: The Facebook authentication provider has the following configuration\noptions: You can enable and configure the Facebook authentication provider from the\nApp Services UI by selecting  Facebook  from the\n Authentication  page. To enable and configure the Facebook authentication provider with\nthe  App Services CLI , define a  configuration\nobject  for it in  /auth/providers.json . Facebook provider configurations have the following form: Field Description Required. The  App ID  of the Facebook app. See  Set Up a Facebook App  for information about setting\nup your Facebook app and finding the  App ID . Required. The name of a  Secret  that stores\nthe  App Secret  of the Facebook app. See  Set Up a Facebook App  for information about setting\nup your Facebook app and finding the  App Secret . Optional. A list of fields describing the authenticated user that\nyour application will request from the  Facebook Graph API . All metadata fields are omitted by default and can be\nrequired on a field-by-field basis. Users must\nexplicitly grant your app permission to access each\nrequired field. If a metadata field is required and\nexists for a particular user, it will be included in\ntheir user object. To require a metadata field from an import/export\nconfiguration file, add an entry for the field to\nthe  metadata_fields  array. Each entry should\nbe a document of the following form: Required for web applications.\nA list of allowed redirect  URIs (Uniform Resource\nIdentifiers) . Once a user completes the authentication process on\nFacebook, App Services redirects them back to either a\nspecified redirect URI or, if no redirect URI is\nspecified, the URL that they initiated the\nauthentication request from. App Services will only redirect\na user to a URI that exactly matches an entry in this\nlist, including the protocol and any trailing\nslashes. Optional. A list of approved  domains \nfor user accounts. If specified, the provider checks the domain of a\nuser's primary email address on Facebook and only\nallows them to authenticate if the domain matches an\nentry in this list. For example, if  example1.com  and  example2.com \nare listed, a Facebook user with a primary email of\n joe.mango@example1.com  would be allowed to log\nin, while a user with a primary email of\n joe.mango@example3.com  would not be allowed to\nlog in. If you've specified any domain restrictions, you\nmust also require the email address field in the\n Metadata Fields  setting. The Facebook authentication provider requires a  Facebook\napp  to manage\nauthentication and user permissions. The following steps\nwalk through creating the app, setting up Facebook Login,\nand configuring the provider to connect with the app. Follow Facebook's  official guide  to create a new\nFacebook app. From the app's  Dashboard  view, find the  Facebook\nLogin  card and click  Set Up . You should see a list of\nquickstart guides for each platform. Follow the guide for your\nplatform to enable Facebook Login. App Services web applications do not require you to install the Facebook\nSDK to use the Facebook authentication provider. If you are\nincorporating Facebook Login into a web application you can skip\nany steps in the quickstart related to setting up the Facebook SDK\nfor JavaScript. When a user completes the login flow for your Facebook app they need\nto be redirected back to App Services. Facebook Login will only allow users\nto redirect to a pre-approved list of URIs. From the  Facebook Login > Settings  page, add an App Services\nauthentication callback URL that corresponds to the  deployment\nregion  of your application to\nthe list of  Valid OAuth Redirect URIs .\nThe following table lists the callback URL for each region: Region App Services Authentication Callback URL To connect your Facebook app to App Services, find your Facebook app's\n App ID  and  App Secret  values on the\n Settings > Basic  page and add them to your authentication\nprovider  configuration . For code examples that demonstrate how to register and log in using\nFacebook authentication, see the documentation for the Realm SDKs: C++ SDK Flutter SDK Java SDK Kotlin SDK .NET SDK Node SDK React Native SDK Swift SDK Web SDK",
            "code": [
                {
                    "lang": "none",
                    "value": "{\n  \"oauth2-facebook\": {\n    \"name\": \"oauth2-facebook\",\n    \"type\": \"oauth2-facebook\",\n    \"disabled\": <boolean>,\n    \"config\": {\n      \"clientId\": <string>\n    },\n    \"secret_config\": {\n      \"clientSecret\": <string>\n    },\n    \"metadata_fields\": [<document>, ...],\n    \"redirect_uris\": [<string>, ...],\n    \"domain_restrictions\": [<string>, ...]\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{ name: \"<metadata field name>\", required: \"<boolean>\" }"
                },
                {
                    "lang": "text",
                    "value": "https://services.cloud.mongodb.com/api/client/v2.0/auth/callback"
                },
                {
                    "lang": "text",
                    "value": "https://us-east-1.aws.services.cloud.mongodb.com/api/client/v2.0/auth/callback"
                },
                {
                    "lang": "text",
                    "value": "https://us-west-2.aws.services.cloud.mongodb.com/api/client/v2.0/auth/callback"
                },
                {
                    "lang": "text",
                    "value": "https://eu-west-1.aws.services.cloud.mongodb.com/api/client/v2.0/auth/callback"
                },
                {
                    "lang": "text",
                    "value": "https://eu-central-1.aws.services.cloud.mongodb.com/api/client/v2.0/auth/callback"
                },
                {
                    "lang": "text",
                    "value": "https://ap-south-1.aws.services.cloud.mongodb.com/api/client/v2.0/auth/callback"
                },
                {
                    "lang": "text",
                    "value": "https://ap-southeast-1.aws.services.cloud.mongodb.com/api/client/v2.0/auth/callback"
                },
                {
                    "lang": "text",
                    "value": "https://ap-southeast-2.aws.services.cloud.mongodb.com/api/client/v2.0/auth/callback"
                }
            ],
            "preview": "The Facebook authentication provider allows users to log in\nwith their existing Facebook account through a companion\nFacebook application. When a user logs in, Facebook provides\nAtlas App Services with an OAuth 2.0 access token for\nthe user. App Services uses the token to identify the user and\naccess approved data from the Facebook API on their behalf.\nFor more information on Facebook Login, see Facebook Login\nfor Apps.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "release-notes/backend",
            "title": "Atlas App Services Changelog",
            "headings": [
                "2024 Releases",
                "6 March 2024 Release",
                "21 February 2024 Release",
                "7 February 2024 Release",
                "24 January 2024 Release",
                "10 January 2024 Release",
                "2023 Releases",
                "13 December 2023 Release",
                "29 November 2023 Release",
                "15 November 2023 Release",
                "01 November 2023 Release",
                "18 October 2023 Release",
                "04 October 2023 Release",
                "18 September 2023 Release",
                "06 September 2023 Release",
                "23 August 2023 Release",
                "09 August 2023 Release",
                "26 July 2023 Release",
                "12 July 2023 Release",
                "28 June 2023 Release",
                "07 June 2023 Release",
                "24 May 2023 Release",
                "10 May 2023 Release",
                "26 April 2023 Release",
                "12 April 2023 Release",
                "29 March 2023 Release",
                "16 March 2023 Release",
                "24 February 2023 Release",
                "08 February 2023 Release",
                "25 January 2023 Release",
                "11 January 2023 Release",
                "2022 Releases",
                "14 December 2022 Release",
                "30 November 2022 Release",
                "17 November 2022 Release",
                "3 November 2022 Release",
                "20 October 2022 Release",
                "10 October 2022 Release",
                "23 September 2022 Release",
                "9 September 2022 Release",
                "29 June 2022 Release",
                "15 June 2022 Release",
                "7 June 2022 Release",
                "19 May 2022 Release",
                "20 April 2022 Release",
                "25 March 2022 Release",
                "26 January 2022 Release",
                "19 January 2022 Release",
                "2021 Releases",
                "02 December 2021 Release",
                "18 November 2021 Release",
                "06 October 2021 Release",
                "08 September 2021 Release",
                "25 August 2021 Release",
                "11 August 2021 Release",
                "28 July 2021 Release",
                "15 July 2021 Release",
                "02 July 2021 Release",
                "16 June 2021 Release",
                "04 June 2021 Release"
            ],
            "paragraphs": "Created a new endpoint to link multiple data sources to an existing App. Updated the timeout variable,  maxTimeMS , to accept  float64  for  db.aggregate \nand  db.coll.aggregate  commands. Added a check to ensure custom user data is not enabled before deleting a\ndatasource. If custom user data is enabled, the user will get a 400 error. If Sync is paused or terminated, schema version information will still be shown in App Services UI. Fixed error where an empty App would sometimes fails if mutation custom resolvers\nwere pushed to the App. Fixed error to allow custom user data changes to be parsed when pushing or\npulling an app even if custom user data is disabled. Configured  custom error handlers  for Database Triggers\nsending events to AWS EventBridge. Created new  trigger error handler log type  in support\nof new AWS EventBridge custom error handling for Database Triggers. Atlas Data Services log pages now use your timezone of preference. Added support for dictionaries in Dart object model generation. Fixed error where Apps linked to serverless datasources were incorrectly able to enable Device Sync. Timestamps displayed in the App Services UI (e.g. log messages, deployment histories) now include the timezone. GET methods for the App Services Admin API now return full endpoints instead of partial endpoints. Raised the default websocket read limit from 16 MB to 20 MB. Added an app-level config setting to raise the read limit if needed. Added an App Services Admin API endpoint to trigger a client reset. Added Edge Server support for multi-architecture builds for Raspberry Pi. Increased max refresh token expiration for user sessions from 180 days to\n5 years. For information about configuring refresh token expiration, refer\nto  Configure Refresh Token Expiration . Released the new  services.cloud.mongodb.com  domain. App Services UI\nweb visits, Admin API requests, and Client API requests through the Atlas\nDevice SDKs can now begin migrating from  realm.mongodb.com  to the new\ndomain. For more information, refer to  Domain Migration . Requests authenticated through the Custom JWT provider now update the user's profile data (e.g.  context.user.data ) based on information from the JWT. Updated App Services schema UI to display the difference between a 64-bit  long  and 32-bit  integer . Disallowed creating schemas with invalid database or collection names, such as names including spaces. MongoDB aggregation operations no longer specify a default value for  allowDiskUse . Added  integer  as a supported type for indexed queryable fields. Device Sync permissions now properly union a read filter set to  false  with its corresponding write filter. Included a prepended forward slash ( / ) for endpoint routes in the App Services \"Create HTTPS Endpoint\" UI. Added support for writing functions to nested folders using the App Services CLI. Released Dark Mode for the App Services UI. To enable Dark Mode,\nopen your Atlas user preferences and set the Appearance to Dark Mode. Added a link to the UI to download the App Services CLI. Disabled the Partion-Based Sync UI for new Apps. Existing Apps will\ncontinue to see both Partition-Based Sync and Flexible Sync\nconfiguration screens until the Partition-Based Sync deprecation date. Fixed syntax highlighting for C++ generated models. Added an exponential backoff to AWS EventBridge retry requests. Updated product UI and documentation to use the new  appservices \nCLI instead of  realm-cli . Added support for triggers that watch all collections in a database\nand deployment. Added Device Sync model generation for the Realm C++ Beta SDK. Apps with Private Endpoints enabled can now be configured to allow\nrequests that originate from outside of the VPC. No longer creates a pending user account if a user tries to register a\nnew email/password identity with an invalid email address. Soft released the new  Atlas App Services CLI version 0.1.0 . Added support for Device Sync data ingest in Flutter object models. App Services now prevents you from deleting a data source that has an\nactive trigger, log forwarder, sync, or other event subscription. Device Sync now supports geographic queries on GeoJSON Point fields. Device Sync clients trying to bootstrap with a bundled realm that has\nbecome invalidated by a termination are now sent a client reset\ncommand. Device Sync developer mode now gracefully handles breaking schema changes. This only applies to new Apps. Device Sync performance thresholds now scale with the size of the linked cluster. Triggers and Device Sync now recover faster from transient issues. Added support for programmatic informational CLI announcements. Added support for  show dbs  and  show collections  on the Atlas Edge Server. UI Improvements Trigger match and project expressions maintain formatting and are prettified on reload. Added support for creating relationships to fields within a list of embedded objects in the UI. Fixed a looping UI error state on the schema page. The schema generator now outputs correct Kotlin SDK models. Logs for AWS EventBridge triggers now include the document size and ID. Added support for  indexed queryable fields  in Device Sync. Raised the Atlas Function memory limit from 256MB to 350MB. New Apps are automatically opted into Flexible Sync and cannot enable\nPartion-Based Sync. The UI now displays a confirmation dialog when you delete an\napplication. This dialog includes the application name and a warning\nthat the deletion is permanent. This change helps prevent accidental\ndeletions. You can now disable GraphQL introspection queries in your GraphQL API\nconfiguration. Added Jakarta, Cape Town, Melbourne, Hyderabad, UAE, and Zurich to the\nsupported regions for EventBridge triggers in the Atlas UI. Added support for multi-lined secrets in the secret input in the UI. Added support for the  maxTimeMS  option to aggregation pipelines in\nFunctions. Added support for functions in nested directories in automatic GitHub\ndeployments. The following kinds of requests are no longer included in billing\nusage metrics: Flexible sync downloads that contain no changesets Requests that fail for transient issues like disconnects Custom User Data ID fields may now contain string or ObjectId values. Extended third-party services deprecation date to November 1, 2024. Substantially increased the speed of static hosting draft creation & deployment. Renamed \"Tiered Device Sync\" to \"Device Sync Edge Server\" Added \"Device Sync\" to the left side navigation in the Atlas UI Added support for the  %stringToUuid  and\n %uuidToString  operators in rule expressions. Added support for  String.prototype.replaceAll()  in\nFunctions. Increased function  execution timeout \nfrom 270 seconds to 300 seconds. Added support for automatically migrating a Device Sync App from\nPartition-Based Sync to Flexible Sync. To learn more, see\n Migrate Device Sync Modes . App cards in the Atlas UI are now ordered by Last Updated time. Fixed an issue where a Device Sync changeset missing required fields\ncould cause temporary JSON schema errors in the underlying cluster. Added suport for  VPC Private Endpoints  with\nAWS PrivateLink. Added support for  axios@^1.3.6  in Functions. Increased function  execution timeout \nfrom 240 seconds to 270 seconds. Updated the Scheduled Trigger configuration UI to use the timezone and\ndate format specified in the user's Atlas preferences. Updated the Realm SDK model generator to output C# models based on\nsource generators with nullable type annotations. Removed the permissions section from the Device Sync configuration UI.\nInstead, define permissions from the unified  Rules  screen. The GraphQL API will now emit a warning if multiple schemas use the\nsame  title  value. Renamed \"Asymmetric Sync\" to \"Data Ingest\" in the UI Increased function  execution timeout \nfrom 210 seconds to 240 seconds. Remove  function_name  in endpoint request body when\n fetching, creating, and modifying HTTPS Endpoints from the Admin API . Expiration times for user refresh tokens can be  configured from the UI . Change the email address associated with email/password authentication via an\n Admin API endpoint . Device Sync is automatically paused \nif it is inactive for 30 days. Updated all existing apps using Atlas Device Sync with Flexible Sync to\n use the same rules and permissions as other services .\nThis update has been rolled out to all existing Apps from 6 March, 2023 through\n15 March, 2023. Add additional  currentPasswordValid  parameter to  password reset function . Update Atlas Device Sync with Flexible Sync to  use the same rules and permissions  as other services. This update is currently only\napplied to newly created Apps. On the week of 26 February, 2023, existing apps\nwill have their Flexible Sync specific-rules migrated to use the same rules and\npermissions as other services. Increase  App concurrent request limit  from 5,000 to 10,000. Configure  user creation function  via the App Services UI. Change App deployment model  from the UI and API. Authenticate Data API and HTTPS Endpoint requests  using Bearer Authentication . Configure  user creation function  via the App Services CLI and Admin API. Raise  Atlas Function request timeout \nfrom 150 seconds to 180 seconds. Expiration times for refresh tokens can be\n configured from the API . Let users create Apps in AWS region Ohio (us-east-2) from the UI. Remove maximum number of clients that can concurrently\n listen to database change streams \nbased on cluster size. Support  App Services deployments  in the AWS region\nOhio (aws-us-east-2). Authenticate Atlas Data API requests \nwith the  apiKey  authentication credential headers for API key authentication. GraphQL API support for query on relationships nested in relationship arrays. When creating or configuring your App in the UI, App Services selects the\n App Services deployment region  geographically closest\nto your selected Atlas data source as the default option. Support  App Services deployments  in the AWS region\nS\u00e3o Paulo (aws-sa-east-1). Updated  Admin API  endpoint to fetch Atlas Triggers and\nAtlas Data API apps. Function context.app.id  returns a string (formerly BSON ObjectId). Support  App Services deployments  in the AWS region\nLondon (aws-eu-west-2). Added  Admin API  endpoint to retrieve App Services metrics. Added option to  auto-resume Database Triggers \nthat were suspended because resume token was lost. Increased  request timeout  from 120 seconds to 150 seconds. Added  mongodb.admin()  and  admin.getDBNames()  in\nAtlas Functions. Support App Services deployments in the following  GCP regions : Ohio ( us-central1 ) Virginia ( us-east4 ) Oregon ( us-west1 ) Belgium ( europe-west1 ) Mumbai ( asia-south1 ) Added support for  database.getCollectionNames()  in Atlas\nFunctions. Introduced a refreshed UI for Rules. Introduced ability to configure  field-level permissions with the Data API . Released Flexible Sync as GA. Released  Asymmetric Sync , which optimizes Flexible Sync for write-heavy workloads. Released  Data API  as GA. Introduced Data API for all Apps. Introduced field-level permissions for Flexible Sync. Introduced  local regions for Azure data sources . Introduced option to encode HTTPS Endpoint responses as  EJSON or JSON . Added support for serverless Atlas instances as data sources. Serverless instances do not yet\nsupport Triggers or Sync. Introduced ability to accept null values as optional types in Realm Schema. Added ability to download logs from the UI. Added Flexible Sync support for queries using  BETWEEN  and the string operators  BEGINSWITH ,  ENDSWITH ,  CONTAINS . Added Flexible Sync support for queries on arrays of primitives. Performance improvements for Functions, particularly aimed at decreasing the\nruntime for those dependencies that make external requests. MongoDB Atlas moved to Let's Encrypt as the new Certificate\nAuthority for TLS certificates for all App Services. Visually refreshed the  Schema UI . Introduced  Log Forwarding , which automatically stores your\napplication's server-side logs in a MongoDB collection or sends them to\nan external service. Introduced  Flexible Sync (preview) , which allows clients to sync data without the need for partition keys. Added ability to import dependencies  from the UI . Deprecated  third party services . Third party services will be fully removed on December 1, 2022. Renamed \"Webhooks\" to \" HTTPS Endpoints \". Requests blocked by an application's  IP Access List  no longer count towards billing. Added the ability to configure an  IP Access List . Increased function  execution timeout  from 90 seconds to 120 seconds. Added the ability to create apps with a Template Starter Application. App Services Events are now available to view on the  Atlas Activity Feed .\nYou can configure these events in the  Atlas Alert Settings . Increased  request limit  from 3000 requests per second to 5000 requests per second. Allows users to store non-Realm files in the App Services\n app structure . Updates to documents that do not match an application's  schema \ncan now enable Atlas Device Sync for those documents. Introduced the ability to export and re-deploy the most recent 25 deploys. Support the option of sending events using  Extended JSON \nin  AWS EventBridge Triggers  to support sending additional data types such as\n Decimal128 . Display the generated data models in SDKs Data Models when  Development Mode \nis enabled for Atlas Device Sync. Deprecated the  Stitch JS SDK . Released the  Trigger Preimages  option for GA. Improved usability of the Admin UI Dashboard. Released  mongodb-realm-cli 2.0 . Added  JWT Authentication  support for\nJWTs with multiple audiences. Introduced support for bi-directional  GitHub Autodeploy . Added the ability to link a Github repository on application create. Introduced ability to link new data sources via  Github Autodeploy . Improves performance of client requests to app servers. Fixes an issue where aggregation pipelines did not support the  $set  operator. Reduces \"Invalid Session\" logs.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "authentication/custom-function",
            "title": "Custom Function Authentication",
            "headings": [
                "Overview",
                "When to Use Custom Function Authentication",
                "The Authentication Function",
                "Receive a Custom Credential Payload",
                "Return an Authenticated User ID",
                "Throw an Error for Failed Authentication",
                "Set Up the Custom Function Provider",
                "Enable the Custom Function Provider",
                "Define the Authentication Function",
                "Deploy the Updated Application",
                "Pull the Latest Version of Your App",
                "Define the Authentication Function",
                "Add a Provider Configuration File",
                "Deploy the Updated Application",
                "Configure Custom User Data",
                "Log In from a Realm SDK"
            ],
            "paragraphs": "The  custom function  authentication provider allows you to define a\ncustom authentication flow using a  serverless function . You can use this provider to implement your own user\nauthentication logic or integrate an external authentication system. Custom function authentication is the most flexible form of\nauthentication. However, it also requires you to manually define and\nconfigure the authentication flow yourself. Before you define a custom function provider, consider if you can one\nuse of the  built-in authentication providers  instead. Consider using the custom function provider in your app if: The following diagram shows the custom function logon process: You want to use an external authentication service that does not have\na built-in provider. If the service uses JSON web tokens, consider\ncreating a  Custom JWT  provider\ninstead. You want to customize the authentication process beyond what's\navailable in a built-in provider. For example, you could use a service\nto send customized confirmation emails instead of the default\nemail/password provider emails. Atlas App Services does not perform any data validation or\nauthentication checks for the custom function provider. Make sure that you validate incoming data and that your\nauthentication system performs appropriate authentication checks,\nsuch as requiring a password,  two-factor authentication , or a\n single sign-on  token. The authentication function is a JavaScript function that holds your\ncustom user authentication code. It runs whenever a user logs in through\nthe custom function provider. The function maps data provided at login, like a username and password\nor an access token, to a string that uniquely identifies the user in\nyour external authentication system. For example, you could use the\nprovided data to log in to an external service over HTTP or using a\npackage from npm. The  payload  object passed to the function contains data that was\nincluded with the custom function provider credential in the client app.\nThe function accepts any value provided from your client app, so the\nactual field names and values depend on your implementation. For examples of how to create a custom credential  payload  object\nusing the Realm SDKs, refer to the documentation for each SDK: C++ SDK Flutter SDK Java SDK Kotlin SDK .NET SDK Node.js SDK React Native SDK Swift SDK Web SDK If authentication is successful, the function should return a unique\nstring identifier for the user. For example, you could return the user\nID value used by your external authentication system. This is the user's\n external ID , which the provider uses to map from your custom system to\nyour app's internal user accounts. If an existing user is already associated with the external ID, the\nprovider logs that user in. If the provider has no record of a given external ID, it creates a new\n user account , adds a custom function provider\nidentity, and then logs in the new user. The identity object for the custom function provider is stored in the\n user object  and resembles the following: The user's external ID is  not  the same as the user's internal\naccount ID, which is exposed as the  id  field of a user object.\nYou access the user's internal ID with  %%user.id  in expressions,\n context.user.id  in functions, and the  User.id  property in the\nSDKs. The authentication function should return a unique external ID as a\nstring: You can also return an object that contains the external ID as its\n id  value: If you want to define a display name for the user, define it in the\n name  field of the returned object: If the user provided invalid credentials or the function otherwise fails\nto authenticate the user, throw an error with a descriptive message.\nThis returns a  401 - Unauthorized  error with the message attached to\nthe SDK. You can configure custom function authentication using any supported\ndeployment method. To enable the provider: Click  App Users  in the left navigation menu. Select the  Providers  tab. Click  Custom Function Authentication . On the provider configuration screen, set the  Provider\nEnabled  toggle to  On . The provider uses a normal function to handle authentication. The\nfunction should return a unique external ID string to identify the\nuser. To define a new  authentication function : Click the  Function  dropdown and select  New Function . Enter a name for the function. This name must be unique among all\nfunctions in your application. Define the source code in the function editor. Click  Save To make custom function authentication available to client\napplications, deploy your application. Click  Deploy  in the left navigation menu Find the draft in the deployment history table and then click\n Review & Deploy Changes . Review the diff of changes and then click  Deploy . To pull a local copy of the latest version of your app, run the\nfollowing: You can also download a copy of your application's configuration files from\nthe  Deploy > Export App  screen in the App Services UI. The provider uses a normal function to handle authentication. The\nfunction should return a unique external ID string to identify the\nuser. user. Save the  authentication function  code\nin the  functions  directory. To enable and configure the Custom Function authentication provider,\ndefine a a  configuration object  for it in\n /auth/providers.json . Custom function provider configurations have the following form: Once you have created the authentication function and configured the provider,\nyou can push the updated configurations to your remote app. App Services CLI\nimmediately deploys the new schema on push. You can associate  custom data  in a MongoDB\nAtlas collection with user accounts in your app. This can be helpful if\nyou often need to access a user's data but is not required to use the\ncustom function provider. A user's custom data document may contain any data. For apps that use\nthe custom function provider, we recommend storing both the user's\ninternal user account ID and their external ID. For example, you might use the following format: You can use the following approach to create custom user documents for\ncustom function provider users: Configure custom user data for a collection in your linked cluster.\nThe User ID field stores the user's internal account ID. Configure the custom function authentication provider and return a\nunique external user ID from the authentication function. App Services\nstores this ID in the  id  field of the user's  custom-function \nidentity. Set up an authentication trigger that listens for  CREATE  events\nfrom the  custom-function  provider. In the trigger function, add a\nnew document to the custom user data collection that includes both\nthe user's internal ID and external ID. To log in from a client application, use a Custom Function credential\nthat contains your login payload data. For examples, refer to the documentation for a specific SDK: C++ SDK Flutter SDK Java SDK Kotlin SDK .NET SDK Node.js SDK React Native SDK Swift SDK Web SDK",
            "code": [
                {
                    "lang": "javascript",
                    "value": "exports = async function (payload) {\n  // 1. Parse the `payload` object, which holds data from the\n  //    FunctionCredential sent by the SDK.\n  const { username, password } = payload;\n\n  // 2. Create a new user or log in an existing user in the external\n  //    authentication service.\n\n  // You can use a client library from npm\n  const auth = require(\"fake-auth-service\");\n  const user = await auth.login({ username, password });\n\n  // Or you can communicate directly over HTTP\n  const userFromHttp = await context.http.post({\n    url: \"https://example.com/auth/login\",\n    headers: {\n      Authorization: [\"Basic bmlja0BleGFtcGxlLmNvbTpQYTU1dzByZA==\"],\n    },\n    body: JSON.stringify({ username, password }),\n  });\n\n  // 3. Return a unique identifier for the user. Typically this is the\n  //    user's ID in the external authentication system or the _id of a\n  //    stored MongoDB document that describes them.\n  //\n  //    !!! This is NOT the user's internal account ID for your app !!!\n  return user.id;\n};\n"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"id\": \"<Internal User Account ID>\",\n  \"identities\": [\n     {\n       \"providerType\": \"custom-function\",\n       \"id\": \"<External User ID>\",\n     }\n  ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "return \"5f650356a8631da45dd4784c\""
                },
                {
                    "lang": "javascript",
                    "value": "return { \"id\": \"5f650356a8631da45dd4784c\" }"
                },
                {
                    "lang": "javascript",
                    "value": "return {\n  \"id\": \"5f650356a8631da45dd4784c\",\n  \"name\": \"James Bond\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "const auth = require(\"some-external-auth-system\");\ntry {\n  const user = await auth.login(payload);\n  return user.id;\n} catch (err) {\n  throw new Error(`Authentication failed with reason: ${err.message}`);\n}\n"
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "shell",
                    "value": "touch functions/handleCustomFunctionAuthentication.js"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"custom-function\": {\n    \"name\": \"custom-function\",\n    \"type\": \"custom-function\",\n    \"config\": {\n      \"authFunctionName\": \"<Authentication Function Name>\"\n    },\n    \"disabled\": false\n  }\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=\"<Your App ID>\""
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": \"<Generated ObjectId>\",\n  \"user_id\": \"<Internal User ID>\",\n  \"external_id\": \"<External User ID>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"mongo_service_name\": \"mongodb-atlas\",\n  \"database_name\": \"myApp\",\n  \"collection_name\": \"users\",\n  \"user_id_field\": \"user_id\",\n  \"enabled\": true\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function handleCustomFunctionAuth(payload) {\n  const auth = require(\"some-external-auth-system\");\n  const user = await auth.login(payload);\n  return user.id;\n};\n"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function onNewCustomFunctionUser({ user }) {\n  // This is the user's internal account ID that was generated by your app\n  const internalId = user.id;\n\n  // This is the external ID returned from the authentication function\n  const customFunctionIdentity = user.identities.find((id) => {\n    return id.provider_type === \"custom-function\";\n  });\n  const externalId = customFunctionIdentity.id;\n\n  // Create a custom user data document for the user\n  const mdb = context.services.get(\"mongodb-atlas\");\n  const users = mdb.db(\"myApp\").collection(\"users\");\n  return await users.insertOne({\n    // Include both the internal ID and external ID\n    user_id: internalId,\n    external_id: externalId,\n    // Add any other data you want to include\n    created_at: new Date(),\n  });\n};\n"
                }
            ],
            "preview": "The custom function authentication provider allows you to define a\ncustom authentication flow using a serverless function. You can use this provider to implement your own user\nauthentication logic or integrate an external authentication system.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "authentication/anonymous",
            "title": "Anonymous Authentication",
            "headings": [
                "Overview",
                "Account Linking",
                "Configuration",
                "Examples",
                "Summary"
            ],
            "paragraphs": "The Anonymous authentication provider allows users to log in to your\napplication without providing credentials. Anonymous user objects have a\nunique ID value but no other metadata fields or configuration options. Potential use cases for anonymous authentication include: An Anonymous user object is not intended to persist data. Once a user logs\nout, the user cannot retrieve any previous user data. Authenticating the readers of a blog or news service. Allowing end users to try the features of an application before\nregistering for an account. Simplifying the creation of users while developing and testing the\nclient application. When someone authenticates anonymously, the provider generates an anonymous\nuser object. If the app does not explicitly log this anonymous user out,\nthe same anonymous user is reused. The anonymous user persists until either\nof these things occur: To persist data associated with an Anonymous user, you can associate\nthat existing Anonymous identity with a user account created by a\ndifferent authentication provider. Explicit logout. User deletion. Atlas App Services deletes anonymous user objects that have been inactive\nfor 90 days. App Services may delete an Anonymous user object that is 90 days old (or\nolder). When an account is deleted, it is not recoverable and any\nassociated user data is lost. Documents created or modified by the\nuser remain unaffected. You can enable the Anonymous authentication provider from the\nApp Services UI in the  Authentication  options. Select  Authentication  in the left sidebar. Select the \"Allow users to log in anonymously\" entry of the\n Provider  list. Click the  Provider Enabled  toggle to move it\ninto the \"On\" state. Click the  Save  button in the lower right of the\npage to save your changes to the App configuration. Click  Review & Deploy Changes  in the dropdown\nthat appears at the top of the page. Review the changes to your app's configuration and click\n Deploy  in the lower right of the dialog to\nmake your changes available to application users. To enable and configure the Anonymous authentication provider with\nthe  App Services CLI , define a  configuration\nobject  for it in  /auth/providers.json . Anonymous provider configurations have the following form: The  name  of an authentication provider is always the same as its  type . The anonymous authentication provider does not have any\nprovider-specific configuration options. For code examples that demonstrate how to register and log in using\nanonymous authentication, see the documentation for the Realm SDKs: C++ SDK Flutter SDK Java SDK Kotlin SDK .NET SDK Node SDK React Native SDK Swift SDK Web SDK Anonymous authentication allows users to interact with your\napplication creating an identity. To persist data from an anonymous session after a user creates an\nidentity with a different authentication provider, you can link the\ntwo identities.",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"anon-user\": {\n    \"name\": \"anon-user\",\n    \"type\": \"anon-user\",\n    \"disabled\": <boolean>\n  }\n}"
                }
            ],
            "preview": "The Anonymous authentication provider allows users to log in to your\napplication without providing credentials. Anonymous user objects have a\nunique ID value but no other metadata fields or configuration options.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "authentication/email-password",
            "title": "Email/Password Authentication",
            "headings": [
                "Overview",
                "Configuration",
                "User Confirmation",
                "New User Account Status",
                "Send a Confirmation Email",
                "Run a Confirmation Function",
                "Automatically Confirm Users",
                "Password Resets",
                "Send a Password Reset Email",
                "Run a Password Reset Function",
                "Examples",
                "Summary"
            ],
            "paragraphs": "Email/password authentication lets users register and login using an email address.\nAtlas App Services must confirm Email/Password users before they may log in. The following diagram shows the process flow for logging in: User email addresses are case-sensitive. For example, a user with the\nemail address  TestAccount@example.com  could not log in using the\n testaccount@example.com  address. In the left navigation, click Authentication. In the list of Authentication providers, select\n Email/Password . From this screen, you can\nconfigure and enable the provider. Use the following sections to\nguide you through the process. To enable and configure the Email/Password authentication provider with\nthe  App Services CLI , define a  configuration\nobject  for it in  /auth/providers.json . Email/Password provider configurations have the following form: The  name  of an authentication provider is always the same as its  type . Before an email/password user can authenticate, your app must register and\nconfirm the user's new account. Registering an email/password user creates a\nnew user object. App Services then requires confirmation of the account. You can\nselect one of three methods to confirm a new user account: The following diagram shows the process flow for confirming a user: Send a Confirmation Email Run a Confirmation Function Automatically Confirm Users A user account can be in one of 2 states: Pending and Confirmed. When the confirmation process is started, Atlas App Services creates a user\naccount and sets the status to  Pending Confirmation . In this state,\nthe user cannot login. Because the email address is now associated with a\nuser account, any attempt to re-register the same email will fail. With the account in the pending state, the user must confirm their account\nbefore logging in. When the user confirms their account, Atlas App Services sets\nthe status to  Confirmed  and the user can log in. If you are automatically confirming users, the account is created with the\nstatus set to  Confirmed  and the user can log on immediately after\nregistering. You can view a list of pending user accounts in the UI or via the\nApp Services APIs: Only use automatic confirmation when you are developing and testing your app.\nProduction applications should always use a secure confirmation process. In the left navigation, under  Data Access ,\nclick  App Users . At the top of the list of users, select the  Pending  filter. To view pending users with the Realm CLI, use the\n appservices users list \ncommand and include the  --pending  option. To view pending users with the Admin API, use the\n List Users  endpoint. With this confirmation method, users must respond to an email upon\nregistration. The email contains a link to a confirmation URL. The\nuser must visit this link within 30 minutes to confirm that they\ncontrol the email address. Configure the following settings to have App Services automatically send a\nconfirmation email: Email Confirmation URL : the base of the URL included in\nevery confirmation email. App Services appends a unique  token  and\n tokenId  to this URL. These serve as query parameters to create a unique link\nfor every confirmation. To confirm the user, first extract these query\nparameters from the user's unique URL. Then, pass the  token  and  tokenId \nto the Client SDK's  confirmUser  function. Mobile applications can handle email confirmation directly in the app. To do\nthis, configure  deep linking  in\nAndroid or  universal links \nin iOS. Email Confirmation Subject : the subject line for the email\nthat App Services sends. This value is optional if omitted,\nApp Services uses a default subject line instead. Custom email\nconfirmation subjects can contain a maximum of 256 characters. Confirmation emails are not currently customizable beyond the base URL and\nsubject line. In particular, they always come from a  mongodb.com  email\naddress. For production apps, you can use a confirmation function rather than\nthe built-in confirmation email method. Confirmation functions allow you to\nbuild entirely custom email confirmations. See\n Run a Confirmation Function  for more information. You can  Run a Confirmation Function  when a new user registers.\nApp Services passes a confirmation token, a token ID, and the user's email to the\nfunction that you create. Your function then performs the logic you need to confirm\nthe user, and then returns one of the following result objects, which are explained\nin more detail below: Within the function, you define custom logic to confirm users. For example, your\nfunction might: The custom confirmation function signature has one parameter. It is an object\nthat contains user data and confirmation tokens. These fields are: The custom user confirmation function returns an object with a status field.\nThe following table describes the potential values of this field: The following is an example of a user confirmation function that: Unless the custom function automatically confirms the user, you must provide a\nmeans for the user to complete the confirmation process after the function fires.\nAfter completing any additional confirmation steps, call the SDK's Confirm User\nmethod to finalize the user account creation on Atlas App Services. { status: 'success' } { status: 'fail' } { status: 'pending' } Send custom confirmation emails from a specific domain.\nSend these with a particular template using an external service. Read user permissions from a collection in  MongoDB Atlas \nor an external REST service. Send confirmation messages through a service other than email, such as SMS. Field Description username The user's email address. token A unique value used to confirm the user's identity. You use this when\ncalling the SDK's Confirm User function. tokenId A unique value used to confirm the user's identity. You use this when\ncalling the SDK's Confirm User function. Status Effect success App Services confirms the user's identity, allowing the user to log\ninto their new account. pending App Services changes the user's confirmation status to  Pending Confirmation .\nThis does not confirm the user's identity or allow login.\nThe client application must call the SDK's Confirm User function to\nfinalize the process. fail App Services does not create a user account. The user can only retry\naccount confirmation by registering again. Since the previous account\ndoesn't exist, the user can reuse the same username (email). Checks that the email provided is a valid email. Confirms that the given email address has access to a particular service. Sends an SMS message to the user. If the message is sent successfully, informs Atlas App Services to create a\nnew account with a Pending status. You can configure the provider to\n Automatically Confirm Users . When selected, App Services immediately confirms\nnew Email/Password users after registration. App Services does not validate automatically confirmed email addresses. As\na result, there are a few reasons you may not be able to contact such\nusers via email: Exercise caution with this option.  Securely  resetting the\npassword of a user account with no valid contact information\ncan be very difficult. An automatically confirmed user's email address might not actually\nbelong to that user. (e.g. a user could sign up as\n steve.jobs@apple.com ) A user's email address may not even be a valid email address. (e.g.\na user could sign up as  my.name@gmail  or  asdavaskljj ) An email/password user may forget their password and need to reset it.\nFor security reasons, you should confirm the user's identity before completing\nthe password reset. App Services provides two ways to do this: After confirming the user's identity, you can complete the password reset\nrequest. Once the password reset has been completed, the user can log in\nusing the new password. Send a Password Reset Email Run a Password Reset Function You can configure the provider to  Send a Password Reset Email. \nWhen a user requests a password reset, App Services sends a\nPassword Reset URL to a user's email address. The user must visit\nthis URL within 30 minutes. When you configure password reset emails, you can configure the following settings: Mobile applications can handle password resets directly in the app.\nConfigure  deep linking \nin Android or  universal links  in iOS. Password Reset URL : the base of the URL included in every\npassword reset email. App Services appends a unique  token  and\n tokenId  to this URL. These serve as query parameters to create a unique link\nfor every password reset. To reset the user's password, extract these\nquery parameters from the user's unique URL. Pass the token and\ntokenId to the Client SDK's  resetPassword  function. Reset Password Email Subject : the subject line for the\nemail that App Services sends. This value is optional: if omitted,\nApp Services uses a default subject line instead. Custom password\nreset subjects can contain a maximum of 256 characters. Password reset emails are not currently customizable beyond the base URL and\nsubject line. In particular, they always come from a  mongodb.com  email\naddress. For production apps, you can use a custom password reset function\nrather than the built-in password reset email method. Custom functions\nallow you to build entirely custom email confirmations, or use a method other\nthan email to confirm a password reset request. You can configure the provider to  Run a Password Reset\nFunction . You define a  Function  for App Services to\nrun when you  callResetPasswordFunction()  in the SDK.\nApp Services passes the user's email, the desired new password, a confirmation\ntoken, and a token ID to the function that you create. Your function then\nperforms the logic you need to confirm the user's identity prior to resetting\nthe password. App Services can immediately reset the user's password. Or you can require\nadditional confirmation from the client application. The App Services function must return an object containing a  status  key.\nThe key maps to a string with one of the following values, which are explained\nin more detail below: You can use a custom password reset function to define your own password\nreset flows: The custom password reset function signature is\n variadic : it accepts any number of\nparameters. The first is always an object containing user data and confirmation tokens.\nAll following parameters are custom parameters. They are passed into the Client SDK\nas an argument collection. For instance, the Client SDK call: Becomes this customized signature for the password reset function: In the custom password reset function, an object must be passed as the\nfirst parameter. The following table describes the fields found in this object: The custom password reset function must return an object that contains a\n status  field. The value of this  status  field may be one of: A custom password reset function may include elements similar to this example: { status: 'success' } { status: 'fail' } { status: 'pending' } The Realm SDK function  callResetPasswordFunction()  is  not \nauthenticated. Its password recovery is intended only for users who\ncannot otherwise log into their account. As a result, you cannot associate\nany call to this function with a specific App Services user. Returning a\n success  status permanently changes the password to the new value in the\n password  parameter of the function. So returning  success  can result\nin  any user  resetting the password of  any other application user . For\nsecurity, you should send the account owner a message via\na trusted mode of communication and return  pending . Send custom password reset emails from a specific domain with a\nparticular template using an external service. Interface with a MongoDB Atlas collection to implement a password\nreset \"cooldown period\". This can prevent too many password reset\nattempts on a single account in a particular time range. Send custom password reset messages through a service other than\nemail. Using a password reset function is mutually exclusive with configuring\nApp Services to send a password reset email. When you configure password\nreset to use a custom function, calling  sendResetPasswordEmail() \nin the client SDK returns an error. Field Description username The email address of the user. password A new proposed password for the user. If this function returns the\nsuccessful status, this is the user's new password. token A unique value used to update the user's password. tokenId A unique value used to confirm the user's identity\nusing the SDK  confirmUser  function. currentPasswordValid A boolean that is true if a user requests to change their password\nto their existing password. Status Password Reset State SDK Effect success App Services changes the user's password to the provided  password \nparameter immediately. Only return  success  if you authenticate the user's\nidentity in the custom function. You might do this with a security question\nor other secure means. If the custom function call returns a  success , your client app\ncan interpret the lack of an error as a successful password reset.\nThe user can sign in using the new password. pending App Services waits for the client application to take some additional\naction to complete the password reset. For example, the custom\npassword reset function may send a  token  and  tokenId  via email\nor SMS to verify the user's identity. The SDK method that calls this function does not take a return value, so\nit does not directly handle a  pending  case. You must implement custom\nlogic in your application to complete the password reset process. For\nexample, you might implement a deep link or universal link in the client\nto extract the  token  and  tokenId , and then call the the\n resetPassword  method to complete a reset. fail Nothing happens. The SDK interprets a returned  fail  as an error or exception\nwhen calling the custom password reset function. For examples of registration, log in, and password reset flows using email/password\nauthentication, see the Realm SDKs: C++ SDK Flutter SDK Java SDK Kotlin SDK .NET SDK Node SDK React Native SDK Swift SDK Web SDK Email/password authentication allows users to create an identity in\nyour application based on a username and a password. To enable Email/password authentication, your application should\nsupport a method of email  confirmation , and a method of\n resetting a user's password . There are multiple\nimplementation options for each of these requirements.",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"local-userpass\": {\n    \"name\": \"local-userpass\",\n    \"type\": \"local-userpass\",\n    \"config\": {\n      \"autoConfirm\": <boolean>,\n      \"emailConfirmationUrl\": <string>,\n      \"confirmEmailSubject\": <string>,\n      \"runConfirmationFunction\": <boolean>,\n      \"confirmationFunctionName\": <string>,\n      \"resetPasswordUrl\": <string>,\n      \"resetPasswordSubject\": <string>,\n      \"runResetFunction\": <boolean>,\n      \"resetFunctionName\": <string>,\n    },\n    \"disabled\": <boolean>\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = ({ token, tokenId, username }) => {\n   // Validate the username\n   const isValidEmail = myCustomValidatorService.validate(username);\n\n   // Check if the user has access to this service\n   const isPrivileged =\n      myCustomAuthorizationService.hasAccess(username)\n\n   // Send a message to the user so that they can confirm themselves\n   const msgSendSuccessful = isValidEmail && isPrivileged\n      && mySmsService.send(username, token, tokenId)\n\n   if ( msgSendSuccessful ) {\n      return { status: 'pending' };\n   } else {\n      return { status: 'fail' };\n   }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "callResetPasswordFunction(\"myUsername\", \"newPassword\", [\"Security Question Answer 1\", \"Security Question Answer 2\", \"securityCode:0510\"])"
                },
                {
                    "lang": "javascript",
                    "value": "resetFunc({username, password, token, tokenId, currentPasswordValid}, securityAnswer1, securityAnswer2, securitySMSCode)"
                },
                {
                    "lang": "javascript",
                    "value": "exports = ({ token, tokenId, username, password, currentPasswordValid }) => {\n   // check if the username corresponds to a real user\n   const isUser = myCustomValidator.validate(username);\n\n   // check if the user is attempting to reset their password to their current password\n   if (currentPasswordValid) {\n     myCustomNotifier.sendMessage(username, \"Cannot reset password to current password.\");\n     return { status: 'fail' };\n   }\n\n   // check if the user has requested a password reset too often recently\n   const isNotCoolingDown = myCustomCooldownService.canReset(username, 'myCustomService')\n\n   // send a message to the user in some way so that the user can confirm themselves\n   const msgSendSuccessful = isUser && isNotCoolingDown && myCustomMsgr.send(username, token, tokenId)\n\n   if ( msgSendSuccessful ) {\n      return { status: 'pending' };\n   } else {\n      return { status: 'fail' };\n   }\n}"
                }
            ],
            "preview": "Email/password authentication lets users register and login using an email address.\nAtlas App Services must confirm Email/Password users before they may log in.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "authentication/custom-jwt",
            "title": "Custom JWT Authentication",
            "headings": [
                "Authentication and Authorization",
                "How JWT Authentication Works",
                "Configuration",
                "Verification Method",
                "Manually Specify Signing Keys",
                "Use a JWK URI",
                "Metadata Fields",
                "Audience",
                "Using JWT Authentication",
                "Realm SDKs",
                "API Services"
            ],
            "paragraphs": "The Custom JWT authentication provider allows users to log in with an\nauthentication credential from a third-party system (external to Atlas\nApp Services) and use that token to access data and services with App Services.\nThe external system must return a signed  JSON Web\nToken (JWT)  that contains a unique ID value for the\nauthenticated user. The third-party JWT provider authenticates users and returns a JWT. App Services\nuses the JWT to identify your application's users and authorize their requests. App Services is agnostic to the authentication methods used by the third-party\nprovider. It does not impose any restrictions on the\nexternal authentication system's requirements or authentication methods.\nFor example, the system could require the user to perform multi-factor\nauthentication (MFA), provide specific credentials, or otherwise identify\nthemselves. There are many resources online that dive into the intricacies of JWT\nauthentication and the token structure. In the context of App Services, the\nfollowing diagram provides the process flow of a user logging on to a\nDevice Sync app. The steps below the diagram provide details. JWT authentication with App Services follows these general steps: The user logs on to the third-party authentication provider by whatever means\nthe provider requires. If authentication succeeds, the provider returns a JWT to the client app. The client app logs on to the App Services app, providing the JWT credential. App Services parses and decodes the JWT. If you manually provided signing keys in App Services, App Services checks if\nthe signing key in the JWT matches one of the signing keys you specified. If\nso, the user is authenticated. If you configured App Services to use a JSON Web Key (JWK) URI, App\nServices passes the JWT and public key to the third-party provider's JWK API. The provider decodes and verifies the signature and returns a JWK. App Services checks if the signature in the JWK matches the JWT's\nsignature. If so, the user is authenticated. App Services always specifies a 30-minute access token expiry even if the\ncustom JWT token specifies a different expiry through the  exp  key.\nApp Services will check the custom JWT token  exp  to ensure that the token\nis still valid before issuing the 30-minute expiry. For more information on\nApp Services access tokens, refer to  Manage User Sessions . You configure Custom JWT Authentication from the UI or with the CLI. Choose your\npreferred method below. You enable the JWT authentication provider from the App\nServices UI by selecting  Custom JWT Authentication \nfrom the  Authentication  page. To enable and configure the Custom JWT authentication provider with\n App Services CLI , define a  configuration\nobject  for it in  /auth/providers.json . A  Custom JWT provider configuration  has\nthe following form: The  Verification Method  field determines how App Services will\nvalidate the JWT returned from the JWT provider. You can configure App Services\nto validate the JWT by using the signing key(s) you provide, or to validate by\nusing a JSON Web Key (JWK) URI issued by the third-party provider. You can configure your app to use one or more signing keys to validate the JWT.\nThere are two settings you need to provide: Field Description The cryptographic method that the external system uses to sign\nthe JWT. Custom authentication supports JWTs signed using either of\nthe following algorithms: HS256 RS256 A list of up to three  Secrets \nthat each contain a signing key used by the third-party\nauthentication system to sign  JWTs (JSON Web Tokens) .\nThe key can only contain ASCII letters, numbers,\nunderscores, and hyphens, and must be between 32 and 512 characters long.\nThe following is a valid 256-bit signing key: If you are uncertain of what value to use, consider visiting a random\nkey generator website, like  keygen.io , and\nusing one of the generated 256-bit values. Set the Signing Algorithm: Create one or more signing keys to sign the JWT. To do\nthis, provide a name for the key (which is solely for your reference\nlater), and then specify a 256-bit signing key. A  Signing Key  is a secret key and anyone with the\nkey can issue valid user credentials for your app. Ensure that\nit's never stored in a publicly accessible location, such as a\ngit repository, message board, or in your code. Some external authentication systems provide a  JSON Web Key Set  (JWKS) that describes the signing\nalgorithm and signing keys the system uses to sign JWTs. You can use the\nJWKS to configure the provider instead of manually specifying the\nsigning algorithm and keys. The returned JWKS must include a\n kid  header that specifies the Key ID of a key from the JWKS. The JWKS\nmay specify up to three signing keys and must use the  RS256  algorithm. There is only one value you need to provide: JWK and JWKS are used synonymously in Atlas App Services. JWK URI , which is the third-party URL that hosts a JWK or JWKS service.\nWhen you choose this option, App Service automatically sets encryption\nto the required  RS256  method. Specify the URL to the third-party JWKS endpoint: Metadata Fields  are additional data that describe each internal\nApp Services user. App Services determines the value of each metadata field from\nthe value of a field included in the third-party JWT.\nFor example, if you set the  name  field of a user, then App Services will use\nthat field in the JWT as the user's display name. There are three values that you need to specify for each metadata field: App Services refreshes a user's metadata whenever they log in and exposes the\nfields in the  data  object of the  user metadata . The length of a JWT token increases with the number of metadata fields in the\ntoken and the size of each field.  App Services limits the length of a\nJWT token to 2048 characters.  If you exceed this limit, App Services\nlogs an error and the ticket is not processed. Field Description If  true  , the metadata field is required for all\nusers associated with the provider. The JWT\nreturned by the external system must have a\nvalue assigned to the field designated by  Path . The name or path to a field in the JWT that\ncontains the value for the metadata field. To specify the path to a field in\nan embedded object, use  dot notation . Optional. A name for the metadata field in the user object's  data \ndocument that maps to the JWT  Path . This field name\nmust be less than 64 characters long. Default Value Rules If this field is not specified, the name defaults to the name of\nthe JWT field that contains the value. If this field is not specified and the  Path  value uses dot notation,\nthe default name will be the last part of the notation. For example, if\nyou specify a path of  location.primary.city , the default value for\nthe name is  city . To define a metadata field, click  Add Field  and\nspecify the mapping between the metadata field in the JWT and its\ncorresponding field name in the user object. To define a metadata field in a Custom JWT authentication\nconfiguration file, add an entry for the field to the\n metadata_fields  array. Each entry should be a document of the\nfollowing form: Use a backslash ( \\ ) to escape period ( . ) characters in JWT keys. For example, in this JSON object, you represent the  \"nested_key\"  in\nthe path name as  valid\\.json\\.key\\.nested_key . An external authentication system returns JWTs that include\nadditional information about each user in the  user_data  field: To include the values from the  user_data  field in each user's\n user object , specify the following\nmetadata fields in your App Services configuration: The  user object , will now include those fields: Path Field Name user_data.name name user_data.aliases aliases The  Audience  of a JWT specifies the intended recipient of the token.\nJWTs describe their audience in the  aud  claim. App Services\nexpects  aud  to contain the App ID of the App for which the provider\nis configured. However, if the external authentication system JWT specifies a\ndifferent  aud  value, then you can configure the provider to use that value\ninstead. There are two fields you configure: Field Description Audience A single value or comma-separated list of values for the\naudience or audiences expected to be found in a client JWT. Require If you provide multiple audiences, you must\nspecify how to handle them. Your options are: All of these audiences : the JWT must include every audience in\nthe list. Any one of these audiences : the JWT only needs to include one\naudience from the list. To set an audience, configure  config.audience . There are two fields\nyou configure: Field Description audience An array of strings specifying the audience or audiences expected\nto be found in a client JWT. requireAnyAudience Boolean. If  false , the JWT must include  all  of the listed\naudiences. If  true , the JWT must only include one or\nmore of the listed audiences. You can register new Custom JWT users and log them in using one of the Realm SDKs\nor an API service. For code examples that demonstrate how to register and log in using\nCustom JWT authentication, see the Realm SDK documentation for your\npreferred language and platform: C++ SDK Flutter SDK Java SDK Kotlin SDK .NET SDK Node SDK React Native SDK Swift SDK Web SDK You can authenticate Data API requests using the Custom JWT\nprovider. You can either require users to create accounts before using a\nservice, or configure your API endpoints to automatically create a new\nuser account if the request contains a valid JWT that does not match an\nexisting user. There are two approaches to using JWTs with your service APIs: For more information, see  Authenticate Data API Requests . specify the JWT directly in the  jwtTokenString  request header start a user session with the JWT and include the session access token\nas an  Authorization  header bearer token.",
            "code": [
                {
                    "lang": "none",
                    "value": "{\n  \"custom-token\": {\n    \"name\": \"custom-token\",\n    \"type\": \"custom-token\",\n    \"config\": {\n      \"audience\": \"<JWT Audience>\",\n      \"requireAnyAudience\": <boolean>,\n      \"signingAlgorithm\": \"<JWT Signing Algorithm>\",\n      \"useJWKURI\": <boolean>,\n      \"jwkURI\": \"<JWK or JWKS URL>\",\n    },\n    \"secret_config\": {\n      \"signingKeys\": [\n        \"<Signing Key Secret Name>\",\n        ...\n      ]\n    },\n    \"metadata_fields\": [\n      {\n        \"required\": <boolean>,\n        \"name\": \"<JWT Field Path>\",\n        \"field_name\": \"<Metadata Field Name>\",\n      },\n      ...\n    ],\n    \"disabled\": <boolean>\n  }\n}"
                },
                {
                    "lang": "sh",
                    "value": "231a58b00632c9c4d8ac02b268ca4caf8dd48fd020e3dffa72666523d860988f"
                },
                {
                    "lang": "javascript",
                    "value": "\"config\": {\n  \"signingAlgorithm\": \"<JWT Signing Algorithm>\",\n},\n\"secret_config\": {\n  \"signingKeys\": [\n    \"<Signing Key Secret Name>\",\n    ...\n  ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "\"config\": {\n  \"useJWKURI\": <boolean>,\n  \"jwkURI\": \"<JWK or JWKS URL>\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  required: <boolean>,\n  name: \"<field path>\",\n  field_name: \"<metadata field name>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"valid.json.key\": {\n    \"nested_key\": \"val\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "(JWT JSON)\n\n{\n  \"aud\": \"myapp-abcde\",\n  \"exp\": 1516239022,\n  \"sub\": \"24601\",\n  \"user_data\": {\n    \"name\": \"Jean Valjean\",\n    \"aliases\": [\n      \"Monsieur Madeleine\",\n      \"Ultime Fauchelevent\",\n      \"Urbain Fabre\"\n    ]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "(USER METADATA OBJECT)\n\n{\n  \"id\": \"59fdd02846244cdse5369ebf\",\n  \"type\": \"normal\",\n  \"data\": {\n    \"name\": \"Jean Valjean\",\n    \"aliases\": [\n      \"Monsieur Madeleine\",\n      \"Ultime Fauchelevent\",\n      \"Urbain Fabre\"\n    ]\n  },\n  identities: [\n    {\n      \"id\": \"24601\",\n      \"provider_type\": \"custom-token\",\n      \"data\": {\n        \"name\": \"Jean Valjean\",\n        \"aliases\": [\n          \"Monsieur Madeleine\",\n          \"Ultime Fauchelevent\",\n          \"Urbain Fabre\"\n        ]\n      },\n    }\n  ]\n}"
                },
                {
                    "lang": "json",
                    "value": "\"config\": {\n  \"audience\": [\n    \"<JWT Audience>\",\n  ],\n  \"requireAnyAudience\": <boolean>,\n}"
                }
            ],
            "preview": "The Custom JWT authentication provider allows users to log in with an\nauthentication credential from a third-party system (external to Atlas\nApp Services) and use that token to access data and services with App Services.\nThe external system must return a signed JSON Web\nToken (JWT) that contains a unique ID value for the\nauthenticated user.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "authentication/okta-jwt",
            "title": "Okta JWT Authentication (Custom JWT)",
            "headings": [
                "Before You Begin",
                "Create an Okta App & Authorization Server",
                "Configure the Custom JWT authentication provider",
                "Log in with an Okta JWT"
            ],
            "paragraphs": "You can configure the  Custom JWT authentication provider  to authenticate users that you manage with\n Okta . You will need the following to use Okta: An Okta project. To learn more, refer to the  Okta documentation . An App Services App that does not already use Custom JWT\nauthentication. To learn how to create a new App Services App, see\n Create an App . If you're using the command line interface, you need  App Services CLI  to be installed and authenticated on your local system. If you're using the Admin API, you need a MongoDB Atlas Admin API\n public/private key pair . The API key\nmust have  Project Owner  permissions. Create an application in Okta that represents your client application.\nThe type of application you create depends on your use case. For\nexample, if you're building a web browser app, you might create a\nSingle-Page Application (SPA) or Web application in Okta. Once you've configured the application, create an authorization server\nin Okta that represents your App Services App. You can use any name and\ndescription. Set the server  Audience  to your App Services\nApp's Client App ID. For example,  myapp-abcde . To learn more about how to set up an Okta application and authorization\nserver, refer to  Create an authorization server \nin the Okta documentation. You can configure Custom JWT authentication from the UI or by modifying\nthe underlying configuration files directly with the CLI or Admin API.\nChoose your preferred method below. In the left navigation menu, click  Authentication .\nThen click the  Authentication Providers  tab and select\nthe  Custom JWT  provider. Now you can configure the Custom JWT authentication provider to\nwork with your Okta project. Click the toggle to enable the provider. Set  Verification Method  to  Use a JWK\nURI . Specify your Okta Authorization Server's JWK URI in the\n JWK URI  field. Your Okta JWK URI should resemble the following: You can get your exact JWK URI from the Okta UI by following\nthe  Metadata URI  link for your Authorization\nserver. Use the value listed in the  jwks_uri  field. Define  Metadata Fields  to map data from the Okta\nJWT to the corresponding App Services user account. You do not have to map metadata fields from the Okta JWT.\nHowever, you might find them useful for getting user\ninformation from Okta into your App. To learn more about\nmetadata fields and how to configure them, see  Custom JWT\nmetadata fields  . Leave the value of  Audience  blank. Click  Save  and deploy your changes Run the following command, replacing the value of  --remote \nwith your App's Client App ID. This downloads a local copy of your\nApp's latest configuration files and navigates to the\nconfiguration file directory, which uses the same name as your\nApp. Add a new Custom JWT authentication provider to your App's\n /auth/providers.json  file. Use the following configuration as\na template. Make sure to: Save your changes to  /auth/providers.json . Then, push the\nupdated configuration file to deploy your App: Replace the  jwkURI  value with your Okta Authorization\nServer's JWK URI. Define  Custom JWT metadata fields  to\nmap data from the Okta JWT. This is optional, however, you might\nfind the field mapping useful for getting user information from\nOkta into your App. Add a new Custom JWT authentication provider to your App using the\n Create an authentication provider  endpoint. Use the following configuration as a template. Make sure to: Specify your App's  $PROJECT_ID  and  $APP_ID Include an Admin API access token in the  Authorization \nheader. Replace the  jwkURI  value in the request body with your Okta\nAuthorization Server's JWK URI. Define  Custom JWT metadata fields  to\nmap data from the Okta JWT. This is optional, however, you might\nfind the field mapping useful for getting user information from\nOkta into your App. Once you've configured the Custom JWT authentication provider to use\nOkta, you can log in to your App Services App with an Okta JWT access\ntoken. Log the user into Okta. To learn how, see the relevant  Okta SDK\ndocumentation  for your platform\nand programming language. Get the user's Okta access token from the login response. Use the Okta access token to authenticate with Atlas App Services.\nYou can  start a session over HTTP  or\nlog in with an SDK. To learn how, see the docs for your SDK: Custom JWT Authentication - Flutter SDK Custom JWT Authentication - Java SDK Custom JWT Authentication - Kotlin SDK Custom JWT Authentication - .NET SDK Custom JWT Authentication - Node.js SDK Custom JWT Authentication - React Native SDK Custom JWT Authentication - Swift SDK Custom JWT Authentication - Web SDK",
            "code": [
                {
                    "lang": null,
                    "value": "https://<Your Okta Domain>/oauth2/<Your Authorization Server ID>/v1/keys"
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote \"myapp-abcde\"\ncd myapp"
                },
                {
                    "lang": "bash",
                    "value": "appservices push"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"custom-token\": {\n    \"name\": \"custom-token\",\n    \"type\": \"custom-token\",\n    \"disabled\": false,\n    \"config\": {\n      \"audience\": [],\n      \"jwkURI\": \"https://<Your Okta Domain>/oauth2/<Your Authorization Server ID>/v1/keys\",\n      \"useJWKURI\": true\n    },\n    \"secret_config\": {\n      \"signingKeys\": []\n    },\n    \"metadata_fields\": []\n  }\n}\n"
                },
                {
                    "lang": "bash",
                    "value": "curl \"https://services.cloud.mongodb.com/api/admin/v3.0/groups/$PROJECT_ID/apps/$APP_ID/auth_providers\" \\\n  -X \"POST\" \\\n  -H \"Authorization: Bearer $ACCESS_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"custom-token\",\n    \"type\": \"custom-token\",\n    \"disabled\": false,\n    \"config\": {\n      \"audience\": [],\n      \"jwkURI\": \"https://<Your Okta Domain>/oauth2/<Your Authorization Server ID>/v1/keys\",\n      \"useJWKURI\": true\n    },\n    \"secret_config\": {\n      \"signingKeys\": []\n    },\n    \"metadata_fields\": []\n  }'\n"
                }
            ],
            "preview": "You can configure the Custom JWT authentication provider to authenticate users that you manage with\nOkta.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "authentication/google",
            "title": "Google Authentication",
            "headings": [
                "Overview",
                "Set Up a Project in the Google API Console",
                "Create a Project in the Google API Console",
                "Generate OAuth Client Credentials",
                "Configure the Google Authentication Provider",
                "Configure in App Services",
                "Examples"
            ],
            "paragraphs": "The Google authentication provider allows users to log in with their\nexisting Google account through  Google Sign-In . When a user logs in, Google\nprovides Atlas App Services with an  OAuth 2.0 access token  for the user.\nApp Services uses the token to identify the user and access approved data from\nGoogle APIs on their behalf. The following diagram shows the OAuth logic flow: You must set up a project in the Google API Console before you  configure\nGoogle Authentication in App Services . Follow the\nSDK-specific steps to set up Google Authentication in the Google API Console. The Google authentication provider requires a  project in the Google\nAPI Console  to manage authentication and user permissions. The\nfollowing steps walk through creating the project, generating OAuth\ncredentials, and configuring the provider to connect with the project. Follow Google's  official guide  to create a new\nGCP project. Follow Google's support guide on  Setting up OAuth 2.0  for your\nproject. For iOS client applications, you need to create both a Web\nOAuth Client ID and an iOS OAuth Client ID. The former is\nused by App Services, while the latter will be used by the app\nitself. Refer to the  Web  tab of this section for\ninstructions on creating the web application Client ID for\nApp Services. Use the following values when configuring your iOS application\n Client ID : Application Type iOS Name The name you wish to associate with this  Client ID . Bundle ID The Bundle ID for your iOS application. You can find\nthis value in XCode on the  General  tab for\nthe app's primary target. Use the following values when configuring your Android\napplication  Client ID : Application Type Android Name The name you wish to associate with this  Client ID . Signing-certificate Fingerprint The SHA-1 fingerprint of your application signing\ncertificate. See  Authenticating Your\nClient  for instructions\non generating this value. You will need to create a web application  Client ID \nand provide several App Services-related values. For  Authorized JavaScript Origins , enter the\nfollowing URL: For  Authorized Redirect URIs , enter the App Services\nauthentication callback URL that corresponds to the\n deployment region \nof your application. The following table lists the callback URL\nfor each region: Region App Services Authentication Callback URL To connect your GCP project to App Services add the OAuth 2.0\n Client ID  and  Client Secret  you generated in\nthe previous step to your authentication provider  configuration . Make sure that you add the  web application  credentials\nto the provider configuration. If you add the iOS\ncredentials instead, Google authentication will fail. The Google authentication provider has the following configuration\noptions: You can enable and configure the Google authentication provider\nfrom the App Services UI by selecting  Google  from the\n Authentication  page. To enable and configure the Google authentication provider with\nthe  App Services CLI , define a  configuration\nobject  for it in  /auth/providers.json . Google provider configurations have the following form: Field Description Required. An  OAuth 2.0 Client ID  for your project\nin the Google API Console. See  Set Up a Project in the Google API Console  for information about setting\nup OAuth Credentials for your GCP project. Required. The name of a  Secret  that stores\nan  OAuth 2.0 Client Secret  for your project from the\nGoogle API Console. See  Set Up a Project in the Google API Console  for information about setting\nup OAuth Credentials for your GCP project. Optional. A list of fields describing the authenticated user that\nyour application will request from the  Google Identity API . All metadata fields are omitted by default and can be required on\na field-by-field basis. Users must explicitly grant your app\npermission to access each required field.\nIf a metadata field is required and exists for a particular user,\nit will be included in their user object. To require a metadata field from an import/export\nconfiguration file, add an entry for the field to the\n metadata_fields  array. Each entry should be a document of\nthe following form: Required for web applications.\nA list of allowed redirect  URIs (Uniform Resource\nIdentifiers) . Once a user completes the authentication process on Google, App Services\nredirects them back to either a specified redirect URI or, if no\nredirect URI is specified, the URL that they initiated the\nauthentication request from. App Services will only redirect a user to a\nURI that exactly matches an entry in this list, including the\nprotocol and any trailing slashes. Optional. A list of approved  domains \nfor user accounts. If specified, the provider checks the domain of a user's primary\nemail address on Google and only allows them to authenticate if\nthe domain matches an entry in this list. For example, if  example1.com  and  example2.com  are listed,\na Google user with a primary email of  joe.mango@example1.com \nwould be allowed to log in, while a user with a primary email of\n joe.mango@example3.com  would not be allowed to log in. If you've specified any domain restrictions, you must also\nrequire the email address field in the  Metadata\nFields  setting. Optional. Default  false . If  true , the provider uses  OpenID Connect  to authenticate users. Refer to SDK-specific documentation about support for OpenID Connect.\nNot all SDK implementations of Google Authentication support OpenID Connect,\nwhile others require it. Google supports OpenID as part of their OAuth 2.0 implementation but does\nnot provide access to restricted scopes for OpenID authentication. This\nmeans that App Services cannot access metadata fields for users authenticated\nwith OpenID Connect. For code examples that demonstrate how to register and log in using\nGoogle authentication, see the documentation for the Realm SDKs: C++ SDK Flutter SDK Java SDK Kotlin SDK .NET SDK Node SDK React Native SDK Swift SDK Web SDK",
            "code": [
                {
                    "lang": "none",
                    "value": "https://services.cloud.mongodb.com"
                },
                {
                    "lang": "text",
                    "value": "https://services.cloud.mongodb.com/api/client/v2.0/auth/callback"
                },
                {
                    "lang": "text",
                    "value": "https://us-east-1.aws.services.cloud.mongodb.com/api/client/v2.0/auth/callback"
                },
                {
                    "lang": "text",
                    "value": "https://us-west-2.aws.services.cloud.mongodb.com/api/client/v2.0/auth/callback"
                },
                {
                    "lang": "text",
                    "value": "https://eu-west-1.aws.services.cloud.mongodb.com/api/client/v2.0/auth/callback"
                },
                {
                    "lang": "text",
                    "value": "https://eu-central-1.aws.services.cloud.mongodb.com/api/client/v2.0/auth/callback"
                },
                {
                    "lang": "text",
                    "value": "https://ap-south-1.aws.services.cloud.mongodb.com/api/client/v2.0/auth/callback"
                },
                {
                    "lang": "text",
                    "value": "https://ap-southeast-1.aws.services.cloud.mongodb.com/api/client/v2.0/auth/callback"
                },
                {
                    "lang": "text",
                    "value": "https://ap-southeast-2.aws.services.cloud.mongodb.com/api/client/v2.0/auth/callback"
                },
                {
                    "lang": "none",
                    "value": "{\n  \"oauth2-google\": {\n    \"name\": \"oauth2-google\",\n    \"type\": \"oauth2-google\",\n    \"disabled\": <boolean>,\n    \"config\": {\n      \"clientId\": <string>,\n      \"openId\": <boolean>\n    },\n    \"secret_config\": {\n      \"clientSecret\": <string>\n    },\n    \"metadata_fields\": [<document>, ...],\n    \"redirect_uris\": [<string>, ...],\n    \"domain_restrictions\": [<string>, ...]\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{ name: \"<metadata field name>\", required: \"<boolean>\" }"
                }
            ],
            "preview": "The Google authentication provider allows users to log in with their\nexisting Google account through Google Sign-In. When a user logs in, Google\nprovides Atlas App Services with an OAuth 2.0 access token for the user.\nApp Services uses the token to identify the user and access approved data from\nGoogle APIs on their behalf.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "triggers/disable",
            "title": "Disable a Trigger",
            "headings": [
                "Overview",
                "Find the Trigger",
                "Disable the Trigger",
                "Deploy Your Changes",
                "Pull Your App's Latest Configuration Files",
                "Verify that the Trigger Configuration File Exists",
                "Disable the Trigger",
                "Deploy Your Changes",
                "Restoring from a Snapshot"
            ],
            "paragraphs": "Triggers may enter a  suspended  state in response to\nan event that prevents the Trigger's change stream from continuing, such\nas a network disruption or change to the underlying cluster. When a\nTrigger enters a suspended state, it does not receive change events and will not\nfire. You can suspend a Trigger from the Atlas App Services UI or by\nimporting an application directory with the  App Services CLI . In the event of a suspended or failed trigger, Atlas App Services sends the\nproject owner an email alerting them of the issue. On the  Database Triggers  tab of the  Triggers \npage, find the trigger that you want to disable in the list of\nTriggers. Switch the  Enabled  toggle to the \"off\" setting. If Development Mode is not enabled, press the\n review draft & deploy  button to release your changes. If you exported a new copy of your application, it should already include an\nup-to-date configuration file for the suspended trigger. You can confirm that\nthe configuration file exists by looking in the  /triggers  directory for a\n trigger configuration file  with the same name\nas the trigger. After you have verified that the trigger configuration file exists, add\na field named  \"disabled\"  with the value  true  to the top level\nof the trigger json definition: Finally, push the configuration back to your app: Consider the following scenario: In this case, the trigger picks up all of the newly-added documents and fires\nfor each document. It will not fire again for events that have already been\nprocessed. A database trigger is disabled or suspended. New documents are added while the trigger is disabled. The database is restored from a snapshot to a time prior to the new documents\nbeing added. The database trigger is restarted. If a previously-enabled database trigger is running during snapshot restoration,\nyou will see an error in the Edit Trigger section of the Atlas UI because the\ntrigger cannot connect to the Atlas cluster during the restore process. Once\nsnapshot restoration completes, the error disappears and the trigger continues\nto execute normally.",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices pull --remote=<App ID>"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"id\": \"6142146e2f052a39d38e1605\",\n   \"name\": \"steve\",\n   \"type\": \"SCHEDULED\",\n   \"config\": {\n      \"schedule\": \"*/1 * * * *\"\n   },\n   \"function_name\": \"myFunc\",\n   \"disabled\": true\n}"
                },
                {
                    "lang": "shell",
                    "value": "appservices push"
                }
            ],
            "preview": "Triggers may enter a suspended state in response to\nan event that prevents the Trigger's change stream from continuing, such\nas a network disruption or change to the underlying cluster. When a\nTrigger enters a suspended state, it does not receive change events and will not\nfire.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "authentication/apple",
            "title": "Apple ID Authentication",
            "headings": [
                "Overview",
                "What You'll Need",
                "Configure Apple ID Authentication",
                "Add Sign-in With Apple to Your App",
                "Create a Private Key",
                "Create the Client Secret JWT",
                "Configure the Provider in App Services",
                "Create an App ID",
                "Create a Services ID",
                "Create a Private Key",
                "Create the Client Secret JWT",
                "Configure the Provider in App Services",
                "Examples"
            ],
            "paragraphs": "The Apple ID authentication provider allows users to log in with their\nApple ID credentials through  Sign in with Apple . This authentication\nmethod uses the industry-standard  OAuth 2.0 \nauthorization protocol. When a user successfully logs in through Sign in with Apple, Apple\nreturns a credential object that includes a  JSON Web Token  that the Apple ID provider uses to authenticate the\nuser. If the user has granted permissions to your app, the credential\nobject may also include the user's name and email address. For additional information on how to implement Sign in with Apple, check\nout: The following diagram shows the OAuth logic flow: The official  Sign in with Apple documentation \non Apple's Developer Portal The  Introducing Sign In with Apple \nsession from WWDC 2019 The associated  reference application . An iOS app that uses Apple ID authentication must target iOS 13 or newer. Before you can implement Apple ID authentication you will need the\nfollowing: An active  Apple Developer Program  account. Xcode 11  or newer. When using Sign-in with Apple with Atlas App Services, you can configure\nthe Apple authentication provider for  either  a mobile application or a\nweb application. If you would like to use Sign in with Apple for both, you could set up your\nown manual Sign in with Apple flow for either the web or mobile app. Then,\nuse the JWT that returns with the  Custom JWT authentication provider . In the  Realm SDKs , you\ncan then link the user identities for each authentication provider to a\nsingle user. To use Sign in with Apple exclusively with either a web or a mobile application, choose your application type and follow this guide. Your app must have the  Sign-in with Apple  entitlement in\norder to use this capability. Then, confirm it is enabled in the Apple Developer Portal. Note the  Bundle ID  of your app. You need the Bundle\nID when you create a client secret and when you configure Sign in\nwith Apple in App Services. Select your app target in Xcode. Go to the  Signing & Capabilities  tab, and\nselect  + Capability . Search for the  Sign in with Apple  capability, and select it. Navigate to the  Certificates, Identifiers and Profiles \npage of the  Apple Developer Portal . Select the identifier for your app from the dropdown. This\ntakes you to your  App ID Configuration  pane. Scroll down until you see the checkbox for  Sign In\nwith Apple . If that checkbox is not selected, select it. If\nyou've made changes, press the  Save  button. The client secret for Sign in with Apple is a JSON Web Token that you\ncreate and sign with a  private key . You need to generate the\nprivate key through the Apple Developer Portal. Click  Keys  in the left navigation menu. Click the blue plus icon next to  Keys . On the  Register a New Key  page, enter a descriptive\n Key Name  and then scroll down to find the\n Sign in with Apple  row. Check the checkbox to enable\nSign in with Apple and then click  Configure . On the  Configure Key  page, select the App ID for your\napp as the  Primary App ID  and then click\n Save . Click  Continue  to review your key configuration. When\nyou're sure that you've configured the key correctly, click\n Register . Copy the  Key ID  value somewhere that you can access it\nlater and then click  Download  to download the key as a\n .p8  text file. You will use these to generate the client\nsecret. You can only download the key one time. Make sure that you save\nthe key somewhere safe in case you need it again. If you lose\nthe key, you will need to generate a new one. You can now create the client secret JWT for the Apple ID\nauthentication provider. Make sure that you have the following\ninformation: Once you've confirmed that you have all the required information, you can use\na script to generate the JWT. You may define your own script or use the\nscript in this step. To generate the JWT, we'll use the  jwt  gem. To install\nit, run the following: Create a new file called  generate_client_secret.rb  and copy the following\ncode block into the file. Update the values of  team_id ,  client_id ,  key_id , and\n key_file  to match your application's information and then save\nthe file. When you're ready to generate the JWT, run the script in\nyour shell: The  Bundle ID  of your app. You'll use this\nas the  client_id  in the script below. The  Key ID  of the key that you created and the  .p8 \nfile that contains the key. Your Apple Team ID. You can find this in the top right of the Apple\nDeveloper Portal. The  generate_client_secret.rb  script outputs to stdout.\nWhen we call it, we append stdout to a file called\n client_secret.txt . You will need the JWT to configure\nthe Apple ID provider in App Services. At this point you have configured an Apple application and generated\nthe required OAuth 2.0 credentials. You can now configure the Apple\nID authentication provider with the credentials to allow App Services\nclient application users to log in. Click  Authentication  in the left navigation menu and\nthen click  Apple ID . Turn on the  Provider Enabled  toggle. For the App Services  Client ID , enter your\napplication's  Bundle ID . For  Client Secret , create a new  secret  with a descriptive name and set the\n Client Secret Value  to the JWT string that you\ngenerated. Alternatively, you can select a pre-existing\nsecret that contains the JWT. Click  Save  to finish configuring the provider.\nTo make the provider available to client applications, you\nneed to deploy your changes. Click  Review &\nDeploy Changes  and then click  Deploy . To enable and configure the Apple authentication provider with\nappservices, define a  configuration\nobject  for it in  /auth/providers.json . Apple provider configurations have the following form: Once you've created the configuration file, you can make the\nApple ID authentication provider available to client\napplications by deploying your application. To deploy a draft application with App Services CLI: To deploy a draft application with automatic  GitHub deployment : Field Description Client ID Required. Your application's  Bundle ID . Client Secret Required. The name of a  Secret  that stores\nthe  Client Secret  JWT that you generated. Redirect URIs Required for web applications. Not required for mobile\napplications. An Apple  App ID  represents your application and allows you to\naccess services like Sign in with Apple. To configure the Apple ID\nprovider, you must create a new App ID. Navigate to the  Certificates, Identifiers and Profiles \npage of the  Apple Developer Portal . Click  Identifiers  in the left navigation menu. Click the blue plus icon next to  Identifiers . On the  Register a New Identifier  page, select\n App IDs  and then click  Continue . On the  Register an App ID  page, select\nthe  Platform  that your app runs on and then enter a\nbrief  Description  and a  reverse-dns\nnotation   Bundle ID . Scroll down the  Register an App ID  page until you see\nthe  Sign in with Apple  capability. Check the checkbox\nto enable the capability. Press the  Continue  button at the top of the page. Complete\nany other setup steps that apply to your app, and then press the\n Register  button. An Apple  Services ID  represents a single application and allows\nyou to configure an authorization callback URL and define a private\nkey for the application. Click  Identifiers  in the left navigation menu. Click the blue plus icon next to  Identifiers . On the  Register a New Identifier  page, select\n Services IDs  and then click  Continue . On the  Register a Services ID  page, enter a brief\n Description  and a  reverse-dns notation   Identifier . Press the  Continue  button. Confirm the details, and\nthen press  Register . The  Identifier  value of the Services ID is your\napplication's  Client ID . You will need this value\nlater to configure the Apple ID provider in Atlas App Services. Click into the service you just created. Check the checkbox to enable\n Sign in with Apple  and then click  Configure .\nSelect the App ID that you created as the  Primary App ID . Enter your domains, subdomains, and return URLs for the Services ID.\nPress the  Next  button. Click  Continue  and then click  Save .\nConfirm that you have correctly configured the Services ID and\nthen click  Register . The client secret for Sign in with Apple is a JSON Web Token that you\ncreate and sign with a  private key . You need to generate the\nprivate key through the Apple Developer Portal. Click  Keys  in the left navigation menu. Click the blue plus icon next to  Keys . On the  Register a New Key  page, enter a descriptive\n Key Name  and then scroll down to find the\n Sign in with Apple  row. Check the checkbox to enable\nSign in with Apple and then click  Configure . On the  Configure Key  page, select the App ID that you\ncreated as the  Primary App ID  and then click\n Save . Click  Continue  to review your key configuration. When\nyou're sure that you've configured the key correctly, click\n Register . Copy the  Key ID  value somewhere that you can access it\nlater and then click  Download  to download the key as a\n .p8  text file. You will use these to generate the client\nsecret. You can only download the key one time. Make sure that you save\nthe key somewhere safe in case you need it again. If you lose\nthe key, you will need to generate a new one. You can now create the client secret JWT for the Apple ID\nauthentication provider. Make sure that you have the following\ninformation: Once you've confirmed that you have all the required information, you can use\na script to generate the JWT. You may define your own script or use the\nscript in this step. To generate the JWT, we'll use the  jwt  gem. To install\nit, run the following: Create a new file called  generate_client_secret.rb  and copy the following\ncode block into the file. Update the values of  team_id ,  client_id ,  key_id , and\n key_file  to match your application's information and then save\nthe file. When you're ready to generate the JWT, run the script in\nyour shell: The  Services ID  that you created. You'll use this\nas the  client_id  in the script below. file that contains the key. Developer Portal. The  generate_client_secret.rb  script outputs to stdout.\nWhen we call it, we append stdout to a file called\n client_secret.txt . You will need the JWT to configure\nthe Apple ID provider in App Services. At this point you have configured an Apple application and generated\nthe required OAuth 2.0 credentials. You can now configure the Apple\nID authentication provider with the credentials to allow App Services\nclient application users to log in. Click  Authentication  in the left navigation menu and\nthen click  Apple ID . Turn on the  Provider Enabled  toggle. For the App Services  Client ID , enter the\nApple  Services ID  you got when you created a\n Services ID  in step 2 above. For  Client Secret , create a new  secret  with a descriptive name and set the\n Client Secret Value  to the JWT string that you\ngenerated. Alternatively, you can select a pre-existing\nsecret that contains the JWT. For  Redirect URIs , click  Add Redirect\nURI  and enter the URL that App Services should redirect to once\nthe OAuth process is complete. Provide a URL for a domain\nthat you control and then use a  universal link \nto redirect the user back to your app. Click  Save  to finish configuring the provider.\nTo make the provider available to client applications, you\nneed to deploy your changes. Click  Review &\nDeploy Changes  and then click  Deploy . To enable and configure the Apple authentication provider with\n appservices , define a  configuration\nobject  for it in  /auth/providers.json . Apple provider configurations have the following form: Once you've created the configuration file, you can make the\nApple ID authentication provider available to client\napplications by deploying your application. To deploy a draft application with App Services CLI: To deploy a draft application with automatic  GitHub deployment : Field Description Client ID Required. The Apple  Services ID  that you created\nwhen you completed step 2 above. Client Secret Required. The name of a  Secret  that stores\nthe  Client Secret  JWT that you generated. Redirect URIs Required for web applications.\nA list of allowed redirect  URIs (Uniform Resource\nIdentifiers) . Once a user completes the authentication process,\nApp Services redirects them back to either a specified redirect URI or,\nif no redirect URI is specified, the URL that they initiated the\nauthentication request from. App Services will only redirect a user to\na URI that exactly matches an entry in this list, including the\nprotocol and any trailing slashes. Provide a URL for a domain that you control and then use\na  universal link \nto redirect the user back to your app. For code examples that demonstrate how to register and log in using\nApple authentication, see the documentation for the Realm SDKs: C++ SDK Flutter SDK Java SDK Kotlin SDK .NET SDK Node SDK React Native SDK Swift SDK Web SDK",
            "code": [
                {
                    "lang": "shell",
                    "value": "gem install jwt"
                },
                {
                    "lang": "shell",
                    "value": "ruby generate_client_secret.rb >> client_secret.txt"
                },
                {
                    "lang": "ruby",
                    "value": "# Source: https://developer.okta.com/blog/2019/06/04/what-the-heck-is-sign-in-with-apple\nrequire 'jwt'\n\n# Update these values with your app's information\nteam_id = '<Apple Team ID>'\nclient_id = '<Bundle ID or Services ID>'\nkey_id = '<Apple Key ID>'\nkey_file = '<Key File Path>'\n\n# Define the JWT's headers and claims\nheaders = {\n  # The token must be signed with your key\n  'kid' => key_id\n}\nclaims = {\n  # The token is issued by your Apple team\n  'iss' => team_id,\n  # The token applies to Apple ID authentication\n  'aud' => 'https://appleid.apple.com',\n  # The token is scoped to your application\n  'sub' => client_id,\n  # The token is valid immediately\n  'iat' => Time.now.to_i,\n  # The token expires in 6 months (maximum allowed)\n  'exp' => Time.now.to_i + 86400*180,\n}\n\n# Read in the key and generate the JWT\necdsa_key = OpenSSL::PKey::EC.new IO.read key_file\ntoken = JWT.encode claims, ecdsa_key, 'ES256', headers\n\n# Print the JWT to stdout\nputs token\n"
                },
                {
                    "lang": "none",
                    "value": "{\n  \"oauth2-apple\": {\n     \"name\": \"oauth2-apple\",\n     \"type\": \"oauth2-apple\",\n     \"disabled\": <boolean>,\n     \"config\": {\n        \"clientId\": \"<Bundle ID>\"\n     },\n     \"secret_config\": {\n        \"clientSecret\": \"<Secret Name>\"\n     },\n     \"redirect_uris\": [\"<string>\", ...]\n  }\n}"
                },
                {
                    "lang": "shell",
                    "value": "appservices push"
                },
                {
                    "lang": "shell",
                    "value": "git add ./auth/providers.json\ngit commit -m \"Configure and Enable Apple ID Authentication\"\ngit push origin <branch name>"
                },
                {
                    "lang": "shell",
                    "value": "gem install jwt"
                },
                {
                    "lang": "shell",
                    "value": "ruby generate_client_secret.rb >> client_secret.txt"
                },
                {
                    "lang": "ruby",
                    "value": "# Source: https://developer.okta.com/blog/2019/06/04/what-the-heck-is-sign-in-with-apple\nrequire 'jwt'\n\n# Update these values with your app's information\nteam_id = '<Apple Team ID>'\nclient_id = '<Bundle ID or Services ID>'\nkey_id = '<Apple Key ID>'\nkey_file = '<Key File Path>'\n\n# Define the JWT's headers and claims\nheaders = {\n  # The token must be signed with your key\n  'kid' => key_id\n}\nclaims = {\n  # The token is issued by your Apple team\n  'iss' => team_id,\n  # The token applies to Apple ID authentication\n  'aud' => 'https://appleid.apple.com',\n  # The token is scoped to your application\n  'sub' => client_id,\n  # The token is valid immediately\n  'iat' => Time.now.to_i,\n  # The token expires in 6 months (maximum allowed)\n  'exp' => Time.now.to_i + 86400*180,\n}\n\n# Read in the key and generate the JWT\necdsa_key = OpenSSL::PKey::EC.new IO.read key_file\ntoken = JWT.encode claims, ecdsa_key, 'ES256', headers\n\n# Print the JWT to stdout\nputs token\n"
                },
                {
                    "lang": "none",
                    "value": "{\n  \"oauth2-apple\": {\n     \"name\": \"oauth2-apple\",\n     \"type\": \"oauth2-apple\",\n     \"disabled\": <boolean>,\n     \"config\": {\n        \"clientId\": \"<Apple Services ID>\"\n     },\n     \"secret_config\": {\n        \"clientSecret\": \"<Secret Name>\"\n     },\n     \"redirect_uris\": [\"<string>\", ...]\n  }\n}"
                },
                {
                    "lang": "shell",
                    "value": "appservices push"
                },
                {
                    "lang": "shell",
                    "value": "git add ./auth/providers.json\ngit commit -m \"Configure and Enable Apple ID Authentication\"\ngit push origin <branch name>"
                }
            ],
            "preview": "The Apple ID authentication provider allows users to log in with their\nApple ID credentials through Sign in with Apple. This authentication\nmethod uses the industry-standard OAuth 2.0\nauthorization protocol.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "triggers/authentication-triggers",
            "title": "Authentication Triggers",
            "headings": [
                "Create an Authentication Trigger",
                "Configuration",
                "Authentication Events",
                "Example",
                "Additional Examples"
            ],
            "paragraphs": "An authentication trigger fires when a user interacts with an\n authentication provider . You can\nuse authentication triggers to implement advanced user management. Some uses include: Storing new user data in your linked cluster Maintaining data integrity upon user deletion Calling a service with a user's information when they log in. To open the authentication trigger configuration screen in the Atlas App Services UI,\nclick  Triggers  in the left navigation menu, select the\n Authentication Triggers  tab, and then click  Add a\nTrigger . Configure the trigger and then click  Save  at the bottom of the\npage to add it to your current deployment draft. To create an authentication trigger with  App Services CLI : Add an authentication trigger  configuration file  to the  triggers  subdirectory of a\nlocal application directory. App Services does not enforce specific filenames for Atlas Trigger\nconfiguration files. However, once imported, App Services will\nrename each configuration file to match the name of the\ntrigger it defines, e.g.  mytrigger.json . Deploy  the trigger: Authentication Triggers have the following configuration options: Field Description Trigger Type The type of the trigger. For authentication triggers,\nset this value to  AUTHENTICATION . Trigger Name The name of the trigger. Linked Function The name of the function that the trigger\nexecutes when it fires. An  authentication\nevent object  causes the trigger to fire.\nThis object is the only argument the trigger passes to the function. Operation Type The  authentication operation\ntype  that causes the trigger to\nfire. Providers A list of one or more  authentication provider  types. The trigger only listens for\n authentication events  produced by these\nproviders. Authentication events represent user interactions with an authentication\nprovider. Each event corresponds to a single user action with one of the\nfollowing operation types: Authentication event objects have the following form: Operation Type Description LOGIN Represents a single instance of a user logging in. CREATE Represents the creation of a new user. DELETE Represents the deletion of a user. Field Description operationType The  operation type \nof the authentication event. providers The  authentication providers \nthat emitted the event. One of the following names represents each authentication provider: \"anon-user\" \"local-userpass\" \"api-key\" \"custom-token\" \"custom-function\" \"oauth2-facebook\" \"oauth2-google\" \"oauth2-apple\" Generally, only one authentication provider emits each event.\nHowever, you may need to delete a user linked to multiple providers.\nIn this case, the  DELETE  event for that user includes all linked providers. user The  user object  of the user that interacted with\nthe authentication provider. time The time at which the event occurred. An online store wants to store custom metadata for each of its customers\nin  Atlas .\nEach customer needs a document in the  store.customers  collection.\nThen, the store can record and query metadata in the customer's document. The collection must represent each customer. To guarantee this, the store\ncreates an Authentication Trigger. This Trigger listens for newly created users\nin the  email/password  authentication\nprovider. Then, it passes the\n authentication event object  to its linked\nfunction,  createNewUserDocument . The function creates a new document\nwhich describes the user and their activity. The function then inserts the document\ninto the  store.customers  collection. For additional examples of Triggers integrated into an App Services App,\ncheckout the  example Triggers on Github .",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices push"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"operationType\": <string>,\n  \"providers\": <array of strings>,\n  \"user\": <user object>,\n  \"time\": <ISODate>\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function(authEvent) {\n  const mongodb = context.services.get(\"mongodb-atlas\");\n  const customers = mongodb.db(\"store\").collection(\"customers\");\n\n  const { user, time } = authEvent;\n  const isLinkedUser = user.identities.length > 1;\n\n  if(isLinkedUser) {\n    const { identities } = user;\n    return users.updateOne(\n      { id: user.id },\n      { $set: { identities } }\n    )\n\n  } else {\n    return users.insertOne({ _id: user.id, ...user })\n     .catch(console.error)\n  }\n  await customers.insertOne(newUser);\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"AUTHENTICATION\",\n  \"name\": \"newUserHandler\",\n  \"function_name\": \"createNewUserDocument\",\n  \"config\": {\n    \"providers\": [\"local-userpass\"],\n    \"operation_type\": \"CREATE\"\n  },\n  \"disabled\": false\n}"
                }
            ],
            "preview": "An authentication trigger fires when a user interacts with an\nauthentication provider. You can\nuse authentication triggers to implement advanced user management. Some uses include:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "authentication/firebase-jwt",
            "title": "Firebase JWT Authentication (Custom JWT)",
            "headings": [
                "Before You Begin",
                "Configure the Custom JWT authentication provider",
                "Log in with a Firebase JWT"
            ],
            "paragraphs": "You can configure the  Custom JWT authentication provider  to authenticate users that you manage with\n Firebase Authentication . You will need the following to use Firebase Authentication: A Firebase project with Authentication configured. To learn more,\nrefer to the  Firebase Authentication  documentation. An App Services App that does not already use Custom JWT\nauthentication. To learn how to create a new App Services App, see\n Create an App . If you're using the command line interface, you need  App Services CLI  to be\ninstalled and authenticated on your local system. If you're using the Admin API, you need a MongoDB Atlas Admin API\n public/private key pair . The API key\nmust have  Project Owner  permissions. You can configure Custom JWT authentication from the UI or by modifying\nthe underlying configuration files directly with the CLI or Admin API.\nChoose your preferred method below. In the left navigation menu, click  Authentication .\nThen click the  Authentication Providers  tab and select\nthe  Custom JWT  provider. Now you can configure the Custom JWT authentication provider to\nwork with your Firebase project. Click the toggle to enable the provider. Set  Verification Method  to  Use a JWK\nURI . Specify the following URL for  JWK URI : Define  Metadata Fields  to map data from the\nFirebase JWT to the corresponding App Services user account. None of the metadata fields are required. However, you might\nfind them useful for getting user information from the Firebase\nJWT into your App. The following is a mapping from the Firebase JWT to an App\nServices user. You can add these to the table in the UI as they\nare or modify the mapping as you wish following the\n Metadata Fields documentation . Path Field Name firebase.identities.email emails firebase.sign_in_provider signInProvider user_id userId email_verified emailVerified email email Set  Audience  to your  Firebase Project ID . You  must  set  Audience  to use your Firebase\nProject ID as a Custom JWT provider even though it's labeled\nas optional in the UI. Click  Save  and deploy your changes Run the following command, replacing the value of  --remote \nwith your App's Client App ID. This downloads a local copy of your\nApp's latest configuration files and navigates to the\nconfiguration file directory, which uses the same name as your\nApp. Add a new Custom JWT authentication provider to your App's\n /auth/providers.json  file. Use the following configuration as\na template, replacing the  audience  value with your Firebase\nProject ID. You can use the provided  metadata_fields  as they\nare or modify the mapping as you wish following the  Metadata\nFields documentation . Save your changes to  /auth/providers.json . Then, push the\nupdated configuration file to deploy your App: Add a new Custom JWT authentication provider to your App using the\n Create an authentication provider  endpoint. Use the following configuration as a template. Make sure to: You can use the provided  metadata_fields  as they are or\nmodify the mapping as you wish following the  Metadata\nFields documentation . Specify your App's  $PROJECT_ID  and  $APP_ID Include an Admin API access token in the  Authorization \nheader. Replace the  audience  value in the request body with your\nFirebase Project ID. Once you've configured the Custom JWT authentication provider to use\nFirebase Authentication, you can log in to your App Services App with a\nFirebase JWT. Log the user into Firebase. To learn how, see the relevant\n Firebase SDK documentation  for your platform and\nprogramming language. Get the user's Firebase JWT. To learn how, see  Retrieve ID\ntokens on clients  in the\nFirebase documentation. Use the Firebase JWT to authenticate with Atlas App Services. You can\n start a session over HTTP  or log in\nwith an SDK. To learn how, see the docs for your SDK: Custom JWT Authentication - Flutter SDK Custom JWT Authentication - Java SDK Custom JWT Authentication - Kotlin SDK Custom JWT Authentication - .NET SDK Custom JWT Authentication - Node.js SDK Custom JWT Authentication - React Native SDK Custom JWT Authentication - Swift SDK Custom JWT Authentication - Web SDK",
            "code": [
                {
                    "lang": null,
                    "value": "https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com"
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote \"myapp-abcde\"\ncd myapp"
                },
                {
                    "lang": "bash",
                    "value": "appservices push"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"custom-token\": {\n    \"name\": \"custom-token\",\n    \"type\": \"custom-token\",\n    \"disabled\": false,\n    \"config\": {\n      \"audience\": [\"<Your Firebase Project ID>\"],\n      \"jwkURI\": \"https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com\",\n      \"useJWKURI\": true\n    },\n    \"secret_config\": {\n      \"signingKeys\": []\n    },\n    \"metadata_fields\": [\n      {\n        \"required\": false,\n        \"name\": \"firebase.identities.email\",\n        \"field_name\": \"emails\"\n      },\n      {\n        \"required\": false,\n        \"name\": \"firebase.sign_in_provider\",\n        \"field_name\": \"signInProvider\"\n      },\n      {\n        \"required\": false,\n        \"name\": \"user_id\",\n        \"field_name\": \"userId\"\n      },\n      {\n        \"required\": false,\n        \"name\": \"email_verified\",\n        \"field_name\": \"emailVerified\"\n      },\n      {\n        \"required\": false,\n        \"name\": \"email\",\n        \"field_name\": \"email\"\n      }\n    ]\n  }\n}\n"
                },
                {
                    "lang": "bash",
                    "value": "curl \"https://services.cloud.mongodb.com/api/admin/v3.0/groups/$PROJECT_ID/apps/$APP_ID/auth_providers\" \\\n  -X \"POST\" \\\n  -H \"Authorization: Bearer $ACCESS_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"custom-token\",\n    \"type\": \"custom-token\",\n    \"disabled\": false,\n    \"config\": {\n      \"audience\": [\"<Your Firebase Project ID>\"],\n      \"jwkURI\": \"https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com\",\n      \"useJWKURI\": true\n    },\n    \"secret_config\": {\n      \"signingKeys\": []\n    },\n    \"metadata_fields\": [\n      {\n        \"required\": false,\n        \"name\": \"firebase.identities.email\",\n        \"field_name\": \"emails\"\n      },\n      {\n        \"required\": false,\n        \"name\": \"firebase.sign_in_provider\",\n        \"field_name\": \"signInProvider\"\n      },\n      {\n        \"required\": false,\n        \"name\": \"user_id\",\n        \"field_name\": \"userId\"\n      },\n      {\n        \"required\": false,\n        \"name\": \"email_verified\",\n        \"field_name\": \"emailVerified\"\n      },\n      {\n        \"required\": false,\n        \"name\": \"email\",\n        \"field_name\": \"email\"\n      }\n    ]\n  }'\n"
                }
            ],
            "preview": "You can configure the Custom JWT authentication provider to authenticate users that you manage with\nFirebase Authentication.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "triggers/scheduled-triggers",
            "title": "Scheduled Triggers",
            "headings": [
                "Create a Scheduled Trigger",
                "Configuration",
                "CRON Expressions",
                "Expression Syntax",
                "Format",
                "Field Values",
                "Example",
                "Performance Optimization",
                "Additional Examples"
            ],
            "paragraphs": "Scheduled triggers allow you to execute server-side logic on a\n regular schedule that you define .\nYou can use scheduled triggers to do work that happens on a periodic\nbasis, such as updating a document every minute, generating a nightly\nreport, or sending an automated weekly email newsletter. To create a scheduled Trigger in the Atlas App Services UI: Click  Triggers  under  Build  in the\nleft navigation menu. Click  Add a Trigger  to open the Trigger configuration page. Select  Scheduled  for the  Trigger Type . To create a scheduled Trigger with the  App Services CLI : Add a scheduled Trigger  configuration file  to the  triggers  subdirectory of a local\napplication directory. Scheduled Trigger configuration files have the following form: You cannot create a Trigger that runs on a  Basic \nschedule using App Services CLI. All imported scheduled Trigger\nconfigurations must specify a  CRON expression . Deploy  the trigger: Scheduled Triggers have the following configuration options: Field Description Select  Scheduled . The name of the trigger. Enabled by default. Used to enable or disable the trigger. Disabled by default. If enabled, any change events that occurred while\nthis trigger was disabled will not be processed. Required. You can select  Basic  or  Advanced . A Basic\nschedule executes the Trigger periodically based on the interval you set,\nsuch as \"every five minutes\" or \"every Monday\". An Advanced schedule runs the Trigger based on the custom\n CRON expression  that you define. Within the  Function  section, you choose what action is taken when\nthe trigger fires. You can choose to run a  function  or use\n AWS EventBridge . A Scheduled Trigger does not pass any arguments to its linked\nFunction. CRON expressions are user-defined strings that use standard\n cron  job syntax to define when a  scheduled\ntrigger  should execute.\nApp Services executes Trigger CRON expressions based on  UTC time .\nWhenever all of the fields in a CRON expression match the current date and time,\nApp Services fires the trigger associated with the expression. CRON expressions are strings composed of five space-delimited fields.\nEach field defines a granular portion of the schedule on which its\nassociated trigger executes: Field Valid Values Description minute [0 - 59] Represents one or more minutes within an hour. If the  minute  field of a CRON expression has a value of\n 10 , the field matches any time ten minutes after the hour\n(e.g.  9:10 AM ). hour [0 - 23] Represents one or more hours within a day on a 24-hour clock. If the  hour  field of a CRON expression has a value of\n 15 , the field matches any time between  3:00 PM  and\n 3:59 PM . dayOfMonth [1 - 31] Represents one or more days within a month. If the  dayOfMonth  field of a CRON expression has a value\nof  3 , the field matches any time on the third day of the\nmonth. month Represents one or more months within a year. A month can be represented by either a number (e.g.  2  for\nFebruary) or a three-letter string (e.g.  APR  for April). If the  month  field of a CRON expression has a value of\n 9 , the field matches any time in the month of September. weekday Represents one or more days within a week. A weekday can be represented by either a number (e.g.  2  for a\nTuesday) or a three-letter string (e.g.  THU  for a Thursday). If the  weekday  field of a CRON expression has a value of\n 3 , the field matches any time on a Wednesday. Each field in a CRON expression can contain either a specific value or\nan expression that evaluates to a set of values. The following table\ndescribes valid field values and expressions: Expression Type Description Matches all possible field values. Available in all expression fields. The following CRON expression schedules a trigger to execute\nonce every minute of every day: Matches a specific field value. For fields other than  weekday \nand  month  this value will always be an integer. A  weekday \nor  month  field can be either an integer or a three-letter\nstring (e.g.  TUE  or  AUG ). Available in all expression fields. The following CRON expression schedules a trigger to execute\nonce every day at 11:00 AM UTC: Matches a list of two or more field expressions or specific\nvalues. Available in all expression fields. The following CRON expression schedules a trigger to execute\nonce every day in January, March, and July at 11:00 AM UTC: Matches a continuous range of field values between and including\ntwo specific field values. Available in all expression fields. The following CRON expression schedules a trigger to execute\nonce every day from January 1st through the end of April at\n11:00 AM UTC: Matches any time where the step value evenly divides the\nfield value with no remainder (i.e. when  Value % Step == 0 ). Available in the  minute  and  hour  expression fields. The following CRON expression schedules a trigger to execute\non the 0th, 25th, and 50th minutes of every hour: An online store wants to generate a daily report of all sales from the\nprevious day. They record all orders in the  store.orders  collection\nas documents that resemble the following: To generate the daily report, the store creates a scheduled Trigger\nthat fires every day at  7:00 AM UTC . When the\nTrigger fires, it calls its linked Atlas Function,\n generateDailyReport , which runs an aggregation\nquery on the  store.orders  collection to generate the report. The\nFunction then stores the result of the aggregation in the\n store.reports  collection. Use the Query API with a a  $match \nexpression to reduce the number of documents your Function looks at.\nThis helps your Function improve performance and not reach\n Function memory limits . Refer the Example section for a Scheduled Trigger using a $match expression. For additional examples of Triggers integrated into an App Services App,\ncheckout the  example Triggers on Github .",
            "code": [
                {
                    "lang": "none",
                    "value": "{\n   \"type\": \"SCHEDULED\",\n   \"name\": \"<Trigger Name>\",\n   \"function_name\": \"<Trigger Function Name>\",\n   \"config\": {\n     \"schedule\": \"<CRON expression>\"\n   },\n   \"disabled\": <boolean>\n}"
                },
                {
                    "lang": "shell",
                    "value": "appservices push"
                },
                {
                    "lang": "text",
                    "value": "* * * * *\n\u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 weekday...........[0 (SUN) - 6 (SAT)]\n\u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500 month.............[1 (JAN) - 12 (DEC)]\n\u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500 dayOfMonth........[1 - 31]\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour..............[0 - 23]\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute............[0 - 59]"
                },
                {
                    "lang": "text",
                    "value": "* * * * *"
                },
                {
                    "lang": "text",
                    "value": "0 11 * * *"
                },
                {
                    "lang": "text",
                    "value": "0 11 * 1,3,7 *"
                },
                {
                    "lang": "text",
                    "value": "0 11 * 1-4 *"
                },
                {
                    "lang": "text",
                    "value": "*/25 * * * *"
                },
                {
                    "lang": "json",
                    "value": "{\n  _id: ObjectId(\"59cf1860a95168b8f685e378\"),\n  customerId: ObjectId(\"59cf17e1a95168b8f685e377\"),\n  orderDate: ISODate(\"2018-06-26T16:20:42.313Z\"),\n  shipDate: ISODate(\"2018-06-27T08:20:23.311Z\"),\n  orderContents: [\n    { qty: 1, name: \"Earl Grey Tea Bags - 100ct\", price: Decimal128(\"10.99\") }\n  ],\n  shippingLocation: [\n    { location: \"Memphis\", time: ISODate(\"2018-06-27T18:22:33.243Z\") },\n  ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = function() {\n  // Instantiate MongoDB collection handles\n  const mongodb = context.services.get(\"mongodb-atlas\");\n  const orders = mongodb.db(\"store\").collection(\"orders\");\n  const reports = mongodb.db(\"store\").collection(\"reports\");\n\n  // Generate the daily report\n  return orders.aggregate([\n    // Only report on orders placed since yesterday morning\n    { $match: {\n        orderDate: {\n          $gte: makeYesterdayMorningDate(),\n          $lt: makeThisMorningDate()\n        }\n    } },\n    // Add a boolean field that indicates if the order has already shipped\n    { $addFields: {\n        orderHasShipped: {\n          $cond: {\n            if: \"$shipDate\", // if shipDate field exists\n            then: 1,\n            else: 0\n          }\n        }\n    } },\n    // Unwind individual items within each order\n    { $unwind: {\n        path: \"$orderContents\"\n    } },\n    // Calculate summary metrics for yesterday's orders\n    { $group: {\n        _id: \"$orderDate\",\n        orderIds: { $addToSet: \"$_id\" },\n        numSKUsOrdered: { $sum: 1 },\n        numItemsOrdered: { $sum: \"$orderContents.qty\" },\n        totalSales: { $sum: \"$orderContents.price\" },\n        averageOrderSales: { $avg: \"$orderContents.price\" },\n        numItemsShipped: { $sum: \"$orderHasShipped\" },\n    } },\n    // Add the total number of orders placed\n    { $addFields: {\n        numOrders: { $size: \"$orderIds\" }\n    } }\n  ]).next()\n    .then(dailyReport => {\n      reports.insertOne(dailyReport);\n    })\n    .catch(err => console.error(\"Failed to generate report:\", err));\n};\n\nfunction makeThisMorningDate() {\n  return setTimeToMorning(new Date());\n}\n\nfunction makeYesterdayMorningDate() {\n  const thisMorning = makeThisMorningDate();\n  const yesterdayMorning = new Date(thisMorning);\n  yesterdayMorning.setDate(thisMorning.getDate() - 1);\n  return yesterdayMorning;\n}\n\nfunction setTimeToMorning(date) {\n  date.setHours(7);\n  date.setMinutes(0);\n  date.setSeconds(0);\n  date.setMilliseconds(0);\n  return date;\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"SCHEDULED\",\n  \"name\": \"reportDailyOrders\",\n  \"function_name\": \"generateDailyReport\",\n  \"config\": {\n    \"schedule\": \"0 7 * * *\"\n  },\n  \"disabled\": false\n}"
                }
            ],
            "preview": "Scheduled triggers allow you to execute server-side logic on a\nregular schedule that you define.\nYou can use scheduled triggers to do work that happens on a periodic\nbasis, such as updating a document every minute, generating a nightly\nreport, or sending an automated weekly email newsletter.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-schema-validate",
            "title": "appservices schema validate",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Validate a given schema against documents of a collection. Validates a collection's documents, using the deployed schema by default.\nIf the \"--use-local-app\" flag is passed in, validate using the local version of the schema in the pulled app found at \"data_sources/<data source name>/<database name>/<collection name>/schema.json\".\nDifferent filters can be added to determine which documents should be validated. Name Type Required Description -a, --app string false Specify the name or ID of an App Service on which to validate a schema --project string false Specify the ID of a MongoDB Atlas project -s, --datasource string false Specify the name or ID of a datasource -e, --error-path string false Specify the filepath for validation error details -d, --database string false Specify the name of a database -c, --collection string false Specify the name of a collection -l, --limit int false Specify the maximum number of documents to return This value defaults to 500. --skip int false Specify the numbers of documents to skip --filter string false Specify a filter (Allowed format: '{\"field\": \"value\"}') --sort string false Specify the sort order of the returned documents (Allowed format: '{\"field\": -1}') --local string false Specify the local filepath of an app services project (Note: The local filepath must be an absolute path or the command will fail) --use-local-app false Use local version of schema -h, --help false help for validate Name Type Required Description --profile string false Specify your profile (Default value: \"default\") [Learn more:  https://www.mongodb.com/docs/atlas/app-services/cli/#cli-profiles ] --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices schema validate [options]"
                }
            ],
            "preview": "Validate a given schema against documents of a collection.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-apps-delete",
            "title": "appservices apps delete",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Delete an app If you have more than one app, you will be prompted to select one or multiple\napp(s) that you would like to delete from a list of all your apps. The list\nincludes apps from all projects associated with your user profile. Name Type Required Description -a, --app strings false Specify the name(s) or ID(s) of apps to delete --project string false Specify the ID of a MongoDB Atlas project -h, --help false help for delete Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices apps delete [options]"
                }
            ],
            "preview": "Delete an app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-accessList-delete",
            "title": "appservices accessList delete",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Delete an IP address or CIDR block from the Access List of your app Removes an existing entry from the Access List of your app. You will be\nprompted to select an IP address or CIDR block if none are provided in the\ninitial command. Name Type Required Description -a, --app string false Specify the name or ID of an App Service to remove entries from its Access List --project string false Specify the ID of a MongoDB Atlas project --ip strings false Specify the IP address(es) or CIDR block(s) to delete -h, --help false help for delete Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices accessList delete [options]"
                }
            ],
            "preview": "Delete an IP address or CIDR block from the Access List of your app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "triggers/database-triggers",
            "title": "Database Triggers",
            "headings": [
                "Create a Database Trigger",
                "Configuration",
                "Trigger Details",
                "Trigger Source Details",
                "Function",
                "Advanced",
                "Change Event Types",
                "Database Trigger Example",
                "Suspended Triggers",
                "Automatically Resume a Suspended Trigger",
                "Manually Resume a Suspended Trigger",
                "Find the Suspended Trigger",
                "Restart the Trigger",
                "Pull Your App's Latest Configuration Files",
                "Verify that the Trigger Configuration File Exists",
                "Redeploy the Trigger",
                "Trigger Time Reporting",
                "Performance Optimization",
                "Disable Event Ordering for Burst Operations",
                "Disable Collection-Level Preimages",
                "Use Match Expressions to Limit Trigger Invocations",
                "Testing Match Expressions",
                "Use Project Expressions to Reduce Input Data Size",
                "Additional Examples"
            ],
            "paragraphs": "Database Triggers allow you to execute server-side logic whenever a database\nchange occurs on a linked MongoDB Atlas cluster. You can configure triggers on\nindividual collections, entire databases, and on an entire cluster. Unlike SQL data triggers, which run on the database server, triggers run\non a serverless compute layer that scales independently of the database\nserver. Triggers automatically call  Atlas Functions \nand can forward events to external handlers through AWS EventBridge. Use database triggers to implement event-driven data interactions. For\nexample, you can automatically update information in one document when a\nrelated document changes or send a request to an external service\nwhenever a new document is inserted. Database triggers use MongoDB  change streams \nto watch for real-time changes in a collection. A change stream is a\nseries of  database events  that each\ndescribe an operation on a document in the collection. Your app opens a\nsingle change stream for each collection with at least one enabled\ntrigger. If multiple triggers are enabled for a collection they all\nshare the same change stream. You control which operations cause a trigger to fire as well as what\nhappens when it does. For example, you can run a function whenever a\nspecific field of a document is updated. The function can access the\nentire change event, so you always know what changed. You can also pass\nthe change event to  AWS EventBridge  to handle\nthe event outside of Atlas. Triggers support  $match \nexpressions to filter change events and  $project \nexpressions to limit the data included in each event. There are limits on the total number of change streams you can open\non a cluster, depending on the cluster's size. Refer to  change\nstream limitations  for\nmore information. You cannot define a database trigger on a  serverless instance  or  Federated database instance  because they do not support change streams. In deployment and database level triggers, it is possible to configure triggers\nin a way that causes other triggers to fire, resulting in recursion.\nExamples include a database-level trigger writing to a collection within the\nsame database, or a cluster-level logger or log forwarder writing logs to\nanother database in the same cluster. To open the database trigger configuration screen in the App Services UI, click\n Triggers  in the left navigation menu, select the\n Database Triggers  tab, and then click  Add a\nTrigger . Configure the trigger and then click  Save  at the bottom of\nthe page to add it to your current deployment draft. To create a database trigger with the  App Services CLI : Add a database trigger  configuration file  to the  triggers  subdirectory of a\nlocal application directory. Deploy  the trigger: Atlas App Services does not enforce specific filenames for Trigger\nconfiguration files. However, once imported, Atlas App Services will rename\neach configuration file to match the name of the Trigger it defines,\ne.g.  mytrigger.json . Database Triggers have the following configuration options: Field Description Trigger Type The type of the Trigger. Set this value to  Database  for\ndatabase Triggers Name The name of the Trigger. Enabled by default. Used to enable or disable the trigger. Skip Events On Re-Enable Disabled by default. If enabled, any change events that occurred while this\ntrigger was disabled will not be processed. Event Ordering If enabled, trigger events are processed in the order in which they occur.\nIf disabled, events can be processed in parallel, which is faster when\nmany events occur at the same time. If event ordering is enabled, multiple executions of this Trigger will occur\nsequentially based on the timestamps of the change events. If event ordering is\ndisabled, multiple executions of this Trigger will occur independently. Improve performance for Triggers that respond to bulk database operations\nby disabling event ordering.\n Learn more. Within the  Trigger Source Details  section, you first select the\n Watch Against , based on the level of granularity you want. Your\noptions are: Depending on which source type you are using, the additional options differ. The\nfollowing table describes these options. Collection , when a change occurs on a specified collection Database , when a change occurs on any collection in a\nspecified database Deployment , when deployment changes occur on a specified\ncluster. If you select the Deployment source type, the following\ndatabases are  not watched  for changes: The admin databases  admin ,  local , and  config The sync databases  __realm_sync  and  __realm_sync_<app_id> The deployment-level source type is only available on dedicated tiers. Source Type Options Collection Trigger is associated with. Database Name . The MongoDB database that contains the watched\ncollection. Collection Name . The MongoDB collection to watch. Optional.\nIf you leave this option blank, the Source Type changes to \"Database.\" Operation Type . The The  operation types  that cause the Trigger to fire.\nSelect the operation types you want the trigger to respond to. Options\ninclude: Insert Update Replace Delete Update operations executed from MongoDB Compass or the MongoDB Atlas\nData Explorer fully replace the previous document. As a result,\nupdate operations from these clients will generate  Replace \nchange events rather than  Update  events. Full Document . If enabled,  Update  change events include\nthe latest  majority-committed \nversion of the modified document  after  the change was applied in\nthe  fullDocument  field. Regardless of this setting,  Insert  and  Replace  events always\ninclude the  fullDocument  field.  Delete  events never include\nthe  fullDocument  field. Document Preimage . When enabled, change events include a\ncopy of the modified document from immediately  before  the change was\napplied in the  fullDocumentBeforeChange  field. This has\n performance considerations . All change events\nexcept for  Insert  events include the document preimage. Database Cluster Name . The name of the MongoDB cluster that the\nTrigger is associated with. Database Name . The MongoDB database to watch. Optional.\nIf you leave this option blank, the Source Type changes to \"Deployment,\"\nunless you are on a shared tier, in which case App Services will not\nlet you save the trigger. Operation Type . The  operation types  that cause the Trigger to fire.\nSelect the operation types you want the  trigger to respond to.\nOptions include: Create Collection Modify Collection Rename Collection Drop Collection Shard Collection Reshard Collection Refine Collection Shard Key Update operations executed from MongoDB Compass or the MongoDB Atlas\nData Explorer fully replace the previous document. As a result,\nupdate operations from these clients will generate  Replace \nchange events rather than  Update  events. Full Document . If enabled,  Update  change events include\nthe latest  majority-committed \nversion of the modified document  after  the change was applied in\nthe  fullDocument  field. Regardless of this setting,  Insert  and  Replace  events always\ninclude the  fullDocument  field.  Delete  events never include\nthe  fullDocument  field. Document Preimage . When enabled, change events include a\ncopy of the modified document from immediately  before  the change was\napplied in the  fullDocumentBeforeChange  field. This has\n performance considerations . All change events\nexcept for  Insert  events include the document preimage. Disabled\nfor Database and Deployment sources to limit unnecessary watches on the\ncluster for a new collection being created. Deployment Cluster Name . The name of the MongoDB cluster that the\nTrigger is associated with. Operation Type . The The  operation types  that occur in the cluster that cause\nthe Trigger to fire. Select the operation types you want the  trigger\nto respond to. Options include: Drop Database Full Document . If enabled,  Update  change events include\nthe latest  majority-committed \nversion of the modified document  after  the change was applied in\nthe  fullDocument  field. Regardless of this setting,  Insert  and  Replace  events always\ninclude the  fullDocument  field.  Delete  events never include\nthe  fullDocument  field. Document Preimage . When enabled, change events include a\ncopy of the modified document from immediately  before  the change was\napplied in the  fullDocumentBeforeChange  field. This has\n performance considerations . All change events\nexcept for  Insert  events include the document preimage. Disabled\nfor Database and Deployment sources to limit unnecessary watches on the\ncluster for a new collection being created. Preimages require additional storage overhead that may affect\nperformance. If you're not using preimages on a collection,\nyou should disable preimages. To learn more, see  Disable\nCollection-Level Preimages . Document preimages are supported on non-sharded Atlas clusters running\nMongoDB 4.4+, and on sharded Atlas clusters running MongoDB 5.3 and later.\nYou can upgrade a non-sharded cluster (with preimages) to a\nsharded cluster, as long as the cluster is running 5.3 or later. Within the  Function  section, you choose what action is taken when\nthe trigger fires. You can choose to run a  function  or use\n AWS EventBridge . Within the  Advanced  section, the following  optional  configuration\noptions are available: Field Description Match Expression A  $match  expression document\nthat App Services uses to filter which change events cause the Trigger to\nfire. The Trigger evaluates all change event objects that it receives against\nthis match expression and only executes if the expression evaluates to  true \nfor a given change event. MongoDB performs a full equality match for embedded documents in a match\nexpression. If you want to match a specific field in an embedded document,\nrefer to the field directly using  dot-notation . For more information, see\n Query on Embedded Documents  in\nthe MongoDB server manual. Limit the number of fields that the Trigger processes by using a\n $match  expression.\n Learn more. Project Expression A  $project \nexpression that selects a subset of fields from each event in the change\nstream. You can use this to  optimize the trigger's execution . The expression is an object that maps the name of fields in the change\nevent to either a  0 , which excludes the field, or a  1 , which\nincludes it. An expression can have values of either  0  or  1  but\nnot both together. This splits projections into two categories,\ninclusive and exclusive: An  inclusive  project expression specifies fields to include in each\nchange event document. The expression is an object that maps the name\nof fields to include to a  1 . If you don't include a field, it is\nnot included in the projected change event. The following projection includes only the  _id  and\n fullDocument  fields: An  exclusive  project expression specifies fields to exclude from\neach change event document. The expression is an object that maps the\nname of fields to include to a  0 . If you don't exclude a field, it\nis included in the projected change event. The following projection excludes the  _id  and\n fullDocument  fields: You cannot exclude the  operation_type  field with a projection.\nThis ensures that the trigger can always check if it should run for\na given event's operation type. Auto-Resume Triggers If enabled, when this Trigger's resume token\ncannot be found in the cluster's oplog, the Trigger automatically resumes\nprocessing events at the next relevant change stream event.\nAll change stream events from when the Trigger was suspended until the Trigger\nresumes execution do not have the Trigger fire for them. Maximum Throughput Triggers If the linked data source is a dedicated server (M10+ Tier),\nyou can increase the  maximum throughput \nbeyond the default 10,000 concurrent processes. Before increasing the maximum throughput, consider whether one or more of\nyour triggers are calling a rate-limited external API. Increasing the\ntrigger rate might result in exceeding those limits. Increasing the throughput may also add a larger workload, affecting\noverall cluster performance. To enable maximum throughput, you must disable Event Ordering. Database change events represent individual changes in a specific\ncollection of your linked MongoDB Atlas cluster. Every database event has the same operation type and structure as the\n change event  object that was\nemitted by the underlying change stream. Change events have the\nfollowing operation types: Database change event objects have the following general form: Operation Type Description Insert Document  (All trigger types) Represents a new document added to the collection. Update Document  (All trigger types) Represents a change to an existing document in the collection. Delete Document  (All trigger types) Represents a document deleted from the collection. Replace Document  (All trigger types) Represents a new document that replaced a document in the collection. Create Collection  (Database and Deployment trigger types only) Represents the creation of a new collection. Modify Collection  (Database and Deployment trigger types only) Represents the modification collection. Rename Collection  (Database and Deployment trigger types only) Represents collection being renamed. Drop Collection  (Database and Deployment trigger types only) Represents a collection being dropped. Shard Collection  (Database and Deployment trigger types only) Represents a collection changing from unsharded to sharded. Reshard Collection  (Database and Deployment trigger types only) Represents a change to a collection's sharding. Refine Collection Shard Key  (Database and Deployment trigger types only) Represents a change in the shard key of a collection. Create Indexes  (Database and Deployment trigger types only) Represents the creation of a new index. Drop Indexes  (Database and Deployment trigger types only) Represents an index being dropped. Drop Database  (Deployment trigger type only) Represents a database being dropped. An online store wants to notify its customers whenever one of their\norders changes location. They record each order in the  store.orders \ncollection as a document that resembles the following: To automate this process, the store creates a Database Trigger that\nlistens for  Update  change events in the  store.orders  collection.\nWhen the trigger observes an  Update  event, it passes the\n change event object  to its associated Function,\n textShippingUpdate . The Function checks the change event for any\nchanges to the  shippingLocation  field and, if it was updated, sends\na text message to the customer with the new location of the order. Database Triggers may enter a suspended state in response to an event\nthat prevents the Trigger's change stream from continuing. Events that\ncan suspend a Trigger include: In the event of a suspended or failed trigger, Atlas App Services sends the\nproject owner an email alerting them of the issue. invalidate events \nsuch as  dropDatabase ,  renameCollection , or those caused by\na network disruption. the  resume token  required to resume the change stream is no longer in the\ncluster  oplog . The App logs\nrefer to this as a  ChangeStreamHistoryLost  error. You can configure a Trigger to automatically resume if the Trigger was suspended\nbecause the resume token is no longer in the oplog.\nThe Trigger does not process any missed change stream events between\nwhen the resume token is lost and when the resume process completes. When  creating or updating a Database Trigger \nin the App Services UI, navigate to the configuration page of the Trigger\nyou want to automatically resume if suspended. In the  Advanced (Optional)  section, select  Auto-Resume Triggers . Save and deploy the changes. When  creating or updating a Database Trigger \nwith the Realm CLI, create or navigate to the configuration file for the Trigger\nyou want to automatically resume if suspended. In the  Trigger's configuration file ,\ninclude the following: Deploy the changes with the following command: When you manually resume a suspended Trigger, your App attempts to resume the Trigger\nat the next change stream event after the change stream stopped.\nIf the resume token is no longer in the cluster oplog, the Trigger\nmust be started without a resume token. This means the Trigger begins\nlistening to new events but does not process any missed past events. You can adjust the oplog size to keep the resume token for more time after\na suspension by  scaling your Atlas cluster .\nMaintain an oplog size a few times greater than\nyour cluster's peak oplog throughput (GB/hour) to reduce the risk of a\nsuspended trigger's resume token dropping off the oplog\nbefore the trigger executes.\nView your cluster's oplog throughput in the  Oplog GB/Hour  graph in the\n Atlas cluster metrics . You can attempt to restart a suspended Trigger from the App Services UI or by\nimporting an application directory with the  App Services CLI . On the  Database Triggers  tab of the  Triggers \npage, find the trigger that you want to resume in the list of\ntriggers. App Services marks suspended triggers\nwith a  Status  of  Suspended . Click  Restart  in the trigger's  Actions  column.\nYou can choose to restart the trigger with a change stream\n resume token  or\nopen a new change stream. Indicate whether or not to use a resume\ntoken and then click  Resume Database Trigger . If you use a  resume token , App Services\nattempts to resume the trigger's underlying change\nstream at the event immediately following the last\nchange event it processed. If successful, the trigger\nprocesses any events that occurred while it was\nsuspended. If you do not use a resume token, the\ntrigger begins listening for new events but will not\nfire for any events that occurred while it was\nsuspended. If you exported a new copy of your application, it should already\ninclude an up-to-date configuration file for the suspended trigger.\nYou can confirm that the configuration file exists by looking\nin the  /triggers  directory for a  trigger configuration file  with the same name as the trigger. After you have verified that the trigger configuration file exists,\npush the configuration back to your app. App Services\nautomatically attempts to resume any suspended triggers included\nin the deployment. The list of Triggers in the Atlas App Services UI shows three timestamps: Last Modified This is the time the Trigger was created or most recently changed. Latest Heartbeat Atlas App Services keeps track of the last time a trigger was run. If the trigger\nis not sending any events, the server sends a heartbeat to ensure the trigger's\nresume token stays fresh. Whichever event is most recent is shown as the\n Latest Heartbeat . Last Cluster Time Processed Atlas App Services also keeps track of the  Last Cluster Time Processed ,\nwhich is the last time the change stream backing a Trigger emitted an event. It\nwill be older than the  Latest Heartbeat  if there have been no events\nsince the most recent heartbeat. Consider disabling event ordering if your trigger fires on a collection that\nreceives short bursts of events (e.g. inserting data as part of a daily batch\njob). Ordered Triggers wait to execute a Function for a particular event until\nthe Functions of previous events have finished executing. As a\nconsequence, ordered Triggers are effectively rate-limited by the run\ntime of each sequential Trigger function. This may cause a significant\ndelay between the database event appearing on the change stream and the\nTrigger firing. In certain extreme cases, database events might fall off\nthe oplog before a long-running ordered trigger processes them. Unordered Triggers execute functions in parallel if possible, which can be\nsignificantly faster (depending on your use case) but does not guarantee that\nmultiple executions of a Trigger Function occur in event order. Document preimages require your cluster to record additional data about\neach operation on a collection. Once you enable preimages for any\ntrigger on a collection, your cluster stores preimages for every\noperation on the collection. The additional storage space and compute overhead may degrade trigger\nperformance depending on your cluster configuration. To avoid the storage and compute overhead of preimages, you must disable\npreimages for the entire underlying MongoDB collection. This is a\nseparate setting from any individual trigger's preimage setting. If you disable collection-level preimages, then no active trigger on\nthat collection can use preimages. However, if you delete or disable all\npreimage triggers on a collection, then you can also disable\ncollection-level preimages. To learn how, see  Disable Preimages for a Collection . You can limit the number of Trigger invocations by specifying a  $match  expression in the  Match\nExpression  field. App Services evaluates the match expression against the\nchange event document and invokes the Trigger only if the expression evaluates\nto true for the given change event. The match expression is a JSON document that specifies the query conditions\nusing the  MongoDB read query syntax . We recommend only using match expressions when the volume of Trigger events\nmeasurably becomes a performance issue. Until then, receive all events and\nhandle them individually in the Trigger function code. The exact shape of the change event document depends on the event that caused\nthe trigger to fire. For details, see the reference for each event type: insert update replace delete create modify rename drop shardCollection reshardCollection refineCollectionShardKey dropDatabase The following match expression allows the Trigger to fire\nonly if the change event object specifies that the  status  field in\na document changed. updateDescription  is a field of the  update Event object . The following match expression allows the Trigger to fire only when a\ndocument's  needsTriggerResponse  field is  true . The  fullDocument \nfield of the  insert ,\n update , and  replace  events represents a document after the\ngiven operation. To receive the  fullDocument  field, you must enable\n Full Document  in your Trigger configuration. The following procedure shows one way to test whether your match expression\nworks as expected: Download  the MongoDB Shell (mongosh)  and use it to\n connect to your cluster . Replacing  DB_NAME  with your database name,  COLLECTION_NAME  with your\ncollection name, and  YOUR_MATCH_EXPRESSION  with the match expression you\nwant to test, paste the following into mongosh to open a change stream on an\nexisting collection: In another terminal window, use mongosh to make changes to some test\ndocuments in the collection. Observe what the change stream filters in and out. In the  Project Expression  field,\nlimit the number of fields that the Trigger processes by using a\n $project  expression. When using Triggers, a projection expression is inclusive  only .\nProject does not support mixing inclusions and exclusions.\nThe project expression must be inclusive because Triggers require you\nto include  operationType . If you want to exclude a single field, the projection expression must\ninclude every field  except  the one you want to exclude.\nYou can only explicitly exclude  _id , which is included by default. A trigger is configured with the following  Project Expression : The change event object that App Services passes to the trigger function\nonly includes the fields specifed in the projection, as in the following\nexample: For additional examples of Triggers integrated into an App Services App,\ncheckout the  example Triggers on Github .",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices push"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  _id: 1,\n  fullDocument: 1\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  _id: 0,\n  fullDocument: 0\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   _id : <ObjectId>,\n   \"operationType\": <string>,\n   \"fullDocument\": <document>,\n   \"fullDocumentBeforeChange\": <document>,\n   \"ns\": {\n      \"db\" : <string>,\n      \"coll\" : <string>\n   },\n   \"documentKey\": {\n     \"_id\": <ObjectId>\n   },\n   \"updateDescription\": <document>,\n   \"clusterTime\": <Timestamp>\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  _id: ObjectId(\"59cf1860a95168b8f685e378\"),\n  customerId: ObjectId(\"59cf17e1a95168b8f685e377\"),\n  orderDate: ISODate(\"2018-06-26T16:20:42.313Z\"),\n  shipDate: ISODate(\"2018-06-27T08:20:23.311Z\"),\n  orderContents: [\n    { qty: 1, name: \"Earl Grey Tea Bags - 100ct\", price: NumberDecimal(\"10.99\") }\n  ],\n  shippingLocation: [\n    { location: \"Memphis\", time: ISODate(\"2018-06-27T18:22:33.243Z\") },\n  ]\n}"
                },
                {
                    "lang": "javascript",
                    "value": "exports = async function (changeEvent) {\n  // Destructure out fields from the change stream event object\n  const { updateDescription, fullDocument } = changeEvent;\n\n  // Check if the shippingLocation field was updated\n  const updatedFields = Object.keys(updateDescription.updatedFields);\n  const isNewLocation = updatedFields.some(field =>\n    field.match(/shippingLocation/)\n  );\n\n  // If the location changed, text the customer the updated location.\n  if (isNewLocation) {\n    const { customerId, shippingLocation } = fullDocument;\n    const mongodb = context.services.get(\"mongodb-atlas\");\n    const customers = mongodb.db(\"store\").collection(\"customers\");\n    const { location } = shippingLocation.pop();\n    const customer = await customers.findOne({ _id: customerId });\n\n    const twilio = require('twilio')(\n      // Your Account SID and Auth Token from the Twilio console:\n      context.values.get(\"TwilioAccountSID\"),\n      context.values.get(\"TwilioAuthToken\"),\n    );\n\n    await twilio.messages.create({\n      To: customer.phoneNumber,\n      From: context.values.get(\"ourPhoneNumber\"),\n      Body: `Your order has moved! The new location is ${location}.`\n    })\n  }\n};"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"DATABASE\",\n  \"name\": \"shippingLocationUpdater\",\n  \"function_name\": \"textShippingUpdate\",\n  \"config\": {\n    \"service_name\": \"mongodb-atlas\",\n    \"database\": \"store\",\n    \"collection\": \"orders\",\n    \"operation_types\": [\"UPDATE\"],\n    \"unordered\": false,\n    \"full_document\": true,\n    \"match\": {}\n  },\n  \"disabled\": false\n}"
                },
                {
                    "lang": "js",
                    "value": "{\n  \"name\": \"<Trigger Name>\",\n  \"type\": \"DATABASE\",\n  \"config\": {\n    \"tolerate_resume_errors\": true,\n    // ...rest of Database Trigger configuration\n  },\n  // ...rest of Trigger general configuration\n}"
                },
                {
                    "lang": "shell",
                    "value": "appservices push --remote=<App ID>"
                },
                {
                    "lang": "shell",
                    "value": "appservices pull --remote=<App ID>"
                },
                {
                    "lang": "shell",
                    "value": "appservices push"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"updateDescription.updatedFields.status\": {\n    \"$exists\": true\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"fullDocument.needsTriggerResponse\": true\n}"
                },
                {
                    "lang": "js",
                    "value": "db.getSiblingDB(DB_NAME).COLLECTION_NAME.watch([{$match: YOUR_MATCH_EXPRESSION}])\nwhile (!watchCursor.isClosed()) {\n  if (watchCursor.hasNext()) {\n    print(tojson(watchCursor.next()));\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": 0,\n  \"operationType\": 1,\n  \"updateDescription.updatedFields.status\": 1\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"operationType\": \"update\",\n  \"updateDescription\": {\n    \"updatedFields\": {\n      \"status\": \"InProgress\"\n    }\n  }\n}"
                }
            ],
            "preview": "Database Triggers allow you to execute server-side logic whenever a database\nchange occurs on a linked MongoDB Atlas cluster. You can configure triggers on\nindividual collections, entire databases, and on an entire cluster.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-deploy-describe",
            "title": "appservices deploy describe",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Displays automatic deployment configuration details for your app Name Type Required Description -a, --app string false Specify the name or ID of an App Service on which to describe the automatic deployment configuration --project string false Specify the ID of a MongoDB Atlas project -h, --help false help for describe Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices deploy describe [options]"
                }
            ],
            "preview": "Displays automatic deployment configuration details for your app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "triggers/aws-eventbridge",
            "title": "Send Trigger Events to AWS EventBridge",
            "headings": [
                "Overview",
                "Procedure",
                "Set Up the MongoDB Partner Event Source",
                "Configure the Trigger",
                "Associate the Trigger Event Source with an Event Bus",
                "Custom Error Handling",
                "Create a New Custom Error Handler",
                "Create a New Error Handler",
                "Name the New Function",
                "Write the Function Code",
                "Test the Function",
                "Save the Function",
                "Write the Error Handler",
                "Add an Error Handler to Your Trigger Configuration",
                "Authenticate a MongoDB Atlas User",
                "Create a Deployment Draft (Optional)",
                "Create the Error Handler Function",
                "Create the AWS EventBridge Trigger",
                "Deploy the Draft",
                "Error Handler Parameters",
                "error",
                "changeEvent",
                "Error Codes",
                "DOCUMENT_TOO_LARGE",
                "OTHER",
                "Error Handler Logs",
                "Example Event",
                "Performance Optimization"
            ],
            "paragraphs": "MongoDB offers an  AWS Eventbridge  partner event source that lets\nyou send Atlas Trigger events to an event bus instead of\ncalling an Atlas Function. You can configure any Trigger type to send events to\nEventBridge. Database Triggers also support custom error handling,\nto reduce trigger suspensions due to non-critical errors. All you need to send Trigger events to EventBridge is an AWS account ID.\nThis guide walks through finding your account ID, configuring the\nTrigger, associating the Trigger event source with an event bus, and setting\nup custom error handling. This guide is based on Amazon's  Receiving Events from a\nSaaS Partner \ndocumentation. The AWS put entry for an EventBridge trigger event must be smaller than 256 KB. Learn how to reduce the size of your PutEvents entry in the  Performance Optimization  section. To send trigger events to AWS EventBridge, you need the  AWS\naccount ID  of the account that should receive the events.\nOpen the  Amazon EventBridge console  and click\n Partner event sources  in the navigation menu. Search for\nthe  MongoDB  partner event source and then click\n Set up . On the  MongoDB  partner event source page, click\n Copy  to copy your AWS account ID to the clipboard. Once you have the  AWS account ID , you can configure a\ntrigger to send events to EventBridge. In the App Services UI, create and configure a new  database\ntrigger ,  authentication\ntrigger , or  scheduled\ntrigger  and select the\n EventBridge  event type. Paste in the  AWS Account ID  that you copied from\nEventBridge and select an  AWS Region  to send the trigger events\nto. Optionally, you can configure a function for handling trigger errors.\nCustom error handling is only valid for database triggers.\nFor more details, refer to the  Custom Error Handling \nsection on this page. By default, triggers convert the BSON types in event objects into\nstandard JSON types. To preserve BSON type information, you can\nserialize event objects into  Extended JSON format  instead. Extended JSON preserves type\ninformation at the expense of readability and interoperability. To enable Extended JSON,\nclick the  Enable Extended JSON  toggle in the\n Advanced (Optional)  section. Create a  trigger configuration file \nin the  /triggers  directory. Omit the  function_name  field\nand define an  AWS_EVENTBRIDGE  event processor. Set the  account_id  field to the  AWS Account ID \nthat you copied from EventBridge and set the  region  field to\nan AWS Region. By default, triggers convert the BSON types in event objects into\nstandard JSON types. To preserve BSON type information, you can\nserialize event objects into  Extended JSON format  instead. Extended JSON preserves type\ninformation at the expense of readability and interoperability. To enable Extended JSON, set the  extended_json_enabled  field to  true . Optionally, you can configure a function for handling trigger errors.\nCustom error handling is only valid for database triggers.\nFor more details, refer to the  Custom Error Handling \nsection on this page. The trigger configuration file should resemble the following: For a full list of supported AWS regions, refer to Amazon's\n Receiving Events from a SaaS Partner \nguide. Go back to the EventBridge console and choose Partner event sources in\nthe navigation pane. In the  Partner event sources  table,\nfind and select the  Pending  trigger source and then click\n Associate with event bus . On the  Associate with event bus  screen, define any\nrequired access permissions for other accounts and organizations and\nthen click  Associate . Once confirmed, the status of the trigger event source changes from\n Pending  to  Active , and the name of the event\nbus updates to match the event source name. You can now start creating\nrules that trigger on events from that partner event source. For more\ninformation, see  Creating a Rule That Triggers on a SaaS Partner Event . You can create an error handler to be executed on a trigger failure,\nwhen retry does not succeed. Custom error handling allows you to determine\nwhether an error from AWS EventBridge is critical enough to suspend the Trigger,\nor if it is acceptable to ignore the error and continue processing other events.\nFor more information on suspended database triggers, refer to\n Suspended Triggers . Currently, only database triggers support custom error handling.\nAuthentication triggers and scheduled triggers do not support\ncustom error handling at this time. You can create the new function directly in the Create a Trigger page, as below,\nor from the Functions tab. For more information on how to define functions in\nApp Services, refer to  Define a Function . In the  Configure Error Function  section, select\n + New Function . You can also select an existing Function, if one is already defined,\nfrom the dropdown. Enter a unique, identifying name for the function in the  Name  field.\nThis name must be distinct from all other functions in the application. In the  Function  section, write the JavaScript code directly in\nthe function editor. The function editor contains a default function that\nyou can edit as needed. For more information on creating functions, refer\nto the  Functions  documentation. In the  Testing Console  tab beneath the function editor, you can\ntest the function by passing in example values to the  error  and\n changeEvent  parameters, as shown in the comments of the testing console. For more information on these paramaters, refer to the\n Error Handler Parameters \nsection on this page. Click  Run  to run the test. Once you are satisfied with the custom error handler, click\n Save . In order to update your trigger's configuration with an error handler,\nfollow these steps to  Update an App . When you\nupdate your configuration files in Step 3, do the following: Follow the steps in  Define a Function \nto write your error handler source code and configuration file. For the error handler source code, see the following template error handler: Add an  error_handler  attribute to your trigger configuration file\nin the  Triggers  folder. The trigger configuration file should\nresemble the following: For more information on trigger configuration files, see\n Trigger Configuration Files . Call the admin user authentication endpoint with your MongoDB Atlas API\nkey pair: If authentication succeeds, the response body contains a JSON object\nwith an  access_token  value: The  access_token  grants access to the App Services Admin API. You\nmust include it as a Bearer token in the  Authorization  header for\nall Admin API requests. API Authentication Documentation A draft represents a group of application changes that you\ncan deploy or discard as a single unit. If you don't create\na draft, updates automatically deploy individually. To create a draft, send a  POST  request with no body to\nthe  Create a Deployment Draft  endpoint: Create the function to handle errors for a failed AWS\nEventBridge trigger via a  POST  request to the\n Create a new\nFunction  endpoint. Create the AWS EventBridge Trigger with error handling\nenabled via a  POST  request to the\n Create a Trigger  endpoint. If you created a draft, you can deploy all changes in\nthe draft by sending a  POST  request with no body to the\n Deploy a deployment draft  endpoint.\nIf you did not create a draft as a first step, the\nindividual function and trigger requests deployed automatically. The default error handler has two parameters:  error  and  changeEvent . Has the following two attributes: code : The code for the errored EventBridge put request. For a list of\nerror codes used by the error handler, see the below section. message : The unfiltered error message from an errored EventBridge\nput request. The requested change to your data made by EventBridge. For more information\non types of change events and their configurations, see\n Change Event Types . If an error was recevied from EventBridge, the event processor will parse the\nerror as either  DOCUMENT_TOO_LARGE  or  OTHER . This parsed error is passed\nto the error handler function through the  error  parameter. If the put entry for an EventBridge trigger event is larger\nthan 256 KB, EventBridge will throw an error. The error will contain either: For more information on reducing put entry size, see the below  Performance\nOptimization  section. status code: 400  and\n total size of the entries in the request is over the limit . status code: 413 ,\nwhich indicates a too large payload. The default bucket for all other errors. You can make special error handling cases for\nyour most common error messages to optimize your error handling for\nerrors with an  OTHER  code. To determine which errors need\nspecial cases, we recommended keeping track of\nthe most common error messages you receive in  error.message . You can view  Trigger Error Handler logs  for\nyour EventBridge Trigger error handler in the application logs. To learn more about viewing application logs, see  View Application Logs . Click  Logs  in the left navigation of the App Services UI. Click the  Filter by Type  dropdown and select\n Triggers Error Handlers  to view all error handler\nlogs for the App. Pass the  trigger_error_handler  value to the  --type  flag to\nview all error handler logs for the App. Retrieve  TRIGGER_ERROR_HANDLER  type logs via a  GET  request to\nthe  Retreive App Services Logs  endpoint: The following object configures a trigger to send events to AWS\nEventbridge and handle errors: The AWS put entry for an EventBridge trigger event must be smaller than 256 KB. For more information, see the  AWS Documentation to calculate Amazon\nPutEvents event entry size . When using Database Triggers, the Project Expression can be useful reduce the document size\nbefore sending messages to EventBridge.\nThis expression lets you include only specified fields, reducing document size. Learn more in the Database Trigger Project Expression documentation.",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"...\",\n  \"type\": \"...\",\n  \"event_processors\": {\n     \"AWS_EVENTBRIDGE\": {\n         \"config\": {\n            \"account_id\": \"<AWS Account ID>\",\n            \"region\": \"<AWS Region>\",\n            \"extended_json_enabled\": <boolean>\n         }\n      }\n   }\n}"
                },
                {
                    "lang": "js",
                    "value": "exports = async function(error, changeEvent) {\n   // This sample function will log additional details if the error is not\n   // a DOCUMENT_TOO_LARGE error\n   if (error.code === 'DOCUMENT_TOO_LARGE') {\n      console.log('Document too large error');\n\n      // Comment out the line below in order to skip this event and not suspend the Trigger\n      throw new Error(`Encountered error: ${error.code}`);\n   }\n\n   console.log('Error sending event to EventBridge');\n   console.log(`DB: ${changeEvent.ns.db}`);\n   console.log(`Collection: ${changeEvent.ns.coll}`);\n   console.log(`Operation type: ${changeEvent.operationType}`);\n\n   // Throw an error in your function to suspend the trigger and stop processing additional events\n   throw new Error(`Encountered error: ${error.message}`);\n};"
                },
                {
                    "lang": "json",
                    "value": "      {\n         \"name\": \"...\",\n         \"type\": \"DATABASE\",\n         \"event_processors\": {\n            \"AWS_EVENTBRIDGE\": {\n               \"config\": {\n                  \"account_id\": \"<AWS Account ID>\",\n                  \"region\": \"<AWS Region>\",\n                  \"extended_json_enabled\": <boolean>\n               }\n            }\n         },\n         \"error_handler\": {\n            \"config\": {\n               \"enabled\": <boolean>,\n               \"function_name\": \"<Error Handler Function Name>\"\n            }\n         }\n      }"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/auth/providers/mongodb-cloud/login \\\n  -H 'Content-Type: application/json' \\\n  -H 'Accept: application/json' \\\n  -d '{\n    \"username\": \"<Public API Key>\",\n    \"apiKey\": \"<Private API Key>\"\n  }'"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"access_token\": \"<access_token>\",\n  \"refresh_token\": \"<refresh_token>\",\n  \"user_id\": \"<user_id>\",\n  \"device_id\": \"<device_id>\"\n}"
                },
                {
                    "lang": "bash",
                    "value": "curl -X POST 'https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/drafts' \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer <access_token>'"
                },
                {
                    "lang": "bash",
                    "value": "curl -X POST \\\n   https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/functions \\\n   -H 'Authorization: Bearer <access_token>' \\\n   -d '{\n      \"name\": \"string\",\n      \"private\": true,\n      \"source\": \"string\",\n      \"run_as_system\": true\n      }'"
                },
                {
                    "lang": "bash",
                    "value": "curl -X POST \\\n   https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/triggers \\\n   -H 'Authorization: Bearer <access_token>' \\\n   -d '{\n         \"name\": \"string\",\n         \"type\": \"DATABASE\",\n         \"config\": {\n            \"service_id\": \"string\",\n            \"database\": \"string\",\n            \"collection\": \"string\",\n            \"operation_types\": {\n               \"string\"\n            },\n            \"match\": ,\n            \"full_document\": false,\n            \"full_document_before_change\": false,\n            \"unordered\": true\n         },\n         \"event_processors\": {\n            \"AWS_EVENTBRIDGE\": {\n               \"account_id\": \"string\",\n               \"region\": \"string\",\n               \"extended_json_enabled\": false\n            },\n         },\n         \"error_handler\": {\n            \"enabled\": true,\n            \"function_id\": \"string\"\n         }\n      }'"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST \\\n'https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/drafts/{draftId}/deployment' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer <access_token>' \\"
                },
                {
                    "lang": "shell",
                    "value": "appservices logs list --type=trigger_error_handler"
                },
                {
                    "lang": "shell",
                    "value": "curl -X GET 'https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/logs' \\\n   -H 'Content-Type: application/json' \\\n   -H 'Authorization: Bearer <access_token>'\n   -d '{\n      \"type\": \"TRIGGER_ERROR_HANDLER\"\n      }'"
                },
                {
                    "lang": "json",
                    "value": "\"event_processors\": {\n   \"AWS_EVENTBRIDGE\": {\n      \"config\": {\n         \"account_id\": \"012345678901\",\n         \"region\": \"us-east-1\"\n      }\n   }\n},\n \"error_handler\": {\n   \"config\": {\n      \"enabled\": true,\n      \"function_name\": \"myErrorHandler.js\"\n   }\n}"
                }
            ],
            "preview": "Learn how to set up AWS EventBridge to handle Atlas Trigger events.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-users-enable",
            "title": "appservices users enable",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Enable an application User of your app Activates a User on your app. A User that has been enabled will have no\nrestrictions with logging in. Name Type Required Description -a, --app string false Specify the name or ID of an App Service to enable its users\u2019 --project string false Specify the ID of a MongoDB Atlas project -u, --user strings false Specify your app's users' ID(s) to enable -h, --help false help for enable Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices users enable [options]"
                }
            ],
            "preview": "Enable an application User of your app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-users-delete",
            "title": "appservices users delete",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Delete an application user from your app You can remove multiple Users at once with the \"--user\" flag. You can only\nspecify these Users using their ID values. Name Type Required Description -a, --app string false Specify the name or ID of an App Service to delete its users --project string false Specify the ID of a MongoDB Atlas project -u, --user strings false Specify your app's users' ID(s) to delete --pending false View the Realm app's pending users --state string false Filter the Realm app's users by state (Default value: <none>; Allowed values: enabled, disabled) --provider Set false Filter the Realm app's users by provider type (Default value: <none>; Allowed values: <none>, \"local-userpass\", \"api-key\", \"oauth2-facebook\", \"oauth2-google\", \"anon-user\", \"custom-token\", \"oauth2-apple\", \"custom-function\") This value defaults to []. -h, --help false help for delete Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices users delete [options]"
                }
            ],
            "preview": "Delete an application user from your app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-login",
            "title": "appservices login",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Log the CLI into App Services using a MongoDB Cloud API Key Begins an authenticated session with App Services. To get a MongoDB Cloud API\nKey, open your app in the App Services UI. Navigate to \"Deployment\" in the left\nnavigation menu, and select the \"Export App\" tab. From there, create a\nprogrammatic API Key to authenticate your appservices session. Name Type Required Description --api-key string false Specify the public portion of your Atlas programmatic API Key --private-api-key string false Specify the private portion of your Atlas programmatic API Key --browser false Direct browser to project access page to create a new API Key for logging into a project (Note: Can not be used in combination with login credentials) -h, --help false help for login Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices login [options]"
                }
            ],
            "preview": "Log the CLI into App Services using a MongoDB Cloud API Key",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-accessList-create",
            "title": "appservices accessList create",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Create an IP address or CIDR block in the Access List for your app You will be prompted to input an IP address or CIDR block if none is provided in\nthe initial command. Name Type Required Description -a, --app string false Specify the name or ID of an App Service to create an entry in its Access List --project string false Specify the ID of a MongoDB Atlas project --ip string false Specify the IP address or CIDR block that you would like to add --comment string false Add a comment to the IP address or CIDR block (Note: This action is optional) --use-current false Add your current IP address to your Access List --allow-all false Allows all IP addresses to access your app (Note: \u201c0.0.0.0/0\u201d will be added as an entry) -h, --help false help for create Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices accessList create [options]"
                }
            ],
            "preview": "Create an IP address or CIDR block in the Access List for your app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-secrets-create",
            "title": "appservices secrets create",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Create a Secret for your app You will be prompted to name your Secret and define the value of your Secret. Name Type Required Description -a, --app string false Specify the name or ID of an App Service to create its secrets --project string false Specify the ID of a MongoDB Atlas project -n, --name string false Name the secret -v, --value string false Specify the secret value -h, --help false help for create Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices secrets create [options]"
                }
            ],
            "preview": "Create a Secret for your app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-apps-describe",
            "title": "appservices apps describe",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Displays information about your app View all of the aspects of your app to see what is configured and enabled (e.g.\nservices, functions, etc.). If you have more than one app, you will be prompted\nto select an app to view. Name Type Required Description -a, --app string false Specify the name or ID of an App Service to describe --project string false Specify the ID of a MongoDB Atlas project -h, --help false help for describe Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices apps describe [options]"
                }
            ],
            "preview": "Displays information about your app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-logs",
            "title": "appservices logs",
            "headings": [
                "Options",
                "Inherited Options",
                "Related Commands"
            ],
            "paragraphs": "Interact with the Logs of your app (alias: log) Name Type Required Description -h, --help false help for logs Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts appservices logs list  - Lists the Logs in your app (alias: ls)",
            "code": [],
            "preview": "Interact with the Logs of your app (alias: log)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-deploy",
            "title": "appservices deploy",
            "headings": [
                "Options",
                "Inherited Options",
                "Related Commands"
            ],
            "paragraphs": "Manage automatic deployments of your app Name Type Required Description -h, --help false help for deploy Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts appservices deploy configure  - Configure the settings for automatic deployment appservices deploy describe  - Displays automatic deployment configuration details for your app appservices deploy disable  - Disable automatic deployments appservices deploy enable  - Enable automatic deployments",
            "code": [],
            "preview": "Manage automatic deployments of your app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-apps-list",
            "title": "appservices apps list",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "List the apps you have access to (alias: ls) Lists and filters your apps. Name Type Required Description -a, --app string false Filter the list of apps by name --project string false Specify the ID of a MongoDB Atlas project -h, --help false help for list Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices apps list [options]"
                }
            ],
            "preview": "List the apps you have access to (alias: ls)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-apps-create",
            "title": "appservices apps create",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Create a new app (or a template app) from your current working directory and deploy it to the App Services server Creates a new app by saving your configuration files in a local directory and\ndeploying the new app to the App Services server. This command will create a new\ndirectory for your project. You can specify a \"--remote\" flag to create an app from an existing app;\nif you do not specify a \"--remote\" flag, the CLI will create a default app. NOTE: To create an app without deploying it, use \"app init\". Name Type Required Description --remote string false Specify the name or ID of a remote app to clone --local string false Specify the local filepath of an app to be created -n, --name string false Name your new app --provider-region string false Select the app's provider region [Learn more:  https://www.mongodb.com/docs/atlas/app-services/manage-apps/deploy/deployment-models-and-regions/#cloud-deployment-regions ] -d, --deployment-model string false Select the app's deployment model (Default value: <none>; Allowed values: GLOBAL, LOCAL) [Learn more:  https://www.mongodb.com/docs/atlas/app-services/manage-apps/deploy/deployment-models-and-regions/#deployment-models ] -e, --environment string false Select the app's environment (Default value: <none>; Allowed values: development, testing, qa, production) [Learn more:  https://www.mongodb.com/docs/atlas/app-services/manage-apps/configure/environments/ ] --cluster strings false Link Atlas cluster(s) to your app (Note: Only one cluster can be linked during app creation if creating a template app) --cluster-service-name strings false Specify the app's Service name to reference your Atlas cluster (Note: Service names will be overwritten when creating a template app) --serverless-instance strings false Link Atlas Serverless instance(s) to your app (Note: Serverless instances cannot be used to create template apps) --serverless-instance-service-name strings false Specify the app's Service name to reference your Atlas Serverless instance --federated-database strings false Link Atlas Federated Database instance(s) to your app (Note: Federated Database instances cannot be used to create template apps) --federated-database-service-name strings false Specify the app's Service name to reference your Atlas Federated Database instance --template string false Create your app from an available template [Learn more:  https://www.mongodb.com/docs/atlas/app-services/reference/template-apps/#template-apps-available ] -x, --dry-run false Run without writing any changes to the local filepath or pushing any changes to the App Services server --project string false Specify the ID of a MongoDB Atlas project -h, --help false help for create Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices apps create [options]"
                }
            ],
            "preview": "Create a new app (or a template app) from your current working directory and deploy it to the App Services server",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-secrets-delete",
            "title": "appservices secrets delete",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Delete a Secret from your app Remove multiple Secrets at once with \"--secret\" flags. You can specify these\nSecrets using their ID or Name values Name Type Required Description -a, --app string false Specify the name or ID of an App Service to delete its secrets --project string false Specify the ID of a MongoDB Atlas project -s, --secret strings false Specify the name or ID of the secret to delete -h, --help false help for delete Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices secrets delete [options]"
                }
            ],
            "preview": "Delete a Secret from your app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-function",
            "title": "appservices function",
            "headings": [
                "Options",
                "Inherited Options",
                "Related Commands"
            ],
            "paragraphs": "Interact with the Functions of your app (alias: functions) Name Type Required Description -h, --help false help for function Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts appservices function run  - Run a Function from your app",
            "code": [],
            "preview": "Interact with the Functions of your app (alias: functions)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-pull",
            "title": "appservices pull",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Exports the latest version of your app into your local directory (alias: export) Pulls changes from your remote app into your local directory. If applicable,\nHosting Files and/or Dependencies associated with your app will be exported as\nwell. Name Type Required Description --local string false Specify a local filepath to export an app to --remote string false Specify the name or ID of a remote app to export --include-node-modules false Export and include app dependencies from a node_modules archive (Note: The allowed formats are as a directory or compressed into a .zip, .tar, .tar.gz, or .tgz file) --include-package-json false Export and include app dependencies from a package.json file -s, --include-hosting false Export and include app hosting files -x, --dry-run false Run without writing any changes to the local filepath -t, --template strings false Specify the frontend Template ID(s) to export. (Note: Specified templates must be compatible with the remote app) [Learn more:  https://www.mongodb.com/docs/atlas/app-services/reference/template-apps/#template-apps-available ] --project string false Specify the ID of a MongoDB Atlas project -h, --help false help for pull Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices pull [options]"
                }
            ],
            "preview": "Exports the latest version of your app into your local directory (alias: export)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-users-disable",
            "title": "appservices users disable",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Disable an application User of your app Deactivates a User on your app. A User that has been disabled will not be\nallowed to log in, even if they provide valid credentials. Name Type Required Description -a, --app string false Specify the name or ID of an App Service to disable its users --project string false Specify the ID of a MongoDB Atlas project -u, --user strings false Specify your app's users' ID(s) to disable -h, --help false help for disable Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices users disable [options]"
                }
            ],
            "preview": "Disable an application User of your app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-whoami",
            "title": "appservices whoami",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Display information about the current user Displays a table that includes your Public and redacted Private Atlas\nprogrammatic API Key (e.g.  ********-****-****-****-3ba985aa367a ). No session\ndata will be surfaced if you are not logged in. NOTE: To log in and authenticate your session, use \"appservices login\" Name Type Required Description --show-projects false Show projects associated with this profile's API Key -h, --help false help for whoami Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices whoami [options]"
                }
            ],
            "preview": "Display information about the current user",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-apps",
            "title": "appservices apps",
            "headings": [
                "Options",
                "Inherited Options",
                "Related Commands"
            ],
            "paragraphs": "Manage the App Service Apps associated with the current user (alias: app) Name Type Required Description -h, --help false help for apps Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts appservices apps create  - Create a new app (or a template app) from your current working directory and deploy it to the App Services server appservices apps delete  - Delete an app appservices apps describe  - Displays information about your app appservices apps diff  - Show differences between your local directory and your app appservices apps init  - Initialize an App Services App in your current working directory (alias: initialize) appservices apps list  - List the apps you have access to (alias: ls)",
            "code": [],
            "preview": "Manage the App Service Apps associated with the current user (alias: app)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-accessList-list",
            "title": "appservices accessList list",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "List the allowed entries in the Access List of your app (alias: ls) This will display the IP addresses and/or CIDR blocks in the Access List of\nyour app Name Type Required Description -a, --app string false Specify the name or ID of an App Service to list its allowed IP addresses and/or CIDR blocks --project string false Specify the ID of a MongoDB Atlas project -h, --help false help for list Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices accessList list [options]"
                }
            ],
            "preview": "List the allowed entries in the Access List of your app (alias: ls)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-secrets",
            "title": "appservices secrets",
            "headings": [
                "Options",
                "Inherited Options",
                "Related Commands"
            ],
            "paragraphs": "Manage the Secrets of your app (alias: secret) Name Type Required Description -h, --help false help for secrets Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts appservices secrets create  - Create a Secret for your app appservices secrets delete  - Delete a Secret from your app appservices secrets list  - List the Secrets in your app (alias: ls) appservices secrets update  - Update a Secret in your app",
            "code": [],
            "preview": "Manage the Secrets of your app (alias: secret)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-users",
            "title": "appservices users",
            "headings": [
                "Options",
                "Inherited Options",
                "Related Commands"
            ],
            "paragraphs": "Manage the Users of your app (alias: user) Name Type Required Description -h, --help false help for users Name Type Required Description --profile string false Specify your profile (Default value: \"default\") [Learn more:  https://www.mongodb.com/docs/atlas/app-services/cli/#cli-profiles ] --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts appservices users count  - Displays the total count of application users of your app appservices users create  - Create an application user for your app appservices users delete  - Delete an application user from your app appservices users disable  - Disable an application User of your app appservices users enable  - Enable an application User of your app appservices users list  - List the application users of your app (alias: ls) appservices users revoke  - Revoke an application User\u2019s sessions from your app",
            "code": [],
            "preview": "Manage the Users of your app (alias: user)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-function-run",
            "title": "appservices function run",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Run a Function from your app Functions allow you to define and execute server-side logic for your app. Once\nyou select and run a Function for your app, the following will be displayed: A list of logs, if present The function result as a document A list of error logs, if present Name Type Required Description -a, --app string false Specify the name or ID of an App Service to run its function --project string false Specify the ID of a MongoDB Atlas project --name string false Specify the name of the function to run --args stringArray false Specify the args to pass to your function [Learn more:  https://www.mongodb.com/docs/atlas/app-services/functions/#call-from-app-services-cli ] --user string false Specify which user to run the function as (Note: Using <none> will run as the System user) (Default value: <none>; Allowed values: <none>, <userID>) -h, --help false help for run Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices function run [options]"
                }
            ],
            "preview": "Run a Function from your app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-logout",
            "title": "appservices logout",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Log the CLI out of App Services Ends the authenticated session and deletes cached auth tokens. To\nre-authenticate, you must call Login with your Atlas programmatic API Key. Name Type Required Description -h, --help false help for logout Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices logout [options]"
                }
            ],
            "preview": "Log the CLI out of App Services",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-apps-init",
            "title": "appservices apps init",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Initialize an App Services App in your current working directory (alias: initialize) Initializes a new App Services App by saving your configuration files in your\ncurrent working directory. You can specify a \"--remote\" flag to initialize an App Services App from an\nexisting app; if you do not specify a \"--remote\" flag, the CLI will initialize a\ndefault App Services App. NOTE: To create a new App Services App and have it deployed, use \"app create\". Name Type Required Description --remote string false Specify the name or ID of a remote app to clone -n, --name string false Name your new app --provider-region string false Select the app's provider region [Learn more:  https://www.mongodb.com/docs/atlas/app-services/manage-apps/deploy/deployment-models-and-regions/#cloud-deployment-regions ] -d, --deployment-model string false Select the app's deployment model (Default value: <none>; Allowed values: GLOBAL, LOCAL) [Learn more:  https://www.mongodb.com/docs/atlas/app-services/manage-apps/deploy/deployment-models-and-regions/#deployment-models ] -e, --environment string false Select the app's environment (Default value: <none>; Allowed values: development, testing, qa, production) [Learn more:  https://www.mongodb.com/docs/atlas/app-services/manage-apps/configure/environments/ ] --project string false Specify the ID of a MongoDB Atlas project -h, --help false help for init Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices apps init [options]"
                }
            ],
            "preview": "Initialize an App Services App in your current working directory (alias: initialize)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-schema-datamodels",
            "title": "appservices schema datamodels",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Generate data models based on your Schema (alias: datamodel) Translates your Schema\u2019s objects into App Services data models. The data models\ndefine your data as native objects, which can be easily integrated into your own\nrepo to use with Device Sync. NOTE: You must have a valid JSON Schema before using this command. Specify the language with a \"--language\" flag Filter which Schema objects you\u2019d like to include in your output with \"--name\" flags Combine your Schema objects into a single output with a \"--flat\" flag Omit import groups from your model with a \"--no-imports\" flag Name Type Required Description -a, --app string false Specify the name or ID of an App Service to generate its data models --project string false Specify the ID of a MongoDB Atlas project -l, --language string false Specify the language to generate schema data models in (Default value: <none>) --flat false View generated data models (and associated imports) as a single code block --no-imports false View generated data models without imports --name strings false Filter generated data models by name(s) -h, --help false help for datamodels Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices schema datamodels [options]"
                }
            ],
            "preview": "Generate data models based on your Schema (alias: datamodel)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-users-count",
            "title": "appservices users count",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Displays the number of total application users of your app, across all Auth Provider types. Name Type Required Description -a, --app string false Specify the name or ID of an App Service to display number of users\u2019 --project string false Specify the ID of a MongoDB Atlas project -h, --help false help for count Name Type Required Description --profile string false Specify your profile (Default value: \"default\") [Learn more:  https://www.mongodb.com/docs/atlas/app-services/cli/#cli-profiles ] --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices users count [options]"
                }
            ],
            "preview": "Displays the number of total application users of your app, across all Auth Provider types.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-apps-diff",
            "title": "appservices apps diff",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Show differences between your local directory and your app Displays file-by-file differences between your local directory and the latest\nversion of your app. If you have more than one app, you will be prompted to\nselect an app to view. Name Type Required Description --local string false Specify the local filepath of an app to diff --remote string false Specify the name or ID of an app to diff --include-node-modules false Include app dependencies in the diff from a node_modules archive (Note: The allowed formats are as a directory or compressed into a .zip, .tar, .tar.gz, or .tgz file) --include-package-json false Include app dependencies in the diff from a package.json file -s, --include-hosting false Include app hosting files in the diff --project string false Specify the ID of a MongoDB Atlas project -h, --help false help for diff Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices apps diff [options]"
                }
            ],
            "preview": "Show differences between your local directory and your app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-logs-list",
            "title": "appservices logs list",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Lists the Logs in your app (alias: ls) Displays a list of your app\u2019s Logs sorted by recentness, with most recent Logs\nappearing towards the bottom. You can specify a \"--tail\" flag to monitor your\nLogs and follow any newly created Logs in real-time. Name Type Required Description -a, --app string false Specify the name or ID of an App Service to list its logs --project string false Specify the ID of a MongoDB Atlas project --type Set false Specify the type(s) of logs to list (Default value: <none>; Allowed values: <none>, \"auth\", \"function\", \"push\", \"service\", \"trigger\", \"graphql\", \"sync\", \"schema\", \"trigger_error_handler\", \"log_forwarder\", \"endpoint\") This value defaults to []. --errors false View your app's error logs --start Date false Specify when to begin listing logs [Learn more:  https://www.mongodb.com/docs/atlas/app-services/logs/cli/#view-logs-for-a-date-range ] --end Date false Specify when to finish listing logs [Learn more:  https://www.mongodb.com/docs/atlas/app-services/logs/cli/#view-logs-for-a-date-range ] --tail false View your app's logs in real-time (Note: \"--start\" and \"--end\" flags do not apply here) -h, --help false help for list Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices logs list [options]"
                }
            ],
            "preview": "Lists the Logs in your app (alias: ls)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-deploy-configure",
            "title": "appservices deploy configure",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Configure the settings for automatic deployment Walks you through configuring automatic deployment for your app Name Type Required Description -a, --app string false Specify the name or ID of an App Service on which to configure automatic deployment --project string false Specify the ID of a MongoDB Atlas project -h, --help false help for configure Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices deploy configure [options]"
                }
            ],
            "preview": "Configure the settings for automatic deployment",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-schema",
            "title": "appservices schema",
            "headings": [
                "Options",
                "Inherited Options",
                "Related Commands"
            ],
            "paragraphs": "Manage the Schemas of your app (alias: schemas) Name Type Required Description -h, --help false help for schema Name Type Required Description --profile string false Specify your profile (Default value: \"default\") [Learn more:  https://www.mongodb.com/docs/atlas/app-services/cli/#cli-profiles ] --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts appservices schema datamodels  - Generate data models based on your Schema (alias: datamodel) appservices schema generate  - Generate a schema based on a sample set of documents in a collection or from a single JSON file appservices schema validate  - Validate a given schema against documents of a collection",
            "code": [],
            "preview": "Manage the Schemas of your app (alias: schemas)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-secrets-update",
            "title": "appservices secrets update",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Update a Secret in your app NOTE: The Name of the Secret cannot be modified. In order to do so, you will\nneed to delete and re-create the Secret. Name Type Required Description -a, --app string false Specify the name or ID of an App Service to update its secrets --project string false Specify the ID of a MongoDB Atlas project -s, --secret string false Specify the name or ID of the secret to update -n, --name string false Re-name the secret -v, --value string false Specify the new secret value -h, --help false help for update Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices secrets update [options]"
                }
            ],
            "preview": "Update a Secret in your app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-push",
            "title": "appservices push",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Imports and deploys changes from your local directory to your app (alias: import) Updates a remote App Services App with your local directory. First, input an\napp that you would like changes pushed to. This input can be either the\napplication Client App ID of an existing app you would like to update, or the\nName of a new app you would like to create. Changes pushed are automatically\ndeployed. Name Type Required Description --local string false Specify the local filepath of an app to be imported --remote string false Specify the name or ID of a remote app to edit --include-node-modules false Import and include app dependencies from a node_modules archive (Note: The allowed formats are as a directory or compressed into a .zip, .tar, .tar.gz, or .tgz file) --include-package-json false Import and include app dependencies from a package.json file -s, --include-hosting false Import and include app hosting files -c, --reset-cdn-cache false Reset the hosting CDN cache of an app -x, --dry-run false Run without pushing any changes to the App Services server --project string false Specify the ID of a MongoDB Atlas project -h, --help false help for push Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices push [options]"
                }
            ],
            "preview": "Imports and deploys changes from your local directory to your app (alias: import)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-deploy-enable",
            "title": "appservices deploy enable",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Enable automatic deployments Name Type Required Description -a, --app string false Specify the name or ID of an App Service on which to enable automatic deployment --project string false Specify the ID of a MongoDB Atlas project -h, --help false help for enable Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices deploy enable [options]"
                }
            ],
            "preview": "Enable automatic deployments",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-deploy-disable",
            "title": "appservices deploy disable",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Disable automatic deployments Name Type Required Description -a, --app string false Specify the name or ID of an App Service on which to disable automatic deployment --project string false Specify the ID of a MongoDB Atlas project -h, --help false help for disable Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices deploy disable [options]"
                }
            ],
            "preview": "Disable automatic deployments",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-users-revoke",
            "title": "appservices users revoke",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Revoke an application User's sessions from your app Logs a User out of your app. A revoked User can log in again if they provide\nvalid credentials. Name Type Required Description -a, --app string false Specify the name or ID of an App Service to revoke its users' sessions --project string false Specify the ID of a MongoDB Atlas project -u, --user strings false Specify the app's users' ID(s) to revoke sessions for --pending false View the Realm app's pending users --state string false Filter the Realm app's users by state (Default value: <none>; Allowed values: enabled, disabled) --provider Set false Filter the Realm app's users by provider type (Default value: <none>; Allowed values: <none>, \"local-userpass\", \"api-key\", \"oauth2-facebook\", \"oauth2-google\", \"anon-user\", \"custom-token\", \"oauth2-apple\", \"custom-function\") This value defaults to []. -h, --help false help for revoke Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices users revoke [options]"
                }
            ],
            "preview": "Revoke an application User's sessions from your app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-accessList",
            "title": "appservices accessList",
            "headings": [
                "Options",
                "Inherited Options",
                "Related Commands"
            ],
            "paragraphs": "Manage the allowed IP addresses and CIDR blocks of your app (aliases: accesslist, access-list) Name Type Required Description -h, --help false help for accessList Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts appservices accessList create  - Create an IP address or CIDR block in the Access List for your app appservices accessList delete  - Delete an IP address or CIDR block from the Access List of your app appservices accessList list  - List the allowed entries in the Access List of your app (alias: ls) appservices accessList update  - Modify an IP address or CIDR block in the Access List of your app",
            "code": [],
            "preview": "Manage the allowed IP addresses and CIDR blocks of your app (aliases: accesslist, access-list)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-users-create",
            "title": "appservices users create",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Create an application user for your app Adds a new User to your app. You can create a User for the following enabled\nAuth Providers: \"Email/Password\", or \"API Key\". Name Type Required Description -a, --app string false Specify the name or ID of an App Service to create its users --project string false Specify the ID of a MongoDB Atlas project --type string false Select the type of user to create (Default value: <none>; Allowed values: api-key, email) --name string false Specify the name of the new API Key --email string false Specify the email of the new user --password string false Specify the password of the new user -h, --help false help for create Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices users create [options]"
                }
            ],
            "preview": "Create an application user for your app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-accessList-update",
            "title": "appservices accessList update",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Modify an IP address or CIDR block in the Access List of your app Changes an existing entry from the Access List of your app. You will be\nprompted to select an IP address or CIDR block to update if neither is\nspecified. Name Type Required Description -a, --app string false Specify the name or ID of an App Service to modify an entry in its Access List --project string false Specify the ID of a MongoDB Atlas project --ip string false Specify the existing IP address or CIDR block that you would like to modify --new-ip string false Specify the new IP address or CIDR block that will replace the existing entry --comment string false Add or edit a comment to the IP address or CIDR block that is being modified -h, --help false help for update Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices accessList update [options]"
                }
            ],
            "preview": "Modify an IP address or CIDR block in the Access List of your app",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-users-list",
            "title": "appservices users list",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "List the application users of your app (alias: ls) Displays a list of your app's Users' details. The list is grouped by Auth\nProvider type and sorted by Last Authentication Date. Name Type Required Description -a, --app string false Specify the name or ID of an App Service to list its users\u2019 --project string false Specify the ID of a MongoDB Atlas project -u, --user strings false Filter the App Service's users by ID(s) --pending false View the Realm app's pending users --state string false Filter the Realm app's users by state (Default value: <none>; Allowed values: enabled, disabled) --provider Set false Filter the Realm app's users by provider type (Default value: <none>; Allowed values: <none>, \"local-userpass\", \"api-key\", \"oauth2-facebook\", \"oauth2-google\", \"anon-user\", \"custom-token\", \"oauth2-apple\", \"custom-function\") This value defaults to []. -h, --help false help for list Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices users list [options]"
                }
            ],
            "preview": "List the application users of your app (alias: ls)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-secrets-list",
            "title": "appservices secrets list",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "List the Secrets in your app (alias: ls) This will display the IDs and Names of the Secrets in your app. Name Type Required Description -a, --app string false Specify the name or ID of an App Service to list its secrets --project string false Specify the ID of a MongoDB Atlas project -h, --help false help for list Name Type Required Description --profile string false Specify your profile (Default value: \"default\") --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices secrets list [options]"
                }
            ],
            "preview": "List the Secrets in your app (alias: ls)",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "cli/appservices-schema-generate",
            "title": "appservices schema generate",
            "headings": [
                "Syntax",
                "Options",
                "Inherited Options"
            ],
            "paragraphs": "Generate a schema based on a sample set of documents in a collection or from a single JSON file. When run with a selected database and collection, generates a schema based on a sample of documents; when run with the \u201c--path\u201d flag, generates a schema based on the JSON file at the provided path. Name Type Required Description -a, --app string false Specify the name or ID of an App Service on which to generate a schema --project string false Specify the ID of a MongoDB Atlas project --local string false Specify the local filepath of an app to generate a schema -s, --datasource string false Specify the name or ID of the datasource to sample from -d, --database string false Specify the name of the database to sample from -c, --collection string false Specify the name or ID of the collection to sample from --path string false Specify the path to the JSON file to generate a schema from --title string false Specify a title to name the generated schema (Note: Title can only be used with the path flag) -l, --limit int false Specify the maximum number of documents to sample from This value defaults to 500. --skip int false Specify the numbers of documents to skip --filter string false Specify a filter document to sample from a subset of the data (Allowed format: '{\"field\": \"value\"}') --projection string false Specify which fields to return in the schema (Allowed format: '{\"field\": 1}') --sort string false Specify the sort order of the returned documents (Allowed format: '{\"field\": -1}') --save false Automatically save the schema to the filesystem (Note: The --save flag will automatically save the schema to the corresponding data_sources/datasource/database/collection folder, and prompt for the datasource, database, and collection if not given) -h, --help false help for generate Name Type Required Description --profile string false Specify your profile (Default value: \"default\") [Learn more:  https://www.mongodb.com/docs/atlas/app-services/cli/#cli-profiles ] --telemetry string false Enable/Disable CLI usage tracking for your current profile (Default value: \"on\"; Allowed values: \"on\", \"off\") -o, --output-target string false Write CLI output to the specified filepath -f, --output-format string false Set the CLI output format (Default value: <blank>; Allowed values: <blank>, \"json\") --disable-colors false Disable all CLI output styling (e.g. colors, font styles, etc.) -y, --yes false Automatically proceed through CLI commands by agreeing to any required user prompts",
            "code": [
                {
                    "lang": null,
                    "value": "appservices schema generate [options]"
                }
            ],
            "preview": "Generate a schema based on a sample set of documents in a collection or from a single JSON file.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "apps/deployment-models-and-regions",
            "title": "Deployment Models & Regions",
            "headings": [
                "Overview",
                "Deployment Models",
                "Global Deployment",
                "Local Deployment",
                "Cloud Deployment Regions"
            ],
            "paragraphs": "When you first create an App, you must select a\n deployment type  that determines where\nyour application's data is stored and processed. You can deploy your\nApp globally or limit the deployment to a specific\n cloud deployment region . A globally deployed App is hosted across the world in all of the\nfollowing AWS regions: An App can handle most types of requests from any region. For example,\nany region can run serverless functions, evaluate rules, validate object\nschemas, or authenticate users. All write operations to a linked MongoDB data source are handled within\na single region that you specify when you create the App. You can choose\nany global region to handle writes, but should generally choose the\nregion closest to your MongoDB cluster. If a server in another region\nneeds to handle a write request, it forwards the request to the\nconfigured write region, which adds latency to the write operation. Realm SDKs automatically execute application requests, such as reading\nuser information or calling a function, in the global deployment region\nthat's closest to the user that issued the request. Requests sent to a\nglobal URL (for example, with the Data API) are forwarded to the\nclosest server at the DNS level. Requests sent to a region-specific URL\nexecute in that region regardless of the user's location. Ireland (IE) -  aws-eu-west-1 Oregon (US-OR) -  aws-us-west-2 Sydney (AU) -  aws-ap-southeast-2 Virginia (US-VA) -  aws-us-east-1 A locally deployed App is available in a single cloud provider region.\nAll requests and MongoDB write operations are handled exclusively in the\nregion, which you specify when you create the App. You can deploy to any region, but should generally choose the region\nclosest to your application's users. You can deploy your App to cloud regions hosted around the world by the major\ncloud providers.  Support for a given region depends on your  deployment\nmodel . Global deployment has fewer supported regions\nthan local deployment. If you don't specify a region when creating or configuring your App, App\nServices selects the region geographically closest to your selected data source. App Services supports the following regions: Cloud Region Location Region ID Local Global AWS eu-west-1 Ireland (IE) aws-eu-west-1 \u2713 \u2713 AWS us-west-2 Oregon (US-OR) aws-us-west-2 \u2713 \u2713 AWS ap-southeast-2 Sydney (AU) aws-ap-southeast-2 \u2713 \u2713 AWS us-east-1 Virginia (US-VA) aws-us-east-1 \u2713 \u2713 AWS us-east-2 Ohio (US-VA) aws-us-east-2 \u2713 AWS eu-west-2 London (IE) aws-eu-west-2 \u2713 AWS eu-central-1 Frankfurt (DE-FF) aws-eu-central-1 \u2713 AWS ap-south-1 Mumbai (IN-MB) aws-ap-south-1 \u2713 AWS ap-southeast-1 Singapore (SG) aws-ap-southeast-1 \u2713 AWS sa-east-1 S\u00e3o Paulo (BR-SP) aws-sa-east-1 \u2713 Azure eastus2 Virginia (US-VA) azure-eastus2 \u2713 Azure westus California (US-OR) azure-westus \u2713 Azure westeurope Netherlands (DE-FF) azure-westeurope \u2713 Azure southeastasia Singapore (SG) azure-southeastasia \u2713 Azure eastasia Hong Kong (IN-MB) azure-eastasia \u2713 GCP us-central1 Ohio (US-VA) gcp-us-central1 \u2713 GCP us-east4 Virginia (US-VA) gcp-us-east4 \u2713 GCP us-west1 Oregon (US-OR) gcp-us-west1 \u2713 GCP europe-west1 Belgium (DE-FF) gcp-europe-west1 \u2713 GCP asia-south1 Mumbai (IN-MB) gcp-asia-south1 \u2713",
            "code": [],
            "preview": "When you first create an App, you must select a\ndeployment type that determines where\nyour application's data is stored and processed. You can deploy your\nApp globally or limit the deployment to a specific\ncloud deployment region.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "apps/copy",
            "title": "Copy an App",
            "headings": [
                "Overview",
                "Before You Begin",
                "Procedure",
                "Authenticate a MongoDB Atlas User",
                "Create a New App",
                "Migrate Secrets to the New App",
                "Copy Your Configuration Files",
                "Push the Copied Configuration Files",
                "Create a New Configuration Directory",
                "Create a New App",
                "Set Up Automatic Github Deployment",
                "Migrate Secrets to the New App",
                "Copy Your Configuration Files",
                "Push New App to Github"
            ],
            "paragraphs": "You can make a copy of an existing App by reusing the App's\nconfiguration files and manually porting Secrets. You might want to copy an application if: You use feature branches for development. Use a unique copy of the\nApp for each feature branch to avoid conflicts. You run tests against a working version of the App. You can copy an\nApp for each test run to to ensure a consistent start state. You deploy the same app to clusters across regions using a  local\ndeployment model . You can copy an App to serve\nmultiple regions locally. You will need the following to copy an App in the CLI: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . A MongoDB Atlas Admin API  public/private key pair . The API key must\nhave  Project Owner  permissions to work with App\nServices Admin API. A copy of App Services CLI installed and added to your local system  PATH . To\nlearn how, see  Install App Services CLI . To copy an App based on configuration files in a GitHub\nrepository, you must enable  Automatic GitHub Deployment  for the App. If  Automatic GitHub Deploymeny  is enabled,\ndo not push changes to your App with App Services CLI. For more\ninformation, see  Avoid Making Changes from the CLI . Use your MongoDB Atlas Admin API Key to log in to the CLI: Create a new blank App. Choose a name and set the deployment\nmodel and region to be what you want the copied App to use.\nFor more information, see  Create an App . An App's configuration files do not include the names or values of any\n Secrets . You must have access to the original App's existing Secret values and\nadd them manually to the new App. If your app doesn't have any Secrets,\nyou can skip this step. To add the secrets from your original App: Get the names of all the secrets from the exported app by following\nthe  view secrets documentation . Save the names of all the secrets to a secure location. The list\nwon't include the actual Secret values, but it's useful to have a\nlist of the Secret names to add to your new App. Find the value for each of the original App's Secrets. Add each Secret individually to the new App. To learn how, see\n Define a Secret . Some App Services features require you to have one or more Secrets\ndefined before you can define and use the feature. For example, OAuth\nauthentication providers require a Secret that contains a\n clientSecret  value. If you push configuration files that reference undefined Secrets, the\ndeployment will fail. Pull the latest version of your original App's configuration\nfiles to your local filesystem. To learn how, see\n Export an App . Copy all configuration files from your original App, except for\n realm_config.json , to the new App's configuration directory. You\nshould use the new App's  realm_config.json  and overwrite any other\nconfiguration files. Push the configuration files you copied from your original\nApp. The new App will automatically update and deploy with\nthe copied configuration files. Create a new directory to store the copied App's\nconfiguration files. You can create a new repository for the\ncopied App or keep both Apps' configurations in the same\nrepository using branches or subdirectories. Create a new blank App. Choose the same name as the original\nApp and set the deployment model and region to be what you\nwant the copied App to use. For more information, see\n Create an App . Once created, save the new App's configuration files to the\ndirectory you created in the previous step if they aren't\nalready. In the new App, set up and enable  Automatic Github\nDeployment . Make sure to point to the\nrepository, branch, and directory that you created for the\nnew App, not your original App. An App's configuration files do not include the names or values of any\n Secrets . You must have access to the original App's existing Secret values and\nadd them manually to the new App. If your app doesn't have any Secrets,\nyou can skip this step. To add the secrets from your original App: Get the names of all the secrets from the exported app by following\nthe  view secrets documentation . Save the names of all the secrets to a secure location. The list\nwon't include the actual Secret values, but it's useful to have a\nlist of the Secret names to add to your new App. Find the value for each of the original App's Secrets. Add each Secret individually to the new App. To learn how, see\n Define a Secret . Some App Services features require you to have one or more Secrets\ndefined before you can define and use the feature. For example, OAuth\nauthentication providers require a Secret that contains a\n clientSecret  value. If you push configuration files that reference undefined Secrets, the\ndeployment will fail. Copy all configuration files from your original App, except for\n realm_config.json , to the new App's configuration directory. You\nshould use the new App's  realm_config.json  and overwrite any other\nconfiguration files. Commit the copied application configuration files and then\npush them to GitHub. The new App will automatically update\nand deploy with the copied configuration files.",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices login --api-key=\"<my api key>\" --private-api-key=\"<my private api key>\""
                },
                {
                    "lang": "bash",
                    "value": "# Create the new App\nappservices app create \\\n  --name \"myapp-copy\" \\\n  --deployment-model \"LOCAL\" \\\n  --provider-region \"aws-us-west-2\""
                },
                {
                    "lang": "bash",
                    "value": "# Pull the config files for an existing App\nappservices pull --remote=\"myapp-abcde\""
                },
                {
                    "lang": "bash",
                    "value": "# Copy all configuration files except for realm_config.json\ncp -r myapp myapp-temp\nrm myapp-temp/realm_config.json\ncp -r myapp-temp/* myapp-copy\nrm -rf myapp-temp"
                },
                {
                    "lang": "bash",
                    "value": "# Navigate back to the new App\ncd myapp-copy\n# Push the copied configuration files to App Services\nappservices push"
                },
                {
                    "lang": "bash",
                    "value": "# Create a new directory for the copied App\nmkdir myapp-copy"
                },
                {
                    "lang": "bash",
                    "value": "# Navigate to the new App's directory\ncd myapp-copy\n# Create the new App. The create command saves the new\n# App's configuration file directory in the current directory\nappservices app create \\\n  --name \"myapp-copy\" \\\n  --deployment-model \"LOCAL\" \\\n  --provider-region \"aws-us-west-2\"\ncp -r myapp-copy/* .\nrm -rf myapp-copy\n# Navigate back to the root of the repo\ncd .."
                },
                {
                    "lang": "bash",
                    "value": "# Copy all configuration files except for realm_config.json\ncp -r myapp myapp-temp\nrm myapp-temp/realm_config.json\ncp -r myapp-temp/* myapp-copy\nrm -rf myapp-temp"
                },
                {
                    "lang": "bash",
                    "value": "# Navigate back to the new App\ncd myapp-copy\n# Push the copied configuration files to GitHub\ngit add -A\ngit commit -m \"Copy configuration from myapp\"\ngit push origin main"
                }
            ],
            "preview": "You can make a copy of an existing App by reusing the App's\nconfiguration files and manually porting Secrets.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "apps/create",
            "title": "Create an App Services App",
            "headings": [
                "Overview",
                "Before You Begin",
                "Procedure",
                "Navigate to the App Services UI",
                "Choose an App Type",
                "Configure App Settings",
                "Create the App",
                "Authenticate a MongoDB Atlas User",
                "Run the App Creation Command",
                "Get an Admin API Access Token",
                "Get the Project ID",
                "Create a new App"
            ],
            "paragraphs": "You can create a new App Services App from the App Services UI, CLI, or\nAdmin API. An App Services App is a managed backend instance that contains your\napplication's services. Each app belongs to a specific Atlas project. To create an App in a project, you must be logged in to  MongoDB Atlas  and have\n Project Owner  permissions. You will need the following to create an App in the Atlas UI: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . You will need the following to create an App in the CLI: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . A MongoDB Atlas Admin API  public/private key pair . The API key must\nhave  Project Owner  permissions to work with App\nServices Admin API. A copy of App Services CLI installed and added to your local system  PATH . To\nlearn how, see  Install App Services CLI . You will need the following to create an App with the Admin API: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . A MongoDB Atlas Admin API  public/private key pair . The API key must\nhave  Project Owner  permissions to work with App\nServices Admin API. In the Atlas UI Project dashboard, click the  App\nServices  tab. You can either create a new blank App or start from a\ntemplate that has some services pre-configured for you. Some\ntemplate Apps also come with corresponding client\napplications. To learn more, see  Template Apps . If the project has no Apps, you will be prompted to either\ncreate a new App from a template or to start from scratch. Otherwise, you can choose between two create buttons at the\ntop right of the  App Services  tab: To create a blank App, click the  Create a New App  button. To use a template, click the  Create App from\nTemplate  and select a template from the dropdown menu. The UI prompts you to configure the following settings: You can also configure the following optional settings under\n Advanced Configuration : The  Application Name . Your App name is public and\nincluded in the App ID used by clients. Application names must be between 1 and 32 characters and may\nonly contain ASCII letters, numbers, underscores, and hyphens. A  Linked Data Source . This is an Atlas cluster that\nthe App can access. If the project has no clusters, you\ncan have App Services create one for you automatically. In order to use Atlas Device Sync, your Atlas cluster must use MongoDB version 4.4 or higher.\nWhen setting up your cluster, select  MongoDB 4.4  (or a higher version)\nfrom the dropdown menu under  Additional Settings . The  Application Region  which determines where your App\nexecutes and where its metadata is stored. To learn more,\nsee  Deployment Models & Regions . The  App Environment  that determines which environment\nvalues to use. To learn more, see  Configure an App\nEnvironment . Once you've configured the App, click  Create App\nService . Upon creation of your App, the Atlas UI automatically\nredirects you to the App Services Admin UI. Use your MongoDB Atlas Admin API Key to log in to the CLI: The CLI can create an app and copy its  configuration files  to a local directory with a single command. The command\nasks you to specify the app name and the Atlas project that should contain the\napp. You can configure the command with flags or interactively. The command also supports additional flags that you can optionally include\nto customize your app. The following table lists common flags you might use: --template Choose an App template to start from. If you don't\nspecify a template, the command creates a blank app. For a list of all template apps, see to\n Available Template Apps . --deployment-model Defines the App's  deployment model . Valid values: GLOBAL LOCAL --provider-region Defines the App's deployment region. For a list of available regions, see  Cloud Deployment Regions . --environment Sets the App's  environment tag . Valid values: development testing qa production For more details and additional flags, see the\n CLI documentation for the create command . Call the admin user authentication endpoint with your MongoDB Atlas API\nkey pair: If authentication succeeds, the response body contains a JSON object\nwith an  access_token  value: The  access_token  grants access to the App Services Admin API. You\nmust include it as a Bearer token in the  Authorization  header for\nall Admin API requests. API Authentication Documentation Every App Services App belongs to a MongoDB Atlas project. To create a App you need the Project ID (sometimes referred to as\nthe Group ID) of the MongoDB Atlas project that contains the app.\nYou will include this ID in the URL of API requests. You can create a new App through the\n Create an App  endpoint. Your request must include: If you want to create an app from a  template , you must also include the following in the\nrequest body: If your App is created successfully, the API returns a\n 201  response with information about your new App. An  Authorization  header with your Admin API access\ntoken as a  Bearer token . A  name  for the new App The  template_id  field with one of the available\ntemplate apps. For a list of all template apps, refer to\n Available Template Apps . The  data_source  field with information about an\nexisting Atlas cluster.",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices login --api-key=\"<my api key>\" --private-api-key=\"<my private api key>\""
                },
                {
                    "lang": "shell",
                    "value": "appservices apps create"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/auth/providers/mongodb-cloud/login \\\n  -H 'Content-Type: application/json' \\\n  -H 'Accept: application/json' \\\n  -d '{\n    \"username\": \"<Public API Key>\",\n    \"apiKey\": \"<Private API Key>\"\n  }'"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"access_token\": \"<access_token>\",\n  \"refresh_token\": \"<refresh_token>\",\n  \"user_id\": \"<user_id>\",\n  \"device_id\": \"<device_id>\"\n}"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps \\\n  --header 'Authorization:  Bearer <access_token>' \\\n  --data '{ \"name\": \"<App Name>\" }'"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps \\\n  -H 'Authorization:  Bearer <access_token>' \\\n  -d '{\n    \"name\": \"<App Name>\",\n    \"template_id\": \"<Template App Id>\",\n    \"data_source\": {\n      \"name\": \"mongodb-atlas\",\n      \"type\": \"mongodb-atlas\",\n      \"config\": {\n        \"clusterName\": \"<Atlas Cluster Name>\"\n      }\n    }\n  }'"
                }
            ],
            "preview": "You can create a new App Services App from the App Services UI, CLI, or\nAdmin API.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "apps/rollback",
            "title": "Roll Back a Deployment",
            "headings": [
                "Overview",
                "Before You Begin",
                "Procedure",
                "View Your App's Deployment History",
                "Redeploy a Deployment",
                "Confirm Rollback Success",
                "Authenticate a MongoDB Atlas User",
                "Find the Deployment ID",
                "Redeploy the App"
            ],
            "paragraphs": "You can roll back an App's configuration to a previously deployed\nversion. App Services can automatically redeploy any of an App's\n25 most recent deployments. For older\ndeployments, you must manually get and deploy the configuration files\n(e.g. from source control). A redeploy rolls back your application's configuration to match the\nearlier version. You will need the following to rollback an App in the Atlas UI: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . You will need the following to rollback an App with the Admin API: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . A MongoDB Atlas Admin API  public/private key pair . The API key must\nhave  Project Owner  permissions to work with App\nServices Admin API. Your App's internal ObjectId hex string and the Project ID of the\nAtlas Project that contains your App. To learn how to find these, see\n Get App Metadata . You can redeploy from your App's Deployment History page. To\nget there, click  Deployment  in the left\nnavigation menu and then select the  History  tab. In the deployment history list, find the deployment you want\nto roll back to. Click the  Re-Deploy  button for\nthat deployment. After the deployment completes, you'll see the deployment success banner.\nThe new deployment appears in your Deployment History. If you have GitHub deployment enabled, it also contains a link to the\ncommit in the GitHub repository. In the linked GitHub repository, the commit author is the\n mongodb-realm  bot. Call the admin user authentication endpoint with your MongoDB Atlas API\nkey pair: If authentication succeeds, the response body contains a JSON object\nwith an  access_token  value: The  access_token  grants access to the App Services Admin API. You\nmust include it as a Bearer token in the  Authorization  header for\nall Admin API requests. API Authentication Documentation You need the  _id  of the deployment you want to redeploy.\nTo find it, call the  List Deployments  endpoint, which responds\nwith a list of the App's 25 most\nrecent deployments. Find the deployment you want to redeploy in the list and\ncopy its  _id . You can roll back to a previous deployment by calling the\n Redeploy a Deployment  endpoint. Pass the\n _id  of the deployment you want to redeploy as the\n deploymentId  parameter.",
            "code": [
                {
                    "lang": "shell",
                    "value": "curl -X POST \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/auth/providers/mongodb-cloud/login \\\n  -H 'Content-Type: application/json' \\\n  -H 'Accept: application/json' \\\n  -d '{\n    \"username\": \"<Public API Key>\",\n    \"apiKey\": \"<Private API Key>\"\n  }'"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"access_token\": \"<access_token>\",\n  \"refresh_token\": \"<refresh_token>\",\n  \"user_id\": \"<user_id>\",\n  \"device_id\": \"<device_id>\"\n}"
                },
                {
                    "lang": "bash",
                    "value": "curl -X GET \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/deployments \\\n  -H 'Authorization: Bearer <access_token>'"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    \"_id\": \"638662e881ad88c37dcb7656\",\n    \"app_id\": \"63844243ec3f52ed48923213\",\n    \"draft_id\": \"638662d6cc4b58e348927a7a\",\n    \"user_id\": \"59231005d323af2275135258\",\n    \"deployed_at\": 1669751529,\n    \"origin\": \"UI\",\n    \"commit\": \"\",\n    \"status\": \"successful\",\n    \"status_error_message\": \"\",\n    \"diff_url\": \"\",\n    \"name\": \"638662e881ad88c37dcb7656\",\n    \"remote_location\": \"US-VA\"\n  },\n  {\n    \"_id\": \"638662b2fd660afffb39df01\",\n    \"app_id\": \"63844243ec3f52ed48923213\",\n    \"draft_id\": \"638662928d19776b743d3b30\",\n    \"user_id\": \"593f1105d383ad2275165258\",\n    \"deployed_at\": 1669751475,\n    \"origin\": \"UI\",\n    \"commit\": \"\",\n    \"status\": \"successful\",\n    \"status_error_message\": \"\",\n    \"diff_url\": \"\",\n    \"name\": \"638662b2fd660afffb39df01\",\n    \"remote_location\": \"US-VA\"\n  }\n]"
                },
                {
                    "lang": "bash",
                    "value": "curl -X GET \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/deployments/{deploymentId}/redeploy \\\n  -H 'Authorization: Bearer <access_token>'"
                }
            ],
            "preview": "You can roll back an App's configuration to a previously deployed\nversion. App Services can automatically redeploy any of an App's\n25 most recent deployments. For older\ndeployments, you must manually get and deploy the configuration files\n(e.g. from source control).",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "apps/delete",
            "title": "Delete an App",
            "headings": [
                "Overview",
                "Before You Begin",
                "Procedure",
                "Navigate to the App Services Tab",
                "Delete the App",
                "Authenticate a MongoDB Atlas User",
                "Run the App Delete Command",
                "Authenticate a MongoDB Atlas User",
                "Delete the App"
            ],
            "paragraphs": "You can delete an App Services App from the App Services UI, CLI, or\nAdmin API. You will need the following to delete an App in the Atlas UI: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . You will need the following to delete an App in the CLI: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . A MongoDB Atlas Admin API  public/private key pair . The API key must\nhave  Project Owner  permissions to work with App\nServices Admin API. A copy of App Services CLI installed and added to your local system  PATH . To\nlearn how, see  Install App Services CLI . Your App's client App ID. This is the unique string that contains your\nApp name, e.g.  \"myapp-abcde\" . To learn how to find your App ID, see\n Get App Metadata . You will need the following to delete an App with the Admin API: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . A MongoDB Atlas Admin API  public/private key pair . The API key must\nhave  Project Owner  permissions to work with App\nServices Admin API. Your App's internal ObjectId hex string and the Project ID of the\nAtlas Project that contains your App. To learn how to find these, see\n Get App Metadata . Go to the Project Dashboard where you want to delete the App.\nSelect the  App Services  tab. To delete an app, click the  ...  menu at the upper\nright corner of the tile for the app you want to delete. Then, select  Delete App  from the drop-down menu\nthat appears. The page prompts you to confirm that you want to delete this\napp. Click  Delete App  to confirm and delete the\nApp. Use your MongoDB Atlas Admin API Key to log in to the CLI: The CLI can delete one or more apps with the following command: If you have more than one App, you will be prompted to select one or\nmore apps that you would like to delete from a list of all your Apps. If you already know the name or id of the app you would like\nto delete, you can specify it with the  --app  flag: For more details and additional flags, see the\nCLI documentation for the  app delete  command. Call the admin user authentication endpoint with your MongoDB Atlas API\nkey pair: If authentication succeeds, the response body contains a JSON object\nwith an  access_token  value: The  access_token  grants access to the App Services Admin API. You\nmust include it as a Bearer token in the  Authorization  header for\nall Admin API requests. API Authentication Documentation Send a request to the  Delete an App  endpoint. Make sure to include your Admin API  access_token , the\n groupId  of the Atlas project containing your App, and\nthe App's internal  appId  hex string: If your application deletes successfully, App Services returns a  204  response.",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices login --api-key=\"<my api key>\" --private-api-key=\"<my private api key>\""
                },
                {
                    "lang": "shell",
                    "value": "appservices apps delete"
                },
                {
                    "lang": "shell",
                    "value": "appservices apps delete --app <App ID | App Name>"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/auth/providers/mongodb-cloud/login \\\n  -H 'Content-Type: application/json' \\\n  -H 'Accept: application/json' \\\n  -d '{\n    \"username\": \"<Public API Key>\",\n    \"apiKey\": \"<Private API Key>\"\n  }'"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"access_token\": \"<access_token>\",\n  \"refresh_token\": \"<refresh_token>\",\n  \"user_id\": \"<user_id>\",\n  \"device_id\": \"<device_id>\"\n}"
                },
                {
                    "lang": "shell",
                    "value": "curl --request DELETE 'https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}' \\\n  --header 'Authorization:  Bearer <access_token>' \\\n  --header 'Content-Type: application/json'"
                }
            ],
            "preview": "You can delete an App Services App from the App Services UI, CLI, or\nAdmin API.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "apps/export",
            "title": "Export an App",
            "headings": [
                "Overview",
                "Before You Begin",
                "Procedure",
                "Navigate to Your App",
                "Export the App",
                "Authenticate a MongoDB Atlas User",
                "Run the App Export Command",
                "Authenticate a MongoDB Atlas User",
                "Export Your App",
                "Output"
            ],
            "paragraphs": "You can download a directory of your App's configuration files by\nexporting them. This allows you to save your App's configuration in\nsource control and to work locally with App Services CLI and your preferred\ntext editor. The exported directory contains configuration files for most components\nof your App, including Data Sources, Functions, Triggers, and other\nservices. For more information on the contents of an exported App, see\n App Configuration . The export does not include any  Secrets . If you\ncreate a new App based on exported configuration files, you must\nmanually add Secrets to the new App. For more information, see\n Copy an App . You will need the following to export an App in the Atlas UI: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . You will need the following to export an App in the CLI: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . A MongoDB Atlas Admin API  public/private key pair . The API key must\nhave  Project Owner  permissions to work with App\nServices Admin API. A copy of App Services CLI installed and added to your local system  PATH . To\nlearn how, see  Install App Services CLI . Your App's client App ID. This is the unique string that contains your\nApp name, e.g.  \"myapp-abcde\" . To learn how to find your App ID, see\n Get App Metadata . You will need the following to export an App with the Admin API: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . A MongoDB Atlas Admin API  public/private key pair . The API key must\nhave  Project Owner  permissions to work with App\nServices Admin API. Your App's internal ObjectId hex string and the Project ID of the\nAtlas Project that contains your App. To learn how to find these, see\n Get App Metadata . In the Atlas UI, click the  App Services  tab.\nThen, click the tile for the App you want to export. Click  Deployment  in the left-hand navigation. From here, there are two ways to export your app: Export App  tab, then click the\n Export App  button. deployment, click the  History  tab and find the\ndeployment you want to export. Once you've found the deployment, click the\n Export  button at the end of the row. Use your MongoDB Atlas Admin API Key to log in to the CLI: The  pull  command downloads an application's\nconfiguration files to a local directory. You choose which\napp to export by specifying its Client App ID with the\n --remote  flag. By default the command pulls files into the current working\ndirectory. You can configure the command to create a new\ndirectory instead by specifying the directory path with the\n --local  flag. The command also supports  additional flags  that you can optionally include to\ncustomize your app. The following table lists common flags\nyou might use: The directory specified by  --local  must not already\nexist, otherwise the CLI throws an error and does not\npull the configuration files. --include-dependencies If specified, external dependencies are exported as a\n node_modules  archive in the  /functions \ndirectory. --include-hosting If specified, hosted static assets are exported\nin the  /hosting/files  directory. Call the admin user authentication endpoint with your MongoDB Atlas API\nkey pair: If authentication succeeds, the response body contains a JSON object\nwith an  access_token  value: The  access_token  grants access to the App Services Admin API. You\nmust include it as a Bearer token in the  Authorization  header for\nall Admin API requests. API Authentication Documentation To export your App, send a request to the\n Export an App  endpoint. Make sure to include your Admin API  access_token , the\n groupId  of the Atlas project containing your App, and\nthe App's internal  appId  hex string: You can export the configuration for a specific deployment\nby specifying the deployment  _id  as a URL query\nparameter: If the request is successful, the endpoint returns a  200 \nresponse that contains a  .zip  directory of your\napplication's current configuration files. An exported App contains configuration files for every component of the\nApp. For detailed information about each type of configuration file, see\n App Configuration . An exported configuration directory has the following structure:",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices login --api-key=\"<my api key>\" --private-api-key=\"<my private api key>\""
                },
                {
                    "lang": "shell",
                    "value": "appservices pull --remote=<Client App ID>"
                },
                {
                    "lang": "shell",
                    "value": "appservices pull --remote=<App ID> --local=<Output Directory Path>"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/auth/providers/mongodb-cloud/login \\\n  -H 'Content-Type: application/json' \\\n  -H 'Accept: application/json' \\\n  -d '{\n    \"username\": \"<Public API Key>\",\n    \"apiKey\": \"<Private API Key>\"\n  }'"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"access_token\": \"<access_token>\",\n  \"refresh_token\": \"<refresh_token>\",\n  \"user_id\": \"<user_id>\",\n  \"device_id\": \"<device_id>\"\n}"
                },
                {
                    "lang": "shell",
                    "value": "curl -X GET \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/export \\\n  -H 'Authorization: Bearer <access_token>' \\\n  -H 'Content-Type: application/json' \\\n  --output ./exported-app.zip"
                },
                {
                    "lang": "text",
                    "value": "https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/export?deployment={deploymentId}"
                },
                {
                    "lang": "none",
                    "value": "app/\n\u251c\u2500\u2500 realm_config.json\n\u251c\u2500\u2500 auth/\n\u2502   \u251c\u2500\u2500 providers.json\n\u2502   \u2514\u2500\u2500 custom_user_data.json\n\u251c\u2500\u2500 data_sources/\n\u2502   \u2514\u2500\u2500 <service name>/\n\u2502       \u251c\u2500\u2500 config.json\n\u2502       \u2514\u2500\u2500 <database>/\n\u2502           \u2514\u2500\u2500 <collection>/\n\u2502               \u251c\u2500\u2500 schema.json\n\u2502               \u2514\u2500\u2500 rules.json\n\u251c\u2500\u2500 environments/\n\u2502   \u251c\u2500\u2500 no-environment.json\n\u2502   \u251c\u2500\u2500 development.json\n\u2502   \u251c\u2500\u2500 testing.json\n\u2502   \u251c\u2500\u2500 qa.json\n\u2502   \u2514\u2500\u2500 production.json\n\u251c\u2500\u2500 functions/\n\u2502   \u251c\u2500\u2500 config.json\n\u2502   \u251c\u2500\u2500 <function>.js\n\u2502   \u2514\u2500\u2500 <directory>/\n\u2502       \u2514\u2500\u2500 <function>.js\n\u251c\u2500\u2500 graphql/\n\u2502   \u251c\u2500\u2500 config.json\n\u2502   \u2514\u2500\u2500 custom_resolvers\n\u2502       \u2514\u2500\u2500 <resolver name>.json\n\u251c\u2500\u2500 hosting/\n\u2502   \u251c\u2500\u2500 config.json\n\u2502   \u251c\u2500\u2500 metadata.json\n\u2502   \u2514\u2500\u2500 files/\n\u2502       \u2514\u2500\u2500 <files to host>\n\u251c\u2500\u2500 http_endpoints/\n\u2502   \u251c\u2500\u2500 config.json\n\u2502   \u251c\u2500\u2500 data_api_config.json\n\u2502   \u2514\u2500\u2500 [Deprecated] <service name>/\n\u2502       \u251c\u2500\u2500 config.json\n\u2502       \u2514\u2500\u2500 incoming_webhooks/\n\u2502           \u251c\u2500\u2500 config.json\n\u2502           \u2514\u2500\u2500 source.js\n\u251c\u2500\u2500 log_forwarders/\n\u2502   \u2514\u2500\u2500 <name>.json\n\u251c\u2500\u2500 sync/\n\u2502   \u2514\u2500\u2500 config.json\n\u251c\u2500\u2500 triggers/\n\u2502   \u2514\u2500\u2500 <trigger name>.json\n\u2514\u2500\u2500 values/\n    \u2514\u2500\u2500 <value name>.json"
                }
            ],
            "preview": "You can download a directory of your App's configuration files by\nexporting them. This allows you to save your App's configuration in\nsource control and to work locally with App Services CLI and your preferred\ntext editor.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "apps/cicd",
            "title": "Set Up a CI/CD Pipeline",
            "headings": [
                "Overview",
                "Pipeline Stages",
                "Development",
                "Staging",
                "Production",
                "Build Tasks",
                "Configure the Environment",
                "Set Up App Services CLI",
                "Create an App",
                "Update an App",
                "Run Tests Against the App",
                "Clean Up the Job"
            ],
            "paragraphs": "Many developers use  continuous integration, delivery, and deployment\npipelines  to automatically test and publish their applications whenever\nthey make changes. This is most common and useful for larger apps where multiple\npeople work on the codebase in parallel using a shared version control system\nlike git. This guide covers the high level stages common to most CI/CD pipelines and\ndescribes what you might do in each stage. Further, it includes a list of common\ntasks and actions you might perform within your pipelines to configure and test\nyour Atlas App Services Apps. If you'd like to see an example CI/CD pipeline that manages testing,\ndeployment, and other tasks for a real application, check out the article\n How to Build CI/CD Pipelines for App Services Apps Using GitHub Actions \non the MongoDB Developer Hub. At a high level most pipelines share a common pattern of moving through multiple\nstages that each handle different concerns. The Development stage is the first step for creating new features and fixing\nbugs in an App. In this stage you work with your application's\nconfiguration files and source code to implement your desired changes. To develop new features for an existing app: Fork the main app and  deploy a new development copy . This instance will have a different App ID than your\nproduction app. You can also use  environment value templates  to use development data sources and other services that\nare not linked to production. Develop your application. This could involve updating or adding a client app\nscreen, adding a new database trigger, or any other application features. You\ncan use  Development Mode  if you need to\nmake changes to your synced Realm Object Schema. Run automated tests locally to ensure that your code does not introduce any\nnew errors. Tests that pass locally do not guarantee that your app is free of\nintegration bugs, but increase confidence that your changes do not include any\nregressions or unintended behavior. The Staging stage, which you might also call QA (Quality Assurance), Testing, or\nPre-Production, is a step that simulates your development changes in an\nenvironment that is as similar to production as possible. This gives you a\nusable version of your app for review and can help you catch integration bugs\nwith live services without affecting production data. The specifics of your staging deployment depend on your application's needs.\nHowever, you can use the following high level procedure to set it up: Set up your staging environment.  Use separate, non-production services and\ndata sources with configurations that mirror production as closely as\npossible. For example, you might use an Atlas cluster named  staging  that\notherwise has the same configuration as your  production  cluster.\nDepending on your use case, you may have a consistent App that you\nreuse for all staging builds or you may create a new App for each\nstaging build. Create or use an existing staging build.  You can automatically create a\nstaging build as part of your CI/CD process, such as when you create a new\nnew pull request. You can  use a new app  for each\nstaging build or you can  reuse a prebuilt environment  that you share across builds. Verify that your app behaves as expected.  This might involve running an\nautomated test suite against your staging environment, manually checking\nbehavior, or getting approval through a user-acceptance test. The Production stage is the final deployment step where your modified app is\ndeployed into your production environment. Ideally at this stage you have\nalready tested your changes locally and in staging to confirm that they are safe\nto deploy. You can either deploy to production manually or automatically as part\nof your CI/CD workflow by  updating your production app . This section outlines common tasks that you will perform in your CI/CD pipeline.\nYou may not always do all of these tasks depending on your use case and pipeline\nstage, but in general most pipelines will perform all of these at least once. The configuration and code for your app should generally be similar between\ndevelopment stages. However, you'll want to change the value of certain\nconfiguration options depending on the environment. Determine what stage you're building and set the appropriate configuration\nvalues. For example, you might configure the app with the App ID of a new app in\nthe Development stage or use your production App ID in the Production stage. You might not always be able to hardcode your App ID. You can look up a\nspecific App ID with App Services CLI. For an example, see  Create an App . The App Services CLI is the easiest way to programmatically\ncreate, configure, and manage Atlas Apps. You should install and\nuse the latest version in your deployment scripts. You'll also need a MongoDB Atlas public/private API key pair to authenticate and\nuse the CLI. For more information and a walkthrough of how to get an API key,\nsee  Programmatic API Keys . To log in, save your API keys in a new named profile configuration and then log\nin with that profile: App Services CLI is available on  npm . To install the CLI on your system,\nensure that you have  Node.js \ninstalled and then run the following command in your shell: Make sure to use the  --profile  flag in all of your commands, otherwise\nApp Services CLI won't recognize that you're logged in. You can use App Services CLI to create new apps to use in development and testing. If\nyour pipeline is in the Development or Staging phase, you should deploy and test\nchanges with an app other than your live production app. To use a new app for your development or staging branch: Create a New App Push a new app based on your branch of the app's  configuration\nfiles : Save the App ID The new app has a unique App ID value that you'll need to identify it later\nin your pipeline and in your client app. You should save the value to an\nenvironment variable, file, or other location. You can use App Services CLI to update an existing app, like a shared staging app or\nyour production deployment. The app already exists, so you should be able to\nlook up its App ID. To update an existing app, specify its App ID in the  --remote  flag: Your app should include automated unit and integration test suites that you can\nrun to verify that everything works. The specifics of your test setup will vary\ndepending upon your app, but you may need to run tests across multiple platforms\nusing a variety of simulators. If you have integration tests, you could checkout previous releases and run your\nintegration tests against the current version of the App to ensure\nbackwards compatibility. At the end of a CI/CD stage or pipeline you may want to clean up resources that\nyou created specifically for that test. For example, if you create a new\nDevelopment or Staging app, you might delete the apps and any databases\nassociated with them once your changes are merged. Alternatively, you would not\nwant to clean up your production app or a persistent staging app if you use one. Before you clean up, consider what resources may be useful in future. For\nexample, you could choose to skip deleting apps and their databases if your\ntests fail. That way you can manually investigate the issue and find any app\nsettings or data that caused the failure.",
            "code": [
                {
                    "lang": "bash",
                    "value": "# Use the production App ID for the main branch\nexport REALM_APP_ID=\"myapp-abcde\"\n# Use a staging App ID for the QA branch\nexport REALM_APP_ID=\"myapp-staging-fghij\"\n# Use a new App ID for development branches - you'll need to create the app first!\nexport REALM_APP_ID=\"myapp-dev-zyxwv\""
                },
                {
                    "lang": "yaml",
                    "value": "<Profile Name>:\n  public_api_key: \"<MongoDB Atlas Public API Key>\"\n  private_api_key: \"<MongoDB Atlas Private API Key>\"\n  atlas_base_url: \"https://cloud.mongodb.com\"\n  realm_base_url: \"https://services.cloud.mongodb.com\"\n  telemetry_mode: \"\""
                },
                {
                    "lang": "bash",
                    "value": "appservices login --profile=\"<Profile Name>\""
                },
                {
                    "lang": "shell",
                    "value": "npm install -g atlas-app-services-cli"
                },
                {
                    "lang": "bash",
                    "value": "cd path/to/realmApp\nappservices push -y --project=\"<MongoDB Atlas Project ID>\" # e.g. --project=\"609ea544934fe445460219a2\""
                },
                {
                    "lang": "bash",
                    "value": "# Save to an environment variable\noutput=$(appservices app describe)\napp_id=$(echo $output | sed 's/^.*client_app_id\": \"\\([^\"]*\\).*/\\1/')\nexport REALM_APP_ID=app_id\n# Save to a file\necho $REALM_APP_ID > ./clients/ios/realm-app-id.txt"
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=$REALM_APP_ID -y"
                }
            ],
            "preview": "Many developers use continuous integration, delivery, and deployment\npipelines to automatically test and publish their applications whenever\nthey make changes. This is most common and useful for larger apps where multiple\npeople work on the codebase in parallel using a shared version control system\nlike git.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "apps/change-deployment-models",
            "title": "Change Deployment Models",
            "headings": [
                "Overview",
                "Run a Deployment Migration",
                "Before You Begin",
                "Procedure",
                "Navigate to the Deployment Screen",
                "Choose a Target Deployment Model and Region",
                "Start the Migration",
                "Monitor the Migration Status",
                "Clean Up After the Migration",
                "Choose a Target Deployment Model and Region",
                "Start the Migration",
                "Monitor the Migration Status",
                "Clean Up After the Migration",
                "Deployment Migration Process",
                "Data Affected"
            ],
            "paragraphs": "You can change an existing App's deployment configuration to deploy it\nin a new model, region, and/or cloud provider. For example, you might\nswitch your App from a global deployment to a specific local AWS region\nor switch an App deployed in the Eastern USA from AWS to Azure. For a\nlist of all available options, see  Deployment Models & Regions . To move an existing App, you begin a  Deployment Migration  that\nautomatically moves your App's data and configuration to the new\nconfiguration. The migration process is designed to be as seamless as\npossible but requires 5 to 30 minutes of downtime. You can\nmonitor the migration status, but no requests will be processed until the\nmigration is complete. For more information, see  Deployment\nMigration Process . We recommend that you contact MongoDB technical support if you are\nplanning to change your deployment model for a production App. To\nlearn how, visit the  MongoDB Support  portal. You can begin a deployment migration at any time. Only one migration may\nbe in progress at a time for a single App. If you try to start a migration while another\nis in progress, the new migration does not run and fails with an error. If you connect to your App from a Realm SDK, you must update your SDK to\na version that supports changing deployment models. If your App's SDK\nversion does not support changing deployment models, you'll need to reinstall\nyour app. If you change deployment models before upgrading, the SDK will\nnot be able to connect and requests will fail. Minimum SDK version: Realm C++ SDK v0.2.0 Realm Flutter SDK v1.2.0 Realm Kotlin SDK v1.10.0 Realm .NET SDK v11.1.0 Realm Node.js SDK v12.0.0 (pending release) Realm React Native SDK v12.0.0 (pending release) Realm Swift SDK v10.40.0 Before changing deployment models, please note the following: All logs and drafts will be lost. Suspended Triggers will restart. Any existing Private Endpoints will need to be recreated for the new region. Triggers and Device Sync operations will be paused during the migration. Changing your deployment model is not a draft. This change\ncannot be reversed after it is saved. You will need the following to modify an App's deployment model in\nthe Atlas UI: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . You will need the following to modify an App's deployment model\nwith the Admin API: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . A MongoDB Atlas Admin API  public/private key pair . The API key must\nhave  Project Owner  permissions to work with App\nServices Admin API. Your App's internal ObjectId hex string and the Project ID of the\nAtlas Project that contains your App. To learn how to find these, see\n Get App Metadata . Click  App Settings  in the left navigation menu.\nOn the settings page, find the  Deployment Region \nsection and click the  Edit  button. You can migrate an App to any valid deployment configuration. First, choose to migrate to either a specific  LOCAL \nregion or a  GLOBAL  deployment distributed around the\nworld. If you choose  GLOBAL , choose one of the global regions\nto host your App's configuration data. If you choose  LOCAL , choose a specific cloud provider\nand region to deploy to. For a list of all available\noptions, see  Deployment Models & Regions . Once you've specified your desired deployment configuration,\nclick  Continue . You should see a confirmation\nprompt with a checklist. Review and check each item in the\nlist to confirm that you understand the impact of the\nmigration. Once you've reviewed the checklist, being the migration by\nclicking  Change region . While the migration is in progress, the UI displays a banner\nat the top of the page with the current migration status. Some features and services will not continue to work after a\nmigration and must be reconfigured. If you use any of these\nfeatures, follow the clean up steps below to restore\nfunctionality: Feature Clean Up Steps VPC Private Endpoints VPC Private Endpoints are region-specific. After\nmigrating to a new region, you must create new VPC\nPrivate Endpoints in the new region and update your\napplication to use the new endpoints. You cannot use VPC Private Endpoints if you migrated\nto a global deployment or to a local region in Azure\nor GCP. Sending Requests Once migration is complete, you must send requests\nusing new URLs, if applicable. You can migrate an App to any valid deployment configuration. First, choose to migrate to either a specific  LOCAL \nregion or a  GLOBAL  deployment distributed around the\nworld. If you choose  GLOBAL , choose one of the global regions\nto host your App's configuration data. If you choose  LOCAL , choose a specific cloud provider\nand region to deploy to. For a list of all available\noptions, see  Deployment Models & Regions . To start a migration, call the  Create a\nDeployment Migration  endpoint with\nthe deployment model and region you want to migrate to in\nthe request body. To get the current status of a deployment migration, call\nthe  Get a Deployment Migration \nendpoint. Some features and services will not continue to work after a\nmigration and must be reconfigured. If you use any of these\nfeatures, follow the clean up steps below to restore\nfunctionality: Feature Clean Up Steps VPC Private Endpoints VPC Private Endpoints are region-specific. After\nmigrating to a new region, you must create new VPC\nPrivate Endpoints in the new region and update your\napplication to use the new endpoints. You cannot use VPC Private Endpoints if you migrated\nto a global deployment or to a local region in Azure\nor GCP. Sending Requests Once migration is complete, you must send requests\nusing new URLs, if applicable. Deployment migrations move your App's data and configuration to one or\nmore new regions in a series of stages. At each stage, the process\nmigrates a portion of your App to the new model and cleans up any\nartifacts from the previous model. A deployment migration moves through the following stages in order: \"started\" : the migration has been started \"downtime\" : the App is unavailable while the migration is in progress \"enabling_event_subscriptions\" : the App's Triggers and Device\nSync translators are being enabled \"cleanup\" : deployment artifacts are being cleaned up The migration is complete and can be in one of two states: \"successful\" : the migration completed successfully \"failed\" : the migration failed The migration process does not affect any application data stored in a\nMongoDB Atlas cluster. The migration process migrates the following components of your App: The migration process  does not  migrate the following data: User accounts App configuration files Triggers API Services Device Sync translators Application logs Deployment drafts Data stored in a MongoDB Atlas cluster",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"deployment_model\": \"GLOBAL\",\n  \"provider_region\": \"aws-us-east-1\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"deployment_model\": \"LOCAL\",\n  \"provider_region\": \"azure-westus\"\n}"
                },
                {
                    "lang": "sh",
                    "value": "curl -X PUT \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/deployment_migration \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer <AccessToken>' \\\n  -d '{\n    \"deployment_model\": \"<DeploymentModel>\",\n    \"provider_region\": \"<RegionID>\"\n  }'"
                },
                {
                    "lang": "sh",
                    "value": "curl -X GET \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/<groupId>/apps/<appId>/deployment_migration \\\n  -H 'Authorization: Bearer <AccessToken>'"
                }
            ],
            "preview": "You can change an existing App's deployment configuration to deploy it\nin a new model, region, and/or cloud provider. For example, you might\nswitch your App from a global deployment to a specific local AWS region\nor switch an App deployed in the Eastern USA from AWS to Azure. For a\nlist of all available options, see Deployment Models & Regions.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "apps/environment",
            "title": "Configure an App Environment",
            "headings": [
                "Overview",
                "Environment Values",
                "How to Use Environments",
                "Before You Begin",
                "Procedure",
                "Create a New Value",
                "Define Values for Each Environment",
                "Specify Your App Environment",
                "Save and Deploy",
                "Authenticate a MongoDB Atlas User",
                "Pull the Latest Version of Your App",
                "Define One or More Environments",
                "Set Your App Environment",
                "Deploy Your App",
                "Authenticate a MongoDB Atlas User",
                "Define Environment Values",
                "Set Your App Environment"
            ],
            "paragraphs": "App Environments are a way to organize your development workflow and\neffectively reuse code. You can use App Environments to define different\nmultiple versions of global configuration values and then switch between\nthem by changing the App's current environment. App Services supports a set of built-in environment names that each\nrepresent a stage of your development workflow: \"\" \"development\" \"testing\" \"qa\" \"production\" For each environment, you can define a set of  environment values  that\nare available to your application when that environment is active. For example, you might use different values for an API's  baseUrl \nconfiguration depending on whether you're developing a new feature or\ndeploying to production. The values for an environment are stored as a single JSON object. You\ncan set the field names and corresponding values to whatever you want. You can access values from the current environment by field name: Use  context.environment  in Functions. Use  %%environment  in rule\nexpressions. Use  Templated Configurations  in your\nApp's configuration files. Environment tags each represent a specific stage of your development\nprocess. You can define a separate App for each environment, where each\nApp uses the same set of configuration files but has a distinct\nenvironment tag. To learn how, see  Copy an App . For example, a typical development process may have three development\nstages:  development ,  testing , and  production . You could use\na separate App for each stage and use different environment values for\neach App. You can also use unique applications to test individual feature\nbranches. For example, you might have a core  development  App that\ndevelopers fork for their feature branches. When a feature branch moves\ninto testing, you can use the core  testing  App instead. To learn how to incorporate environments into your CI/CD workflow, see\n Set Up a CI/CD Pipeline . For a full example that\nmanages testing, deployment, and other tasks for a real application,\nsee:  How to Build CI/CD Pipelines for App Services Apps using GitHub\nActions . You will need the following to define an App Environment in the Atlas UI: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . You will need the following to define an App Environment in the CLI: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . A MongoDB Atlas Admin API  public/private key pair . The API key must\nhave  Project Owner  permissions to work with App\nServices Admin API. A copy of App Services CLI installed and added to your local system  PATH . To\nlearn how, see  Install App Services CLI . Your App's client App ID. This is the unique string that contains your\nApp name, e.g.  \"myapp-abcde\" . To learn how to find your App ID, see\n Get App Metadata . You will need the following to define an App Environment with the Admin API: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . A MongoDB Atlas Admin API  public/private key pair . The API key must\nhave  Project Owner  permissions to work with App\nServices Admin API. Your App's internal ObjectId hex string and the Project ID of the\nAtlas Project that contains your App. To learn how to find these, see\n Get App Metadata . In the App Services UI, click  Values  in the left\nnavigation menu, then click  Create New Value . Enter a name for the value and select\n Environment  for the value type. Define a value for each environment tag that you want to\naccess the value from. You can define a different value for\neach environment and may leave a value undefined in any\nenvironment. Your App always runs in a specific environment, which affects the value of all\nenvironment values. You can specify the current environment for your App on\nthe  Deploy > Environment  screen. Once you've defined the environment value, click  Save  to update\nthe configuration. If your application has deployment drafts enabled, click\n Review & Deploy  to deploy the changes. Use your MongoDB Atlas Admin API Key to log in to the CLI: Get a local copy of your App's configuration files. To get the latest\nversion of your App, run the following command: You can also export a copy of your application's configuration files\nfrom the UI or with the Admin API. To learn how, see  Export an App . The  /environments  directory contains a  .json  file\nfor each supported environment tag. Each file defines all of\nthe environment values for its corresponding environment. App Services supports the following environment tags: Open the file for each environment you want to use. In each\nfile, add an entry to the  values  subdocument that maps\nthe value's name to its value in that environment. \"\" \"development\" \"testing\" \"qa\" \"production\" The following configurations define the  baseUrl  environment value in\nthe  production  and  development  environments. Your app always runs in a specific environment, which affects the value of all\nenvironment values. You can specify the current environment for your app in\n realm_config.json : Once you've defined values for each environment, you can push the updated\nconfigurations to your remote app. App Services CLI immediately deploys the updated\nenvironment values on push. Call the admin user authentication endpoint with your MongoDB Atlas API\nkey pair: If authentication succeeds, the response body contains a JSON object\nwith an  access_token  value: The  access_token  grants access to the App Services Admin API. You\nmust include it as a Bearer token in the  Authorization  header for\nall Admin API requests. API Authentication Documentation You can define environment values for your App by calling\nthe  Create an Environment Value \nendpoint. Each environment value has a name and a  values \nobject that maps environment tags to the value in the\nenvironment. You can set your App's environment tag by calling the\n Set the App Environment  endpoint.",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices login --api-key=\"<my api key>\" --private-api-key=\"<my private api key>\""
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "json",
                    "value": "{\n  \"values\": {\n    \"baseUrl\": \"https://example.com\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"values\": {\n    \"baseUrl\": \"https://dev.example.com\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  ...,\n  \"environment\": \"development\"\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=\"<Your App ID>\""
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/auth/providers/mongodb-cloud/login \\\n  -H 'Content-Type: application/json' \\\n  -H 'Accept: application/json' \\\n  -d '{\n    \"username\": \"<Public API Key>\",\n    \"apiKey\": \"<Private API Key>\"\n  }'"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"access_token\": \"<access_token>\",\n  \"refresh_token\": \"<refresh_token>\",\n  \"user_id\": \"<user_id>\",\n  \"device_id\": \"<device_id>\"\n}"
                },
                {
                    "lang": "bash",
                    "value": "curl -X GET \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/environment_values/{environmentValueId} \\\n  -H 'Authorization: Bearer <access_token>' \\\n  -d '{\n    \"name\": \"myEnvironmentValue\",\n    \"values\": {\n      \"none\": \"alpha\",\n      \"development\": \"beta\",\n      \"testing\": \"gamma\",\n      \"qa\": \"delta\",\n      \"production\": \"epsilon\"\n    }\n  }'"
                },
                {
                    "lang": "bash",
                    "value": "curl -X PUT \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/environment \\\n  -H 'Authorization: Bearer <access_token>' \\\n  -d '{ \"environment\": \"production\" }'"
                }
            ],
            "preview": "App Environments are a way to organize your development workflow and\neffectively reuse code. You can use App Environments to define different\nmultiple versions of global configuration values and then switch between\nthem by changing the App's current environment.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "schemas/types",
            "title": "Schema Types",
            "headings": [
                "All Schema Types",
                "BSON Types",
                "Array",
                "Boolean",
                "Mixed",
                "Number",
                "Object",
                "ObjectId",
                "String",
                "UUID",
                "Binary Data",
                "Realm Database Types",
                "Set",
                "Dictionary",
                "Dictionary of a BSON Type",
                "Dictionary of Mixed BSON Types",
                "Dictionary of Embedded Objects",
                "Geospatial Data",
                "GeoJSON Point",
                "GeoJSON Points and Device Sync Queries",
                "Querying"
            ],
            "paragraphs": "The following fields are available for all BSON schemas regardless of type: Field Name Description bsonType The BSON type of the property the schema describes. If the\nproperty's value can be of multiple types, specify an array of\nBSON types. Cannot be used with the  type  field. BSON types include all JSON types as well as additional types\nthat you can reference by name: double string object array objectId date bool null regex int timestamp long decimal uuid binData mixed type The JSON type of the property the schema describes. If the\nproperty's value can be of multiple types, specify an array of\nJSON types. Cannot be used with  bsonType . The following standard JSON types are available: Atlas App Services supports the  type  field to maintain compatibility with the JSON\nschema standard. However, we recommend that you use the  bsonType \nfield instead. BSON types include all JSON schema types and support even\nmore data types. object array number boolean string null MongoDB's JSON Schema implementation does not support the\n integer  JSON type. Instead, use the  bsonType  field\nwith  int  or  long  as the value. enum An array that includes all valid values for the data that the\nschema describes. title A short title or name for the data that the schema models. This\nfield is used for metadata purposes only and has no impact on\nschema validation. description A detailed description of the data that the schema models. This\nfield is used for metadata purposes only and has no impact on\nschema validation. An  array  contains multiple values of a specific type. BSON  array  schemas\nuse the standard  JSON Schema array \nformat. Field Name Description items A schema for all array items, or an array of schemas where\norder matters. additionalItems Default:  true . If  true , the array may contain additional values that are\nnot defined in the schema. If  false , only values that are\nexplicitly listed in the  items  array may appear in the\narray. If the value is a schema object, any additional fields must\nvalidate against the schema. The  additionalItems  field only affects array schemas\nthat have an array-valued  items  field. If the  items \nfield is a single schema object,  additionalItems  has no\neffect. maxItems The maximum length of the array. minItems The minimum length of the array. uniqueItems Default:  false If  true , each item in the array must be unique.\nIf  false , multiple array items may be identical. To model a  Set , use the  array  schema type\nwith  uniqueItems  set to  true . A  bool  is either  true  or  false . A  mixed  field may contain any schema type except for  arrays ,  embedded objects ,\n sets , and  dictionaries . App Services does not enforce a consistent type across\ndocuments, so two different documents may have values of different types. Mixed fields can represent relationships. Sync translates these relationships\nto MongoDB as a  DBRef  to preserve\nthe database name, collection name, and primary key of the link. A  number  generically configures some type of number. BSON schemas extend\n JSON Schema numerics \nwith additional types to define integers, floats, and decimals. Field Name Description multipleOf An integer divisor of the field value. For example, if\n multipleOf  is set to  3 ,   6  is a valid value but\n 7  is not. maximum The maximum value of the number. exclusiveMaximum Default:  false If  true , the field value must be strictly less than the\n maximum  value. If  false , the field value may also be\nequal to the  maximum  value. minimum The minimum value of the number. exclusiveMinimum Default:  false If  true , the field value must be strictly greater than the\n minimum  value. If  false , the field value may also be\nequal to the  minimum  value. An  object  is a structured object with  string  keys that each have a typed\nvalue. Objects represent Realm objects and embedded objects in synced realms as\nwell as the documents they map to in MongoDB. Field Name Description required An array of field names that must be included in the document. title A type name for the object. App Services uses this value to name\nthe document's type in the  GraphQL API . GraphQL is\ndeprecated.  Learn More . properties An object where each field maps to a field in the parent\nobject by name. The value of each field is a schema document\nthat configures the value of the field. minProperties The minimum number of fields allowed in the object. maxProperties The maximum number of fields allowed in the object. patternProperties An object where each field is a regular expression string that\nmaps to all fields in the parent object that match. The value\nof each field is a schema document that configures the value\nof matched fields. additionalProperties Default:  true . If  true , a document may contain additional fields that are\nnot defined in the schema. If  false , only fields that are\nexplicitly defined in the schema may appear in a document. If the value is a schema object, any additional fields must\nvalidate against the schema. dependencies Specify property and schema dependencies. To model  dictionaries , use\nthe  object  schema type with  additionalProperties  set to\nthe object type of the values stored in the dictionary. An  objectId  is a 12-byte identifier for  BSON  objects. ObjectId values are most commonly used as the\nunique  _id  values of documents in a MongoDB collection or objects in a\nsynced realm. A  string  is text encoded as a series of characters. BSON  string  schemas\nuse the standard  JSON Schema string \nformat. Field Name Description maxLength The maximum number of characters in the string. minLength The minimum number of characters in the string. pattern A regular expression string that must match the string value. A  uuid  (Universal Unique Identifier) is a  standardized \n16-byte unique object identifier. A  binData  is a piece of unstructured binary data. Maps to the  binary\nBSON type . Always uses subtype 0. A set is a collection of unique values. A set schema is an  array  schema where  uniqueItems  is set to  true . A dictionary is a collection of dynamic and unique  string  keys paired with\nvalues of a given type. A dictionary is functionally an object or document\nwithout pre-defined field names. A dictionary schema is an  object  schema where  properties  is not defined\nand the value of  additionalProperties  is a schema for the dictionary value's\ntype. To store a dictionary with values of a BSON type, set  additionalProperties \nto the type's schema. To store a dictionary with  mixed  values, set\n additionalProperties  to  true : Alternatively, you can define a full  mixed  schema: To store a dictionary with embedded object values, define an  object  schema\nwith the  title  field set to the embedded object's type name: Geospatial data describes points and other data on the earth's surface. App\nServices does not have built-in geospatial types. Instead, you model geographic\ndata using standard GeoJSON objects. If you are not using Device Sync, you can query your geospatial data with regular\n geospatial operators . However, if you are\nusing Device Sync, there are additional requirements in how you store and\nquery the data. A GeoJSON Point (GeoPoint) is a single location on the Earth's surface. The GeoPoint\nschema must a  required   type  field, of type string, which is always set\nto \"Point\". You must also provide a  coordinates  field, which is an array of doubles.\nThe  coordinates  array must contain  at least 2 double values, and may\ncontain a third: To store a GeoPoint in Atlas for later querying with  Atlas Device Sync ,\nthe following rules also apply to the GeoPoint object: The following is the Device Sync schema of an object with an embedded GeoPoint\nproperty named \"location\": The following code block shows a document that follows the schema. It has a\nsingle embedded GeoPoint object named \"location\". The first double is the longitude of the point, between -180 and 180. The second is the latitude, between -90 and 90. The optional third value represents the elevation/altitude of the point,\nin meters. Atlas ignores this value when performing queries. You cannot set an array as required in a Device Sync schema, so the Sync\nserver checks that the coordinates field is present and meets the\nrequirements. It must be of type  object . It must be embedded within another type. Additional values are allowed within the  coordinates  array, as long as\nthey are doubles. Additional properties in the GeoPoint object are also allowed. GeoPoints are the only form of geospatial data that Device Sync can\nquery. The Device Sync server validates every embedded GeoPoint property with\nthe rules outlined above. If a device writes an object that fails validation, it will be reverted with a\n compensating write error . If a document that doesn't\nmeet these requirements is written directly to the MongoDB collection, the server\nmarks that document as  unsyncable . You can query a GeoPoint using  Realm Query Language (RQL) ,\nusing  geoWithin  to do so. Although GeoPoints are the only stored type you\ncan query, your query may contain other geospatial types. For example,\nthis partial query looks for all points within the specified GeoCircle: For SDK-specific details about querying against geospatial data, see: Geospatial - Flutter SDK Geospatial - Kotlin SDK Geospatial - .NET SDK Geospatial - Node.js SDK Geospatial - React Native SDK",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"bsonType\": \"<BSON Type>\" | [\"<BSON Type>\", ...],\n  \"type\": \"<JSON Type>\" | [\"<JSON Type>\", ...],\n  \"enum\": [<Value 1>, <Value 2>, ...],\n  \"description\": \"<Descriptive Text>,\n  \"title\": \"<Short Description>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"bsonType\": \"array\",\n  \"items\": <Schema Document> | [<Schema Document>, ...],\n  \"additionalItems\": <boolean> | <Schema Document>,\n  \"maxItems\": <integer>,\n  \"minItems\": <integer>,\n  \"uniqueItems\": <boolean>\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"bsonType\": \"bool\" }"
                },
                {
                    "lang": "json",
                    "value": "{ \"bsonType\": \"mixed\" }"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"bsonType\": \"number\" | \"int\" | \"long\" | \"double\" | \"decimal\",\n  \"multipleOf\": <number>,\n  \"maximum\": <number>,\n  \"exclusiveMaximum\": <boolean>,\n  \"minimum\": <number>,\n  \"exclusiveMinimum\": <boolean>\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"bsonType\": \"object\",\n  \"title\": \"<Type Name>\",\n  \"required\": [\"<Required Field Name>\", ...],\n  \"properties\": {\n    \"<Field Name>\": <Schema Document>\n  },\n  \"minProperties\": <integer>,\n  \"maxProperties\": <integer>,\n  \"patternProperties\": {\n    \"<Field Name Regex>\": <Schema Document>\n  },\n  \"additionalProperties\": <boolean> | <Schema Document>,\n  \"dependencies\": {\n    \"<Field Name>\": <Schema Document> | [\"<Field Name>\", ...]\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"bsonType\": \"objectId\" }"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"bsonType\": \"string\",\n  \"maxLength\": <integer>,\n  \"minLength\": <integer>,\n  \"pattern\": \"<Regular Expression>\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"bsonType\": \"uuid\" }"
                },
                {
                    "lang": "json",
                    "value": "{ \"bsonType\": \"binData\" }"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"bsonType\": \"array\",\n   \"uniqueItems\": true,\n   \"items\": {\n      \"bsonType\": \"long\"\n   }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"bsonType\": \"object\",\n      \"additionalProperties\": {\n         \"bsonType\": \"string\"\n      }\n   }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"bsonType\": \"object\",\n      \"additionalProperties\": true\n   }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"bsonType\": \"object\",\n      \"additionalProperties\": {\n         \"bsonType\": \"mixed\"\n      }\n   }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"bsonType\": \"object\",\n      \"additionalProperties\": {\n         \"bsonType\": \"object\",\n         \"title\": \u201cAddress\u201d,\n         \"properties\": {\n           \"streetNumber\": { \"bsonType\": \"string\" },\n           \"street\": { \"bsonType\": \"string\" },\n           \"city\": { \"bsonType\": \"string\" },\n           \"province\": { \"bsonType\": \"string\" },\n           \"country\": { \"bsonType\": \"string\" },\n           \"postalCode\": { \"bsonType\": \"string\" }\n         }\n   }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"MyObject\",\n  \"properties\": {\n     \"_id\": {\n       \"bsonType\": \"objectId\"\n     },\n     \"location\": {\n       \"bsonType\": \"object\",\n       \"required\": [ \"type\" ],\n       \"properties\": {\n         \"type\": {\n           \"bsonType\": \"string\",\n         },\n         \"coordinates\": {\n           \"bsonType\": \"array\",\n           \"items\": {\n             \"bsonType\": \"double\"\n           }\n         }\n       }\n     }\n   }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": { \"$oid\": \"65039d09fe4e46dddee31a3f\" },\n  \"location\": {\n    \"type\": \"Point\",\n    \"coordinates\": [-122.4, 48.12, 23.0]\n   }\n}"
                },
                {
                    "lang": "json",
                    "value": "location geoWithin GeoCircle([-73.981209, 40.725055], 0.00050464271)"
                }
            ],
            "preview": "The following fields are available for all BSON schemas regardless of type:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "apps/metadata",
            "title": "Get App Metadata",
            "headings": [
                "Find Your App ID",
                "Find Your App in the Atlas UI",
                "Copy Your App ID",
                "Find Your Atlas Project ID",
                "Choose Your Atlas Organization",
                "Find Your Project",
                "Copy the Project ID"
            ],
            "paragraphs": "Every App has a unique ID. You use App IDs to specify which app you want\nto use or modify. You can find your Project ID in the App Services UI or use the Admin API\nor App Services CLI to find it programmatically. Log into the  MongoDB Atlas UI  and then click the\n App Services  tab. Find your App and click on its summary card to open the App. Click the  Copy App ID  button next to the name of your\nApp near the top of the left navigation menu. The UI displays your App name in the top left. When you click the\ncopy button, the UI copies your App ID, not your App's name, into\nyour clipboard. For example, the UI would display the name\n\"exampleApp\" but copy the id \"exampleApp-wlkgs\". To find an App ID, run  appservices apps list  and find the App you're interested in the\nlist returned by the command. Each entry in the list shows an App's project ID as well as two App\nID values you may want: The Client App ID, which is a unique string that includes the\nApp name. Use this value to connect to your App through the Data API, or\na Realm SDK. The  _id  value, which is an internal ObjectId hex string. Use\nthis value in place of  {appId}  in Admin API endpoints. To find an App ID, send a request to the  List\nAll Apps  API endpoint. The endpoint returns a list of Apps associated with the Atlas\nproject. Find the App you're interested in and copy its App ID. There are two App ID values you may want: The  _id  value, which is an internal ObjectId hex string. Use this\nvalue in place of  {appId}  in Admin API endpoints. The  client_app_id , which is a unique string that includes the App\nname. Use this value to connect to your App through the Data API or a\nRealm SDK. Make sure to replace  <access_token>  and  {groupId}  with\nyour own values. Every App is associated with a single Atlas project. You use the\nproject's unique Project ID value, sometimes referred to as a \"Group\nID\", to identify and work with your App programatically. You can find your Project ID in the Atlas UI or use the Atlas Admin API\nor Atlas CLI to find it programmatically. You can find your Project ID in the App Services Admin UI. In the upper left-hand corner of your Atlas Dashboard, you'll see\na drop-down menu that contains the Organizations you can access.\nSelect the Organization that contains the Project you'd like to\nview. Click  Projects  in the left navigation menu to view the\nlist of project in the Organization. Find your project in the\nlist. The right side of the project list view contains an  Actions \ncolumn. Click the ellipses ( ... ) icon to expand the Actions menu, and select\n Copy Project ID . You can list all Atlas projects that you have access to with the\nAtlas CLI's  projects list  command. The command returns a list of projects in JSON format. Find\nyour project in the list and copy its  id  value. The App Services CLI includes Project IDs when you list Apps. If you\nwant to find the Project ID of an existing App, you can run\n appservices apps list . To learn more, see\n Find Your App ID . You can list the Atlas projects that have access to with the Atlas\nAdmin API's  List All Projects \nendpoint. The endpoint returns a list of projects in JSON format. Find your\nproject in the list and copy its  id  value.",
            "code": [
                {
                    "lang": "bash",
                    "value": "appservices apps list"
                },
                {
                    "lang": "text",
                    "value": "Found 3 apps\n  Client App ID                    Project ID                _id\n  -------------------------------  ------------------------  ------------------------\n  myapp-abcde                      5b2ef33692f119212341b213  64343a2b2107b2523e60fb59\n  myapp-dev-uvxyz                  5b2ef33692f119212341b213  6526d3582b032aff90351070\n  my-other-app-abcde               5b2ef33692f119212341b213  674c19acef214b9ad99a0bbe"
                },
                {
                    "lang": "bash",
                    "value": "curl https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps \\\n  --header 'Authorization: Bearer <access_token>'"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    \"_id\": \"63ea9328dddad2523e60fb59\",\n    \"client_app_id\": \"myapp-abcde\",\n    \"group_id\": \"57879f6cc4b32dbe440bb8c5\",\n    \"domain_id\": \"5886619e46124e4c42fb5dd8\",\n    \"name\": \"myapp\",\n    \"location\": \"US-VA\",\n    \"deployment_model\": \"GLOBAL\",\n    \"last_used\": 1615153544,\n    \"last_modified\": 0,\n    \"product\": \"standard\",\n    \"environment\": \"\"\n  }\n]"
                },
                {
                    "lang": "bash",
                    "value": "atlas projects list"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"links\": [\n    {\n      \"rel\": \"self\",\n      \"href\": \"https://cloud.mongodb.com/api/atlas/v1.0/groups?pageNum=1\\u0026itemsPerPage=100\"\n    }\n  ],\n  \"results\": [\n    {\n      \"id\": \"5d0171d479328f10cb4f3037\",\n      \"orgId\": \"629e04e11633f764462ea109\",\n      \"name\": \"MyProject\",\n      \"clusterCount\": 1,\n      \"created\": \"2019-06-23T05:08:35Z\",\n      \"links\": [\n        {\n          \"rel\": \"self\",\n          \"href\": \"https://cloud.mongodb.com/api/atlas/v1.0/groups/5d0171d479328f10cb4f3037\"\n        }\n      ]\n    }\n  ],\n  \"totalCount\": 1\n}"
                },
                {
                    "lang": "bash",
                    "value": "curl https://cloud.mongodb.com/api/atlas/v1.0/groups \\\n  --digest -u <Atlas Public API Key>:<Atlas Private API Key>"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"links\": [\n    {\n      \"rel\": \"self\",\n      \"href\": \"https://cloud.mongodb.com/api/atlas/v1.0/groups?pageNum=1\\u0026itemsPerPage=100\"\n    }\n  ],\n  \"results\": [\n    {\n      \"id\": \"5d0171d479328f10cb4f3037\",\n      \"orgId\": \"629e04e11633f764462ea109\",\n      \"name\": \"MyProject\",\n      \"clusterCount\": 1,\n      \"created\": \"2019-06-23T05:08:35Z\",\n      \"links\": [\n        {\n          \"rel\": \"self\",\n          \"href\": \"https://cloud.mongodb.com/api/atlas/v1.0/groups/5d0171d479328f10cb4f3037\"\n        }\n      ]\n    }\n  ],\n  \"totalCount\": 1\n}"
                }
            ],
            "preview": "Every App has a unique ID. You use App IDs to specify which app you want\nto use or modify.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "apps/deploy-github",
            "title": "Deploy Automatically with GitHub",
            "headings": [
                "Overview",
                "Prerequisites",
                "Enable Automatic Deployment with GitHub",
                "Install the Atlas App Services GitHub App",
                "Specify a GitHub Repository",
                "Initialize the Repository",
                "Enable Automatic Deployment",
                "Make Changes to Your Application",
                "Commit and Push Your Changes",
                "Authenticate a MongoDB Atlas User",
                "Run the Deploy Configure Command",
                "Disable, Enable, or View Configuration Details (Optional)",
                "Authenticate a MongoDB Atlas User",
                "Get an Existing Deployment Configuration (Optional)",
                "Update the Deployment Configuration",
                "Make Changes from the UI",
                "Make Changes from the CLI",
                "Summary"
            ],
            "paragraphs": "You can configure an Atlas App Services App to automatically deploy\nwhenever you push App configuration files to a  GitHub \nrepository. You can clone the GitHub repository to your computer and\nthen use standard Git commands to pull down the latest versions and\ndeploy new changes. A  GitHub  account and repository. The repository should\neither be empty or contain an exported configuration directory for an\nexisting App. For information on how to create an empty repository,\nsee GitHub's  create a repo  guide. An installed copy of the Git CLI. If you do not have  git \ninstalled, see the official guide for  Installing Git . A  MongoDB Atlas \n Programmatic API Key \nto authenticate and authorize access to your app's underlying Git\nrepo. If you have not yet generated a programmatic API key for your\nAtlas organization, do so now. You can enable automatic deployment with GitHub using the App Services UI,\nthe App Services CLI, or the App Services Admin API. In order to automatically deploy based on a GitHub repository,\nAtlas App Services requires that you install a GitHub app that has, at minimum,\nread access to the repository. To install the app, click  Deployment  in the left navigation menu of the App Services UI.\nSelect the  Configuration  tab and then click  Install\nApp Services on GitHub . A new browser window will open to the GitHub application\ninstallation flow. Select the GitHub account or organization for which you want to install the\napp: Specify one or more repositories for which you want to grant App Services read\naccess. You can either select specific repositories or grant access to all of\nyour repositories on GitHub. Select the repositories you want to use and then\nclick  Install . After you install the application, return to the  Configuration \ntab and click the  Authorize  button to finish connecting\nApp Services to GitHub. This leads you to a github permissions screen.\nClick  Authorize MongoDB Atlas App Services . GitHub may require you to provide your GitHub account credentials before\nyou can install the app. You can manage permissions for the App Services GitHub application from\nthe  Installed GitHub Apps  page in your\nGitHub settings. Once you have linked your GitHub account to your App, you can specify a\nrepository that App Services should automatically deploy. Specify the\n Repository ,  Branch , and  Directory  that\ncontain the App's configuration files and then click  Save . The list of repositories will only contain repos that you have\ngranted App Services read access to. Clone a local copy of the Git repository that you specified: The configured branch and directory must contain configuration\nfiles that define your application. You can create the\nconfiguration manually or  export the application directory  of an existing app. Add the application directory to the repository and then commit the changes: Once you have committed the latest version of the application to the\nrepository, push it to your GitHub repository: After you have specified a repository to deploy and initialized it with the\nlatest version of your app, you need to enable automatic deployment to begin\ndeploying it. On the  Configuration  tab of the  Deploy \npage, click  Enable Automatic Deployment . In the modal that\nappears, click  Enable Automatic Deployment  to confirm your\nselection. A deployment contains one or more changes that you have made to your\napplication since the previous deployment. Make any additions, modifications,\nor deletions to the repository that you want to include in your deployment. Refer to the  App Configuration  reference page and individual component\nreference pages for details on the structure and schema of your application\ndirectory. Because  breaking - also called destructive - schema changes  require special handling, App Services\ndoes not support making breaking schema changes via automated deploy\nwith GitHub. Instead, you should make breaking changes from the\nApp Services UI. Once you have updated your application configuration, you can deploy the\nupdates as a new version of your App by pushing them to the GitHub repo\nthat you specified. If Automatic GitHub Deployment is enabled,\nApp Services immediately deploys the latest commit for the configured\nbranch and directory. When you are ready to deploy, commit all of the files that you want to include\nand then push them to GitHub: Once you successfully push your changes to GitHub, App Services\nimmediately deploys a new version of your application that matches the state\nof the latest commit. Client applications will automatically use the newest\nversion, so make sure that you also update your client code to use the new\nversion if necessary. You can see the currently deployed version of your\napplication as well as a historical log of previous\ndeployments in the  Deployment History  table in the App Services UI. You can use the CLI to configure, enable, or disable automatic deployment. Use your MongoDB Atlas Admin API Key to log in to the CLI: The App Services CLI can walk you through the process of configuring automatic\ndeployment. This is an interactive command. The command also supports additional flags that you can optionally\ninclude. For information on these flags, refer to\n appservices deploy configure . App Services CLI immediately enables automatic deployment when you complete\nthis command. You don't need to run the  enable  command as\nan additional step. The ability to configure automatic deployment using App Services CLI\nrelies on a GitHub feature that is currently in beta. If\nGitHub makes breaking changes to this feature, you may be unable\nto configure automatic deployment using the App Services CLI. In this\ncase, use the App Services UI or Admin API to configure\nautomatic deployment. The GitHub beta feature only affects configuring automatic\ndeployment. If you have previously configured automatic\ndeployment for your application, you can enable or disable it\nusing the App Services CLI regardless of the status of this beta GitHub\nfeature. You can use the App Services CLI to view configuration details after you have\nconfigured automatic deployment. You can also disable and enable automatic deployment using the App Services CLI.\nFor more information about these commands and their options, refer\nto  appservices deploy . Call the admin user authentication endpoint with your MongoDB Atlas API\nkey pair: If authentication succeeds, the response body contains a JSON object\nwith an  access_token  value: The  access_token  grants access to the App Services Admin API. You\nmust include it as a Bearer token in the  Authorization  header for\nall Admin API requests. API Authentication Documentation You can get the existing deployment configuration for your App by\nmaking a  GET  request to the  Get the\nDeployment Configuration \nendpoint. You can update the existing deployment configuration for your App by\nmaking a  PATCH  request to the  Configure\nDeployment  endpoint. Automatic GitHub deployment does not prevent you from making changes\nto your App from the App Services UI. You can make changes to your\napp via the UI, and App Services automatically commits changes back to\nyour linked GitHub repository. When changes are committed to your linked GitHub repository, you'll see a\n Commit: <commit-hash>  link in your app's Deployment History. You can click that link to view the commit in GitHub. The  mongodb-realm \nbot appears as the commit author. If your linked GitHub repository contains new commits that are not reflected\nin your App Services UI changes, App Services can't automatically\ncommit those changes to your repo. However, you can use the\n Export  button to download the updated configuration files\nto your local machine, and then you push them manually to your GitHub\nrepository. Previously, we have recommended that you avoid making changes to your app\nwith App Services CLI after configuring automatic deployment. Starting with\nApp Services CLI v1.1.0 and later, you can safely use the App Services CLI to make changes to\nyour app. App Services CLI version 1.1.0 and later In App Services CLI v1.1.0 and later, a push from the App Services CLI automatically generates\na corresponding commit in GitHub. This keeps your code changes synchronized\nwith your version-controlled GitHub source of truth. App Services CLI version 1.0.3 and earlier In App Services CLI v1.0.3 and earlier, avoid making changes to your app with the App Services CLI\nafter configuring automatic deployment, because: If you push changes with the CLI while GitHub deployment is enabled, App Services\ndeploys the changes but does not commit them back to your linked repository. The configuration files in your repository no longer reflect the current\nstate of your app. To get the changes you pushed, a contributor must manually pull the latest\nconfiguration files directly from your App. The GitHub repository is\nno longer the source of truth. If a contributor adds a new commit to a stale repo, they overwrite any\ndeployed but uncommitted changes. You can deploy your App by modifying a repo hosted on GitHub. While Automatic GitHub Deployment is enabled, App Services immediately\ndeploys the latest commit. To deploy, commit your changes locally, then push\nthem to your repo on GitHub. When you make changes via the App Services UI after enabling Automatic\nGitHub Deployment, those changes are automatically committed back to your\nlinked repository. If you would like to use App Services CLI to update configuration files after enabling\nAutomatic GitHub Deployment, use App Services CLI v1.1.0 or later.",
            "code": [
                {
                    "lang": "shell",
                    "value": "git clone https://github.com/<organization>/<repo>.git"
                },
                {
                    "lang": "shell",
                    "value": "git add -A\ngit commit -m \"Adds App Services Application Directory\""
                },
                {
                    "lang": "shell",
                    "value": "git push origin <branch name>"
                },
                {
                    "lang": "sh",
                    "value": "git add -A\ngit commit -m \"<commit message>\"\ngit push origin <branch name>"
                },
                {
                    "lang": "shell",
                    "value": "appservices login --api-key=\"<my api key>\" --private-api-key=\"<my private api key>\""
                },
                {
                    "lang": "shell",
                    "value": "appservices deploy configure"
                },
                {
                    "lang": "shell",
                    "value": "appservices deploy describe"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST \\\n  https://services.cloud.mongodb.com/api/admin/v3.0/auth/providers/mongodb-cloud/login \\\n  -H 'Content-Type: application/json' \\\n  -H 'Accept: application/json' \\\n  -d '{\n    \"username\": \"<Public API Key>\",\n    \"apiKey\": \"<Private API Key>\"\n  }'"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"access_token\": \"<access_token>\",\n  \"refresh_token\": \"<refresh_token>\",\n  \"user_id\": \"<user_id>\",\n  \"device_id\": \"<device_id>\"\n}"
                },
                {
                    "lang": "bash",
                    "value": "curl -X GET \\\n   https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/deploy/config \\\n   -H 'Authorization: Bearer <access_token>'"
                },
                {
                    "lang": "bash",
                    "value": "curl -X PATCH \\\n   https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/deploy/config \\\n   -H 'Authorization: Bearer <access_token>' \\\n   -d '{\n      \"ui_drafts_disabled\": true,\n      \"automatic_deployment\": {\n         \"enabled\": true,\n         \"provider\": \"github\",\n         \"installation_ids\": [\n            \"string\"\n         ]\n      },\n      \"last_modified\": 0\n      }'"
                }
            ],
            "preview": "Learn how to automatically deploy changes to your App from GitHub.",
            "tags": "code example, curl, shell",
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "schemas/remove-a-schema",
            "title": "Remove a Schema",
            "headings": [
                "Overview",
                "Procedure",
                "Navigate to the Collection Schema Screen",
                "Remove the Schema for a Collection",
                "Remove All Schemas"
            ],
            "paragraphs": "When you make  breaking changes \nto your schema, you must remove the existing schema. You must do this\nafter you  terminate Atlas Device Sync  and before you\n re-enable  it. This guide shows you how to remove the schema. In the left navigation menu, click  Schema  beneath  Data\nAccess  to open the schema editor. Atlas App Services scans your linked cluster for\nexisting collections and lists them on the left side of the schema\neditor. By default, you see all the collections in your linked Atlas cluster.\nAny collection that has a schema shows a bolded name in this list,\nwhile a collection with no schema shows a light-gray, italicized name. From here, you can remove the schema for a single collection, or\nremove the schema for all collections. At this point, the collection no longer has a schema, and\nyou can choose to  generate a schema  or\n enable Development Mode . Hover over the collection name, and a  ...  menu appears. When you click this  ...  element, you get two options. One of\nthem is to  Delete Schema from this Collection . Choose this\noption. A modal pops up asking you to confirm that you want to delete the schema\nfor the collection. Press the  Delete  button. If you have already terminated Device Sync, App Services deletes the schema. If you have\nnot yet  terminated Device Sync ,\na modal displays, letting you know that this is a destructive change and\nclients will experience a  Client Reset . If you'd\nlike to continue, press the  Save Changes & Reinitialize Sync . If breaking changes impact multiple collections and you don't want to\nremove the schema collection-by-collection, you may remove all the schemas\nin a linked Atlas cluster. The first item in the  Collections  pane is the name of the\nlinked data source. You can see a  ...  menu next to the name. Select the  ...  element. You can see a few options; the last of which is\n Delete Schemas from All Collections . Select this option. A modal pops up asking you to confirm that you want to delete all\nschemas for the data source. Press the  Delete  button.",
            "code": [],
            "preview": "When you make breaking changes\nto your schema, you must remove the existing schema. You must do this\nafter you terminate Atlas Device Sync and before you\nre-enable it.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "apps/update",
            "title": "Update an App",
            "headings": [
                "Overview",
                "Update an Existing App",
                "Before You Begin",
                "Procedure",
                "Make Changes to Your Application",
                "Review Draft Changes",
                "Deploy a New App Version",
                "Authenticate a MongoDB Atlas User",
                "Pull the Latest Version of Your App",
                "Update Your Application",
                "Deploy the Updated App",
                "Create a New Draft (Optional)",
                "Make Changes to Your Application",
                "Deploy the Draft",
                "Deployment Drafts",
                "Draft Conflicts",
                "Disable UI Deployment Drafts"
            ],
            "paragraphs": "You can update an existing App by changing its underlying\nconfiguration and then deploying those changes. There are multiple\nways you can do this: The App Services Admin UI: a web application built into MongoDB\nAtlas. It includes a graphical interface for anything you want to\nconfigure in your App. The App Services CLI: a command line tool that you can use to configure and\ndeploy your App locally. You work directly with your App's underlying\n configuration files . The Admin API: a REST API that you can use to configure and deploy\nyour App programmatically. Admin API requests read and write your\nApp's underlying configuration files. Automatic GitHub Deployment: a GitHub integration that syncs your App\nconfiguration with a GitHub repository. You can use GitHub to manage\nyour App's configuration files and deploy your App by pushing changes\nto the repository. You will need the following to update an App in the Atlas UI: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . You will need the following to update an App in the CLI: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . A MongoDB Atlas Admin API  public/private key pair . The API key must\nhave  Project Owner  permissions to work with App\nServices Admin API. A copy of App Services CLI installed and added to your local system  PATH . To\nlearn how, see  Install App Services CLI . You will need the following to update an App with the Admin API: A MongoDB Atlas account with Project Owner permissions. To learn how to\nsign up for a free account, see  Get Started with Atlas . A MongoDB Atlas Admin API  public/private key pair . The API key must\nhave  Project Owner  permissions to work with App\nServices Admin API. To update an App based on configuration files in a GitHub\nrepository, you must enable  Automatic GitHub Deployment  for the App. Start by making any additions, modifications, or deletions that you\nwant to include in the deployment through the App Services UI.\nIf you have drafts enabled, your changes will be saved automatically as a draft. Guide to  Disable Deployment Drafts . You can only create draft changes through the App Services UI or Admin\nAPI. Any changes that you make with GitHub or the App Services CLI are\ndeployed independently of draft changes. To avoid possible conflicts or\ninconsistencies, its best to make your changes in one place at a time. You can review all of the draft changes to your application prior to\ndeploying them. Click  Deployment  in the left navigation menu to navigate to\nyour app's deployment history table. The top row of the table\nrepresents all of the draft changes that you've made since the\nprevious deployment. Click the  Review Draft and Deploy  button in the top row to\nsee a diff of the underlying  app configuration files  that represents the draft changes that you've made. You may want to download a draft of your changes because of merge conflicts\nor to save the draft outside of the UI. You can do so in the\n Review Draft and Deploy  modal by clicking  Export State \nin the top right corner. You can use the download to deploy your changes in the\nappservices or as a reference as you reapply changes in the UI. After you've reviewed your draft changes and confirmed that you want\nto deploy them, click  Deploy . App Services deploys a\nnew version of your application that includes the draft changes.\nClient applications will automatically use the newest version, so make\nsure that you also update your client code to use the new version if\nnecessary. Use your MongoDB Atlas Admin API Key to log in to the CLI: Get a local copy of your App's configuration files. To get the latest\nversion of your App, run the following command: You can also export a copy of your application's configuration files\nfrom the UI or with the Admin API. To learn how, see  Export an App . Add, delete, and modify  configuration files  for\nyour application's various components. Deploy your changes by pushing the updated configuration\nfiles to your remote app. A draft represents a group of application changes that you\ncan deploy or discard as a single unit. If you don't create\na draft, updates automatically deploy individually. To create a draft, send a  POST  request with no body to\nthe  Create a new deployment draft  endpoint: Each user can only create a single draft at a time, either through\nthe UI or the Admin API. If you already have an existing draft, you\ncan discard the changes associated with it by sending a  DELETE \nrequest to the  Discard a deployment draft  endpoint: You can configure your app and add, modify, or delete\ncomponents by calling the Admin API endpoints for each\noperation. For examples and detailed reference information,\nsee  Atlas App Services Admin API . If you did not create a draft, every successful Admin API\nrequest immediately deploys the associated updates to the\nApp. If you created a draft, every successful Admin API request\nadds the associated updates to the draft. Send requests for\nall of the changes that you want to include in the draft. If you created a draft, you can deploy all changes in\nthe draft by sending a  POST  request with no body to the\n Deploy a deployment draft  endpoint: With Automatic GitHub Deployment enabled, your App always uses\nconfiguration files from the most recent commit in your linked\nrepository. For details on how to organize and define\nconfiguration files, see  App Configuration . Update your application by adding, modifying, and deleting\nconfiguration files locally. When you are ready to deploy your changes, commit the\nconfiguration files and then push them to GitHub: After you push, the GitHub integration automatically deploys an\nupdated version of your App based on the configuration files in\nthe latest commit. A deployment draft is a set of one or more updates to your App that you\ncan apply or discard as a single group. Deploying a draft is useful when\nyour changes include services or functions that interact with each\nother. The App Services UI uses deployment drafts by default. While enabled,\nany changes you make to your App's configuration in the App Services UI\nare added to a draft that you  manually deploy . You\ncan  disable UI drafts  if you prefer\nto deploy immediately when you save in the UI. The App Services CLI and  GitHub Deployment  both\nautomatically create and deploy drafts for you. When you run the CLI\n push  command or  git push  to your deployment\nbranch, the CLI or GitHub app creates a diff of your local configuration\nfiles against the currently deployed configuration. Then it creates and\ndeploys a draft based on the diff. The App Services Admin API allows you to manually create and deploy\ndrafts. To learn how, see  Deploy a Group of Changes . Avoid having multiple users make conflicting changes in UI or API drafts\nat the same time. If there are multiple concurrent drafts that contain\nconflicting changes, you will not be able to deploy one or more of the\ndrafts. For example, if you deploy changes through the API but have an existing\nUI draft with conflicting changes, your UI draft will become invalid and\nyou will not be able to deploy it. To recover from a conflict, you can download configuration files for a\nUI draft from the  Deployment  page. You can use these as a\nreference to reapply changes in the UI or deploy the changes directly\nwith the CLI or Admin API. You can disable UI drafts if you prefer to deploy immediately when you\nsave in the UI. This setting does not prevent you from manually creating\ndrafts with the Admin API. To disable UI drafts directly from the UI: Click  Deployment  in the left navigation menu Click the  Configuration  tab Under the  Disable Drafts in Atlas App Services \nheading, click  Disable Drafts  and then confirm your\ndecision in the modal window. To disable UI drafts with the Admin API, send a request to the\n Configure Deployment  endpoint with\n ui_drafts_disabled  set to  true .",
            "code": [
                {
                    "lang": "shell",
                    "value": "appservices login --api-key=\"<my api key>\" --private-api-key=\"<my private api key>\""
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=\"<Your App ID>\""
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST 'https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/drafts' \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer <access_token>'"
                },
                {
                    "lang": "shell",
                    "value": "curl -X DELETE 'https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/drafts/{draftId}' \\\n  -H 'Authorization: Bearer <access_token>'"
                },
                {
                    "lang": "shell",
                    "value": "curl -X POST \\\n  'https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/drafts/{draftId}/deployment' \\\n  --header 'Content-Type: application/json' \\\n  --header 'Authorization: Bearer <access_token>' \\"
                },
                {
                    "lang": "sh",
                    "value": "git add -A\ngit commit -m \"<commit message>\"\ngit push origin <branch name>"
                },
                {
                    "lang": "bash",
                    "value": "curl -X PATCH https://services.cloud.mongodb.com/api/admin/v3.0/groups/{groupId}/apps/{appId}/deploy/config \\\n  -H 'Authorization: Bearer <access_token>' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"ui_drafts_disabled\": true,\n    \"automatic_deployment\": { \"enabled\": false, \"installationIds\": [], \"lastModified\": 0 }\n  }'"
                }
            ],
            "preview": "You can update an existing App by changing its underlying\nconfiguration and then deploying those changes. There are multiple\nways you can do this:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "schemas/enforce-a-schema",
            "title": "Enforce a Schema",
            "headings": [
                "Overview",
                "Procedure",
                "Navigate to the Collection Schema Screen",
                "Generate a Schema",
                "Add Field-Level Schema Definitions",
                "Add Change Validation Expressions",
                "Save the Schema",
                "Validate Documents Against the Schema",
                "Log In to MongoDB Cloud",
                "Pull the Latest Version of Your App",
                "Define the Schema",
                "Define Change Validation",
                "Deploy the Updated Schema",
                "Validate Null Types"
            ],
            "paragraphs": "You can control the shape and contents of documents in a collection by defining\na  schema . Schemas let you require specific fields, control the\ntype of a field's value, and validate changes before committing write\noperations. This guide shows you how to define, configure, and deploy a schema for a linked\nMongoDB Atlas collection. Federated data sources  do not support rules or schemas . You can only access a Federated data source\nfrom a system function. In the left navigation menu, click  Schema  beneath  Data\nAccess  to open the schema editor. Atlas App Services scans your linked cluster for\nexisting collections and lists them on the left side of the schema\neditor. Find and select your collection from the menu to open the collection\nconfiguration screen. If you already have data in your collection, App Services can\nsample a subset of the documents in the collection and generate a\nschema for you based on the sample. This step is optional, so if you\nalready have a schema or prefer to define one manually, skip this\nstep. You can configure the number of documents to sample and use the\nMongoDB query language to limit the sample to specific documents. By\ndefault, App Services randomly samples up to 500 documents from\nthe entire collection. To generate a schema from existing data: Click the  Generate Schema  button to open the sample configuration screen. Specify the sample size, up to a maximum of 50,000 documents. Optionally, click  Advanced  and specify a query,\nprojection, and/or sort to refine the sample query. Use the query\nand sort filters to sample a specific subset of documents and use\nthe projection filter to sample a specific subset of fields from\neach document. Click  Generate  to run the sample query and generate a\nschema. The process may take up to a minute depending on the number\nand contents of the sampled documents. You may define a single BSON schema for each collection. The root-level type\nof each collection schema is an  object  schema that represents a document\nin the collection. You can define additional schemas for document fields\nwithin the root schema's  properties  field. The fields available in a JSON schema object depend on the type of value that\nthe schema defines. For a list of supported types and details on how to\nconfigure them, see  schema types . A group is using App Services to run a voting app where users aged 13 or\nolder can submit a ranked list of their favorite colors. They store\nthe votes in a MongoDB collection named  votes  where each\ndocument represents a single person's vote. Each vote must include\nthe person's name, age, and an array of their favorite colors. Each\ncolor has a  rank ,  name , and a  hexCode .\nThe following is a typical document in the  votes  collection: The group can use the following schema to guarantee that the listed\nconstraints are satisfied for each document in the  votes \ncollection: In addition to configuring the content of each field, you can validate changes\nto documents by defining a validation  expression  in the\n validate  field of a schema. Validation expressions can use the\n %%prev  and  %%prevRoot  expansions to access\na field or document's values  before  the insert or update operation occurred. Consider a collection where the document's  owner_id  field\nrepresents the owner of each document. The business rules for this\ncollection specify that once a document has an owner, the value of\n owner_id  should never change. We can enforce this constraint\nwith the following validation expression that ensures that update\noperations do not change the  owner_id  value unless its to\nassign an owner where there was none previously: We could also use the  %%prevRoot  expansion to\ncreate the following equivalent validation expression: After you have configured the schema, click  Save . After\nsaving, App Services immediately begins enforcing the schema for all future\nqueries. Existing documents that do not match the schema are not\nautomatically updated or validated, so future changes to these\ndocuments may cause schema validation failures. Once you have saved the collection schema, you can validate that\nexisting documents in the collection conform to the schema. App Services samples documents for validation as it does for\nautomatic schema generation. To validate the data in a collection: Once validation is complete, App Services lists any validation\nerrors from the sample in the  JSON Validation Errors \ntable. Each row of the table represents a specific type of validation\nerror for a particular field and indicates the number of documents\nthat failed to validate in that way. For each validation error, you can use the  Actions  menu to\ndownload the raw documents that failed or copy a MongoDB query that\nfinds the failed documents. Click the  Run Validation  button to open the validation configuration window. Specify the sample size, up to a maximum of 50,000 documents. Optionally, click  Advanced  and specify a query and/or\nsort to refine the validation to a specific subset of documents. Click  Run Validation  to sample documents from the collection\nand validate each document against the schema. To configure your app with appservices, you must log in to MongoDB Cloud using\nan  API key  scoped to the organization or\nproject that contains the app. To define a document schema with  appservices , you need a local copy of your\napplication's configuration files. To pull a local copy of the latest version of your app, run the following: You can also download a copy of your application's configuration files from\nthe  Deploy > Import/Export App  screen in the App Services UI. To define or modify the schema for a collection, open the  schema.json \nconfiguration file within the collection's configuration directory. The root-level schema for every document must be of type  object . You can\nuse additional  schema  to configure specific fields within the\n properties  array. At a minimum, the schema should resemble the following: If you haven't already defined rules or a schema for the collection, you\nneed to manually create its configuration directory and  schema.json : You can validate changes to documents by defining a validation\n expression  in the  validate  field of a schema.\nValidation expressions can use the  %%prev  and\n %%prevRoot  expansions to access a field or document's values\n before  the insert or update operation occurred. Consider a collection where the document's  owner_id  field represents\nthe owner of each document. The business rules for this collection specify\nthat once a document has an owner, the value of  owner_id  should never\nchange. We can enforce this constraint with the following validation\nexpression that ensures that update operations do not change the\n owner_id  value unless its to assign an owner where there was none\npreviously: We could also use the  %%prevRoot  expansion to create the\nfollowing equivalent validation expression: Once you've defined and saved  schema.json  you can push the updated config\nto your remote app. App Services CLI immediately deploys the new schema on push. App Services's default behavior is to only accept a single type for each field.\nSchema fields are not nullable by default because  null  is a unique\n BSON type . You can configure App Services to pass schema validation when you use  null \nvalues with optional fields.\nEnabling null type validation allows the value for a field to be persisted as\nthe type in the schema or the  BSON null  type.\nIf you do not enable null type schema validation, App Services rejects  null \nvalues passed to optional fields. Even if you enable null type validation,\nrequired fields are never nullable. To enable null type schema validation in the App Services UI: In the left navigation menu below the  Manage  header, select  App Settings . On the  General  tab, navigate to the  Null Type Schema Validation \nsection. Toggle the switch to  ON . Click the  Save  button.",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"Jane Doe\",\n  \"age\": 42,\n  \"favoriteColors\": [\n    { \"rank\": 1, \"name\": \"RebeccaPurple\", \"hexCode\": \"#663399\" },\n    { \"rank\": 2, \"name\": \"DodgerBlue\", \"hexCode\": \"#1E90FF\" },\n    { \"rank\": 3, \"name\": \"SeaGreen\", \"hexCode\": \"#2E8B57\" },\n  ]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"bsonType\": \"object\",\n  \"required\": [\"name\", \"age\", \"favoriteColors\"],\n  \"properties\": {\n    \"name\": {\n      \"bsonType\": \"string\"\n    },\n    \"age\": {\n      \"bsonType\": \"int\",\n      \"minimum\": 13,\n      \"exclusiveMinimum\": false\n    },\n    \"favoriteColors\": {\n      \"bsonType\": \"array\",\n      \"uniqueItems\": true,\n      \"items\": {\n        \"bsonType\": \"object\",\n        \"properties\": {\n          \"rank\": { \"bsonType\": \"int\" },\n          \"name\": { \"bsonType\": \"string\" },\n          \"hexCode\": {\n            \"bsonType\": \"string\",\n            \"pattern\": \"^#([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3})$\"\n          }\n        }\n      }\n    }\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "\"owner_id\": {\n  \"validate\": {\n    \"%or\": [\n      { \"%%prev\": { \"%exists\": false } },\n      { \"%%prev\": \"%%this\" }\n    ]\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "\"owner_id\": {\n  \"validate\": {\n    \"%or\": [\n      { \"%%prevRoot.owner_id\": { \"%exists\": false } },\n      { \"%%prevRoot.owner_id\": \"%%root.owner_id\" }\n    ]\n  }\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices login --api-key=\"<MongoDB Cloud Public API Key>\" --private-api-key=\"<MongoDB Cloud Private API Key>\""
                },
                {
                    "lang": "bash",
                    "value": "appservices pull --remote=\"<Your App ID>\""
                },
                {
                    "lang": "javascript",
                    "value": "{\n  \"bsonType\": \"object\",\n  \"properties\": {\n    \"<Field Name>\": <Schema Document>,\n    ...\n  }\n}"
                },
                {
                    "lang": "bash",
                    "value": "# Create the collection's configuration directory\nmkdir -p data_sources/<service>/<db>/<collection>\n# Create the collection's schema file\necho '{}' >> data_sources/<service>/<db>/<collection>/schema.json"
                },
                {
                    "lang": "javascript",
                    "value": "\"owner_id\": {\n  \"validate\": {\n    \"%or\": [\n      { \"%%prev\": { \"%exists\": false } },\n      { \"%%prev\": \"%%this\" }\n    ]\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "\"owner_id\": {\n  \"validate\": {\n    \"%or\": [\n      { \"%%prevRoot.owner_id\": { \"%exists\": false } },\n      { \"%%prevRoot.owner_id\": \"%%root.owner_id\" }\n    ]\n  }\n}"
                },
                {
                    "lang": "bash",
                    "value": "appservices push --remote=\"<Your App ID>\""
                }
            ],
            "preview": "You can control the shape and contents of documents in a collection by defining\na schema. Schemas let you require specific fields, control the\ntype of a field's value, and validate changes before committing write\noperations.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        },
        {
            "slug": "schemas/relationships",
            "title": "Relationships",
            "headings": [
                "Overview",
                "Cardinality",
                "To-One",
                "To-Many",
                "Embedded Object Relationships",
                "Embedded Object to Another Collection",
                "Embedded Object Within a List",
                "Define a Relationship",
                "1. Define Schemas",
                "2. Create a New Relationship",
                "3. Configure the Relationship",
                "4. Deploy the Relationship"
            ],
            "paragraphs": "A relationship is a connection between two documents. Relationships\nallow you to reference and query related documents in read and write\noperations, even if the documents are in separate databases or\ncollections. You define a relationship for a \"source\" MongoDB collection and link to\ndocuments in a \"foreign\" collection. Atlas App Services automatically resolves\nrelationships in synced SDK data models by replacing the values in a source\nfield with the foreign documents that they reference. Relationships are unidirectional and don't enforce uniqueness or other\nforeign key constraints. If you reference a non-existent foreign value in\na source field, App Services automatically omits the reference from resolved\nrelationships. Consider an application that has two collections: The app defines this relationship on the  customers  collection. It\npoints from the array of account id values stored in the  accounts \nfield to the  account_id  field of each document in the\n accounts  collection. With this relationship defined, App Services can return a customer and all\nof their accounts in client queries. Without a relationship,\nthe queries would return a list of just  account_id  values instead\nof the full  Account  objects. The  accounts  collection contains documents that each describe\na customer account. The  customers  collection contains documents that each describe\na single customer that can have one or more accounts. Every\ndocument in the  customers  collection has an  accounts  field\nthat contains an array of every  account_id  value from the\n accounts  collection that applies to the customer. A relationship's cardinality determines the number of foreign documents\nthat it can reference. App Services supports two relationship cardinalities:\n\"to-one\" and \"to-many\". A to-one relationship links each source document with either a single\ndocument or an array of documents from the foreign collection. To indicate that a relationship has \"to-one\" cardinality, set\n is_list  to  false : App Services automatically replaces source values with the referenced objects\nor a null value in SDK models: A to-many relationship links each source document with a list of\ndocuments from the foreign collection. To indicate that a relationship has \"to-many\" cardinality, set\n is_list  to  true : App Services automatically replaces source values with the referenced objects\nor a null value in SDK models: Embedded Objects can have relationships with foreign collections. Use\ndot notation to access properties in embedded objects. An embedded object can have a relationship with an object in a foreign\ncollection. Use dot notation to specify the embedded object property that has a\nrelationship with the foreign collection. Then, you can specify the\nforeign collection details and foreign key field. An embedded object that is within a list property can have a relationship\nwith a foreign collection. To access a embedded object property contained in a list, use:\n field1.[].field2 , e.g.  pets.[].favoriteToyBrand . From there,\nyou can specify the foreign collection details and foreign key field. You can use this same  field1.[].field2  syntax when creating relationships\nwithin dictionaries and sets. In the example above,  is_list  is set to false. The field at the end\nof the relationship string here is a primitive, not a list. The embedded\nobject is  contained  in a list, but the  favoriteToyBrand  property\nitself is not a dictionary, set or list. When you define a relationship, keep these limitations in mind: The reference field must not be  _id The reference field must not be a  required  field The foreign key must be the  _id  field of the collection the field is referencing In order to define a relationship, you must have a schema defined for\nboth the source collection and the foreign collection. To learn how to\ndefine schemas, see  Enforce a Schema . You define a relationship for collections in a linked MongoDB data\nsource alongside the schema. To create a new relationship: Click  Schema  in the left navigation menu Click the  Relationships  tab Click  Add Relationship To create a new relationship, add a relationship configuration\nobject to the source collection's  relationships.json  file: A relationship definition maps from a field included in the source\ncollection's schema and points to a field of the same type in the\nforeign collection's schema. To configure the relationship: Specify a field in the source collection to create the relationship on Specify the foreign database, collection, and a field in the\nforeign collection to match with the source field Click  Add To configure the relationship, specify the source field name as a\nroot-level field in  relationships.json  and then add the\nfollowing configuration options in the field's value: Specify the foreign collection in the  ref  field using the\nfollowing format: Specify the field to match in the  foreign_key  field If the source field contains an array, set  is_list  to\n true , otherwise set it to  false . Click  Save  and then deploy your updated application. Save the relationship configuration file and push your changes to\ndeploy them:",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"accounts\": {\n    \"ref\": \"#/relationship/mongodb-atlas/sample_analytics/accounts\",\n    \"foreign_key\": \"account_id\",\n    \"is_list\": true\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"Account\",\n  \"properties\": {\n    \"_id\": { \"bsonType\": \"objectId\" },\n    \"account_id\": { \"bsonType\": \"string\" },\n    \"products\": {\n      \"bsonType\": \"array\",\n      \"items\": { \"bsonType\": \"string\" }\n    },\n    ...\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"Customer\",\n  \"properties\": {\n    \"username\": { \"bsonType\": \"string\" },\n    \"accounts\": {\n      \"bsonType\": \"array\",\n      \"items\": { \"bsonType\": \"string\" }\n    },\n    ...\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"owner\": {\n    \"ref\": \"#/relationship/mongodb-atlas/example/people\",\n    \"foreign_key\": \"_id\",\n    \"is_list\": false\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"Pet\",\n  \"properties\": {\n    \"name\": \"string\",\n    \"owner\": \"Person\"\n  }\n}\n{\n  \"name\": \"Person\",\n  \"properties\": {\n    \"name\": \"string\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"pets\": {\n    \"ref\": \"#/relationship/mongodb-atlas/example/pets\",\n    \"foreign_key\": \"_id\",\n    \"is_list\": true\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"Pet\",\n  \"properties\": {\n    \"name\": \"string\"\n  }\n}\n{\n  \"name\": \"Person\",\n  \"properties\": {\n    \"name\": \"string\",\n    \"pets\": \"Pet[]\"\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"Person\",\n  \"properties\": {\n    \"_id\": { \"bsonType\": \"objectId\" },\n    \"pet\": {\n      \"bsonType\":\"object\",\n      \"properties\": {\n        \"favoriteToyBrand\": { \"bsonType\": \"objectId\" }\n      }\n    }\n    // ...additional model properties\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{ \"pet.favoriteToyBrand\":\n   {\n      \"ref\": \"#/relationship/mongodb-atlas/example/ToyBrand\",\n      \"foreign_key\": \"_id\",\n      \"is_list\": false }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"title\": \"Person\",\n  \"properties\": {\n    \"_id\": { \"bsonType\": \"objectId\" },\n    \"pets\": {\n      \"bsonType\":\"array\",\n      \"items\": {\n        \"bsonType\": \"object\",\n        \"properties\": {\n          \"favoriteToyBrand\": { \"bsonType\": \"objectId\" }\n        }\n      }\n    }\n   // ...additional model properties\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"pets.[].favoriteToyBrand\": {\n    \"ref\": \"#/relationship/mongodb-atlas/example/ToyBrand\",\n    \"foreign_key\": \"_id\",\n    \"is_list\": false\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"<source field>\": {\n    \"ref\": \"#/relationship/<data source>/<db>/<collection>\",\n    \"foreign_key\": \"<foreign field>\",\n    \"is_list\": <boolean>\n  }\n}"
                },
                {
                    "lang": "text",
                    "value": "#/relationship/<data source>/<db>/<collection>"
                },
                {
                    "lang": "sh",
                    "value": "appservices push"
                }
            ],
            "preview": "A relationship is a connection between two documents. Relationships\nallow you to reference and query related documents in read and write\noperations, even if the documents are in separate databases or\ncollections.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "atlas"
                ],
                "target_product>atlas>sub_product": [
                    "atlas-app-services"
                ]
            }
        }
    ]
}