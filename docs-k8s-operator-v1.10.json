{
    "url": "http://mongodb.com/docs/kubernetes-operator/v1.10",
    "includeInGlobalSearch": false,
    "documents": [
        {
            "slug": "openshift-quick-start",
            "title": "OpenShift Quick Start",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Clone the MongoDB Enterprise Kubernetes Operator repository.",
                "Create a  for your  deployment.",
                "Configure kubectl to default to your namespace.",
                "Create a  that contains credentials authorized to pull images from the registry.connect.redhat.com repository.",
                "Install the ",
                "Create credentials and store them as a secret.",
                "Invoke the following command to create a ConfigMap.",
                "Deploy the replica set resource.",
                "Create a secret with your database user password",
                "Create a database user.",
                "Optional: View the newly created user in .",
                "Connect to the replica set."
            ],
            "paragraphs": " uses the   API and tools to manage MongoDB\nclusters.   works together with MongoDB  . This\ntutorial demonstrates how to deploy and connect to your first replica\nset in   from OpenShift with  . This tutorial requires: A running   cluster. By default, The   uses the  mongodb  namespace. To\nsimplify your installation, consider creating a namespace labeled\n mongodb  using the following   command: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you created: If you have not already, obtain a Red Hat subscription. Create a  Registry Service Account . Click on your Registry Service Account, then click the\n Docker Configuration  tab. Download the  <account-name>-auth.json  file and open it in a\ntext editor. Copy the  registry.redhat.io  object, and paste another instance\nof this object into the file. Remember to add a comma after the\nfirst object. Rename the second object\n registry.connect.redhat.com , then save the file: Create a  openshift-pull-secret.yaml  file with the contents of\nthe modified  <account-name>-auth.json  file as  stringData \nnamed  .dockerconfigjson : The value you provide in the  metadata.name  field contains\nthe secret name. Provide this value when asked for the\n <openshift-pull-secret> . Create a   from the  openshift-pull-secret.yaml \nfile: Invoke the following   command to install the   for\nMongoDB deployments: Add your  <openshift-pull-secret>  to the  ServiceAccount \ndefinitions in the     file: Invoke the following   command to install  : Run the following command: Provide your Public and Private Key values for the following\nparameters. To learn more, see  Create Credentials for the  . Provide your values for the following parameters. To learn more,\nsee the  parameter descriptions . Run the following command: You can choose to use a cleartext password or a Base64-encoded\npassword. Plaintext passwords use  stringData.password  and\nBase64-encoded passwords use  data.password . For a cleartext password, create and save the following YAML file: For a Base64-encoded password, create and save the following YAML\nfile: Provide your values for the following parameters. To learn more,\nsee the  parameter descriptions . Run the following command: You can view the newly-created user in  : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. Perform the following steps in the   or  \napplication, depending on where your clusters are hosted: Click  Deployment  in the left navigation. Click   for the deployment to which you want\nto connect. Click  Connect to this instance . Run the connection command in a terminal to connect to the\ndeployment.",
            "code": [
                {
                    "lang": "sh",
                    "value": "git clone https://github.com/mongodb/mongodb-enterprise-kubernetes.git"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=mongodb"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"auths\": {\n    \"registry.redhat.io\": {\n      \"auth\": \"<encoded-string>\"\n    },\n    \"registry.connect.redhat.com\": {\n      \"auth\": \"<encoded-string>\"\n    }\n  }\n}"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: openshift-pull-secret\nstringData:\n  .dockerconfigjson: |\n      {\n        \"auths\": {\n          \"registry.redhat.io\": {\n            \"auth\": \"<encoded-string>\"\n          },\n          \"registry.connect.redhat.com\": {\n            \"auth\": \"<encoded-string>\"\n          }\n        }\n      }\ntype: kubernetes.io/dockerconfigjson"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f openshift-pull-secret.yaml -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-appdb\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-database-pods\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f mongodb-enterprise-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb \\\n  create secret generic ops-manager-admin-key \\\n  --from-literal=\"user=<publicKey>\" \\\n  --from-literal=\"publicApiKey=<privateKey>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap myconfigmap \\\n  --from-literal=\"baseUrl=<myOpsManagerURL>\" \\\n  --from-literal=\"projectName=<myOpsManagerProjectName>\" \\ #Optional\n  --from-literal=\"orgId=<orgID>\" #Required for Global API Keys"
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | kubectl apply -f -\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: demo-mongodb-cluster-1\n  namespace: mongodb\nspec:\n  members: 3\n  version: 4.4.5-ent\n  type: ReplicaSet\n  authentication:\n    enabled: true\n    modes: [\"SHA\"]\n  opsManager:\n    configMapRef:\n      name: myconfigmap\n  credentials: ops-manager-admin-key\n  persistent: true\n  podSpec:\n    podTemplate:\n      spec:\n        containers:\n        - name: mongodb-enterprise-database\n          resources:\n            limits:\n              cpu: 2\n              memory: 1.5G\n            requests:\n              cpu: 1\n              memory: 1G\n            persistence:\n              single:\n                storage: 10Gi\nEOF"
                },
                {
                    "lang": "sh",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mms-user-1-password\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\nstringData:\n  password: <my-plain-text-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key"
                },
                {
                    "lang": "sh",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mms-user-1-password\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\ndata:\n  password: <base-64-encoded-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key"
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | kubectl apply -f -\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: mms-scram-user-1\nspec:\n  passwordSecretKeyRef:\n    name: mms-user-1-password\n    # Match to metadata.name of the User Secret\n    key: password\n  username: \"mms-scram-user-1\"\n  db: \"admin\" #\n  mongodbResourceRef:\n    name: \"demo-mongodb-cluster-1\"\n    # Match to MongoDB resource using authenticaiton\n  roles:\n  - db: \"admin\"\n    name: \"clusterAdmin\"\n  - db: \"admin\"\n    name: \"userAdminAnyDatabase\"\n  - db: \"admin\"\n    name: \"readWrite\"\n  - db: \"admin\"\n    name: \"userAdminAnyDatabase\"\nEOF"
                }
            ],
            "preview": " uses the  API and tools to manage MongoDB\nclusters.  works together with MongoDB . This\ntutorial demonstrates how to deploy and connect to your first replica\nset in  from OpenShift with .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "manage-users",
            "title": "Manage Database Users",
            "headings": [],
            "paragraphs": "Manage database users using SCRAM authentication on MongoDB\ndeployments. Manage database users for deployments running with TLS and X.509\ninternal cluster authentication enabled.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "deploy",
            "title": "Deploy a MongoDB Database Resource",
            "headings": [],
            "paragraphs": "Use   to deploy a new standalone MongoDB instance. Use   to deploy a replica set. Use   to deploy a sharded cluster.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "kind-quick-start",
            "title": "Kind Quick Start",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Clone the MongoDB Enterprise Kubernetes Operator repository.",
                "Create a  for your  deployment.",
                "Configure kubectl to default to your namespace.",
                "Install the ",
                "Create credentials and store them as a secret.",
                "Invoke the following command to create a ConfigMap.",
                "Deploy the replica set resource.",
                "Create a secret with your database user password",
                "Create a database user.",
                "Optional: View the newly created user in .",
                "Connect to the replica set."
            ],
            "paragraphs": " uses the   API and tools to manage MongoDB\nclusters.   works together with MongoDB  . This\ntutorial demonstrates how to deploy and connect to your first replica\nset in   from   with  . This tutorial requires: A running   cluster. By default, The   uses the  mongodb  namespace. To\nsimplify your installation, consider creating a namespace labeled\n mongodb  using the following   command: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you created: Change to the directory in which you cloned the repository. Install the   for MongoDB deployments using the\nfollowing   command: Install the   using the following   command: Run the following command: Provide your Public and Private Key values for the following\nparameters. To learn more, see  Create Credentials for the  . Provide your values for the following parameters. To learn more,\nsee the  parameter descriptions . Run the following command: You can choose to use a cleartext password or a Base64-encoded\npassword. Plaintext passwords use  stringData.password  and\nBase64-encoded passwords use  data.password . For a cleartext password, create and save the following YAML file: For a Base64-encoded password, create and save the following YAML\nfile: Provide your values for the following parameters. To learn more,\nsee the  parameter descriptions . Run the following command: You can view the newly-created user in  : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. Perform the following steps in the   or  \napplication, depending on where your clusters are hosted: Click  Deployment  in the left navigation. Click   for the deployment to which you want\nto connect. Click  Connect to this instance . Run the connection command in a terminal to connect to the\ndeployment.",
            "code": [
                {
                    "lang": "sh",
                    "value": "git clone https://github.com/mongodb/mongodb-enterprise-kubernetes.git"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f crds.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f mongodb-enterprise.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb \\\n  create secret generic ops-manager-admin-key \\\n  --from-literal=\"user=<publicKey>\" \\\n  --from-literal=\"publicApiKey=<privateKey>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap myconfigmap \\\n  --from-literal=\"baseUrl=<myOpsManagerURL>\" \\\n  --from-literal=\"projectName=<myOpsManagerProjectName>\" \\ #Optional\n  --from-literal=\"orgId=<orgID>\" #Required for Global API Keys"
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | kubectl apply -f -\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: demo-mongodb-cluster-1\n  namespace: mongodb\nspec:\n  members: 3\n  version: 4.4.5-ent\n  type: ReplicaSet\n  authentication:\n    enabled: true\n    modes: [\"SHA\"]\n  opsManager:\n    configMapRef:\n      name: myconfigmap\n  credentials: ops-manager-admin-key\n  persistent: true\n  podSpec:\n    podTemplate:\n      spec:\n        containers:\n        - name: mongodb-enterprise-database\n          resources:\n            limits:\n              cpu: 2\n              memory: 1.5G\n            requests:\n              cpu: 1\n              memory: 1G\n            persistence:\n              single:\n                storage: 10Gi\nEOF"
                },
                {
                    "lang": "sh",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mms-user-1-password\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\nstringData:\n  password: <my-plain-text-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key"
                },
                {
                    "lang": "sh",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mms-user-1-password\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\ndata:\n  password: <base-64-encoded-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key"
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | kubectl apply -f -\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: mms-scram-user-1\nspec:\n  passwordSecretKeyRef:\n    name: mms-user-1-password\n    # Match to metadata.name of the User Secret\n    key: password\n  username: \"mms-scram-user-1\"\n  db: \"admin\" #\n  mongodbResourceRef:\n    name: \"demo-mongodb-cluster-1\"\n    # Match to MongoDB resource using authenticaiton\n  roles:\n  - db: \"admin\"\n    name: \"clusterAdmin\"\n  - db: \"admin\"\n    name: \"userAdminAnyDatabase\"\n  - db: \"admin\"\n    name: \"readWrite\"\n  - db: \"admin\"\n    name: \"userAdminAnyDatabase\"\nEOF"
                }
            ],
            "preview": " uses the  API and tools to manage MongoDB\nclusters.  works together with MongoDB . This\ntutorial demonstrates how to deploy and connect to your first replica\nset in  from  with .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "secure",
            "title": "Secure a Database Resource",
            "headings": [],
            "paragraphs": "Configure   for   deployments. Configure X.509 for client authentication. Configure   and X.509 for internal authentication.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference",
            "title": "Reference",
            "headings": [],
            "paragraphs": "Review the     object specification. Review the MongoDB   object specifications. Review the   installation settings. Review settings that only the   can set. Review the EOL dates for   versions. Open Source licenses that the   uses.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "mdb-resources",
            "title": "Deploy and Configure MongoDB Database Resources",
            "headings": [],
            "paragraphs": "You can use the   to deploy and manage MongoDB clusters\nfrom the    , without having to configure them in\n  or  . Review the MongoDB database custom resources architecture. Configure the   to deploy MongoDB database resources. Deploy a standalone, replica set, or sharded cluster resource. Modify the configuration of a MongoDB database resource. Configure authentication for client applications and encrypt\nconnections to your MongoDB resources. Configure authentication for MongoDB database users. Access database resources from inside or outside  .",
            "code": [],
            "preview": "You can use the  to deploy and manage MongoDB clusters\nfrom the  , without having to configure them in\n or .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "upgrade",
            "title": "Upgrade the  from Prior Versions",
            "headings": [],
            "paragraphs": "Upgrade the   to its latest version. Starting in   version 1.3.0, you can only have one MongoDB\nresource per project. This document explains how to migrate existing\nprojects which have multiple MongoDB resources into configurations with\na single resource per project.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "",
            "title": "MongoDB Enterprise Kubernetes Operator",
            "headings": [],
            "paragraphs": "The   translates the human knowledge of creating a MongoDB\ninstance into a scalable, repeatable, and standardized method.\nKubernetes needs help creating and managing stateful applications like\ndatabases. It needs to configure the network, persist storage, and\ndedicate computing capacity without additional human effort on each\ncontainer. The   manages the typical lifecycle events for a MongoDB\ncluster: provisioning storage and computing power, configuring network\nconnections, setting up users, and changing these settings as needed.\nIt accomplishes this using the Kubernetes API and tools. You provide the Operator with the specifications for your MongoDB\ncluster. The Operator uses this information to tell Kubernetes how to\nconfigure that cluster including provisioning storage, setting up the\nnetwork connections, and configuring other resources. The   works together with MongoDB  , which further\nconfigures to MongoDB clusters. When MongoDB is deployed and running in\nKubernetes, you can manage MongoDB tasks using  . You can then deploy MongoDB databases as you deploy them now after the\ncluster is created. You can use the   console to run MongoDB at\noptimal performance.",
            "code": [],
            "preview": "The  translates the human knowledge of creating a MongoDB\ninstance into a scalable, repeatable, and standardized method.\nKubernetes needs help creating and managing stateful applications like\ndatabases. It needs to configure the network, persist storage, and\ndedicate computing capacity without additional human effort on each\ncontainer.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "connect",
            "title": "Access Database Resources",
            "headings": [],
            "paragraphs": "The following pages describe how to connect to a MongoDB\ndatabase resource that is deployed to  : Connect to a MongoDB database resource from inside\nof the   cluster. Connect to a MongoDB database resource from outside\nof the   cluster.",
            "code": [],
            "preview": "The following pages describe how to connect to a MongoDB\ndatabase resource that is deployed to :",
            "tags": null,
            "facets": null
        },
        {
            "slug": "installation",
            "title": "Install and Configure the ",
            "headings": [],
            "paragraphs": "Review   deployment architecture, scopes, considerations, and\nprerequisites. Install the  . Upgrade from earlier versions of  .",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "configure-k8s-operator-for-mdb-resources",
            "title": "Configure the  for MongoDB Database Resources",
            "headings": [],
            "paragraphs": "Create a   so the   can create and update\n  in your   Project. Create a   to link the   to your  \nProject. Create an X.509 certificate to connect to an X.509-enabled\nMongoDB deployment.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "om-resources",
            "title": "Deploy and Configure Ops Manager Resources",
            "headings": [],
            "paragraphs": "Review the   resource architecture. Review the   resource considerations and\nprerequisites. Use the   to deploy an   instance. Use the   to configure   to operate in\n Remote  mode. In Remote mode, the Backup Daemons and managed\nMongoDB resources download installation archives from HTTP endpoints\non a web server or S3-compatible file store deployed to your  \ncluster instead of from the Internet. Use the   to configure   to operate in\n Local  mode. In Local mode, the Backup Daemons and managed\nMongoDB resources download installation archives from a   that\nyou create for the   StatefulSet instead of from the Internet. Encrypt the connection between the application database replica set\nmembers. Configure queryable backups for   deployments created with the\n .",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "third-party-licenses",
            "title": "Third-Party Licenses",
            "headings": [
                "Apache License 2.0",
                "BSD (Berkeley Software Distribution) 2-Clause",
                "BSD (Berkeley Software Distribution) 3-Clause",
                "ISC (Internet Systems Consortium) License",
                "MIT (Massachusetts Institute of Technology) License",
                "MPL (Mozilla Public License) 2.0"
            ],
            "paragraphs": "MongoDB   uses third-party libraries or other resources\nthat may be distributed under licenses different than the MongoDB\nsoftware.  depends upon the following third-party packages. These\npackages are licensed as shown in the following list. Should MongoDB\nhave accidentally failed to list a required license, please\n contact the MongoDB Legal Department . License:  TL;DR  |  Full Text k8s.io/api k8s.io/apiextensions-apiserver k8s.io/apimachinery k8s.io/client-go k8s.io/kube-openapi License:  TL;DR  |  Full Text errors License:  TL;DR  |  Full Text cmp fsnotify.v1 goautoneg Go-diff Go-difflib inf.v0 mergo net oauth2 protobuf pflag protobuf patch text time/rate ssh/terminal unix uuid xerrors License:  TL;DR  |  Full Text go-spew License:  TL;DR  |  Full Text assert atomic cast flowRate go json iterator multierr semver quantile zap yaml License:  TL;DR  |  Full Text lru multierror errwrap",
            "code": [],
            "preview": "MongoDB  uses third-party libraries or other resources\nthat may be distributed under licenses different than the MongoDB\nsoftware.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/k8s-op-exclusive-settings",
            "title": "MongoDB  Exclusive Settings",
            "headings": [
                "Kubernetes Operator Overrides Some Ops Manager Settings"
            ],
            "paragraphs": "At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . Some settings that you configure using   cannot be set or\noverridden in the  . Settings that the   does\nnot manage are accepted. The following list of settings are exclusive to  . This list may\nchange at a later date. These settings can be found on the\n Automation Configuration \npage: In addition to the list of Automation settings, the   uses attributes\noutside of the deployment from the Monitoring and Backup Agent configurations. auth.autoAuthMechanisms auth.authoritativeSet auth.autoPwd auth.autoUser auth.deploymentAuthMechanisms auth.disabled auth.key auth.keyfile auth.keyfileWindows auth.usersWanted auth.usersWanted[n].mechanisms auth.usersWanted[n].roles auth.usersWanted[n].roles[m].role auth.usersWanted[n].roles[m].db auth.usersWanted[n].user auth.usersWanted[n].authenticationRestrictions processes.args2_6.net.port processes.args2_6.net.tls.certificateKeyFile processes.args2_6.net.tls.clusterFile processes.args2_6.net.tls.PEMKeyFile processes.args2_6.replication.replSetName processes.args2_6.sharding.clusterRole processes.args2_6.security.clusterAuthMode processes.args2_6.storage.dbPath processes.args2_6.systemLog.destination processes.args2_6.systemLog.path processes.authSchemaVersion processes.cluster  (mongos processes) processes.featureCompatibilityVersion processes.hostname processes.name processes.version replicaSets._id replicaSets.members._id replicaSets.members.host replicaSets.members replicaSets.version sharding.clusterRole  (config server) sharding.configServerReplica sharding.name sharding.shards._id sharding.shards.rs ssl.CAFilePath ssl.autoPEMKeyFilePath ssl.clientCertificateMode backupAgentTemplate.username backupAgentTemplate.sslPEMKeyFile monitoringAgentTemplate.username monitoringAgentTemplate.sslPEMKeyFile  creates a replica set of 3 members. You changed  storage.wiredTiger.engineConfig.cacheSizeGB \nto  40 . This setting is not in the   exclusive settings\nlist. You then use the   to scale the replica set to\n5 members. The  storage.wiredTiger.engineConfig.cacheSizeGB  on the\nnew members should still be  40 .",
            "code": [],
            "preview": "Some settings that you configure using  cannot be set or\noverridden in the . Settings that the  does\nnot manage are accepted.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "release-notes",
            "title": "Release Notes for ",
            "headings": [
                " 1.10 Series",
                " 1.10.0",
                "",
                "Changes",
                "Bug Fixes",
                "MongoDBOpsManager Resource",
                "Known Issues",
                " 1.9 Series",
                " 1.9.2",
                "",
                "Bug Fixes",
                "MongoDBOpsManager Resource",
                "Known Issues",
                "New Images",
                " 1.9.1",
                "",
                "Bug Fixes",
                "MongoDB Resource",
                "Bug Fixes",
                "MongoDBOpsManager Resource",
                "Known Issues",
                "Changes",
                "New Images",
                " 1.9.0",
                "",
                "Bug Fixes",
                "MongoDB Resource",
                "Changes",
                "MongoDBOpsManager Resource",
                "Known Issues",
                "Changes",
                "New Images",
                " 1.8 Series",
                " 1.8.2",
                "Known Issues",
                "Bug Fix",
                " 1.8.1",
                "Known Issues",
                "Bug Fixes",
                "Improvements",
                "New Images",
                "New  Images",
                " 1.8.0",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes",
                "Bug Fixes",
                "Known Issues",
                " 1.7 Series",
                " 1.7.1",
                "MongoDB Resource Changes",
                "Bug Fixes",
                "Known Issues",
                " 1.7.0",
                "Docker Image Changes",
                "MongoDB Resource Changes",
                "Bug Fixes",
                "Known Issues",
                " 1.6 Series",
                " 1.6.1",
                "Ops Manager Resource Changes",
                "Docker Image Changes",
                "Bug Fixes",
                " 1.6.0",
                "MongoDB Resource Changes",
                "Kubernetes Operator Changes",
                "Bug Fixes",
                " 1.5 Series",
                " 1.5.5",
                "MongoDB Resource Changes",
                "Bug Fixes",
                " 1.5.4",
                "MongoDB Resource Changes",
                "Kubernetes Operator Changes",
                "Bug Fixes",
                " 1.5.3",
                "Kubernetes Operator Changes",
                "Bug Fixes",
                " 1.5.2",
                "Ops Manager Resource Changes",
                "Kubernetes Operator Changes",
                "Bug Fixes",
                " 1.5.1",
                "Bug Fixes",
                "Known Issues",
                " 1.5.0",
                " Changes",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes",
                " 1.4 Series",
                " 1.4.5",
                "CVE Description",
                "Common Weakness Enumeration",
                "Affected Versions",
                "Fixed Versions",
                " 1.4.4",
                "MongoDB Resource Changes",
                " Resource Changes (Beta Release)",
                "Bug Fixes",
                " 1.4.3",
                " Changes",
                "MongoDB Resource Changes",
                " Resource Changes (Beta Release)",
                "Bug Fixes",
                " 1.4.2",
                "MongoDB Resource Changes",
                " Resource Changes (Beta Release)",
                "Bug Fixes",
                " 1.4.1",
                " 1.4.0",
                "MongoDB Resource Changes",
                " Resource Changes (Beta Release)",
                "Bug Fixes",
                " 1.3 Series",
                " 1.3.1",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes (Alpha Release)",
                " 1.3.0",
                "Specification Schema Changes",
                "Ops Manager Resource Changes (Alpha Release)",
                "Bug Fixes",
                " 1.2 Series",
                " 1.2.5",
                "CVE Description",
                "Common Weakness Enumeration",
                "Affected Versions",
                "Fixed Versions",
                " 1.2.4",
                " 1.2.3",
                " 1.2.2",
                " 1.2.1",
                " 1.2.0",
                "GA Release",
                "Alpha Release",
                " 1.1 Series",
                " 1.1",
                " 1.0 Series",
                " 1.0",
                " Beta Series",
                " 0.12",
                " 0.11",
                " 0.10",
                " 0.9",
                " 0.8",
                " 0.7",
                " 0.6",
                " 0.5",
                " 0.4",
                " 0.3",
                " 0.2",
                " 0.1"
            ],
            "paragraphs": "Released 2020-03-25 Updates the   from the  v1beta1  version to the  v1 \nversion. Clusters on   1.16 and higher should remain unimpacted. The   cannot install in clusters on   versions lower than 1.16. Fixes an issue that prevented multiple   resources from\nhaving the same name in different namespaces. Fixes an issue that caused new MongoDB resources created with\n spec.backup.mode=disabled  to fail. Fixes an issue with saving changes on the  S3 Store  page. Fixes an issue that changed the replica set status to  Fail ,\nincreased the replica set members, and disabled  . When you use remote or hybrid mode, and set\n automation.versions.download.baseUrl , you must set the\n automation.versions.download.baseUrl.allowOnlyAvailableBuilds \nproperty to  false .   4.4.11 fixes this issue. Released 2020-02-05 Fixes errors in the CSV (This only effects the Red Hat market) You can't use MongoDB 4.4 as an application database for an  \nresource. You can find all images in the following registries: mongodb-enterprise-operator:1.9.2 Ubuntu-based images :  https://quay.io/repository/mongodb RHEL-based images :  https://catalog.redhat.com/software/containers/mongodb/enterprise-operator/5b8052d069aea356ff258479 Released 2020-01-15 Fixes an issue where you could not specify the\n service-account-name  in the    podSpec \noverride. Removes the unnecessary  delete service  permission from Operator\nrole. Fixes an issue where removing the\n spec.security.roles.privileges  array in\n spec.security.roles  caused the resource to\nenter a bad state. This release introduces: A new Application Database image,\n mongodb-enterprise-appdb:10.2.15.5958-1_4.2.11-ent . The image\nincludes MongoDB  4.2.11-ent  instead of\n 4.2.2-ent . You must push the new image to any private\nrepositories that your   installation uses, otherwise\nthe  MongoDBOpsManager  resource won't start. A new required environment variable,\n APPDB_AGENT_VERSION . If you don't set  APPDB_AGENT_VERSION ,\nthe  MongoDBOpsManager  resource can't fetch the MongoDB Agent\nversion for the Application Database. You can't use MongoDB 4.4 as an application database for an  \nresource. The   user now has  backup ,  restore  and  hostManager  roles, allowing for backups\nand restores on the Application Database. If you omit  spec.applicationDatabase.version , the\n  uses  4.2.11-ent  as the default MongoDB version. You can find all images in the following registries: mongodb-enterprise-operator:1.9.1 mongodb-enterprise-appdb:10.2.15.5958-1_4.2.11-ent mongodb-enterprise-init-appdb:1.0.2 mongodb-enterprise-init-database:1.0.6 Ubuntu-based images :  https://quay.io/repository/mongodb RHEL-based images :  https://catalog.redhat.com/software/containers/mongodb/enterprise-operator/5b8052d069aea356ff258479 Released 2020-12-08 Fixes an issue where the   didn't close connections to\n , causing too many open file descriptors. You can now configure continuous backup for a MongoDB database\nresource in its  . To enable continuous backup in the MongoDB  , you must\n enable backup  in an  \ninstance that you deployed using the  . You can't use MongoDB 4.4 as an application database for an  \nresource. When you upgrade the   to this version, the\n  deletes and re-creates the Backup Daemon statefulset. This is a safe operation. The new   service that enables Queryable Backups requires a change\nto the  matchLabels  Backup Daemon   attribute. The   changes the way it collects the status of\nMongoDB Agents in Application Database  . You can find all images in the following registries: mongodb-enterprise-operator:1.9.0 Ubuntu-based images :  https://quay.io/repository/mongodb RHEL-based images :  https://catalog.redhat.com/software/containers/mongodb/enterprise-operator/5b8052d069aea356ff258479 Released 2020-11-16 You can't use MongoDB 4.4 as an application database for an  \nresource. Fixes an issue where the   resource would reach a  Failing \nstate when both  spec.externalConnectivity  and\n spec.backup.enabled  were enabled. Released 2020-11-13 You can't use MongoDB 4.4 as an application database for an  \nresource. When both  spec.externalConnectivity  and\n spec.backup.enabled  are enabled in   at the same\ntime, the   resource fails to reconcile. Fixes a bug where\n spec.security.authentication.ignoreUnknownUsers  could not\nbe modified after creating a MongoDB resource. Fixes failed queryable backups. The   now creates a\n  Service that   uses to access backups. Fixes an issue that made it impossible to move from non-  to a\n -enabled Application Database. Init containers do not run as root.  Backup daemon runs in unprivileged mode. To manage Database Pod resources, use the\n spec.podSpec.podTemplate  MongoDB Custom Resource attribute.\nFor an example resource definition of each supported type, see the\n samples/mongodb/podspec \ndirectory. The following attributes are deprecated: spec.podSpec.cpu spec.podSpec.cpuRequests spec.podSpec.memory spec.podSpec.memoryRequests Init-database 1.0.1 Ubi Ubuntu Init-ops-manager 1.0.3 Ubi Ubuntu Init-appdb 1.0.5 Ubi Ubuntu For a list of the packages installed and any security vulnerabilities\ndetected in the build process, see the Quay repository for the\n MongoDB Enterprise Operator \nand the  MongoDB Enterprise Database . Version 4.4.5 Ubi Ubuntu Version 4.2.21 Ubi Ubuntu Version 4.2.20 Ubi Ubuntu Released 2020-09-30 The MongoDB Enterprise Database image now requires an init container.\nIf you are using a private repository, you must set the  INIT_DATABASE_IMAGE_REPOSITORY \nenvironment variable in the Operator deployment, and the new\ninit container must exist inside this repository. Introduces new configuration fields: spec.security.authentication.requireClientTLSAuthentication \nfor using the MongoDB Agent client certificate authentication in\nconjunction with any other authentication mechanism. spec.security.authentication.agents.clientCertificateSecretRef \nfor configuring the client TLS certificate used by the MongoDB\nAgent when enabling ClientTLSAuthentication. Changes the default permissions of volumes created from secrets from  0644 \nto  0640 . Allows the Application Database to be configured with SCRAM-SHA-256\nauthentication when using   4.4 or newer version. Changes the validation of the    spec.version  field\nto allow for tags that do not match the  semver \nrequirements. The  spec.version  field must start with the\n Major.Minor.Patch  string that represents the   version. To\nlearn more about this field, see  Ops Manager Resource Specification . Fixes an issue that caused the Operator to choose an incorrect project\nname when creating MongoDB users. Fixes an issue that caused the MongoDB   CRD to have the CA\npath in the incorrect location. Fixes a bug where the MongoDB Agent could not correctly recognize the\nparameters that passed through  spec.agent.startupOptions . Fixes an issue that could cause potential deadlock when certain\nconfiguration options are modified in parallel. You can't use MongoDB 4.4 as an application database for an  \nresource. When you enable queryable backup, you must manually create two\nadditional services for: Exposing the queryable backup port (default: 25999) for the  \npod. The Backup Daemon pod, to ensure that it is resolvable from the   pod. If you deploy   in\n local mode  and upgrade from\nv4.4.1, you must upgrade the MongoDB tools located in the\n automation.versions.directory , which defaults to\n /mongodb-ops-manager/mongodb-releases/ . Released 2020-09-02 Supports setting the Distinguished Name (DN) of the LDAP group to\nwhich the MongoDB Agent user belongs with the\n spec.security.authentication.ldap.automationLdapGroupDN \nsetting. Requires you to provide\n spec.security.authentication.agents.mode  if you specify\nmore than one mode in  spec.security.authentication.modes . Supports setting MongoDB Agent startup parameters for MongoDB Database\nresources with the following settings: spec.applicationDatabase.agent.startupOptions spec.agent.startupOptions spec.configSrv.agent.startupOptions spec.mongos.agent.startupOptions spec.shard.agent.startupOptions  resources: Fixes a bug where you could not enable  SCRAM-SHA \nauthentication for application database resources using certain\nMongoDB versions with   4.4. Fixes a bug where application database monitoring was not correctly\nconfigured in   when you enabled   for the application\ndatabase. Fixes a bug to move the     configuration from\n spec.applicationDatabase.security.tls.ca  to\n spec.security.tls.ca . MongoDB resources: Fixes a bug that prevented you from increasing or decreasing the number\nof members in a replica set or a sharded cluster by more than one\nmember at a time for MongoDB 4.4 deployments. Fixes an issue where the   could not enable agent\nauthentication if you enabled  LDAP  authentication for a MongoDB\nresource. Fixes an issue where you could not create  SCRAM  users and enable\n SCRAM  authentication in any order for a MongoDB resource. Fixes an issue where the   did not remove the backup\nautomation configuration before starting the agent on a MongoDB\nresource  . If you enable   on the application database, you must not provide the\n spec.applicationDatabase.version  field in an  \nresource definition. You can't use MongoDB 4.4 as an application database for an  \nresource. When you upgrade to the   1.7.1, you might have to delete\nthe  mongodb-enterprise-operator  deployment due to deployment\nconfiguration changes. This is a safe operation. Deleting the\n mongodb-enterprise-operator  pod does not affect the MongoDB\n . If you use   certificates signed by a custom  , you must: Omit the  spec.version.applicationDatabase  setting from\nyour   resource definition, and Deploy   in  local mode . You must manually copy\ninstallation archives for all MongoDB versions you want to use to\na   for the   StatefulSet. Released 2020-08-14  1.7.x is the final minor version release series that\nsupports OpenShift 3.11. Do not upgrade to any future major or minor\nversion releases if you want to continue to deploy the  \nusing OpenShift 3.11. The planned end of life for the   1.7.x release series\nis July 2021. All   Red Hat Docker images are now based on UBI 8. In\nthe previous release,   Red Hat Docker images were based\non UBI 7. Supports LDAP as an authorization mechanism for MongoDB database\nresources you deploy with the  . For more information,\nsee the sample LDAP configurations on  GitHub Fixes a bug that prevented scaling down a replica set from three\nmembers to one member.  cannot monitor Application Databases secured using  . For MongoDB 4.4 deployments, you can increase or decrease the number\nof members in a replica set or a sharded cluster by only one member\nat a time. Released 2020-07-30  image for version 4.4.0 is available. The Red Hat  database  and  operator  Docker images are now based\non the latest UBI 7 release. Two high criticality issues have been\nresolved. The following Docker images have been released: Image Type Ubuntu 16.04 Red Hat UBI 7 quay.io/mongodb/mongodb-enterprise-operator:1.6.1 quay.io/mongodb/mongodb-enterprise-operator-ubi:1.6.1 MongoDB Database quay.io/mongodb/mongodb-enterprise-database:1.6.1 quay.io/mongodb/mongodb-enterprise-database-ubi:1.6.1 quay.io/mongodb/mongodb-enterprise-ops-manager:4.4.0 quay.io/mongodb/mongodb-enterprise-ops-manager-ubi:4.4.0 Fixes a bug where the   did not store a configuration of\nyour deployed resources in a  . Fixes a bug where the   did not allow passwords of any\nlength or complexity for Application Database, oplog store, and\nblockstore database resources defined in   resources. Fixes a bug where the authentication configuration was not removed\nfrom   or   projects when you remove a MongoDB\ndatabase resource. Released 2020-07-16 Supports LDAP as an authentication mechanism for MongoDB database\nresources you deploy with the  . For more information,\nsee the sample LDAP configurations on  GitHub . LDAP authorization is not yet supported. Preserves backup history by retaining   cluster records when\nyou enable backup. Fixes a bug that prevented the   from raising errors when\na  projectName  contained spaces. Fixes a bug that prevented   to monitor for all MongoDB\ndatabase resources that you deploy with the  . Released 2020-07-02 Provides additional options for more granular configuration of\n  /   processes. You can find an example of how to apply\nthese options in the  /samples/mongodb/mongodb-options  file of the\n MongoDB Enterprise Kubernetes Operator repository . Fixes a bug introduced in 1.5.4 where   would not tag\nprojects correctly when working on   versions older than 4.2.2.\nIn this version,   tags the projects correctly. Released 2020-06-22 Allows modification of authentication settings using the   UI if\nthe  spec.security.authentication  setting is not provided\nin the MongoDB resource object definition. Supports Helm  installation  with\n helm install  in addition to  helm template | kubectl apply .\n helm install  is now the recommended way to install with Helm. Supports configuring the MongoDB Agent authentication mechanism\nindependently from the cluster authentication mechanism. Supports configuring monitoring for the Application Database to send\nmetrics to  . To learn more about the monitoring function of\nthe MongoDB Agent, see\n MongoDB Agent . Fixes a bug that affected transitioning authentication mechanisms\nfrom X.509 to SCRAM. Fixes a bug that prevented the MongoDB Agent from reaching a goal\nstate if SCRAM configuration was changed in the   UI. Released 2020-05-29 Passes   and MongoDB deployment configuration properties as\n Secret environment variables . Correctly configures shutdown timeouts for   and the Backup\nDaemon. Fixes an issue where  -watched Secrets and ConfigMaps\ntriggered unnecessary reconciliations. Fixes an issue where the status of custom resources failed to update\nin OpenShift 3.11. Released 2020-05-08 Runs   and Backup Daemon pods under a dedicated service\naccount. Can configure the   to watch a subset of provided\n . You can find more information in the documentation. Can generate   without using subresources. Some versions of\nOpenshift 3.11 require this capability. To avoid using subresources,\nuse  --set subresourceEnabled=false  when installing the\n  with helm. Fixes setting the  spec.statefulSet  and\n spec.backup.statefulSet  fields on the\n MongoDBOpsManager  Resource. Fixes an issue that requires a restart of the   during\nsetup of webhook. Fixes an issue that could make an   resource to reach an\nunrecoverable state if the provided admin password has insufficient\nstrength. Released 2020-04-30 Deprecates the generation of   certificates by the  .\nIf you use  -generated certificates, warning messages now\nappear in the   logs. To configure secure deployments, see\n Secure a Database Resource . Fixes an issue where, when no authentication is configured by the\n , the   disables authentication in  .\nThe   no longer disables authentication unless you\nexplicitly set  spec.security.authentication.enabled  to\n false . When you configure the\n spec.statefulSet.spec  and\n spec.backup.statefulSet.spec  settings of the\n MongoDBOpsManager  resource, you can only\nconfigure the  spec.statefulSet.spec.template  and\n spec.backup.statefulSet.spec.template  fields. Any other\n spec.statefulSet.spec  or\n spec.backup.statefulSet.spec  field has no effect. Released 2020-04-24 Adds the ability to start the   with some but not all\nMongoDB   installed. Administrators can specify the container\nargument  watch-resource  to limit the   to deploy either\nMonogDB instances or  , or both. Adds the following new   configuration properties: When using a private docker registry, these properties must point\nto the relevant registries after you copy the images from the MongoDB distribution channels. INIT_OPS_MANAGER_IMAGE_REPOSITORY INIT_APPDB_IMAGE_REPOSITORY APPDB_IMAGE_REPOSITORY Increases support for custom   certificates with the\n spec.security.tls.secretRef  and  spec.security.tls.ca \nconfiguration settings. Deprecates   certificate generation by the  .\nMigrating to custom   certificates is recommended. See the  sample YAML files \nfor new feature usage examples. Releases the  MongoDBOpsManager  resource as\nGenerally Available (GA). MongoDB now supports using the  \nto deploy   resources to   in production environments. Supports Backup Blockstore Snapshot Stores. Defaults to the Application Database as a metadata database for Backup\n  Snapshot Stores. Supports  spec.jvmParameters  and  spec.backup.jvmParameters  to add or\noverride JVM parameters in   and Backup Daemon processes. Automatically configures   and Backup Daemon JVM memory\nparameters based on pod memory availability. Supports   for   and the Application Database. Adds more detailed information to the  status  field. Supports   Local Mode for  MongoDBOpsManager  resources with\nmultiple replicas by enabling users to specify\n PersistentVolumeClaimTemplates  in  spec.statefulSet.spec . Implements a new image versioning scheme. Known Issue : To enable  \nSnapshot stores in   4.2.10 and 4.2.12, you must set\n brs.s3.validation.testing: disabled  in the\n spec.configuration  property of your   resource\nspecification. Removes the  spec.podSpec  configuration setting. Use\n spec.statefulSet.spec  instead. Removes the  spec.backup.podSpec  configuration setting. Use\n spec.backup.statefulSet.spec  instead. Fixes CVE-2020-7922:   Operator generates potentially insecure certificates. X.509 certificates generated by the   may allow an attacker with\naccess to the   cluster improper access to MongoDB instances.\nCustomers who do not use X.509 authentication, and those who do not use\nthe   to generate their X.509 certificates are unaffected. CWE-295: Improper Certificate Validation\nCVSS score: 6.4\nCVSS:3.1/AV:A/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:N 1.0, 1.1 1.2.0 - 1.2.4 1.3.0 - 1.3.1 1.4.0 - 1.4.4 1.2.5 1.4.5 and above Supports changes in the  Cloud Manager API . Properly terminates resources with a termination hook. Implements stricter validations. MongoDB resources: Fixes an issue when working with   with custom  \ncertificates. Released 2020-02-24 Adds a  webhook  to validate\na   configuration. Adds support for sidecars for   pods using the\n spec.podSpec.podTemplate  setting. Allows users to change the  PodSecurityContext  to allow privileged\nsidecar containers. Adds the  spec.podSpec  configuration settings for\n , the Backup Daemon, and the Application Database. See\n Ops Manager Resource Specification .  image for version 4.2.8 is available. See the  sample YAML files  for new\nfeature usage examples. MongoDB resources: Fixes potential race conditions when deleting  .  resources: Supports the  spec.clusterDomain  setting for  \nand Application Database resources. No longer starts monitoring and backup processes for the Application\nDatabase. Released 2020-01-24 Runs MongoDB database   pods under a dedicated   service\naccount:  mongodb-enterprise-database-pods . Adds the  spec.podSpec.podTemplate  setting, which allows\nyou to apply templates to   pods that the  \ngenerates for each database  . Renames the  spec.clusterName  setting to\n spec.clusterDomain . Adds  offline mode support  for the Application\nDatabase. Bundles MongoDB Enterprise version 4.2.2 with the\nApplication Database image. Internet access is not required to\ninstall the application database if\n spec.applicationDatabase.version  is set to\n \"4.2.2-ent\"  or omitted. Renames the  spec.clusterName  setting to\n spec.clusterDomain .  images for versions 4.2.6 and 4.2.7 are available. See the  sample YAML files  for new\nfeature usage examples. MongoDB resources: Fixes the order of sharded cluster component creation. Allows   to be enabled on Amazon EKS.  resources: Enables the   to use the  spec.clusterDomain  setting. Released 2019-12-13 Includes  CVE fixes  and\n RHSA security fixes . Fixes an issue that prevented backup from starting on MongoDB 4.0. Released 2019-12-09 Adds split horizon DNS support for MongoDB replica sets, which allows\nclients to connect to a replica set from outside of the  \ncluster. Supports requests for  -generated certificates for\nadditional certificate domains, which makes them valid for the\nspecified subdomains. For more information on how to enable new features, see the sample YAML\nfiles in the  samples directory . Promotes the  MongoDBOpsManager   resource  to Beta.   version\n4.2.4 is available. Supports Backup and restore in  -deployed  \ninstances. This is a semi-automated process that deploys everything\nyou need to enable backups in  . You can enable Backup by\nsetting the  spec.backup.enabled  setting in the  \ncustom resource. You can configure the Head Database, Oplog Store, and\nS3 Snapshot Store by using the  MongoDBOpsManager   resource\nspecification . Supports access to   from outside the  \ncluster through the  spec.externalConnectivity  setting. Enables SCRAM-SHA-1 authentication  on  's\nApplication Database by default. Adds support for OpenShift (Red Hat UBI Images). Improves overall stability of X.509 user management. Released 2019-11-08 This release introduces significant changes that may not be\ncompatible with previous deployments or resource configurations. Read\n Migrate to One Resource per Project (Required for Version 1.3.0)  before\nupgrading the  . Requires one MongoDB resource per   project. If you\nhave more than one MongoDB resource in a project, all resources will\nchange to a  Pending  status and the   won\u2019t perform\nany changes on them. The existing MongoDB databases will still be\naccessible. You must  migrate to one resource per project . Supports  SCRAM-SHA  authentication mode. See  the MongoDB\nEnterprise Kubernetes Operator GitHub repository \nfor examples. Requires that the project ( ConfigMap ) and\ncredentials ( secret )\nreferenced from a MongoDB resource be in the same namespace. Adds OpenShift installation files (  file and Helm chart\nconfiguration). Supports highly available  Ops Manager resources  by introducing the  spec.replicas \nsetting. Runs   as a non-root user. Released 2019-10-25 This release introduces significant changes that may not be\ncompatible with previous deployments or resource configurations. Read\n Migrate to One Resource per Project (Required for Version 1.3.0)  before installing or\nupgrading the  . Moves to a\n one cluster per project configuration .\nThis follows the warnings introduced in a\n previous version of the operator .\nThe operator now requires each cluster to be contained within a new\nproject. Authentication settings are now contained within the\n security section  of the MongoDB resource\nspecification rather than the project ConfigMap. Replaces the  project  field with the\n spec.opsManager.configMapRef.name  or\n spec.cloudManager.configMapRef.name  fields. User resources  now refer to MongoDB\nresources rather than project ConfigMaps. No longer requires  data.projectName  in the project ConfigMap. The\nname of the project defaults to the name of the MongoDB resource in\n . This release introduces signficant changes to the   resource's\narchitecture. The   application database is now managed by\nthe  , not by  . Stops unnecessary recreation of NodePorts. Fixes logging so it's always in JSON format. Sets  USER  in the   Docker image. Fixes CVE-2020-7922:   Operator generates potentially insecure\ncertificates. X.509 certificates generated by the   may allow an attacker with\naccess to the   cluster improper access to MongoDB instances.\nCustomers who do not use X.509 authentication, and those who do not use\nthe   to generate their X.509 certificates are unaffected. CWE-295: Improper Certificate Validation\nCVSS score: 6.4\nCVSS:3.1/AV:A/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:N 1.0, 1.1 1.2.0 - 1.2.4 1.3.0 - 1.3.1 1.4.0 - 1.4.4 1.2.5 1.4.5 and above Released 2019-10-02 Increases stability of Sharded Cluster deployments. Improves internal testing infrastructure. Released 2019-09-13 Update:  The   will remove support for multiple\nclusters per project in a future release. If a project contains more\nthan one cluster, a warning will be added to the status of the\nMongoDB Resources. Additionally, any new cluster being added to a\nnon-empty project will result in a  Failed  state, and won't\nbe processed. Fix:  The overall stability of the operator has been improved. The\noperator is now more conservative in resource updates both on\n  and  . Released 2019-08-30 Security Fix:  Clusters configured by   versions\n1.0 through 1.2.1 used an insufficiently strong keyfile for internal\ncluster authentication between  mongod  processes. This only affects\nclusters which are using X.509 for user authentication, but are not\nusing X.509 for internal cluster authentication. Users are advised to\nupgrade to version 1.2.2, which will replace all managed keyfiles. Security Fix:  Clusters configured by   versions 1.0\nthrough 1.2.1 used an insufficiently strong password to authenticate\nthe MongoDB Agent. This only affects clusters which have been manually\nconfigured to enable  SCRAM-SHA-1 , which is not a supported\nconfiguration. Users are advised to upgrade to version 1.2.2, which\nwill reset these passwords. Released 2019-08-23 Fix:  The   no longer recreates   when X.509\nauthentication is enabled and the approved   have been deleted. Fix:  If the  OPERATOR_ENV  environment variable is set to\nsomething unrecognized by the  , it will no longer result\nin a  CrashLoopBackOff  of the pod. A default value of  prod  is\nused. The   now supports more than 100 agents in a given\nproject. Released 2019-08-13 Adds a\n readinessprobe \nto the MongoDB Pods to improve the reliability of rolling upgrades. This feature is an alpha release. It is not ready for production use. Can use the   to manage   4.2. To\n deploy an |onprem| instance ,\nyou use a new  resource :  MongoDBOpsManager . Released 2019-07-19 Fix:  Adds sample yaml files, in particular, the attribute related\nto\n featureCompatibilityVersion . Fix:    can be disabled in a deployment. Improvement:  Adds\n script \nin the\n support  directory that can gather\ninformation of your MongoDB resources in Kubernetes. Improvement:  In a   environment, the   can use a\ncustom  . All the certificates must be passed as  \nobjects. Released 2019-06-18 Supports Kubernetes v1.11 or later. Provisions any kind of MongoDB deployment in the Kubernetes Cluster\nof your Organization: Standalone Replica Set Sharded Cluster Configures   on the MongoDB deployments and encrypt all traffic.\nHosts and clients can verify each other\u2019s identities. Manages MongoDB users. Supports X.509 authentication to your MongoDB databases. If you have any questions regarding this release, use the\n #enterprise-kubernetes \nSlack channel. Released 2019-06-07 Rolling upgrades of MongoDB resources ensure that\n rs.stepDown()  is called for the primary\nmember. Requires MongoDB patch version 4.0.8 and later or MongoDB\npatch version 4.1.10 and later. During a MongoDB major version upgrade, the\n featureCompatibilityVersion  field can be set. Fixed a bug where replica sets with more than seven members could\nnot be created. X.509 Authentication can be enabled at the\n Project level . Requires  ,\n  patch version 4.0.11 and later, or   patch version\n4.1.7 and later. Internal cluster authentication based on X.509 can be enabled at the\n deployment  level. MongoDB users with X.509 authentication can be created, using the\nnew  MongoDBUser  custom resource. Released 2019-04-29 NodePort  service creation can be disabled.  can be enabled for internal authentication between MongoDB in\nreplica sets and sharded clusters. The   certificates are created\nautomatically by the  . Refer to the sample\n .yaml  files in the\n GitHub repository \nfor examples. Wide or asterisk roles have been replaced with strict listing of\nverbs in  roles.yaml . Printing  mdb  objects with  kubectl  will provide more\ninformation about the MongoDB object: type, state, and MongoDB server\nversion. Released 2019-04-02 The   and database images are now based on ubuntu:16.04. The   now uses a single   named  MongoDB \ninstead of the  MongoDbReplicaSet ,  MongoDbShardedCluster , and\n MongoDbStandalone  CRDs. Follow the  upgrade procedure  to\ntransfer existing  MongoDbReplicaSet ,  MongoDbShardedCluster ,\nand  MongoDbStandalone  resources to the new format. For a list of the packages installed and any security vulnerabilities\ndetected in our build process, see: MongoDB Enterprise Operator MongoDB Enterprise Database Released 2019-03-19 The Operator and Database images are now based on\n debian:stretch-slim  which is the latest and up-to-date Docker\nimage for Debian 9. Released 2019-02-26 Perform   clean-up on deletion of MongoDB resource without the\nuse of finalisers. Bug fix:  Race conditions when communicating with  . Bug fix:   ImagePullSecrets  being incorrectly initialized in\nOpenShift. Bug fix:  Unintended fetching of closed projects. Bug fix:  Creation of duplicate organizations. Bug fix:  Reconciliation could fail for the MongoDB resource if\nsome other resources in   were in error state. Released 2019-02-01 Improved detailed status field for MongoDB resources. The   watches changes to configuration parameters in a\nproject configMap and the credentials secret then performs a rolling\nupgrade for relevant Kubernetes resources. Added   structured logging for Automation Agent pods. Support   records for MongoDB access. Bug fix: Avoiding unnecessary reconciliation. Bug fix: Improved Ops Manager/Cloud Manager state management for\ndeleted resources. Released 2018-12-17 Refactored code to use the  controller-runtime  library to fix issues\nwhere Operator could leave resources in inconsistent state. This also\nintroduced a proper reconciliation process. Added new  status  field for all MongoDB Kubernetes resources. Can configure Operator to watch any single namespace or all\nnamespaces in a cluster (requires cluster role). Improved database logging by adding a new configuration property\n logLevel . This property is set to  INFO  by default.\nAutomation Agent and MongoDB logs are merged in to a single log\nstream. Added new configuration Operator timeout. It defines waiting time\nfor database pods start while updating  . Fix:  Fixed failure detection for  mongos . Released 2018-11-14 Image for database no longer includes the binary for the Automation\nAgent. The container downloads the Automation Agent binary from\n  when it starts. Fix:  Communication with   failed if the project with the same\nname existed in different organization. Released 2018-10-04 If a backup was enabled in   for a Replica Set or Sharded\nCluster that the   created, then the  \ndisables the backup before removing a resource. Improved persistence support: The data, journal and log directories are mounted to three\nmountpoints in one or three volumes depending upon the\n podSpec.persistence  setting. Prior to this release, only the data directory was mounted to\npersistent storage. Setting Mount Directories to podSpec.persistence.single One volume podSpec.persistence.multiple Three volumes A new parameter,  labelSelector , allows you to specify the\nselector for volumes that   should consider mounting. If   is not specified in the  persistence \nconfiguration, then the default  StorageClass  for the cluster is\nused. In most of public cloud providers, this results in dynamic\nvolume provisioning. Released 2018-08-07 The Operator no longer creates the CustomResourceDefinition objects.\nThe user needs to create them manually. Download and apply\n this new yaml file \n( crd.yaml ) to create/configure these objects. ClusterRoles are no longer required. How the Operator watches\nresources has changed. Until the last release, the Operator would\nwatch for any resource on any  . With 0.3, the Operator\nwatches for resources in the same namespace in which it was created.\nTo support multiple namespaces, multiple Operators can be installed.\nThis allows isolation of MongoDB deployments. Permissions changes were made to how PersistentVolumes are mounted. Added configuration to Operator to not create\n SecurityContexts \nfor  . This solves an issue with OpenShift which does not\nallow this setting when  SecurityContextContraints  are used. If you are using Helm, set  managedSecurityContext  to  true .\nThis tells the Operator to not create  SecurityContext  for\n , satisfying the OpenShift requirement. The combination of  projectName  and  orgId  replaces\n projectId  alone to configure the connection to  .\nThe project is created if it doesn't exist. Released 2018-08-03 Calculates WiredTiger memory cache. Released 2018-06-27 Initial Release Can deploy standalone instances, replica sets, sharded clusters\nusing   configuration files.",
            "code": [],
            "preview": "Released 2020-03-25",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/helm-operator-settings",
            "title": " Helm Installation Settings",
            "headings": [
                "appDb.name",
                "appDb.version",
                "database.name",
                "database.version",
                "initAppDb.name",
                "initAppDb.version",
                "initDatabase.name",
                "initDatabase.version",
                "initOpsManager.name",
                "initOpsManager.version",
                "managedSecurityContext",
                "namespace",
                "needsCAInfrastructure",
                "operator.deployment_name",
                "operator.env",
                "operator.name",
                "operator.version",
                "operator.watchNamespace",
                "operator.watchedResources",
                "opsManager.name",
                "registry.appDb",
                "registry.imagePullSecrets",
                "registry.initAppDb",
                "registry.initOpsManager",
                "registry.operator",
                "registry.opsManager",
                "subresourceEnabled"
            ],
            "paragraphs": "To provide optional settings, edit the Helm  values    file that\ncorresponds to your deployment type in the directory where you cloned\nthe   repository: If the setting that you want to add doesn't exist in the  values \n  file, add its key and value: Vanilla  :  helm_chart/values.yaml OpenShift:  helm_chart/values-openshift.yaml Alternatively, you can pass these values as options when you apply the\nHelm Chart: Name of the Application Database image. The default value is  mongodb-enterprise-appdb . Version of the image that contains the MongoDB Agent that the Application\nDatabase uses. The default value is 10.2.15.5958-1_4.2.11-ent. Name of the MongoDB Enterprise Database image. The default value is  mongodb-enterprise-database . Version of the MongoDB Enterprise Database image that the  \ndeploys. Name of the  initContainer  image that contains the Application\nDatabase start-up scripts and the readiness probe. The default value is  mongodb-enterprise-init-appdb . Version of the  initContainer  image that contains the Application\nDatabase start-up scripts and the readiness probe. The default value is  1.0.6 . Name of the  initContainer  image that contains the MongoDB Agent\nstart-up scripts and the readiness probe. The default value is  mongodb-enterprise-init-database . Version of the  initContainer  image that contains the MongoDB Agent\nstart-up scripts and the readiness probe. The default value is  1.0.2 . Version of the  initContainer  image that contains the  \nstart-up scripts and the readiness probe. The default value is  mongodb-enterprise-init-ops-manager . Version of the  initContainer  image that contains the  \nstart-up scripts and the readiness probe. The default value is  1.0.3 . Flag that determines whether or not the   inherits the\n securityContext  settings that your   cluster manages. This value must be  true  if you want to run the  \nin OpenShift or in a restrictive environment. The default value is  false . The default value is  true .  in which you want to deploy the  . To use a namespace other than the default, specify the namespace in\nwhich you want to deploy the  . The default value is  mongodb . Flag that determines whether   creates a   that allows the\n  to sign   certificates using the\n certificates.k8s.io \nAPI. The default value is  true . Name of the   container. The default value is  mongodb-enterprise-operator . Label for the  s deployment environment. This value\naffects the default timeouts and the logging level and format: The default value is  prod . If the value is Log Level is set to Log Format is set to dev debug text prod info json Name that   assigns to   objects, such as Deployments,\nServiceAccounts, Roles, and Pods. This value also corresponds to the name of the container registry where\nthe   is located. The default value is  mongodb-enterprise-operator . Version of the   that you want to deploy. The default value is  \u200b . Namespace that the   watches for  \nchanges. If this   differs from the default, ensure that\nthe Operator's ServiceAccount  can access \nthis namespace. Use  *  to specify  all namespaces . To watch all namespaces, you\nmust also assign the   to the  mongodb-enterprise-operator \nServiceAccount, which is the ServiceAccount used to run the\n . The default value is  <metadata.namespace> . To deploy   and   in a   other than the one\nwhere you deploy the  , see  Set Scope for   Deployment \nfor values you must use and additional steps you might have to\nperform. Custom resources that the   watches. The   installs the   for and watches only the resources you specify. The   accepts the following values: Value Description mongodb Install the CustomResourceDefinitions for the database resources and also\nwatch those resources. mongodbusers Install the CustomResourceDefinitions for the MongoDB user resources and\nalso watch those resources. opsmanagers Install the CustomResourceDefinitions for the   resources and also\nwatch those resources. Name of the   image. The default value is  mongodb-enterprise-ops-manager .  of the repository from which the   downloads\nthe Application Database image. The default value is  quay.io/mongodb . The default value is  registry.connect.redhat.com/mongodb .  that contains the credentials required to pull\nimages from the repository. OpenShift requires this setting. Define it in this file or\npass it when you install the   using Helm.  of the repository from which the  initContainer  image that\ncontains the Application Database start-up scripts and the readiness\nprobe is downloaded. The default value is  quay.io/mongodb . The default value is  registry.connect.redhat.com/mongodb .  of the repository from which the  initContainer  image that\ncontains the   start-up scripts and the readiness probe is\ndownloaded. The default value is  quay.io/mongodb . The default value is  registry.connect.redhat.com/mongodb . Repository from which the   image is pulled.\nSpecify this value if you want to pull the   image\nfrom a private repository. The default value is  quay.io/mongodb . The default value is  registry.connect.redhat.com/mongodb .  of the repository from which the image for an  Ops\nManager resource  is downloaded. The default value is  quay.io/mongodb . The default value is  registry.connect.redhat.com/mongodb . Flag that indicates whether subresources can be defined in the\n   . Set this setting to  false  if installing the  \nCustomResourceDefinition fails on an OpenShift 3.11 cluster. The default value is  true .",
            "code": [
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: quay.io/mongodb"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade <chart-name> helm_chart \\\n     --values helm_chart/values.yaml \\\n     --set namespace=<testNamespace>"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade <chart-name> helm_chart \\\n     --values helm_chart/values-openshift.yaml \\\n     --set namespace=<testNamespace>"
                },
                {
                    "lang": "yaml",
                    "value": "appDb:\n  name: mongodb-enterprise-appdb\n  version: 10.2.15.5958-1_4.2.11-ent"
                },
                {
                    "lang": "yaml",
                    "value": "appDb:\n  name: mongodb-enterprise-appdb\n  version: 10.2.15.5958-1_4.2.11-ent"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-database\n  version: 2.0.0"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-database\n  version: 2.0.0"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-appdb\n  version: 1.0.6"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-appdb\n  version: 1.0.6"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-database\n  version: 1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-database\n  version: 1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-ops-manager\n  version: 1.0.3"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-ops-manager\n  version: 1.0.3"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: true"
                },
                {
                    "lang": "yaml",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "needsCAInfrastructure: true"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  deployment_name: mongodb-enterprise-operator"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  # Execution environment for the operator, dev or prod.\n  # Use dev for more verbose logging\n  env: prod"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  name: mongodb-enterprise-operator"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  version: \u200b"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchedResources:\n    - mongodbusers\n    - mongodb\n    - opsmanagers"
                },
                {
                    "lang": "yaml",
                    "value": "opsManager:\n  name: mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n# The pull secret must be specified\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "subresourceEnabled: true"
                }
            ],
            "preview": "To provide optional settings, edit the Helm values  file that\ncorresponds to your deployment type in the directory where you cloned\nthe  repository:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/troubleshooting",
            "title": "Troubleshoot the ",
            "headings": [
                "Get Status of a Deployed Resource",
                "Review the Logs",
                "Review Logs from the ",
                "Find a Specific Pod",
                "Review Logs from Specific Pod",
                "View All  Specifications",
                "Restore StatefulSet that Failed to Deploy",
                "Replace a ConfigMap to Reflect Changes",
                "Remove  Components",
                "Remove a ",
                "Remove the ",
                "Remove the ",
                "Remove the ",
                "Create a New  after Deleting a Pod",
                "Disable  Feature Controls",
                "Debug a Failing Container",
                "Verify Corrrectness of Domain Names in TLS Certificates"
            ],
            "paragraphs": "To find the status of a resource deployed with the  ,\ninvoke one of the following commands: The following key-value pairs describe the resource deployment statuses: For   resource deployments: The  status.applicationDatabase.phase  field displays the\nApplication Database resource deployment status. The  status.backup.phase  displays the backup daemon resource\ndeployment status. The  status.opsManager.phase  field displays the   resource\ndeployment status. The   controller watches the database resources\ndefined in the following settings: spec.backup.opLogStores spec.backup.s3Stores spec.backup.blockStores For MongoDB resource deployments: The  status.phase  field displays the MongoDB resource deployment\nstatus. Key Value message Message explaining why the resource is in a  Pending  or\n Failed  state. phase Status Meaning Pending The   is unable to reconcile the resource\ndeployment state. This happens when a reconciliation\ntimes out or if the   requires you to take\naction for the resource to enter a running state. If a resource is pending because a reconciliation timed\nout, the   attempts to reconcile the\nresource state in 10 seconds. Reconciling The   is reconciling the resource state. Resources enter this state after you create or update\nthem or if the   is attempting to reconcile\na resource previously in a  Pending  or  Failed \nstate. The   attempts to reconcile the resource\nstate in 10 seconds. Running The resource is running properly. Failed The resource is not running properly. The  message \nfield provides additional details. The   attempts to reconcile the resource\nstate in 10 seconds. lastTransition  when the last reconciliation happened. link Deployment   in  . backup.statusName If you enabled continuous backups with  spec.backup.mode \nin   for your MongoDB resource, this field indicates\nthe status of the backup, such as  backup.statusName:\"STARTED\" .\nPossible values are  STARTED ,  STOPPED , and  TERMINATED . Resource specific fields For descriptions of these fields, see\n MongoDB Database Resource Specification . To see the status of a replica set named  my-replica-set  in\nthe  developer  namespace, run: If  my-replica-set  is running, you should see: If  my-replica-set  is not running, you should see: To review the   logs, invoke this command: You could check the  Ops Manager Logs  as\nwell to see if any issues were reported to  . To find which pods are available, invoke this command first: If you want to narrow your review to a specific  , you can\ninvoke this command: If your  replica set  is labeled  myrs , run: This returns the  Automation Agent Log  for this\nreplica set. To view all   specifications in the provided\n : To read details about the  dublin  standalone resource, run\nthis command: This returns the following response: A StatefulSet   may hang with a status of  Pending  if it\nencounters an error during deployment. Pending    do not automatically terminate, even if you\nmake  and apply  configuration changes to resolve the error. To return the StatefulSet to a healthy state, apply the configuration\nchanges to the MongoDB resource in the  Pending  state, then delete\nthose pods. A host system has a number of running  : my-replica-set-2  is stuck in the  Pending  stage. To gather\nmore data on the error, run: The output indicates an error in memory allocation. Updating the memory allocations in the MongoDB resource is\ninsufficient, as the pod does not terminate automatically after\napplying configuration updates. To remedy this issue, update the configuration, apply the\nconfiguration, then delete the hung pod: Once this hung pod is deleted, the other pods restart with your new\nconfiguration as part of rolling upgrade of the Statefulset. To learn more about this issue, see\n Kubernetes Issue 67250 . If you cannot modify or redeploy an already-deployed resource\n ConfigMap  file using the  kubectl apply  command, run: This deletes and re-creates the  ConfigMap  resource file. This command is useful in cases where you want to make an immediate\nrecursive change, or you need to update resource files that cannot\nbe updated once initialized. To remove any component, you need the following permissions: Cluster Roles mongodb-enterprise-operator-mongodb-webhook mongodb-enterprise-operator-mongodb-certs Cluster Role Bindings mongodb-enterprise-operator-mongodb-webhook-binding mongodb-enterprise-operator-mongodb-certs To remove any instance that   deployed, you must use  . You can only use the   to remove  -deployed\ninstances. If you use   to remove the instance,   throws an\nerror. To remove a single MongoDB instance you created using  : To remove all MongoDB instances you created using  : To remove the  : Remove all Kubernetes resources : Remove the  : To remove the  : Remove all Kubernetes resources : Remove the  : To remove the  : Remove all Kubernetes resources : Remove the  : If you accidentally delete the MongoDB replica set Pod and its  ,\nthe   fails to reschedule the MongoDB Pod and issues\nthe following error message: To recover from this error, you must  manually create a new PVC \nwith the PVC object's name that corresponds to this replica set Pod,\nsuch as  data-<replicaset-pod-name> . When you manage an   project through the  , the\n  places the  EXTERNALLY_MANAGED_LOCK \n feature control policy \non the project. This policy disables certain features in the  \napplication that might compromise your   configuration. If\nyou need to use these blocked features, you can remove the policy\nthrough the  feature controls API ,\nmake changes in the   application, and then restore the original\npolicy through the  API . The following procedure enables you to use features in the  \napplication that are otherwise blocked by the  . Retrieve the feature control policies \nfor your   project. Save the response that the API returns. After you make changes in\nthe   application, you must add these policies back to\nthe project. Your response should be similar to: Note the highlighted fields and values in the following sample\nresponse. You must send these same fields and values in later\nsteps when you remove and add feature control policies. The  externalManagementSystem.version  field corresponds to the\n  version. You must send the exact same field value\nin your requests later in this task. Update \nthe  policies  array with an empty list: The previously blocked features are now available in the\n  application. The values you provide for the  externalManagementSystem \nobject, like the  externalManagementSystem.version  field, must\nmatch values that you received in the response in Step 1. Make your changes in the   application. Update \nthe  policies  array with the original feature control policies: The features are now blocked again, preventing you from making\nfurther changes through the   application. However, the\n  retains any changes you made in the  \napplication while features were available. The values you provide for the  externalManagementSystem \nobject, like the  externalManagementSystem.version  field, must\nmatch values that you received in the response in Step 1. A container might fail with an error that results in   restarting\nthat container in a loop. You may need to interact with that container to inspect files or run\ncommands. This requires you to prevent the container from restarting. In your preferred text editor, open the MongoDB resource you need to\nrepair. To this resource, add a  podSpec  collection that resembles the\nfollowing. The  sleep  command in the\n spec.podSpec.podTemplate.spec  instructs the container to\nwait for the number of seconds you specify. In this example, the\ncontainer will wait for 1 hour. Apply this change to the resource. Invoke the shell inside the container. A MongoDB replica set or sharded cluster may fail to reach\nthe  READY  state if the   certificate is invalid. When you  configure TLS  for MongoDB replica sets or sharded clusters, verify\nthat you specify a valid certificate. If you don't specify the correct Domain Name for each   certificate,\nthe   logs may contain an error message similar to the\nfollowing, where  foo.svc.local  is the incorrectly-specified Domain\nName for the cluster member's Pod: To check whether you have correctly configured   certificates: To learn more about   certificate requirements, see\n TLS prerequisites . Each certificate should include a valid Domain Name. For each replica set or sharded cluster member, the Common Name, also\nknown as the Domain Name, for that member's certificate must match\nthe   of the POD on which this cluster member\nis deployed. The   name in each certificate has the following syntax:\n pod-name.service-name.namespace.svc.cluster.local . This name is\ndifferent for each Pod hosting a member of the replica set or a\nsharded cluster. For example, for a member of a replica set deployed on a Pod with\nthe name  rs-mongos-0-0 , in the   service\nnamed  mongo-0  that is created in the default  mongodb \nnamespace, the   is: Run: Check for  -related messages in the   log files.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl get <resource-name> -n <namespace> -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb my-replica-set -n developer -o yaml"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n    lastTransition: \"2019-01-30T10:51:40Z\"\n    link: http://ec2-3-84-128-187.compute-1.amazonaws.com:9080/v2/5c503a8a1b90141cbdc60a77\n    members: 1\n    phase: Running\n    version: 4.2.2-ent"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  lastTransition: 2019-02-01T13:00:24Z\n  link: http://ec2-34-204-36-217.compute-1.amazonaws.com:9080/v2/5c51c040d6853d1f50a51678\n  members: 1\n  message: 'Failed to create/update replica set in Ops Manager: Status: 400 (Bad Request),\n    Detail: Something went wrong validating your Automation Config. Sorry!'\n  phase: Failed\n  version: 4.2.2-ent"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs -f deployment/mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs <podName> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs myrs-0 -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl get mdb -n <namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl get mdb dublin -n <namespace> -o yaml"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"mongodb.com/v1\",\"kind\":\"MongoDB\",\"metadata\":{\"annotations\":{},\"name\":\"dublin\",\"namespace\":\"mongodb\"},\"spec\":{\"credentials\":\"credentials\",\"persistent\":false,\"podSpec\":{\"memory\":\"1Gi\"},\"project\":\"my-om-config\",\"type\":\"Standalone\",\"version\":\"4.0.0-ent\"}}\n  clusterDomain: \"\"\n  creationTimestamp: 2018-09-12T17:15:32Z\n  generation: 1\n  name: dublin\n  namespace: mongodb\n  resourceVersion: \"337269\"\n  selfLink: /apis/mongodb.com/v1/namespaces/mongodb/mongodbstandalones/dublin\n  uid: 7442095b-b6af-11e8-87df-0800271b001d\nspec:\n  credentials: my-credentials\n  type: Standalone\n  persistent: false\n  podSpec:\n    memory: \"1Gi\"\n  project: my-om-config\n  version: 4.2.2-ent"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods\n\nmy-replica-set-0     1/1 Running 2 2h\nmy-replica-set-1     1/1 Running 2 2h\nmy-replica-set-2     0/1 Pending 0 2h"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe pod my-replica-set-2\n\n<describe output omitted>\n\nWarning FailedScheduling 15s (x3691 over 3h) default-scheduler\n0/3 nodes are available: 1 node(s) had taints that the pod\ndidn't tolerate, 2 Insufficient memory."
                },
                {
                    "lang": "sh",
                    "value": "vi <my-replica-set>.yaml\n\nkubectl apply -f <my-replica-set>.yaml\n\nkubectl delete pod my-replica-set-2"
                },
                {
                    "lang": "shell",
                    "value": "kubectl replace -f <my-config-map>.yaml"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb <name> -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete deployment mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete crd mongodb.mongodb.com\nkubectl delete crd mongodbusers.mongodb.com\nkubectl delete crd opsmanagers.mongodb.com"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete namespace <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "scheduler error: pvc not found to schedule the pod"
                },
                {
                    "lang": "sh",
                    "value": "curl --user \"{USERNAME}:{APIKEY}\" --digest \\\n     --header \"Accept: application/json\" \\\n     --header \"Content-Type: application/json\" \\\n     --include \\\n     --request GET \"https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true\""
                },
                {
                    "lang": "json",
                    "value": "{\n \"created\": \"2020-02-25T04:09:42Z\",\n \"externalManagementSystem\": {\n   \"name\": \"mongodb-enterprise-operator\",\n   \"systemId\": null,\n   \"version\": \"1.4.2\"\n },\n \"policies\": [\n   {\n     \"disabledParams\": [],\n     \"policy\": \"EXTERNALLY_MANAGED_LOCK\"\n   },\n   {\n     \"disabledParams\": [],\n     \"policy\": \"DISABLE_AUTHENTICATION_MECHANISMS\"\n   }\n ],\n \"updated\": \"2020-02-25T04:10:12Z\"\n}"
                },
                {
                    "lang": "sh",
                    "value": "curl --user \"{USERNAME}:{APIKEY}\" --digest \\\n     --header \"Accept: application/json\" \\\n     --header \"Content-Type: application/json\" \\\n     --include \\\n     --request PUT \"https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true\" \\\n     --data\n       '{\n         \"externalManagementSystem\": {\n           \"name\": \"mongodb-enterprise-operator\",\n           \"systemId\": null,\n           \"version\": \"1.4.2\"\n         },\n         \"policies\": []\n       }'"
                },
                {
                    "lang": "sh",
                    "value": "curl --user \"{USERNAME}:{APIKEY}\" --digest \\\n     --header \"Accept: application/json\" \\\n     --header \"Content-Type: application/json\" \\\n     --include \\\n     --request PUT \"https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true\" \\\n     --data\n       '{\n         \"externalManagementSystem\": {\n           \"name\": \"mongodb-enterprise-operator\",\n           \"systemId\": null,\n           \"version\": \"1.4.2\"\n         },\n         \"policies\": [\n           {\n             \"disabledParams\": [],\n             \"policy\": \"EXTERNALLY_MANAGED_LOCK\"\n           },\n           {\n             \"disabledParams\": [],\n             \"policy\": \"DISABLE_AUTHENTICATION_MECHANISMS\"\n           }\n         ]\n       }'"
                },
                {
                    "lang": "yaml",
                    "value": "podSpec:\n  podTemplate:\n    spec:\n      containers:\n      - name: mongodb-enterprise-database\n        command: ['sh', '-c', 'echo \"Hello!\" && sleep 3600' ]"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl exec -it <pod-name> bash"
                },
                {
                    "lang": "sh",
                    "value": "TLS attempt failed : x509: certificate is valid for foo.svc.local,\nnot mongo-0-0.mongo-0.mongodb.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "rs-mongos-0-0.mongo-0.mongodb.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs -f <pod_name>"
                }
            ],
            "preview": "To find the status of a resource deployed with the ,\ninvoke one of the following commands:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/operator-settings",
            "title": " Installation Settings",
            "headings": [],
            "paragraphs": "When you install the  , you can provide optional settings\nthat affect your deployment.  How you provide these settings depends on\nthe environment to which you deploy the  . Review the settings that you can use when you install the\n  using   or  . Review the settings that you can use when you install the\n  using Helm.",
            "code": [],
            "preview": "When you install the , you can provide optional settings\nthat affect your deployment.  How you provide these settings depends on\nthe environment to which you deploy the .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/support-lifecycle",
            "title": "Support Lifecycle",
            "headings": [],
            "paragraphs": "The support lifecycle of the   is governed by the  MongoDB\nSupport Policy , where the\n  is considered an \"extension to  \". The following\ndates are derived from this policy and are published here for guidance\nonly. These dates might be subject to change.  Version End of Life Date 1.0 2020-03-15 1.1 2020-04-23 1.2.x 2020-05-21 1.3.x 2020-07-30 1.4.x 2020-09-13 1.5.x 2021-01-28 1.6.x 2021-04-13 1.7.x 2021-07-01 1.8.x 2021-07-06 1.9.x 2021-09-13",
            "code": [],
            "preview": "The support lifecycle of the  is governed by the MongoDB\nSupport Policy, where the\n is considered an \"extension to \". The following\ndates are derived from this policy and are published here for guidance\nonly. These dates might be subject to change.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/known-issues",
            "title": "Known Issues in the ",
            "headings": [
                "Unable to Use MongoDB 4.4 for an  Application Database",
                "Update Google Firewall Rules to Fix WebHook Issues",
                "Enable  Snapshot Stores in  4.2.10 and 4.2.12",
                "Configure Persistent Storage Correctly",
                "Remove Resources before Removing ",
                "Create Separate Namespaces for  and MongoDB Resources",
                "HTTPS Enabled After Deployment",
                "Difficulties with Updates",
                "Unable to Update the MongoDB Agent on Application Database Pods",
                "Machine Memory vs. Container Memory"
            ],
            "paragraphs": "You can't use MongoDB 4.4 as an application database for an  \nresource. Use an earlier MongoDB version instead. When you deploy   to   private clusters, the\n  or MongoDBOpsManager resource creation could time out.\nThe following message might appear in the logs: Error setting state to reconciling: Timeout: request did not\ncomplete within requested timeout 30s\". Google configures its firewalls to restrict access to your  \n . To use the webhook service,\n add a new firewall rule \nto grant   control plane access to your webhook service. The   webhook service runs on port 443. To enable S3 Snapshot stores in   4.2.10 and 4.2.12, you must\nset  brs.s3.validation.testing: disabled  in\nthe  spec.configuration  property of your  \nresource specification. If there are no\n persistent volumes \navailable when you create a resource, the resulting   stays in\ntransient state and the Operator fails  (after 20 retries) with the\nfollowing error: To prevent this error, either: For testing only, you may also set  persistent : false . This\n must not be used in production , as data is not preserved between\nrestarts. Provide   or Set  persistent : false  for the resource Sometimes   can diverge from  . This mostly occurs when\n  resources are removed manually.   can keep displaying an\nAutomation Agent which has been shut down. If you want to remove deployments of MongoDB on  , use the\nresource specification to delete resources first so no dead Automation\nAgents remain. The best strategy is to create   and its resources in\ndifferent namespaces so that the following operations would work\ncorrectly: or If the   and resources sit in the same  mongodb \n , then operator would also be removed in the same operation.\nThis would mean that it could not clean the configurations, which\nwould have to be done in the  . We recommend that you enable    before  deploying your   resources.\nHowever, if you enable   after deployment,\nyour managed resources can no longer communicate with   and\nthe   reports your resources' status as  Failed . To resolve this issue, you must delete your   by\nrunning the following command for each Pod: After deletion,   automatically restarts the deleted Pods.\nDuring this period, the resource is unreachable and incurs\ndowntime. Configure   to Run over HTTPS Troubleshoot the  In some cases, the   can stop receiving change events. As\nthis problem is hard to reproduce, the recommended workaround is to\ndelete the operator pod.   starts the new  \nautomatically and starts working correctly: Kubernetes Operator installation You can't use   to upgrade the MongoDB Agents that run on the\nApplication Database Pods. The MongoDB Agent version that runs on these\nPods is embedded in the Application Database Docker image. You can use the   to upgrade the MongoDB Agent version on\nApplication Database Pods as MongoDB publishes new images. APPDB_AGENT_VERSION appDb.version MongoDB versions older than 3.6.13, 4.0.9, and 4.1.9 report host system\nRAM, not container RAM.",
            "code": [
                {
                    "lang": "sh",
                    "value": "Failed to update Ops Manager automation config: Some agents failed to register"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete pods --all"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete pod <replicaset-pod-name>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods;\nkubectl delete pod mongodb-enterprise-operator-<podId>`"
                }
            ],
            "preview": "You can't use MongoDB 4.4 as an application database for an \nresource. Use an earlier MongoDB version instead.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/kubectl-operator-settings",
            "title": "  and  Installation Settings",
            "headings": [
                "APPDB_AGENT_VERSION",
                "APPDB_IMAGE_REPOSITORY",
                "DATABASE_VERSION",
                "IMAGE_PULL_POLICY",
                "INIT_APPDB_IMAGE_REPOSITORY",
                "INIT_APPDB_VERSION",
                "INIT_DATABASE_IMAGE_REPOSITORY",
                "INIT_DATABASE_VERSION",
                "INIT_OPS_MANAGER_IMAGE_REPOSITORY",
                "INIT_OPS_MANAGER_VERSION",
                "MANAGED_SECURITY_CONTEXT",
                "MONGODB_ENTERPRISE_DATABASE_IMAGE",
                "OPERATOR_ENV",
                "OPS_MANAGER_IMAGE_PULL_POLICY",
                "OPS_MANAGER_IMAGE_REPOSITORY",
                "WATCH_NAMESPACE"
            ],
            "paragraphs": "To provide optional settings, edit the   file that corresponds to\nyour deployment type in the directory where you cloned the\n  repository: If the setting that you want to add doesn't exist in the   file,\nadd it as a new array of key-value pair mappings in the\n spec.template.spec.containers.name.env.  collection: Vanilla   using  :  mongodb-enterprise.yaml OpenShift using  :  mongodb-enterprise-openshift.yaml Set the value of the  spec.template.spec.containers.name.env.name \nkey to the setting's name. Set the value of the  spec.template.spec.containers.name.env.value \nkey to the setting's value. Version of the image that contains the MongoDB Agent that the Application\nDatabase uses. The default value is 10.2.15.5958-1_4.2.11-ent. The default value varies based on whether you install the  \nto a vanilla   or an OpenShift environment:  of the repository from which the   downloads\nthe Application Database image. The default value is  quay.io/mongodb/mongodb-enterprise-appdb . The default value is  registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb . Version of the MongoDB Enterprise Database image that the  \ndeploys. Pull policy  for the MongoDB\nEnterprise database image the   deploys. The   accepts the following values:   Always ,\n IfNotPresent ,  Never . The default value is  Always .  of the repository from which the  initContainer  image that\ncontains the Application Database start-up scripts and the readiness\nprobe is downloaded. The default value is  quay.io/mongodb/mongodb-enterprise-appdb-init . The default value is  registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb-init . Version of the  initContainer  image that contains the Application\nDatabase start-up scripts and the readiness probe. The default value is  1.0.6 .  of the repository from which the  initContainer  image that\ncontains the MongoDB Agent start-up scripts and the readiness probe is\ndownloaded. The default value is  quay.io/mongodb/mongodb-enterprise-init-database . The default value is  registry.connect.redhat.com/mongodb/mongodb-enterprise-init-database . Version of the  initContainer  image that contains the MongoDB Agent\nstart-up scripts and the readiness probe. The default value is  1.0.2 .  of the repository from which the  initContainer  image that\ncontains the   start-up scripts and the readiness probe is\ndownloaded. The default value is  quay.io/mongodb/mongodb-enterprise-init-ops-manager . The default value is  registry.connect.redhat.com/mongodb/mongodb-enterprise-init-ops-manager . Version of the  initContainer  image that contains the  \nstart-up scripts and the readiness probe. The default value is  1.0.3 . Flag that determines whether or not the   inherits the\n securityContext  settings that your   cluster manages. This value must be  true  if you want to run the  \nin OpenShift or in a restrictive environment. The default value is  false . The default value is  true .  of the MongoDB Enterprise Database image that the  \ndeploys. The default value is  quay.io/mongodb/mongodb-enterprise-database . The default value is  registry.connect.redhat.com/mongodb/mongodb-enterprise-database . Label for the  s deployment environment. This value\naffects the default timeouts and the logging level and format: The default value is  prod . If the value is Log Level is set to Log Format is set to dev debug text prod info json Pull policy  for the\n  images the   deploys. The   accepts the following values:  Always ,\n IfNotPresent , and  Never . The default value is  Always .  of the repository from which the image for an  Ops\nManager resource  is downloaded. The default value is  quay.io/mongodb/mongodb-enterprise-ops-manager . The default value is  registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager . Namespace that the   watches for  \nchanges. If this   differs from the default, ensure that\nthe Operator's ServiceAccount  can access \nthis namespace. Use  *  to specify  all namespaces . To watch all namespaces, you\nmust also assign the   to the  mongodb-enterprise-operator \nServiceAccount, which is the ServiceAccount used to run the\n . The default value is  <metadata.namespace> . To deploy   and   in a   other than the one\nwhere you deploy the  , see  Set Scope for   Deployment \nfor values you must use and additional steps you might have to\nperform.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: APPDB_IMAGE_REPOSITORY\n              value: quay.io/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: APPDB_AGENT_VERSION\n              value: 10.2.15.5958-1_4.2.11-ent"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: APPDB_IMAGE_REPOSITORY\n              value: quay.io/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: APPDB_IMAGE_REPOSITORY\n              value: registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: DATABASE_VERSION\n              value: 2.0.0"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: IMAGE_PULL_POLICY\n              value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_APPDB_IMAGE_REPOSITORY\n              value: quay.io/mongodb/mongodb-enterprise-init-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_APPDB_IMAGE_REPOSITORY\n              value: registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb-init"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_APPDB_VERSION\n              value: 1.0.6"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n          - name: mongodb-enterprise-operator\n            image: <operatorVersionUrl>\n            imagePullPolicy: <policyChoice>\n            env:\n              - name: INIT_DATABASE_IMAGE_REPOSITORY\n                value:\n                quay.io/mongodb/mongodb-enterprise-init-database"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n          - name: mongodb-enterprise-operator\n            image: <operatorVersionUrl>\n            imagePullPolicy: <policyChoice>\n            env:\n              - name: INIT_DATABASE_IMAGE_REPOSITORY\n                value:\n                registry.connect.redhat.com/mongodb/mongodb-enterprise-init-database"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_DATABASE_VERSION\n              value: 1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_OPS_MANAGER_IMAGE_REPOSITORY\n              value: quay.io/mongodb/mongodb-enterprise-init-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_OPS_MANAGER_IMAGE_REPOSITORY\n              value: registry.connect.redhat.com/mongodb/mongodb-enterprise-init-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n        serviceAccountName: mongodb-enterprise-operator\n        containers:\n          - name: mongodb-enterprise-operator\n            image: <operatorVersionUrl>\n            imagePullPolicy: <policyChoice>\n            env:\n              - name: INIT_OPS_MANAGER_VERSION\n                value: 1.0.3"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: MANAGED_SECURITY_CONTEXT\n              value: false"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: MANAGED_SECURITY_CONTEXT\n              value: true"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n              value: quay.io/mongodb/mongodb-enterprise-database"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n              value: registry.connect.redhat.com/mongodb/mongodb-enterprise-database"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: OPERATOR_ENV\n              value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: OPS_MANAGER_IMAGE_PULL_POLICY\n              value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n          - name: OPS_MANAGER_IMAGE_REPOSITORY\n            value: quay.io/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n          - name: OPS_MANAGER_IMAGE_REPOSITORY\n            value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: WATCH_NAMESPACE\n              value: <testNamespace>"
                }
            ],
            "preview": "To provide optional settings, edit the  file that corresponds to\nyour deployment type in the directory where you cloned the\n repository:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/k8s-operator-om-specification",
            "title": "Ops Manager Resource Specification",
            "headings": [
                "Example",
                "Required  Resource Settings",
                "Optional  Resource Settings"
            ],
            "paragraphs": "The  MongoDB Enterprise Kubernetes Operator \ncreates a containerized   deployment from specification files\nthat you write. After you create or update an   resource specification, you\ndirect   to apply this specification to your  \nenvironment.   creates the services and custom  \nresources that   requires, then deploys   and its backing\napplication database in containers in your   environment. Each   resource uses an   specification in   to\ndefine the characteristics and settings of the deployment. The following example shows a resource specification for an  \ndeployment: This section describes settings that you must use for all  \nresources. Type : string Required . Version of the MongoDB   resource schema. Type : string Required . Kind of MongoDB Kubernetes resource to create. Set this\nto  MongoDBOpsManager . Type : string Required . Name of the MongoDB   resource you are creating. Resource names must be 44 characters or less. Type : number Required . Number of   instances to run in parallel. The minimum accepted value is  1 . For high availability, set this value to more than  1 . Multiple\n  instances can read from the same Application\nDatabase, ensuring failover if one instance is unavailable and\nenabling you to update the   resource without downtime. Type : string Required . Version of   that you want to install\non this MongoDB   resource. Type : string Required . Name of the     you created for\nthe   admin user. When you deploy the   resource,\n  creates a user with these credentials. The admin user is granted the\n Global Owner \nrole. Type : integer Required . Number of members in the\n Application Database \nreplica set. Type : string Optional . Version of MongoDB installed on the  \n Application Database .\nIf you don't specify a specific version,   uses the\ndefault value. This value defaults to  4.2.11-ent . Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. To deploy   inside   without an Internet connection,\nomit this setting or leave the value empty. The  \ninstalls the  bundled MongoDB Enterprise  version  4.2.11-ent  by default. If you update this value to a later version, consider setting\n spec.featureCompatibilityVersion  to give yourself the\noption to downgrade if necessary.  resources can use the following settings: Type : collection   Application Database \nresource definition. The following settings from the\n replica set  resource specification are\noptional: spec.applicationDatabase. spec.additionalMongodConfig spec.applicationDatabase. spec.agent spec.applicationDatabase.agent. spec.agent.startupOptions spec.applicationDatabase. spec.featureCompatibilityVersion spec.applicationDatabase. spec.logLevel spec.applicationDatabase. spec.persistent spec.applicationDatabase.podSpec.persistence. spec.podSpec.persistence.single spec.applicationDatabase.podSpec.persistence.multiple. spec.podSpec.persistence.multiple.data spec.applicationDatabase.podSpec.persistence.multiple. spec.podSpec.persistence.multiple.journal spec.applicationDatabase.podSpec.persistence.multiple. spec.podSpec.persistence.multiple.logs spec.applicationDatabase.podSpec. spec.podSpec.podAffinity spec.applicationDatabase.podSpec. spec.podSpec.podAntiAffinityTopologyKey spec.applicationDatabase.podSpec. spec.podSpec.podTemplate spec.applicationDatabase.podSpec. spec.podSpec.nodeAffinity spec.applicationDatabase.version Type : string Name of the  secret  that contains the\npassword for the   database user  mongodb-ops-manager .\n  uses this password to  authenticate to the Application\nDatabase . Type : string Name of the field in the  secret  that\ncontains the password for the   database user\n mongodb-ops-manager .   uses this password to\n authenticate to the Application Database . The default value is  password . Type : string Name of the     containing the   file for\nthe application database. This   signs the certificates that: spec.applicationDatabase.security.tls.ca  is required\nif you use a Custom Certificate Authority to sign your application\ndatabase   certificates. the application database replica set members use to communicate\nwith one another, and  uses to communicate with the application database replica\nset. You must concatenate your custom   file and the entire\n  certificate chain from  downloads.mongodb.com  to prevent\n  from becoming inoperable if the application database\nrestarts. Type : string Name of the     you created to secure the application\ndatabase resources. Type : boolean Flag that enables Backup for your   resource. When set to\n false , Backup is disabled. Default value is  true . Type : collection Configuration settings for the  head database .\n  creates a   with the specified configuration. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum size of   that should be mounted. This value is\nexpressed as an integer followed by a unit of storage in\n  notation. Default value is  30Gi . backup-hardware-requirements If the head database requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage specified in a  . You may create\nthis storage type as a   object before using it in this\n  specification. Make sure to set the    reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a   is removed. Type : array of strings Optional .   parameters passed to the   backup service\nin the container. This   parameter defaults to an empty list.  calculates the   memory heap values of the\nbackup service based on the container's memory. Changing the\n -Xms  and  -Xmx  values can cause issues with  . Type : collection Required if you enable Backup. Array of  oplog stores  used\nfor Backup. Each item in the array references a MongoDB database\nresource deployed in the   cluster by the  . Type : string Required if you enable Backup. Name of the oplog store. Once specified, do not edit the name of the oplog store. Type : string Required if you enable Backup. Name of the MongoDB database resource that you create to store oplog\nslices. You must deploy this database resource in the same namespace\nas the   resource. The Oplog database only supports the  SCRAM  authentication mechanism.\nYou cannot enable other authentication mechanisms. If you enable  SCRAM  authentication on the oplog database, you\nmust: Create a MongoDB user resource to connect   to the oplog\ndatabase. Specify the  name \nof the user in the   resource definition. If you deploy   4.2 with  SCRAM  authentication enabled, you\nmust specify a MongoDB version earlier than than 4.0 in the oplog\n database resource definition . If a MongoDB database resource with this name doesn't exist, the\n backup  resource enters a  Pending  state. The  \nretries every 10 seconds until a MongoDB database resource with this\nname is created. The   begins to reconcile the   resource\nautomatically when you make security changes to the database\nresources you reference in this setting. The   updates\nthe  mongoURI  and  ssl  flags in the   configuration\nbased on your changes. Type : string Required if SCRAM authentication is enabled on the oplog\nstore database. Name of the MongoDB user resource used to connect to the oplog store\ndatabase. Deploy this user resource in the same\nnamespace as the   resource and with the\n readWriteAnyDatabase  and\n dbAdminAnyDatabase  roles. Type : collection Required if you enable Backup using a blockstore. Array of  blockstores  used\nfor Backup. Each item in the array references a MongoDB database\nresource deployed in the   cluster by the  . Type : string Required if you enable Backup using a blockstore. Name of the blockstore. Once specified, do not edit the name of the blockstore. Type : string Required if you enable Backup using a blockstore. Name of the MongoDB database resource that you create for the\nblockstore. You must deploy this database resource in the same\nnamespace as the   resource. The blockstore database only supports the  SCRAM  authentication\nmechanism. You cannot enable other authentication mechanisms. If you enable  SCRAM  authentication on the blockstore database,\nyou must: Create a MongoDB user resource to connect   to the\nblockstore database. Specify the  name \nof the user in the   resource definition. If you deploy   4.2, you must specify a MongoDB version\nearlier than than 4.0 in the blockstore  database resource\ndefinition . If a MongoDB database resource with this name doesn't exist, the\n backup  resource enters a  Pending  state. The  \nretries every 10 seconds until a MongoDB database resource with this\nname is created. The   begins to reconcile the   resource\nautomatically when you make security changes to the database\nresources you reference in this setting. The   updates\nthe  mongoURI  and  ssl  flags in the   configuration\nbased on your changes. Type : string Required if SCRAM authentication is enabled on the blockstore database. Name of the MongoDB user resource used to connect to the blockstore\ndatabase. Deploy this user resource in the same\nnamespace as the   resource and with the\n readWriteAnyDatabase  and\n dbAdminAnyDatabase  roles. Type : collection Specification for the   that the   creates\nfor the  backup-daemon . To review which fields you can add to\n spec.backup.statefulSet.spec , see the\n Kubernetes documentation . Type : collection Template \nfor the   pods in the   that the   creates\nfor the  backup-daemon . The   doesn't validate the fields you provide\nin  spec.backup.statefulSet.spec.template . Type : collection Metadata for the   pods in the   that the\n  creates for the  backup-daemon . To review which fields you can add to\n spec.backup.statefulSet.spec.template.metadata , see\nthe  Kubernetes documentation . Type : collection Specifications of the   pods in the   that the\n  creates for the  backup-daemon . To review the complete list of fields you can add to\n spec.backup.statefulSet.spec.template.spec , see the\n Kubernetes documentation . The following example  spec.backup.statefulSet.spec.template.spec \ndefines minimum and maximum CPU and memory capacity for one\n backup-daemon  container the   deploys: Type : collection List of containers that belong to the   pods in the\n  that the   creates for the\n backup-daemon . To modify the specifications of the  backup-daemon  container,\nyou must provide the exact name of the container using the  name \nfield, as shown in the following example: When you add containers to\n spec.backup.statefulSet.spec.template.spec.containers ,\nthe   adds them to the   pod. These containers\nare appended to the  backup-daemon  containers in the pod. Type : string Minimum CPU capacity that must be available on a     to\nhost the  backup-daemon . The requested value must be less than or equal to\n spec.backup.statefulSet.spec.template.spec.containers.resources.limits.cpu . Type : string Maximum CPU capacity for the   being created to host\nthe  backup-daemon . If omitted, this value is set to\n spec.backup.statefulSet.spec.template.spec.containers.resources.requests.cpu . Type : string Minimum memory capacity that must be available on a    \nto host the  backup-daemon  on  . This value is expressed as\nan integer followed by a unit of memory in   notation. The requested value must be less than or equal to\n spec.backup.statefulSet.spec.template.spec.containers.resources.limits.memory . Set this value to at least  4.5Gi . Values of less than  4.5Gi \nmight result in an error. Type : string Maximum memory capacity for the   being created to host\nthe  backup-daemon . If omitted, this value is set to\n spec.backup.statefulSet.spec.template.spec.containers.resources.requests.memory . The   calculates and sets parameters for Java heap size\nbased on the container's memory. Setting this value to a value greater than 32 GB ( 32Gi ) can\ncause issues with the backup service. Excessive heaps can cause\nunpredictable results in  . Type : string Required if you enable Backup using an S3 store. Name of the   snapshot store. Once specified, do not edit the name of the   snapshot store. This change will likely fail if\nbackups use the old name. The consequences of\na successful change are unpredictable. Type : string Name of the MongoDB database resource that you create to store\nmetadata for the   snapshot store. You must deploy this database\nresource in the same namespace as the   resource. If you enable  SCRAM  authentication on this database, you must: Omit this setting to use the application database to store\nmetadata for the   snapshot store. If you omit this setting, you must also omit the\n spec.backup.s3Stores.mongodbUserRef.name  setting.\nThe   handles  SCRAM  user authentication\ninternally. Create a MongoDB user resource to connect   to the\ndatabase. Specify the\n name  of the\nuser in the   resource definition. If you deploy   4.2, you must specify a MongoDB version\nearlier than than 4.0 in the  resource definition  for the database that stores the metadata for\nthe   snapshot store. Once specified, do not edit the name of the   snapshot store.\nThis change will likely fail if backups use the old name. The\nconsequences of a successful change are unpredictable. If a MongoDB database resource with this name doesn't exist, the\n backup  resource enters a  Pending  state. The  \nretries every 10 seconds until a MongoDB database resource with this\nname is created. The   begins to reconcile the   resource\nautomatically when you make security changes to the database\nresources you reference in this setting. The   updates\nthe  mongoURI  and  ssl  flags in the   configuration\nbased on your changes. Type : string Required if you created a MongoDB database resource to store\n|s3| snapshot metadata and SCRAM is enabled on this database. Name of the MongoDB user resource used to connect to the metadata\ndatabase of the   snapshot store. Deploy this user resource in the\nsame namespace as the   resource and with the\n readWriteAnyDatabase  and\n dbAdminAnyDatabase  roles. Once specified, do not edit the name of the   metadata snapshot\nstore username. Type : string Required if you enable Backup using an S3 store. Name of the secret that contains the  accessKey  and\n secretKey  fields. The  backup-daemon  uses the values\nof these fields as credentials to access your\n    or  -compatible bucket. The  \nsnapshot store can't be configured if the secret is missimg either\nkey. Type : boolean Indicates the style of the bucket endpoint URL. Default value is  true . Value Description Example true Path-style URL s3.amazonaws.com/<bucket> false Virtual-host-style URL <bucket>.s3.amazonaws.com Type : string Required if you enable Backup using an S3 store. URL of the     bucket or  -compatible bucket that hosts the\nsnapshot store. Type : string Required if you enable Backup using an S3 store. Name of the     bucket or  -compatible bucket that hosts\nthe snapshot store. Type : string  assigns each   a  . The   calculates\nthe   for each   using a provided  clusterDomain .\n  does not provide an   to query these hostnames. Type : string  assigns each   a  . The   calculates\nthe   for each   using a provided  clusterName .  \ndoes not provide an   to query these hostnames. Use  spec.clusterDomain  instead. Type : collection  configuration properties.\nSee  Ops Manager Configuration Settings  for property names and descriptions.\nEach property takes a value of type  string . If   will manage MongoDB resources deployed outside of the\n  cluster it's deployed to, you must add the  mms.centralUrl \nsetting to  spec.configuration . Set the value to the URL by which   is exposed outside of the\n  cluster. Type : string The   service's  default server type . Accepted values are:  PRODUCTION_SERVER ,  TEST_SERVER ,  DEV_SERVER , and\n RAM_POOL . Type : collection Configuration object that enables external connectivity to  .\nIf provided, the   creates a    service  that allows traffic\noriginating from outside of the   cluster to reach the  \napplication. If not provided, the   does not create a   service.\nYou must create one manually or use a third-party solution that\nenables you to route external traffic to the   in your\n  cluster. Type : string The   service  ServiceType \nthat exposes   outside of  . Required  if  spec.externalConnectivity.type  is\npresent. Accepted values are:  LoadBalancer  and  NodePort .\n LoadBalancer  is recommended if your cloud provider supports it.\nUse  NodePort  for local deployments. Type : integer If  spec.externalConnectivity.type  is  NodePort , the\nport on the   service from which external traffic is routed to\nthe  . If  spec.externalConnectivity.type  is  LoadBalancer ,\nthe load balancer resource that your cloud provider creates routes\ntraffic to this port on the   service. You don't need to provide\nthis value.   uses an open port within the default range and\nhandles internal traffic routing appropriately. In both cases, if this value is not provided, the   service\nroutes traffic from an available port within the following default\nrange to the  :  30000 - 32767 . You must configure your network's firewall to allow traffic over\nthis port. Type : string The IP address the  LoadBalancer    service uses when the\n  creates it. This setting can only be used if your cloud provider supports it and\n spec.externalConnectivity.type  is  LoadBalancer . To\nlearn more about the\n Type LoadBalancer , see the\n  documentation. Type : string Routing policy for external traffic to the     service.\nThe service routes external traffic to node-local or cluster-wide\nendpoints depending the value of this setting. Accepted values are:  Cluster  and  Local . To learn which of\nvalues meet your requirements, see  Source IPs in Kubernetes  in the   documentation. If you select  Cluster , the  Source-IP  of your clients are\nlost during the network hops that happen at the  \nnetwork boundary. Type : collection Key-value pairs that allow you to provide cloud provider-specific\nconfiguration settings. To learn more about\n Annotations \nand\n TLS support on AWS ,\nsee the   documentation. Type : array of strings Optional .   parameters passed to the   in the\ncontainer. Any parameters given replace the default   parameters\nfor the  . This   parameter defaults to an empty list.  calculates its   memory heap values of the\n  based on the container's memory. Changing the\n -Xms  and  -Xmx  values can cause issues with  . Name of the     that contains a custom  \nfile for  . This   signs the certificates that: spec.security.tls.ca  is required if you use a Custom\nCertificate Authority to sign your     certificates. The   requires that the certificate is named\n mms-ca.crt  in the ConfigMap. clients use to connect to the  , and agents in the application database   use to communicate\nwith  . You must concatenate your custom   file and the entire\n  certificate chain from  downloads.mongodb.com  to prevent\n  from becoming inoperable if the application database\nrestarts. Type : string Name of the     you created for your    \ncertificate. Used when creating an   instance which runs\nover  . To learn how to configure your   instance to run over\n , see  Deploy an   Resource . Type : collection Specification for the   that the   creates\nfor  . To review which fields you can add to\n spec.statefulSet.spec , see the\n Kubernetes documentation . Type : collection Template \nfor the   pods in the   that the   creates\nfor the  . The   doesn't validate the fields you provide\nin  spec.statefulSet.spec.template . Type : collection Metadata for the   pods in the   that the\n  creates for the  . To review which fields you can add to\n spec.statefulSet.spec.template.metadata , see\nthe  Kubernetes documentation . Type : collection Specifications of the   pods in the   that the\n  creates for the  . To review the complete list of fields you can add to\n spec.statefulSet.spec.template.spec , see the\n Kubernetes documentation . The following example  spec.statefulSet.spec.template.spec  defines\nminimum and maximum CPU and memory capacity for one  \ncontainer the   deploys: Type : collection List of containers that belong to the   pods in the\n  that the   creates for the\n . To modify the specifications of the   container,\nyou must provide the exact name of the container using the  name \nfield, as shown in the following example: When you add containers to\n spec.statefulSet.spec.template.spec.containers ,\nthe   adds them to the   pod. These containers\nare appended to the   containers in the pod. Type : string Minimum CPU capacity that must be available on a     to\nhost the  . The requested value must be less than or equal to\n spec.statefulSet.spec.template.spec.containers.resources.limits.cpu . Type : string Maximum CPU capacity for the   being created to host\nthe  . If omitted, this value is set to\n spec.statefulSet.spec.template.spec.containers.resources.requests.cpu . Type : string Minimum memory capacity that must be available on a    \nto host the   on  . This value is expressed as\nan integer followed by a unit of memory in   notation. The requested value must be less than or equal to\n spec.statefulSet.spec.template.spec.containers.resources.limits.memory . If   on   requires 6 gigabytes of memory, set\nthis value to  6Gi . MongoDB recommends setting this value to at least  5Gi . Type : string Maximum memory capacity for the   being created to host\nthe  . If omitted, this value is set to\n spec.statefulSet.spec.template.spec.containers.resources.requests.memory . The   calculates and sets parameters for Java heap size\nbased on the container's memory. Setting this value to a value greater than 32 GB ( 32Gi ) can\ncause issues with the backup service. Excessive heaps can cause\nunpredictable results in  .",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: om\nspec:\n replicas: 1\n version: \"4.2.6\"\n adminCredentials: ops-manager-admin\n configuration:\n  mms.fromEmailAddr: admin@example.com\n  mms.security.allowCORS: \"false\"\n backup:\n  enabled: true\n  headDB:\n   storage: \"30Gi\"\n   labelSelector:\n    matchLabels:\n     app: my-app\n  opLogStores:\n   - name: oplog1\n     mongodbResourceRef:\n      name: my-oplog-db\n     mongodbUserRef:\n      name: my-oplog-user\n  s3Stores:\n   - name: s3store1\n     mongodbResourceRef:\n      name: my-s3-metadata-db\n     mongodbUserRef:\n      name: my-s3-store-user\n     s3SecretRef:\n       name: my-s3-credentials\n     pathStyleAccessEnabled: true\n     s3BucketEndpoint: s3.region.amazonaws.com\n     s3BucketName: my-bucket\n\n applicationDatabase:\n   passwordSecretKeyRef:\n    name: om-db-user-secret\n    key: password\n   members: 3\n   version: \"4.2.11-ent\"\n   persistent: true\n   podSpec:\n     cpu: \"0.25\"\n"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  backup:\n    jvmParameters: [\"-XX:+UseStringCache\"]"
                },
                {
                    "lang": "yaml",
                    "value": "statefulSet:\n  spec:\n    template:\n      spec:\n        containers:\n        - name: mongodb-backup-daemon\n          resources:\n            requests:\n              cpu: \"0.50\"\n              memory: \"4500M\"\n            limits:\n              cpu: \"1\"\n              memory: \"6000M\""
                },
                {
                    "lang": "yaml",
                    "value": "backup:\n statefulSet:\n   spec:\n     template:\n       spec:\n         containers:\n         - name: mongodb-backup-daemon"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  jvmParameters: [\"-XX:+HeapDumpOnOutOfMemoryError\",\"-XX:HeapDumpPath=/tmp\"]"
                },
                {
                    "lang": "yaml",
                    "value": "statefulSet:\n  spec:\n    template:\n      spec:\n        containers:\n          - name: mongodb-ops-manager\n            resources:\n              requests:\n                cpu: \"0.70\"\n                memory: \"6Gi\"\n              limits:\n                cpu: \"1\"\n                memory: \"7000M\""
                },
                {
                    "lang": "yaml",
                    "value": "backup:\n statefulSet:\n   spec:\n     template:\n       spec:\n         containers:\n         - name: mongodb-ops-manager"
                }
            ],
            "preview": "Type: string",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-op-prerequisites",
            "title": "Prerequisites",
            "headings": [
                "Procedure",
                "Have a  solution available to use.",
                "Clone the MongoDB Enterprise Kubernetes Operator repository.",
                "Create a  for your  deployment.",
                "Optional: Have a running .",
                "Required for OpenShift Installs: Create a  that contains credentials authorized to pull images from the registry.connect.redhat.com repository."
            ],
            "paragraphs": "To install the MongoDB  , you must: If you need a   solution, see the  \n documentation on picking the right solution . You can use  Helm  to install the\n . To learn how to install Helm, see its\n documentation on GitHub . By default, The   uses the  mongodb  namespace. To\nsimplify your installation, consider creating a namespace labeled\n mongodb  using the following   command: If you do not want to use the  mongodb  namespace, you can label\nyour namespace anything you like: If you don't deploy an   resource with the\n , you must have an   running outside of your\n  cluster. If you will deploy an   resource in   with\nthe  , skip this prerequisite. Your   installation must run an active   service. If\nthe   host's clock falls out of sync, that host can't\ncommunicate with the  . To learn how to check your   service for your Ops Manager\nhost, see the documentation for\n Ubuntu  or\n RHEL . If you have not already, obtain a Red Hat subscription. Create a  Registry Service Account . Click on your Registry Service Account, then click the\n Docker Configuration  tab. Download the  <account-name>-auth.json  file and open it in a\ntext editor. Copy the  registry.redhat.io  object, and paste another instance\nof this object into the file. Remember to add a comma after the\nfirst object. Rename the second object\n registry.connect.redhat.com , then save the file: Create a  openshift-pull-secret.yaml  file with the contents of\nthe modified  <account-name>-auth.json  file as  stringData \nnamed  .dockerconfigjson : The value you provide in the  metadata.name  field contains\nthe secret name. Provide this value when asked for the\n <openshift-pull-secret> . Create a   from the  openshift-pull-secret.yaml \nfile:",
            "code": [
                {
                    "lang": "sh",
                    "value": "git clone https://github.com/mongodb/mongodb-enterprise-kubernetes.git"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace <namespaceName>"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"auths\": {\n    \"registry.redhat.io\": {\n      \"auth\": \"<encoded-string>\"\n    },\n    \"registry.connect.redhat.com\": {\n      \"auth\": \"<encoded-string>\"\n    }\n  }\n}"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: openshift-pull-secret\nstringData:\n  .dockerconfigjson: |\n      {\n        \"auths\": {\n          \"registry.redhat.io\": {\n            \"auth\": \"<encoded-string>\"\n          },\n          \"registry.connect.redhat.com\": {\n            \"auth\": \"<encoded-string>\"\n          }\n        }\n      }\ntype: kubernetes.io/dockerconfigjson"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f openshift-pull-secret.yaml -n <namespace>"
                }
            ],
            "preview": "To install the MongoDB , you must:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/resize-pv-storage",
            "title": "Resize Storage for One Database Resource",
            "headings": [
                "Prerequisites",
                "Storage Class Must Support Resizing",
                "Procedure",
                "Create or identify a persistent .",
                "Insert data to the database that the resource serves.",
                "Patch each persistence volume.",
                "Remove the StatefulSets.",
                "Update the database resource with a new storage value.",
                "Update the pods in a rolling fashion.",
                "Validate data exists on the updated ."
            ],
            "paragraphs": "Make sure the   and volume plugin provider that the  \nuse supports resize: If you don't have a StorageClass that supports resizing, ask your  \nadministrator to help. Use an existing database resource or create a new one with persistent\nstorage. Wait until the persistent volume gets to the  Running \nstate. A database resource with persistent storage would include: Start   in the   cluster. Insert data into the  test  database. Invoke the following commands for the entire replica set: Wait until each   gets to the following condition: Delete a   resource. This step removes the   only. The pods remain\nunchanged and running. Update the disk size. Open your preferred text editor and make\nchanges similar to this example: To update the disk size of the replica set to 2 GB, change the\n storage  value in database resource specification: Recreate a   resource with the new volume size. Wait until this StatefulSet achieves the  Running  state. Invoke the following command: The new pods mount the resized volume. If the   were reused, the data that you inserted in  Step\n2  can be found on the databases stored in  :",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl patch storageclass/<my-storageclass> --type='json' \\\n        -p='[{\"op\": \"add\", \"path\": \"/allowVolumeExpansion\", \"value\": true }]'"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.4.0\"\n  project: my-project\n  credentials: my-credentials\n  type: ReplicaSet\n  podSpec:\n    memory: \"300M\"\n    persistence:\n      single:\n        storage: \"1Gi\""
                },
                {
                    "lang": "sh",
                    "value": "$kubectl exec -it <my-replica-set>-0 \\\n         /var/lib/mongodb-mms-automation/mongodb-linux-x86_64-4.4.0/bin/mongo"
                },
                {
                    "lang": "javascript",
                    "value": "<my-replica-set>:PRIMARY> use test\n\nswitched to db test\n\n<my-replica-set>:PRIMARY> db.tmp.insertOne({\"foo\":\"bar\"})\n\n{\n  \"acknowledged\" : true,\n  \"insertedId\" : ObjectId(\"61128cb4a783c3c57ae5142d\")\n}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl patch pvc/\"data-<my-replica-set>-0\" -p='{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"2Gi\"}}}}'\nkubectl patch pvc/\"data-<my-replica-set>-1\" -p='{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"2Gi\"}}}}'\nkubectl patch pvc/\"data-<my-replica-set>-2\" -p='{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"2Gi\"}}}}'"
                },
                {
                    "lang": "yaml",
                    "value": "- lastProbeTime: null\n  lastTransitionTime: \"2019-08-01T12:11:39Z\"\n  message: Waiting for user to (re-)start a pod to finish file\n           system resize of volume on node.\n  status: \"True\"\n  type: FileSystemResizePending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete sts --cascade=false <my-replica-set>"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.4.0\"\n  project: my-project\n  credentials: my-credentials\n  type: ReplicaSet\n  podSpec:\n    memory: \"300M\"\n    persistence:\n      single:\n        storage: \"2Gi\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f my-replica-set-vol.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl rollout restart sts <my-replica-set>"
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl exec -it <my-replica-set>-1 \\\n          /var/lib/mongodb-mms-automation/mongodb-linux-x86_64-4.4.0/bin/mongo"
                },
                {
                    "lang": "javascript",
                    "value": "<my-replica-set>:PRIMARY> use test\nswitched to db test\n\n<my-replica-set>:PRIMARY> db.tmp.count()\n1"
                }
            ],
            "preview": "Make sure the  and volume plugin provider that the \nuse supports resize:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/edit-deployment",
            "title": "Edit a Database Resource",
            "headings": [
                "Considerations",
                "Prerequisites",
                "Procedure"
            ],
            "paragraphs": "You can modify a deployment's configuration and topology, including its\nMongoDB versions, storage engines, and numbers of hosts or shards. You\ncan make modifications at all levels of a deployment's topology from a\ntop-level  sharded cluster  or\n replica set  to lower levels, such as a replica set within a\nsharded cluster, or an individual process within a replica set. You can\nalso modify  standalone  processes. Changes cannot be made to individual members of a replica set or\nsharded cluster, only to the whole set or cluster. If a setting isn't available for a MongoDB Kubernetes resource,\nthen the change must be made in the  Ops Manager  or\n Cloud Manager  application. MongoDB custom resources do not support all\n mongod  command line options. If you use an\nunsupported option in your object specification file, the backing\n MongoDB Agent \noverrides the unsupported options. For a complete list of options\nsupported by MongoDB custom resources, see  MongoDB Database Resource Specification . Certain settings can only be configured using  . To\nreview the list of settings, see\n MongoDB   Exclusive Settings . To update a MongoDB  , you need to have completed the following procedures: Install and Configure the  Create Credentials for the  Create One Project using a ConfigMap Deploy a database Edit the   resource specification file. Modify or add any settings you need added or changed. Save your specification file. Invoke the following   command to update your resource.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <standalone-conf>.yaml"
                }
            ],
            "preview": "You can modify a deployment's configuration and topology, including its\nMongoDB versions, storage engines, and numbers of hosts or shards. You\ncan make modifications at all levels of a deployment's topology from a\ntop-level sharded cluster or\nreplica set to lower levels, such as a replica set within a\nsharded cluster, or an individual process within a replica set. You can\nalso modify standalone processes.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-op-considerations",
            "title": "Considerations",
            "headings": [
                "Set MANAGED_SECURITY_CONTEXT for  OpenShift Deployments",
                "Check Messages from the Validation Webhook",
                "Customize the CustomResourceDefinitions that the  Watches"
            ],
            "paragraphs": "When you deploy the   to OpenShift, you must set the\n MANAGED_SECURITY_CONTEXT  flag to  true . This value is set for\nyou: Both files are included in the  MongoDB Enterprise Kubernetes Operator\nGitHub repository . To learn more, see the  Operator installation  and choose the method you want to use. In the  mongodb-enterprise-openshift.yaml \nfile. In the  values-openshift.yaml \nfile. The   uses a validation   to prevent users\nfrom applying invalid resource definitions. The webhook rejects invalid\nrequests. The   and   for the webhook are included in the default\nconfiguration files that you apply during the installation. To create\nthe role and binding, you must have  cluster-admin privileges . If you create an invalid resource definition, the webhook returns\na message similar to the following that describes the error to the shell: When the   reconciles each resource, it also validates that\nresource. The   doesn't require the validation webhook to\ncreate or update resources. If you omit the validation webhook, or if you remove the webhook's role\nand binding from the default configuration, or have insufficient\nprivileges to run the configuration, the   issues warnings,\nas these are not critical errors. If the   encounters a\ncritical error, it marks the resource as  Failed .  has a known issue with the webhook when deploying to private\nclusters. To learn more, see  Update Google Firewall Rules to Fix WebHook Issues . Earlier versions of the   would crash on startup if any\none of the MongoDB   was not present in the cluster. For\ninstance, you had to install the CustomResourceDefinition for  \neven if you did not plan to deploy it with the  . You can now specify which custom resources you want the  \nto watch. This allows you to install the CustomResourceDefinition for\nonly the resources that you want the   to manage. You must use  helm  to configure the   to watch only the\ncustom resources you specify. Follow the relevant  helm \n installation instructions ,\nbut make the following adjustments: Decide which CustomResourceDefinitions you want to install. You can\ninstall any number of the following: Value Description mongodb Install the CustomResourceDefinitions for the database resources and also\nwatch those resources. mongodbusers Install the CustomResourceDefinitions for the MongoDB user resources and\nalso watch those resources. opsmanagers Install the CustomResourceDefinitions for the   resources and also\nwatch those resources. Install each CustomResourceDefinition that you want the\n  to manage from the  helm_chart/crds \ndirectory: Install the Helm Chart and specify which\nCustomResourceDefinitions you want the\n  to watch. Separate each custom resource with a comma:",
            "code": [
                {
                    "lang": "sh",
                    "value": "error when creating \"my-ops-manager.yaml\":\nadmission webhook \"ompolicy.mongodb.com\" denied the request:\nshardPodSpec field is not configurable for application databases as\nit is for sharded clusters and appdb replica sets"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f helm_chart/crds/{value}.mongodb.com.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --operator.watchedResources=\"{mongodb,mongodbusers}\" \\\n     --skip-crds"
                }
            ],
            "preview": "When you deploy the  to OpenShift, you must set the\nMANAGED_SECURITY_CONTEXT flag to true. This value is set for\nyou:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/k8s-operator-specification",
            "title": "MongoDB Database Resource Specification",
            "headings": [
                "Common Resource Settings",
                "Required",
                "Conditional",
                "Optional",
                "Deployment-Specific Resource Settings",
                "Standalone Settings",
                "Replica Set Settings",
                "Sharded Cluster Settings",
                "Security Settings",
                "Examples"
            ],
            "paragraphs": "The  MongoDB Enterprise Kubernetes Operator \ncreates     from specification files that you\nwrote. MongoDB resources are created in Kubernetes as\n custom resources .\nAfter you create or update a   specification, you direct\n  to apply this specification to your   environment.\n  creates the defined  , services and\nother Kubernetes resources. After the Operator finishes creating those\nobjects, it updates the   deployment configuration to\nreflect changes. Each   uses an object specification in   to define the\ncharacteristics and settings of the MongoDB object: standalone,\n replica set , and  sharded cluster . At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . Deployment Type StatefulSets Size of StatefulSet Standalone 1 1  Replica Set 1 1   per member Sharded Cluster <numberOfShards> + 2 1   per  , shard, or config server member Every resource type must use the following settings: Every resource must use  one  of the following settings: Every resource type may use the following settings: Other settings you can and must use in a   specification\ndepend upon which MongoDB deployment item you want to create: Standalone Settings Replica Set Settings Sharded Cluster Settings All of the  Standalone Settings  also apply to replica set\nresources. The following settings apply only to replica set resource types: All of the  Standalone Settings  also apply to replica set\nresources. The following settings apply only to sharded cluster resource types: The following security settings apply only to replica set and sharded\ncluster resource types: The following example shows a resource specification for a\nstandalone deployment with every setting provided: The following example shows a resource specification for a\n replica set  with every setting provided: The following example shows a resource specification for a\n sharded cluster  with every setting provided:",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone\nspec:\n  version: \"4.2.2-ent\"\n  service: my-service\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  persistent: true\n  type: Standalone\n  additionalMongodConfig:\n    systemLog:\n      logAppend: true\n      verbosity: 4\n    operationProfiling:\n      mode: slowOp\n  podSpec:\n    cpu: \"0.25\"\n    memory: \"512M\"\n    persistence:\n      single:\n        storage: \"12Gi\"\n        storageClass: standard\n        labelSelector:\n          matchExpressions:\n          - {key: environment, operator: In, values: [dev]}\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/e2e-az-name\n            operator: In\n            values:\n            - e2e-az1\n            - e2e-az2\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  service: my-service\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  persistent: true\n  type: ReplicaSet\n  podSpec:\n    cpu: \"0.25\"\n    memory: \"512M\"\n    persistence:\n      multiple:\n        data:\n          storage: \"10Gi\"\n        journal:\n          storage: \"1Gi\"\n          labelSelector:\n            matchLabels:\n              app: \"my-app\"\n        logs:\n          storage: \"500M\"\n          storageClass: standard\n    podAntiAffinityTopologyKey: nodeId\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/e2e-az-name\n            operator: In\n            values:\n            - e2e-az1\n            - e2e-az2\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n  additionalMongodConfig:\n    net:\n      ssl:\n        mode: preferSSL\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  service: my-service\n  type: ShardedCluster\n\n  ## Please Note: The default Kubernetes cluster name is\n  ## `cluster.local`.\n  ## If your cluster has been configured with another name, you can\n  ## specify it with the `clusterDomain` attribute.\n\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n\n  persistent: true\n  configSrvPodSpec:\n    cpu: \"0.5\"\n    memory: \"512M\"\n\n    # if \"persistence\" element is omitted then Operator uses the\n    # default size (5Gi) for mounting single Persistent Volume\n\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n  mongosPodSpec:\n    cpu: \"0.8\"\n    memory: \"1Gi\"\n    podAntiAffinityTopologyKey: rackId\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n  shardPodSpec:\n    cpu: \"0.6\"\n    memory: \"3Gi\"\n    persistence:\n      multiple:\n        # if the child of \"multiple\" is omitted then the default size will be used.\n        # 16GB for \"data\", 1GB for \"journal\", 3GB for \"logs\"\n        data:\n          storage: \"20Gi\"\n        logs:\n          storage: \"4Gi\"\n          storageClass: standard\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n  mongos:\n    additionalMongodConfig:\n      systemLog:\n        logAppend: true\n        verbosity: 4\n  configSrv:\n    additionalMongodConfig:\n      operationProfiling:\n        mode: slowOp\n  shard:\n    additionalMongodConfig:\n      storage:\n        journal:\n          commitIntervalMs: 50\n  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n...\n"
                }
            ],
            "preview": "The MongoDB Enterprise Kubernetes Operator\ncreates   from specification files that you\nwrote.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/migrate-to-single-resource",
            "title": "Migrate to One Resource per Project (Required for Version 1.3.0)",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Create a new ConfigMap for each MongoDB resource in the project.",
                "Update MongoDB resource objects.",
                "Update MongoDB user objects.",
                "Delete the original project ConfigMap.",
                "(Optional) Remove Orphaned Clusters from ."
            ],
            "paragraphs": "Starting in   version 1.3.0, you can only have one MongoDB\nresource per project. This document explains how to migrate existing\nprojects which have multiple MongoDB resources into configurations with\na single resource per project. Before completing this procedure, ensure that you have upgraded your\n  to version 1.3.0. For upgrade instructions, see\n Upgrade the  . Complete the following steps for each project that contains multiple\nMongoDB resources: To associate each MongoDB resource with a single project, each\nresource must have a distinct  . The new ConfigMaps: All other fields can remain the same as the original project\nConfigMap. Invoke the following command for each ConfigMap to apply\nthem to  : To learn more about creating a project using a ConfigMap, see\n Create One Project using a ConfigMap . Must have unique  projectName  fields. Cannot contain the  credentials  or  authenticationMode \nfields. For each MongoDB resource in the project: If  X.509 authentication  is enabled, add the\nfollowing fields to the    : Field Type Description spec.security.authentication object Contains authentication specifications for the\ndeployment. spec.security.authentication.enabled boolean Specifies whether authentication is enabled for the\ndeployment. Set this value to  true . spec.security.authentication.modes array Specifies supported authentication mechanisms for the\ndeployment. Set this value to  [\"X509\"] If internal cluster authentication is enabled, set\n spec.security.authentication.internalCluster  to  X509 . Add the  spec.opsManager.configMapRef.name  field to the\n    and set the value to the  metadata.name  value\nof the corresponding ConfigMap you created in step 1. Remove the  spec.project  field from the resource object. Invoke the following command for each resource object to apply\nthe updated configuration(s). When you apply a new configuration,\nthe Operator creates a new project in   containing the\ndeployment from the corresponding MongoDB resource. All\ndata on the resource database remains the same after the migration. For each  MongoDB user  resource: Remove the  spec.project   field. Add the  spec.mongodbResourceRef.name  field and set the value\nto the name of the relevant MongoDB resource in the same namespace. This may require duplicating your  MongoDBUser  resource if\nyou wish to have the same user in multiple clusters. Invoke the following command to delete the original project ConfigMap\nfrom your   namespace: After reconfiguring your deployments to exist in dedicated clusters,\nyou may have clusters remaining in the original project which are no\nlonger managed by the  . You can remove these clusters if\nyou wish. Removing clusters will delete their historical backups and\nmonitoring data.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <myconfigmap.yaml>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <configuration>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete -f <configMap>.yaml"
                }
            ],
            "preview": "Before completing this procedure, ensure that you have upgraded your\n to version 1.3.0. For upgrade instructions, see\nUpgrade the .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/manage-database-users-x509",
            "title": "Manage Database Users Using X.509 Authentication",
            "headings": [
                "Prerequisites",
                "Add a Database User",
                "Configure kubectl to default to your namespace.",
                "Copy the following example .",
                "Open your preferred text editor and paste the example ConfigMap into a new text file.",
                "Change the five highlighted lines.",
                "Add any additional roles for the user to the ConfigMap.",
                "Create the user.",
                "View the newly created user in .",
                "Delete a Database User"
            ],
            "paragraphs": "The   supports managing database users for deployments\nrunning with   and X.509 internal cluster authentication enabled. The   does not support other authentication mechanisms\nin deployments it creates. In an Operator-created deployment, you\ncannot use   to: After enabling X.509 authentication, you can add X.509 users using the   interface or the  . Add other authentication methods to users. Manage users  not  using X.509 authentication. Before managing database users, you must deploy a\n replica set  or\n sharded cluster  with   and X.509\nenabled. If you need to generate X.509 certificates for your MongoDB users,\nsee  Generate X.509 Client Certificates . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Use the following table to guide you through changing the highlighted\nlines in the ConfigMap: Key Type Description Example metadata.name string The name of the database user resource. Resource names must be 44 characters or less. mms-user-1 spec.username string The subject line of the x509 client certificate signed\nby the     (Kube CA). To get the subject line of the X.509 certificate, run the\nfollowing command: The username must comply with the\n RFC 2253 \nLDAPv3 Distinguished Name standard. CN=mms-user,U=My Organizational Unit,O=My Org,L=New York,ST=New York,C=US spec.opsManager.configMapRef.name string The name of the project containing the MongoDB database\nwhere user will be added. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. my-project spec.roles.db string The database the  role  can act on. admin spec.mongodbResourceRef.name string The name of the  MongoDB resource  to\nwhich this user is associated. my-resource spec.roles.name string The name of the  role  to grant the database\nuser. The role name can be any\n built-in MongoDB role  or\n custom role  that exists\nin  . readWriteAnyDatabase You may grant additional roles to this user using the format defined\nin the following example: Invoke the following   command to create your database user: You can view the newly-created user in  : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. To delete a database user, pass the  metadata.name  from the user\n  to the following command:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: <resource-name>\nspec:\n  username: <rfc2253-subject>\n  db: \"$external\"\n  mongodbResourceRef:\n    name: '<MongoDB-Resource-name>'\n  roles:\n    - db: <database-name>\n      name: <role-name>\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "openssl x509 -noout \\\n  -subject -in <my-cert.pem> \\\n  -nameopt RFC2253"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: mms-user-1\nspec:\n  username: CN=mms-user,U=My Organizational Unit,O=My Org,L=New York,ST=New York,C=US\n  project: my-project\n  db: \"$external\"\n  roles:\n    - db: admin\n      name: backup\n    - db: admin\n      name: restore\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mdbu <metadata.name>"
                }
            ],
            "preview": "The  supports managing database users for deployments\nrunning with  and X.509 internal cluster authentication enabled.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/create-project-using-configmap",
            "title": "Create One Project using a ConfigMap",
            "headings": [
                "Considerations",
                "Prerequisites",
                "Create One Project Using a ConfigMap",
                "Configure kubectl to default to your namespace.",
                "Invoke the following command to create a ConfigMap.",
                "Find the organization's ID.",
                "Copy and paste the orgId into the URL.",
                "Find the organization's ID.",
                "Copy and paste the orgId into the URL.",
                "Invoke the following  command to verify your .",
                "Connect to HTTPS-enabled Ops Manager Using a Custom CA",
                "Create a ConfigMap for the Certificate Authority certificate.",
                "Copy the highlighted section of the following example ConfigMap.",
                "Add the highlighted section to your project's ConfigMap.",
                "Specify the TLS settings",
                "Save your updated ConfigMap.",
                "Invoke the  command to verify your .",
                "Next Steps"
            ],
            "paragraphs": "The   uses a     to create or link your\n   Project . To create a\n  ConfigMap, you can edit a few lines of the\n example ConfigMap    file and apply\nthe ConfigMap. Starting in   version 1.3.0, you can only deploy one\nMongoDB resource per project. See  Deploy a MongoDB Database Resource . You can use the   to deploy MongoDB resources with\n  version 4.0.11 or later and Cloud Manager. You can't use the   to deploy MongoDB resources to\n . Kubernetes version 1.11 or later or Openshift version\n3.11 or later.  version 0.11 or later\n installed . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Key Type Description Example configmap-name string Name of the    . Resource names must be 44 characters or less. metadata.name  documentation on  names .\nThis name must follow  RFC1123  naming\nconventions, using only lowercase alphanumeric\ncharacters,  -  or  . , and must start and end with an\nalphanumeric character. myconfigmap baseUrl string  to your   including the   and port\nnumber. If you deploy   with the   and   will\nmanage MongoDB database resources deployed  outside  of the  \ncluster it's deployed to, you must set  data.baseUrl  to the same\nvalue of the\n spec.configuration.mms.centralUrl \nsetting in the   resource specification. If you're using  , set the  data.baseUrl  value\nto  https://cloud.mongodb.com . https://ops.example.com:8443 projectName string Label for your  \n Project . The   creates the   project if it does\nnot exist. If you omit the  projectName , the  \ncreates a project with the same name as your\n  resource. To use an existing project in a  \norganization, locate\nthe  projectName  by clicking the  All Clusters \nlink at the top left of the   page, and\nsearching by name in the  Search \nbox, or scrolling to find the name in the list.\nEach card in this list represents the\ncombination of one    Organization  and  Project . Development orgId string 24 character hex string that uniquely\nidentifies your\n   Organization . Depending on your    credentials , this field is: You can use the   to deploy MongoDB resources with\n  version 4.0.11 or later and Cloud Manager. You can't use the   to deploy MongoDB resources to\n . Required  for  Global Programmatic API Keys . Optional  for  Organization Programmatic API Keys . You must specify an  existing Organization . Do one of the following: If you are using   4.4 or later or\n , click\n Settings  in the left navigation bar. If you are using   4.2 or earlier, click the\n Context  menu. Select your organization, view the current  \nin your browser and copy the value displayed in\nthe  <orgId>  placeholder as follows: https://ops.example.com:8443/ \n v2#/org/<orgId>/projects If specified, the   links to the organization. To find the  orgID  of your organization: If omitted,   creates an organization called\n projectName  that contains a project also called\n projectName . You must have the  Organization Project Creator \nrole to create a new project within an existing\n  organization. Do one of the following: If you are using   4.4 or later or\n , click\n Settings  in the left navigation bar. If you are using   4.2 or earlier, click the\n Context  menu. Select your organization, view the current  \nin your browser and copy the value displayed in\nthe  <orgId>  placeholder as follows: https://ops.example.com:8443/ \n v2#/org/<orgId>/projects 5cc9b333dde384a625a6615 This command returns a ConfigMap description in the shell: You might have chosen to use your own   certificate to enable\n  for your   instance. If you used a custom certificate,\nyou need to add the CA that signed that custom certificate to the\n . To add your custom CA, complete the following: The   requires the root   certificate of the\nCertificate Authority that issued the   host's certificate. Run\nthe following command to create a   containing the root\nCA certificate in the same namespace of your database pods: The   requires that the certificate is named\n mms-ca.crt  in the ConfigMap. Invoke the following command to edit your project's ConfigMap in\nthe default configured editor: Paste the highlighted section in the example   at\nthe end of the project ConfigMap. Change the following   keys: Key Type Description Example sslMMSCAConfigMap string Name of the   created in the first step\ncontaining the root   certificate used to sign the\n  host's certificate. This mounts the CA certificate\nto the   and database resources. my-root-ca sslRequireValidMMSServerCertificates boolean Forces the Operator to require a valid   certificate\nfrom  . The value must be enclosed in single quotes or the\noperator will throw an error. 'true' This command returns a ConfigMap description in the shell:  defaults to an empty namespace if you do not specify the\n -n  option, resulting in deployment failures. The\n ,  , and  s should run in the\nsame unique namespace. Now that you created your ConfigMap,  Create Credentials for the   before\nyou start  deploying MongoDB resources .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap <configmap-name> \\\n  --from-literal=\"baseUrl=<myOpsManagerURL>\" \\\n  --from-literal=\"projectName=<myOpsManagerProjectName>\" \\ #Optional\n  --from-literal=\"orgId=<orgID>\" #Required for Global API Keys"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe configmaps <configmap-name>"
                },
                {
                    "lang": "sh",
                    "value": "Name:           <configmap-name>\nNamespace:      <namespace>\nLabels:         <none>\nAnnotations:    <none>\n\nData\n====\nbaseUrl:\n----\n<myOpsManagerURL>\nEvents:  <none>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n <namespace> create configmap <root-ca-configmap-name> \\\n  --from-file=mms-ca.crt"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: <my-configmap>\n  namespace: <my-namespace>\ndata:\n  projectName: <my-ops-manager-project-name>\n  orgId: <org-id> # Optional\n  baseUrl: https://<my-ops-manager-URL>\n\n"
                },
                {
                    "lang": "yaml",
                    "value": "  sslMMSCAConfigMap: <root-ca-configmap-name>\n  sslRequireValidMMSServerCertificates: \u2018true\u2019\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl edit configmaps <my-configmap> -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe configmaps <my-configmap> -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Name:           <my-configmap>\nNamespace:      <namespace>\nLabels:         <none>\nAnnotations:    <none>\n\nData\n====\nsslMMSCAConfigMap:\n----\n<root-ca-configmap-name>\nsslRequireValidMMSServerCertificates:\n----\ntrue\nEvents:  <none>"
                }
            ],
            "preview": "The  uses a   to create or link your\n Project. To create a\n ConfigMap, you can edit a few lines of the\nexample ConfigMap  file and apply\nthe ConfigMap.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-op-container-images",
            "title": "Container Images",
            "headings": [
                " Image Operating System",
                "Resource Images"
            ],
            "paragraphs": "MongoDB rebuilds   images daily for the latest\noperating system and supporting library updates.  images are built with the latest versions of the\nfollowing operating systems: Registry Image OS Quay.io Ubuntu 16.04 Red Hat UBI 8 Red Hat Catalog Red Hat UBI 8 When you install the  , it pulls the following images from\na container registry: Image Name Description mongodb-enterprise-appdb Application Database image. mongodb-enterprise-init-appdb initContainer  image that contains the Application Database\nstart-up scripts and the readiness probe. mongodb-enterprise-database MongoDB Database environment image. mongodb-enterprise-init-database initContainer  image that contains the MongoDB Agent start-up\nscripts and the readiness probe. mongodb-enterprise-ops-manager Ops Manager image. mongodb-enterprise-init-ops-manager initContainer  image that contains the Ops Manager start-up\nscripts and the readiness probe. Images from Quay.io use Ubuntu 16.04 by default. Append\n -ubi  to an image name to retrieve a version that uses\nRed Hat UBI 8.",
            "code": [],
            "preview": " images are built with the latest versions of the\nfollowing operating systems:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/production-notes",
            "title": " Production Notes",
            "headings": [
                "Deploy the Recommended Number of MongoDB Replica Sets",
                "Ensure Proper Persistence Configuration",
                "Name Your MongoDB Service with its Purpose",
                "Specify CPU and Memory Resource Requirements",
                "Set CPU and Memory Utilization Bounds for the  Pod",
                "Set CPU and Memory Utilization Bounds for MongoDB Pods",
                "Use Multiple Availability Zones",
                "Co-locate mongos Pods with Your Applications",
                "Use Labels to Differentiate Between Deployments",
                "Verify Permissions",
                "Enable HTTPS",
                "Enable TLS",
                "Enable Authentication",
                "Example Deployment CRD",
                "Example User CRD"
            ],
            "paragraphs": "This page details system configuration recommendations for the\n  when running in production. All sizing and performance recommendations for common MongoDB deployments\nthrough the   in this section are subject to change. Do\nnot treat these recommendations as guarantees or limitations of any kind. These recommendations reflect performance testing findings and represent\nour suggestions for production deployments. We ran the tests on a cluster\ncomprised of seven AWS EC2 instances of type  t2.2xlarge  and a master node of\ntype  t2.medium . The recommendations in this section do not take into account individual\ncharacteristics of any deployment. Numerous factors might make your\ndeployment's characteristics differ from the assumptions made to\ncreate these recommendations. Contact MongoDB support for further\nassistance with sizings. We recommend that you use a single instance of the  \nto deploy up to 20 replica sets in parallel. You  may  increase this number to 50 and expect a reasonable\nincrease in the time that the   takes to download,\ninstall, deploy, and reconcile its resources. For 50 replica sets, the time to deploy varies and might take up to\n40 minutes. This time depends on the network bandwidth of the  \ncluster and the time it takes each MongoDB Agent to download MongoDB\ninstallation binaries from the Internet for each MongoDB cluster member. To deploy more than 50 MongoDB replica sets in parallel,\nuse multiple instances of the  . The   deployments orchestrated by the   are\nstateful. The   container uses   to maintain the\ncluster state between restarts. To satisfy the statefulness requirement, the   performs\nthe following actions: To meet your MongoDB cluster's storage needs, make the following\nchanges in your configuration for each replica set deployed with\nthe  : The following abbreviated example shows recommended persistent storage\nsizes. For a full example of persistent volumes configuration, see\n replica-set-persistent-volumes.yaml \nin the  Persistent Volumes Samples  directory. This\ndirectory also contains sample persistent volumes configurations for\nsharded clusters and standalone deployments. Creates   for your MongoDB deployment. Mounts storage devices to one or more directories\ncalled mount points. Creates one persistent volume for each MongoDB mount point. Sets the default path in each   container to  /data . Verify that persistent volumes are enabled in\n spec.persistent . This setting defaults to  true . Specify a sufficient amount of storage for the  \nto allocate for each of the volumes. The volumes store the data\nand the logs. To set multiple volumes, each for data, logs, and the  oplog , use\n spec.podSpec.persistence.multiple.data . To set a single volume to store data, logs, and the  oplog ,\nuse  spec.podSpec.persistence.single . spec.persistent spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data Set the  spec.service  parameter to a value that identifies\nthis deployment's purpose, as illustrated in the following example. spec.service In  , each Pod includes parameters that allow you\nto specify  CPU resources \nand  memory resources  for each\ncontainer in the Pod. To indicate resource bounds,   uses the  requests and limits \nparameters, where: The following sections illustrate how to: For the Pods hosting  , use the\n default resource limits configurations . request  indicates a lower bound of a resource. limit  indicates an upper bound of a resource. set CPU and Memory for the Operator Pod . set CPU and Memory for MongoDB Pods . When you deploy replica sets with the  , CPU usage for\nPod used to host the   is initially high during the\nreconciliation process, however, by the time the deployment completes,\nit lowers. For production deployments, to satisfy deploying up to 50 MongoDB\nreplica sets or sharded clusters in parallel with the  ,\nset the CPU and memory resources and limits for the   Pod\nas follows: If you don't include the unit of measurement for CPUs,   interprets\nit as the number of cores. If you specify  m , such as 500m,  \ninterprets it as  millis . To learn more, see\n Meaning of CPU . The following abbreviated example shows the configuration with\nrecommended CPU and memory bounds for the   Pod in your\ndeployment of 50 replica sets or sharded clusters. If you are\ndeploying fewer than 50 MongoDB clusters, you may use lower\nnumbers in the configuration file for the   Pod. For a full example of CPU and memory utilization resources and limits\nfor the   Pod that satisfy parallel deployment of up to\n50 MongoDB replica sets, see the  mongodb-enterprise.yaml \nfile. spec.template.spec.containers.resources.requests.cpu  to 500m spec.template.spec.containers.resources.limits.cpu  to 1100m spec.template.spec.containers.resources.requests.memory  to 200Mi spec.template.spec.containers.resources.limits.memory  to 1Gi Monitoring tools report the size of the   rather than the\nactual size of the container. Requests and Limits Assign CPU Resources to Containers and Pods The values for Pods hosting replica sets or sharded clusters map\nto the  requests field \nfor CPU and memory for the created Pod. These values are consistent\nwith  considerations \nstated for MongoDB hosts. The   uses its allocated memory for processing, for the\nWiredTiger cache, and for storing packages during the deployments. For production deployments, set the CPU and memory resources and limits\nfor the MongoDB Pod as follows: If you don't include the unit of measurement for CPUs,   interprets\nit as the number of cores. If you specify  m , such as 500m,  \ninterprets it as  millis . To learn more, see\n Meaning of CPU . The following abbreviated example shows the configuration with\nrecommended CPU and memory bounds for each Pod hosting a MongoDB\nreplica set member in your deployment. For a full example of CPU and memory utilization resources and limits\nfor Pods hosting MongoDB replica set members, see the\n replica-set-podspec.yaml \nfile in the the  MongoDB Podspec Samples  directory. This directory also contains sample CPU and memory limits\nconfigurations for Pods used for: spec.podSpec.podTemplate.spec.containers.resources.requests.cpu  to 0.25 spec.podSpec.podTemplate.spec.containers.resources.limits.cpu  to 0.25 spec.podSpec.podTemplate.spec.containers.resources.requests.memory  to 512M spec.podSpec.podTemplate.spec.containers.resources.limits.memory  to 512M A sharded cluster, in the  sharded-cluster-podspec.yaml . Standalone MongoDB deployments, in the  standalone-podspec.yaml . spec.podSpec.podTemplate.spec Requests and Limits Assign CPU Resources to Containers and Pods Set the   and   to distribute all members\nof one replica set to different   to ensure high\navailability. The following abbreviated example shows affinity and multiple\navailability zones configuration. In this example, the   schedules the Pods deployment to\nthe nodes which have the label  kubernetes.io/e2e-az-name  in  e2e-az1  or\n e2e-az2  availability zones. Change  nodeAffinity  to\nschedule the deployment of Pods to the desired availability zones. See the full example of multiple availability zones configuration in\n replica-set-affinity.yaml \nin the  Affinity Samples \ndirectory. This directory also contains sample affinity and multiple zones\nconfigurations for sharded clusters and standalone MongoDB deployments. Running in Multiple Zones Node affinity You can run the lightweight  mongos  instance on the same  \nas your apps using MongoDB. The   supports standard  \n node affinity and anti-affinity \nfeatures. Using these features, you can force install the  mongos \non the same Pod as your application. The following abbreviated example shows affinity and multiple\navailability zones configuration. The  podAffinity  key determines whether to install an application\non the same Pod, node, or data center as another application. To specify Pod affinity: See the full example of multiple availability zones and node affinity\nconfiguration in\n replica-set-affinity.yaml \nin the  Affinity Samples \ndirectory. This directory also contains sample affinity and multiple\nzones configurations for sharded clusters and standalone\nMongoDB deployments. Add a label and value in the  spec.podSpec.podTemplate.metadata.labels \n  collection to tag the deployment. See\n spec.podSpec.podTemplate.metadata ,\nand the\n Kubernetes PodSpec v1 core API . Specify which label the  mongos  uses in the\n spec.mongosPodSpec.podAffinity \n .requiredDuringSchedulingIgnoredDuringExecution.labelSelector \n  collection. The  matchExpressions  collection defines the\n label  that the   uses to identify the Pod for hosting\nthe  mongos . Assigning Pods to Nodes Node affinity and anti-affinity Kubernetes PodSpec v1 core API Use the  Pod affinity \n  feature to: Separate different MongoDB resources, such as  test ,  staging ,\nand  production  environments. Place   on some specific nodes to take advantage of\nfeatures such as   support. Pod affinity Objects in the   configuration  use the following\ndefault permissions. Kubernetes Resources Verbs Configmaps Require the following permissions: get ,  list ,  watch . The   reads the organization\nand project data from the specified  configmap . create ,  update . The   creates and updates  configmap \nobjects for configuring the  Application Database  instances. delete . The   needs the  delete   configmap  permission\nto support its  older versions .\nThis permission will be deleted when older versions reach their\nEnd of Life Date. Secrets Require the following permissions: get ,  list ,  watch . The   reads secret objects to\nretrieve sensitive data, such as  TLS  or\n X.509  access information. For example, it\nreads the credentials from a secret object to connect to the  . create ,  update . The   creates secret\nobjects holding  TLS  or\n X.509  access information. delete . The   deletes secret objects (containing passwords)\nrelated to the  Application Database . Services Require the following permissions: get ,  list ,  watch . The   reads and watches\nMongoDB services. For example, to communicate with the Ops Manager service,\nthe   needs  get ,  list  and  watch \npermissions to use the   service's URL. create ,  update . To communicate with services, the  \ncreates and updates service objects corresponding to  \nand MongoDB custom resources. StatefulSets Require the following permissions: get ,  list ,  watch . The   reacts to the changes in the\nStatefulSets it creates for the MongoDB custom resources. It also reads\nthe fields of  the StatefulSets it manages. create ,  update . The   creates and updates StatefulSets\ncorresponding to the mongoDB custom resources. delete . The   needs permissions to delete the StatefulSets\nwhen you delete the MongoDB custom resource. Pods Require the following permissions: get ,  list ,  watch . The   queries the\nApplication Database Pods to get information about its state. Namespaces Require the following permissions: list ,  watch . When you run the   in the cluster-wide mode,\nit needs  list  and  watch  permissions to all namespaces\nfor the MongoDB custom resources.  Architecture in  The   supports configuring   to run over\n HTTPS . Enable   before deploying your   resources to avoid a situation\nwhere the   reports your resources' status as  Failed . HTTPS Enabled After Deployment The   supports  TLS  encryption.\nUse   with your MongoDB deployment to encrypt your data over\nthe network. The configuration in the following example enables   for the replica\nset. When   is enabled, all traffic between members of the replica\nset and clients is encrypted using   certificates. The   generates   certificates using the  \nCertificate Authority. To learn more, see\n Managing TLS in Kubernetes . The default   mode is  requireTLS . You can customize it using the\n spec.additionalMongodConfig.net.ssl.mode  configuration\nparameter, as shown in the following abbreviated example. See the full   configuration example in\n replica-set.yaml \nin the  TLS \nsamples directory. This directory also contains sample   configurations for\nsharded clusters and standalone deployments. spec.additionalMongodConfig.net.ssl.mode Secure MongoDB Enteprise Kubernetes Operator Deployments Using TLS The   supports  X.509 , LDAP,\nand  SCRAM  user authentication. You must create an additional   for your\nMongoDB users and the MongoDB Agent instances.\nThe   generates and distributes the certificate. See the full X.509 certificates configuration examples in the\n x509 Authentication  directory in\nthe  Authentication \nsamples directory. This directory also contains sample LDAP and SCRAM configurations. For LDAP configuration, see the\n spec.security.authentication.ldap.automationLdapGroupDN \nsetting. spec.security.authentication.ldap.automationLdapGroupDN Manage Database Users Using X.509 Authentication Manage Database Users Using SCRAM Authentication",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-cluster\nspec:\n\n  ...\n  persistent: true\n\n\n  shardPodSpec:\n  ...\n    persistence:\n      multiple:\n        data:\n          storage: \"20Gi\"\n        logs:\n          storage: \"4Gi\"\n          storageClass: standard"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: \"4.4.0-ent\"\n  service: drilling-pumps-geosensors\n  featureCompatibilityVersion: \"4.0\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n name: mongodb-enterprise-operator\n namespace: mongodb\nspec:\n replicas: 1\n selector:\n  matchLabels:\n     app.kubernetes.io/component: controller\n     app.kubernetes.io/name: mongodb-enterprise-operator\n     app.kubernetes.io/instance: mongodb-enterprise-operator\n template:\n  metadata:\n   labels:\n     app.kubernetes.io/component: controller\n     app.kubernetes.io/name: mongodb-enterprise-operator\n     app.kubernetes.io/instance: mongodb-enterprise-operator\n   spec:\n     serviceAccountName: mongodb-enterprise-operator\n     securityContext:\n       runAsNonRoot: true\n       runAsUser: 2000\n     containers:\n     - name: mongodb-enterprise-operator\n       image: quay.io/mongodb/mongodb-enterprise-operator:1.9.2\n       imagePullPolicy: Always\n       args:\n        - \"-watch-resource=mongodb\"\n        - \"-watch-resource=opsmanagers\"\n        - \"-watch-resource=mongodbusers\"\n       command:\n        - \"/usr/local/bin/mongodb-enterprise-operator\"\n       resources:\n         limits:\n           cpu: 1100m\n           memory: 1Gi\n         requests:\n           cpu: 500m\n           memory: 200Mi"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\nname: my-replica-set\nspec:\n  members: 3\n  version: 4.0.0-ent\n  service: my-service\n  ...\n\n  persistent: true\n  podSpec:\n    podTemplate:\n      spec:\n        containers:\n        - name: mongodb-enterprise-database\n          resources:\n            limits:\n              cpu: \"0.25\"\n              memory: 512M"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: 4.2.1-ent\n  service: my-service\n  ...\n    podAntiAffinityTopologyKey: nodeId\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n        matchExpressions:\n        - key: security\n          operator: In\n          values:\n          - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n\n    nodeAffinity:\n       requiredDuringSchedulingIgnoredDuringExecution:\n         nodeSelectorTerms:\n         - matchExpressions:\n           - key: kubernetes.io/e2e-az-name\n           operator: In\n           values:\n           - e2e-az1\n           - e2e-az2"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: 4.2.1-ent\n  service: my-service\n\n  ...\n    podAntiAffinityTopologyKey: nodeId\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n        matchExpressions:\n        - key: security\n          operator: In\n          values:\n          - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n\n    nodeAffinity:\n       requiredDuringSchedulingIgnoredDuringExecution:\n         nodeSelectorTerms:\n         - matchExpressions:\n           - key: kubernetes.io/e2e-az-name\n           operator: In\n           values:\n           - e2e-az1\n           - e2e-az2"
                },
                {
                    "lang": "yaml",
                    "value": "mongosPodSpec:\n  podAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n        matchExpressions:\n        - key: security\n          operator: In\n          values:\n          - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\nname: my-tls-enabled-rs\nspec:\n  type: ReplicaSet\n\nmembers: 3\nversion: 4.0.4-ent\n\nopsManager:\n configMapRef:\n   name: my-project\ncredentials: my-credentials\n\nsecurity:\n  tls:\n    enabled: true\n\n...\nadditionalMongodConfig:\n  net:\n    ssl:\n     mode: \"preferSSL\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-tls-enabled-rs\nspec:\n  type: ReplicaSet\n  members: 3\n  version: \"4.0.4-ent\"\n  project: my-project\n  credentials: my-credentials\n  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: user-with-roles\nspec:\n  username: \"CN=mms-user-1,OU=cloud,O=MongoDB,L=New York,ST=New York,C=US\"\n  db: \"$external\"\n  project: my-project\n  roles:\n    - db: \"admin\"\n      name: \"clusterAdmin\""
                }
            ],
            "preview": "This page details system configuration recommendations for the\n when running in production.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/create-operator-credentials",
            "title": "Create Credentials for the ",
            "headings": [
                "Prerequisites",
                "Procedure"
            ],
            "paragraphs": "For the   to create or update   in your  \nProject, you need to store your\n Programmatic API Key  as a  \n . Creating a secret stores authentication credentials so\nonly   can access them. Multiple secrets can exist in the same namespace. Each user should\nhave their own secret. To create credentials for the  , you must: Have or create an  \n Organization . Unlike earlier   versions, use the Operator to\ncreate your   project. The Operator adds additional metadata\nto Projects that it creates to help manage the deployments. Have or generate a\n Programmatic API Key . Grant this new   the  Project Owner  role. Add the   or   block of any hosts that serve the\n  to the\n API Whitelist . To create your   secret: Make sure you have the Public and Private Keys for your desired\n   . Invoke the following   command to create your secret: The  -n  flag limits the   to which this secret\napplies. All MongoDB   resources must be in the same\nnamespace with the   and  . The\n  does not use either the secrets or ConfigMaps. Invoke the following   command to verify your secret: This command returns a secret description in the shell:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl -n <metadata.namespace> \\\n  create secret generic <myCredentials> \\\n  --from-literal=\"user=<publicKey>\" \\\n  --from-literal=\"publicApiKey=<privateKey>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe secrets/<myCredentials> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Name:         <myCredentials>\nNamespace:    <metadata.namespace>\nLabels:       <none>\nAnnotations:  <none>\n\nType:  Opaque\n\nData\n====\npublicApiKey:  31 bytes\nuser:          22 bytes"
                }
            ],
            "preview": "For the  to create or update  in your \nProject, you need to store your\nProgrammatic API Key as a \n. Creating a secret stores authentication credentials so\nonly  can access them.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/mdb-resources-arch",
            "title": "MongoDB Database Architecture in ",
            "headings": [
                "The MongoDB Custom Resource Definition",
                "Standalone",
                "Replica Set",
                "Sharded Cluster",
                "Reconciling the MongoDB Custom Resource",
                "Diagram of a Replica Set Reconciliation",
                "Diagram of a Sharded Cluster Reconciliation",
                "Reconciliation Workflow",
                "Reconciling the MongoDBUser Custom Resource"
            ],
            "paragraphs": "You can use the   and   to deploy MongoDB database\nresources to a   cluster. You can use an existing  , or deploy\n  in   to manage your databases. The   uses   to manage the following MongoDB database custom resources: Your   specifications define these resources in the  .\nThe   monitors these resources. When you update the\nresource's specification, the   pushes these changes to\n , which make changes to the MongoDB deployment's configuration. MongoDB MongoDBUser The   manages MongoDB database deployments\nwhich are defined by MongoDB custom resources. The MongoDB database   specification defines the\nfollowing types of the MongoDB database custom resources: The following diagram illustrates the composition of each type of the MongoDB\nresource in the  . Standalone ReplicaSet ShardedCluster For the  Standalone  type of the MongoDB database resource, the  \ndeploys a replica set with a single member to the   cluster as a\n . The   creates the StatefulSet, which contains the Pod\nspecification with the number of Pods to create. The  \nrelies on the   StatefulSet Controller to create a Pod for this\nstandalone MongoDB database instance. In  , a  Standalone  resource is equivalent to a  ReplicaSet \nresource with only one member. We recommend that you deploy\na  ReplicaSet  with one member instead of a  Standalone \nbecause a replica set allows you to add members to it in\nthe future. For the  ReplicaSet  type of the MongoDB resource, the  \ndeploys a replica set to the   cluster as a  , with\na number of members equal to the value of  spec.members . The   relies on the   StatefulSet Controller to create\none Pod in the StatefulSet for each member of the replica set. Each Pod in the StatefulSet runs a MongoDB Agent instance. The  ShardedCluster  type of the MongoDB resource consists of one or more\nConfig Servers,   instances, and shard members. For the  ShardedCluster  resource, the   deploys: The   relies on the   StatefulSet Controller to\ncreate one Pod in each of the StatefulSets created for the sharded cluster. One StatefulSet for all Config Servers One StatefulSet for all   instances One StatefulSet for each Shard Member When you apply a MongoDB custom resource specification,\nthe   deploys each resource as a  \nto the   cluster. The  : Watches the custom resource's specification and associated\n  or   for changes. Validates the changes when the specification file, the ConfigMap,\nor the secret change. Makes the appropriate updates to the MongoDB database resources\nin the   cluster. Pushes the changes to  , which make changes to the MongoDB\ndeployment's configuration. The following diagram describes how the   behaves\nif you make changes to the  MongoDB  custom resource specification,\nor to the associated   or  ,\nfor a replica set. The following diagram describes how the   behaves if\nyou make changes to the  MongoDB  custom resource specification,\nor to the associated   or  ,\nfor a sharded cluster. When you create or change a MongoDB resource specification, or when you make\nchanges to an associated   or  , the  \nperforms the following actions to reconcile the changes: Reads the required organization and project configuration\nfrom the  ConfigMap \nthat you used to create or connect to a project in the  . If you change your resource specification, the   identifies\nthat the change took place, and checks the specification for the ConfigMap\nspecified in  spec.opsManager.configMapRef.name . When you configure the   for MongoDB resources,\nyou  create a ConfigMap  to connect\nor create your   project. The MongoDB Agent uses this ConfigMap\nto start or make changes to the deployment for the MongoDB resource. Reads the authentication configuration for   from\nthe secret specified in  spec.credentials  in the\nresource specification. This secret stores the\n Cloud Manager API keys \nor the  Ops Manager API Keys \nrequired for the   to authenticate to  . When you configure the   for MongoDB resources,\nyou  create this secret . The   connects to   and performs the following actions: Reads the organization specified in the  OrgId  field in the ConfigMap,\nor uses the organization that has the same name as the project in  . Reads a project name specified in the  projectName  field in the\nConfigMap, or creates this project in   if it doesn't exist. Checks that the  <project-id>-group-secret  secret\ncreated by the   for the MongoDB Agent exists.\nThe   reads the secret, or, creates it with\n Ops Manager API keys \nor  Cloud Manager API keys . Registers itself as a watcher of the ConfigMap and this secret.\nThis enables the   to react to changes that you make\nto the ConfigMap or the secret. The   verifies any  TLS and X.509 certificates . If  TLS  is enabled for a replica set, the\n  looks for certificates provided in the\n <resource-name>-cert  secret. If  TLS  is enabled for a sharded cluster, the\n  looks for certificates in these secrets: <resource-name>-x-cert  for each shard member. <resource-name>-config-cert  for all config servers. <resource-name>-mongos-cert  for all   instances. If  X.509  or\n internal authentication with X.509 and  TLS \nare enabled, the   checks that their certificates\ncontain the required configuration. The   locates and updates the necessary StatefulSets,\nor creates new StatefulSets if they don't exist. The number of\nStatefulSets depends on the type of the MongoDB resource. For  ReplicaSet  or  Standalone  resources,\nthe   creates a single StatefulSet. For a  ShardedCluster  resource, the   creates: At this point, each Pod runs at least one MongoDB Agent instance,\nbut does not yet contain   instances. One StatefulSet for all config servers. One StatefulSet for all   instances. One StatefulSet for each shard member. Each MongoDB Agent instance starts polling   to receive the\nMongoDB automation configuration. When the MongoDB Agent receives the configuration for the first\ntime, it downloads the MongoDB binaries with the version\nspecified in  spec.version  from the Internet, or\nfrom  , if the MongoDB Agent is configured in the local mode. After the MongoDB Agent receives the automation configuration, it starts a\n  instance on the corresponding Pod. For each Pod of each StatefulSet that the MongoDB custom resource creates,\nexcept for   StatefulSets, the   generates a  .\nYou can override this behavior by setting  spec.persistent  to\n false  in the resource specification. The   updates the automation configuration it received from the\nMongoDB Agent with changes from the specifications and sends it to  . Each MongoDB Agent for each Pod polls   again and receives the\nupdated automation configuration. If you change any field in the specification, the  \nperforms a   of the StatefulSets to start new Pods\nmatching the new specification. The   waits for each MongoDB Agent to report that it reached\nthe ready state. If you change the  security configuration \nof a database resource, or  scale down \nan existing StatefulSet, the   runs step 6 before it\nruns  step 5. The   updates the   services, or for a new MongoDB\nresource, creates the services required for each new StatefulSet. For the    ClusterIP , the   sets\n ClusterIP  to  None , and performs these actions: Creates this service if it doesn't exist. For  ReplicaSet  or  Standalone  resources, the  \nnames the service with the custom resource's name  with  -svc \nappended to it. For a  ShardedCluster  resource, the   uses these\nnaming conventions: For   instances, the   uses the name specified in\n spec.service , or the resource's name  with  -svc \nappended to it. For the config servers, the   uses the resource's name\nwith  -cs  appended to it. For each shard, the   uses the resource's name\nwith  -sh  appended to it. For the port, the   uses the default port 27017, or\nthe  .net.port \nspecified in  spec.additionalMongodConfig . If the user authentication method is set to  SCRAM ,\nthe  MongoDBUser  custom resource depends\non the   that stores the user credentials. You specify the\nsecret in the  spec.passwordSecretKeyRef  settings in the  MongoDBUser \nresource specification. The   watches the secret for changes. If you make changes\nto the secret's configuration, the   reconciles the\nchanges. It takes the following actions: The following diagram describes how the   behaves if you make\nchanges to the user secret or the  MongoDBUser \ncustom resource specification. Determines the MongoDB user's resource based on the value\nspecified in the  spec.MongoDBResourceRef.name  setting in the\n MongoDBUser  resource specification. Connects to  : Reads the organization's name from  OrgId  in the ConfigMap, or\nuses the same name for the organization as the project's name in  . Reads a project's name from  projectName  in the ConfigMap,\nor creates this project in   if it doesn't exist. Checks that the  <project-id>-group-secret  created by the\n  for the MongoDB Agent exists.\nThe   reads the secret, or, creates it with\n Ops Manager API keys \nor  Cloud Manager API keys . Updates the user's credentials in  , or creates a new user if it doesn't exist. If the user authentication method is  SCRAM ,\nreads the password from the secret. Reads the user name. If the user name has changed, the  \nremoves the old name and adds a new one. Ensures that the user exists in  .",
            "code": [],
            "preview": "You can use the  and  to deploy MongoDB database\nresources to a  cluster. You can use an existing , or deploy\n in  to manage your databases.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-replica-set",
            "title": "Deploy a Replica Set",
            "headings": [
                "Prerequisites",
                "Considerations",
                "Deploy a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example to create a new replica set resource.",
                "Change the highlighted settings to your preferred values.",
                "Add any additional accepted settings for a replica set deployment.",
                "Save this replica set config file with a .yaml extension.",
                "Start your replica set deployment.",
                "Track the status of your replica set deployment.",
                "Enable External Access for a Replica Set",
                "Deploy a replica set with the .",
                "Add Subject Alternate Names to your  certificates.",
                "Create a NodePort for each .",
                "Discover the dynamically assigned NodePorts.",
                "Open your replica set resource  file.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Change the highlighted settings to your preferred values.",
                "Confirm the external hostnames and NodePort values in your replica set resource.",
                "Save your replica set config file.",
                "Update and restart your replica set deployment.",
                "Test the connection to the replica set."
            ],
            "paragraphs": "A  replica set  is a group of MongoDB deployments that maintain\nthe same data set. Replica sets provide redundancy and high\navailability and are the basis for all production deployments. To learn more about replica sets, see the\n Replication Introduction  in\nthe MongoDB manual. Use this procedure to deploy a new replica set that   manages.\nAfter deployment, use   to manage the replica set, including such\noperations as adding, removing, and reconfiguring members. At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . You can use the   to deploy MongoDB resources with\n  version 4.0.11 or later and Cloud Manager. You can't use the   to deploy MongoDB resources to\n . To deploy a  replica set  using an  , you need to complete the\nfollowing procedures: Install Kubernetes Operator Create One Project using a ConfigMap Create Credentials for the  Starting in   version 1.3.0, you can only have one\nMongoDB resource per project. To learn how to migrate your project to\na single-cluster configuration, see  Migrate to One Resource per Project (Required for Version 1.3.0) . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\ninto a new text file. Key Type Description Example metadata.name string Label for this    replica set   . Resource names must be 44 characters or less. metadata.name Kubernetes documentation on\n names . myproject spec.members integer Number of members of the  replica set . 3 spec.version string Version of MongoDB that this  replica set  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. 3.6.7 string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value  must  match the namespace in which you created the\n   project ConfigMap . The   tracks any changes to the ConfigMap and\nreconciles the state of the  . <myconfigmap> spec.credentials string Name of the     you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . This value  must  match the namespace in which you created the\nsecret and the  name  value you provided for your  \n   Secret . The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. ReplicaSet spec.persistent string Optional. Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. If this value is  true , then\n spec.podSpec.persistence.single  is set to its\ndefault value of  16Gi . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: If you want one   for each  , configure the\n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs Your containers must have permissions to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the deployment item does not fix\nissues with your  , contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n  specification file for a  replica set  deployment: spec.additionalMongodConfig spec.backup.mode spec.clusterDomain spec.featureCompatibilityVersion spec.logLevel spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs spec.podSpec.podAffinity spec.podSpec.podAntiAffinityTopologyKey spec.podSpec.nodeAffinity spec.podSpec.podTemplate.metadata spec.podSpec.podTemplate.spec You must set  spec.clusterDomain  if your   cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n  might not function as expected. Invoke the following   command to create your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. If you haven't deployed a replica set, follow the instructions to\n deploy one . You must enable   for the replica set with the\n spec.security.tls.enabled  setting. The replica set must use\na custom certificate stored with  spec.security.tls.ca . Add each external   name to the certificate  . Invoke the following commands to create the NodePorts: Discover the dynamically assigned NodePorts: NodePorts range from 30000 to 32767, inclusive. Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. Key Type Necessity Description Example boolean Optional Set this value to  true  to enable   on the MongoDB\ndeployment. By default,   requires hosts to use and accept\n  encrypted connections. To connect to a replica set from outside  , set this\nvalue to  true . true collection Conditional Add this parameter and values if you need your database to be\naccessed outside of  . This setting allows you to provide\ndifferent   settings within the   cluster and to the\n  cluster. The   uses split horizon   for\nreplica set members. This feature allows communication both\nwithin the   cluster and from outside  . You may add multiple external mappings per host. Make sure that each value in this array is unique. Make sure that the number of entries in this array matches\nthe value given in  spec.members . Set the  spec.security.tls.enabled  to  true  to\nenable  . This method to use split horizons requires\nthe Server Name Indication extension of the   protocol. See Setting Confirm that the external hostnames in the\n spec.connectivity.replicaSetHorizons  setting are correct. External hostnames should match the   names of   worker nodes.\nThese can be  any  nodes in the   cluster.   do internal\nrouting if the pod is run on another node. Set the ports in  spec.connectivity.replicaSetHorizons  to\nthe NodePort values that you discovered. Invoke the following   command to update and restart your\n replica set : If the connection succeeds, you should see: Don't use the  --sslAllowInvalidCertificates  flag in production.\nIn production, share the     files with client tools\nor applications.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl expose pod/<my-replica-set>-0 --type=\"NodePort\" --port 27017\nkubectl expose pod/<my-replica-set>-1 --type=\"NodePort\" --port 27017\nkubectl expose pod/<my-replica-set>-2 --type=\"NodePort\" --port 27017"
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl get svc | grep <my-replica-set>\n<my-replica-set>-0     NodePort   172.30.39.228   <none>  27017:30907/TCP  16m\n<my-replica-set>-1     NodePort   172.30.185.136  <none>  27017:32350/TCP  16m\n<my-replica-set>-2     NodePort   172.30.84.192   <none>  27017:31185/TCP  17m\n<my-replica-set>-svc   ClusterIP  None            <none>  27017/TCP        38m"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  type: ReplicaSet\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:30907\"\n      - \"example-website\": \"web2.example.com:32350\"\n      - \"example-website\": \"web3.example.com:31185\"\n..."
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:30907\"\n      - \"example-website\": \"web2.example.com:32350\"\n      - \"example-website\": \"web3.example.com:31185\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host <my-replica-set>/web1.example.com:30907,web2.example.com:32350,web3.example.com:31185 \\\n      --ssl \\\n      --sslAllowInvalidCertificates"
                },
                {
                    "lang": "javascript",
                    "value": "MongoDB Enterprise <my-replica-set>:PRIMARY"
                }
            ],
            "preview": "A replica set is a group of MongoDB deployments that maintain\nthe same data set. Replica sets provide redundancy and high\navailability and are the basis for all production deployments.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/install-k8s-operator",
            "title": "Install the ",
            "headings": [
                "Prerequisites and Considerations",
                "Procedure",
                "Change to the directory in which you cloned the repository.",
                "Install the  for MongoDB deployments using the following  command:",
                "Optional: Customize the   before installing it.",
                "Install the  using the following  command:",
                "Change to the directory in which you cloned the repository.",
                "Install the  using the following helm command:",
                "Use docker to request the files on a host connected to the Internet.",
                "Export the  images as .tar archive files:",
                "Copy these .tar files to the host running the  docker daemon.",
                "Import the .tar files into docker.",
                "Install the  with modified pull policy values using the following helm command:",
                "Change to the directory in which you cloned the repository.",
                "Install the  for MongoDB deployments.",
                "Optional: Customize the   before installing it.",
                "Install the  using the following  command:",
                "Change to the directory in which you cloned the repository.",
                "Add your OpenShift Pull Secret to the OpenShift Values file.",
                "Install the  using helm.",
                "Use docker to request the files on a host connected to the Internet.",
                "Export the  images as .tar archive files:",
                "Copy these .tar files to the host running the  docker daemon.",
                "Import the .tar files into docker.",
                "Add your OpenShift Pull Secret to the OpenShift Values file.",
                "Install the  with modified pull policy values.",
                "Verify the Installation",
                "Install a Specific Daily Build with Helm",
                "Next Steps"
            ],
            "paragraphs": "Before you install the  , make sure you\n plan for your installation : Choose a  deployment topology . Read the  Considerations . Complete the  Prerequisites . This tutorial presumes some knowledge of  , but does link to\nrelevant   documentation where possible. If you are unfamiliar\nwith  , please review that documentation first. The install procedure varies based on how you want to configure your\nenvironment: The following examples assume that you created a  \nusing the default   namespace of  mongodb .\nIf you specified a different label for your namespace when\nyou  created it , change all\nvalues for  metadata.namespace  to that namespace. To change the label for the namespace for the following\ndeployment to  production , edit all values for\n metadata.namespace  in  mongodb-enterprise.yaml : Invoke the following   command: You might need to add one or more optional settings. To learn about optional   installation settings,\nsee  Operator kubectl and oc Installation Settings . Invoke the following   command: You can install the   with  Helm 3 . Invoke the following  helm  command: To learn about optional   installation settings, see\n Operator Helm Installation Settings . To install the   on a host not connected to the Internet: You can install the   with  Helm 3 . Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the version of the MongoDB Enterprise Database\nimage that you want to use. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the version of the MongoDB Enterprise Database\nimage that you want to use. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the version of the MongoDB Enterprise Database\nimage that you want to use. Invoke the following  helm  command: To learn about optional   installation settings, see\n Operator Helm Installation Settings . The following examples assume that you created a  \nusing the default   namespace of  mongodb .\nIf you specified a different label for your namespace when\nyou  created it , change all\nvalues for  metadata.namespace  to that namespace. To change the label for the namespace for the following\ndeployment to  production , edit all values for\n metadata.namespace  in  mongodb-enterprise-openshift.yaml : Invoke the following   command: If you run OpenShift 3.11 or earlier, you must first manually edit the   to remove subresources. In each  , remove the\nfollowing option: You must add your  <openshift-pull-secret>  to the\n ServiceAccount  definitions: You might need to add one or more optional settings. To learn about optional   installation settings,\nsee  Operator kubectl and oc Installation Settings . Invoke the following   command: You can install the   with  Helm 3 . Add the name of your  <openshift-pull-secret>  to the\n registry.imagePullSecrets  setting in the\n helm_chart/values-openshift.yaml  file: Invoke the following  helm  command: To learn about optional   installation settings, see\n Operator Helm Installation Settings . If you run OpenShift 3.11 or earlier, you must first manually edit the   to remove subresources. In each  , remove the\nfollowing option: To install the   on a host not connected to the Internet: You can install the   with  Helm 3 . Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the version of the MongoDB Enterprise Database\nimage that you want to use. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the version of the MongoDB Enterprise Database\nimage that you want to use. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the version of the MongoDB Enterprise Database\nimage that you want to use. Add the name of your  <openshift-pull-secret>  to the\n registry.imagePullSecrets  setting in the\n helm_chart/values-openshift.yaml  file: Invoke the following  helm  command: To learn about optional   installation settings, see\n Operator Helm Installation Settings . If you run OpenShift 3.11 or earlier, you must first manually edit the   to remove subresources. In each  , remove the\nfollowing option: To verify that the   installed correctly, run the\nfollowing command and verify the output: By default, deployments exist in the  mongodb  namespace. If the\nfollowing error message appears, ensure you use the correct\nnamespace: To troubleshoot your  , see\n Review Logs from the  . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . MongoDB rebuilds   images every day to integrate the\nlatest security and OS updates. By default,  helm  installs the latest build for the version of\nthe   you specify. To install an earlier build, specify the build ID as a parameter with\n --set build=<build-id> . Build IDs are always in the format\n -b<YYYYMMDD>T000000Z , where  <YYYYMMDD>  is the date that the\nbuild you want to use was created. This example shows how to install the   with the latest\nimage: This example shows how to install the   with the image\ncreated at midnight on February 5th, 2021: MongoDB recommends using the default (latest) build. After installing the  , you can: Create an instance of Ops Manager Configure the Kubernetes Operator to deploy MongoDB resources",
            "code": [
                {
                    "lang": "yaml",
                    "value": "##---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-operator\n  namespace: production\n##---\n# Source: mongodb-enterprise-operator/templates/operator.yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-enterprise-operator\n  namespace: production\n\n---\n# Example truncated\n---\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f crds.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f mongodb-enterprise.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --values helm_chart/values.yaml"
                },
                {
                    "lang": "sh",
                    "value": "docker pull quay.io/mongodb/mongodb-enterprise-operator:<op-version>; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-database:<db-version>; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-ops-manager:<om-version>; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-appdb:10.2.15.5958-1_4.2.11-ent; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-init-ops-manager:1.0.3; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-init-appdb:1.0.6; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-init-database:1.0.2;"
                },
                {
                    "lang": "sh",
                    "value": "docker save quay.io/mongodb/mongodb-enterprise-operator:<op-version> -o mongodb-enterprise-operator.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-database:<db-version> -o mongodb-enterprise-database.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-ops-manager:<om-version> -o mongodb-enterprise-ops-manager.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-appdb:10.2.15.5958-1_4.2.11-ent -o mongodb-enterprise-appdb.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-init-ops-manager:1.0.3 -o mongodb-enterprise-init-ops-manager.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-init-appdb:1.0.6 -o mongodb-enterprise-init-appdb.tar;\ndocker save quay.io/mongodb/mongodb-enterprise-init-database:1.0.2 -o mongodb-enterprise-init-database.tar;"
                },
                {
                    "lang": "sh",
                    "value": "docker load -i mongodb-enterprise-operator.tar; \\\ndocker load -i mongodb-enterprise-database.tar; \\\ndocker load -i mongodb-enterprise-ops-manager.tar; \\\ndocker load -i mongodb-enterprise-appdb.tar; \\\ndocker load -i mongodb-enterprise-init-ops-manager.tar; \\\ndocker load -i mongodb-enterprise-init-appdb.tar; \\\ndocker load -i mongodb-enterprise-init-database.tar;"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --values helm_chart/values.yaml \\\n     --set registry.pullPolicy=IfNotPresent"
                },
                {
                    "lang": "yaml",
                    "value": "##---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n  namespace: production\n##---\n# Source: mongodb-enterprise-operator/templates/operator.yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: enterprise-operator\n  namespace: production\n\n---\n# Example truncated\n---\n..."
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  subresources:\n    status: {}\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-appdb\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-database-pods\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f mongodb-enterprise-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "registry:\n# The pull secret must be specified\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --values helm_chart/values-openshift.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  subresources:\n    status: {}\n"
                },
                {
                    "lang": "sh",
                    "value": "docker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-operator:<op-version>; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-database:<db-version>; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager:<om-version>; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb:10.2.15.5958-1_4.2.11-ent; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-init-ops-manager:1.0.3; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-init-appdb:1.0.6; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-init-database:1.0.2;"
                },
                {
                    "lang": "sh",
                    "value": "docker save registry.connect.redhat.com/mongodb/mongodb-enterprise-operator:<op-version> -o mongodb-enterprise-operator.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-database:<db-version> -o mongodb-enterprise-database.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager:<om-version> -o mongodb-enterprise-ops-manager.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb:10.2.15.5958-1_4.2.11-ent -o mongodb-enterprise-appdb.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-init-ops-manager:1.0.3 -o mongodb-enterprise-init-ops-manager.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-init-appdb:1.0.6 -o mongodb-enterprise-init-appdb.tar;\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-init-database:1.0.2 -o mongodb-enterprise-init-database.tar;"
                },
                {
                    "lang": "sh",
                    "value": "docker load -i mongodb-enterprise-operator.tar; \\\ndocker load -i mongodb-enterprise-database.tar; \\\ndocker load -i mongodb-enterprise-ops-manager.tar; \\\ndocker load -i mongodb-enterprise-appdb.tar; \\\ndocker load -i mongodb-enterprise-init-ops-manager.tar; \\\ndocker load -i mongodb-enterprise-init-appdb.tar; \\\ndocker load -i mongodb-enterprise-init-database.tar;"
                },
                {
                    "lang": "sh",
                    "value": "registry:\n# The pull secret must be specified\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --values helm_chart/values-openshift.yaml \\\n     --set registry.pullPolicy=IfNotPresent \\\n     --set registry.imagePullSecrets=<openshift-pull-secret>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  subresources:\n    status: {}\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe deployments mongodb-enterprise-operator -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Error from server (NotFound): deployments.apps \"mongodb-enterprise-operator\" not found"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --values helm_chart/values.yaml \\"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --values helm_chart/values.yaml \\\n     --set build=-b20210205T000000Z"
                }
            ],
            "preview": "Before you install the , make sure you\nplan for your installation:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-om-container-remote-mode",
            "title": "Configure an  Resource to use Remote Mode",
            "headings": [
                "Prerequisites and Considerations",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Create a  for Nginx.",
                "Deploy Nginx to your  cluster.",
                "Create a  service to make Nginx accessible from other pods in your cluster.",
                "Copy and update the highlighted fields of this  resource.",
                "Paste the copied example section into your existing  resource.",
                "Save your  config file.",
                "Apply changes to your  deployment.",
                "Track the status of your  instance.",
                "Deploy a MongoDB Database Resource."
            ],
            "paragraphs": "In a default configuration, the MongoDB Agents and Backup Daemons\naccess MongoDB installation archives over the Internet from  You can configure   to run in  Remote Mode  with the\n  if the nodes in your   cluster don't have access to\nthe Internet. The Backup Daemons and managed MongoDB resources download\ninstallation archives only from  , which proxies download\nrequests to an HTTP endpoint on a local web server or S3-compatible\nstore deployed to your   cluster. This procedure covers deploying an Nginx HTTP server to your  \ncluster to host the MongoDB installation archives. Deploy an   Resource . The following procedure shows you how to\nupdate your       to enable Remote Mode. You can enable Remote Mode only on   4.4 or later. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : The ConfigMap in this tutorial configures Nginx to: Run an HTTP server named  localhost  listening on port  80  on a\nnode in your   cluster, and Route HTTP requests for specific resources to locations that serve\nthe the MongoDB Server and MongoDB Database Tools installation\narchives. Paste the following example Nginx ConfigMap into a text editor: Save this file with a  .yaml  file extension. Create the Nginx ConfigMap by invoking the following\n kubectl  command on the ConfigMap file you created: The Nginx resource configuration in this tutorial: Deploys one Nginx replica, Creates volume mounts to store MongoDB Server and MongoDB Database\nTools installation archives, and Defines  init containers  that use  curl \ncommands to download the installation archives that Nginx serves to\nMongoDB Database resources you deploy in your   cluster. Paste the following example Nginx resource configuration\ninto a text editor: Modify the lines highlighted in the example to specify the\nMongoDB Server versions that you want to install. For example, to replace MongoDB version  4.0.2  with\na different database version, update the following block: Update this block to modify the MongoDB Database Tools\nversion: to the appropriate initContainer for each version you want\nNginx to serve. For example, to configure Nginx to serve MongoDB  4.2.0 \nand  4.4.0 : Save this file with a  .yaml  file extension. Deploy Nginx by invoking the following  kubectl \ncommand on the Nginx resource file you created: Paste the following example Nginx resource configuration\ninto a text editor: Modify the lines highlighted in the example to specify the\nMongoDB Server versions that you want to install. For example, to replace MongoDB version  4.0.2  with\na different database version, update the following block: Update this block to modify the MongoDB Database Tools\nversion: To load multiple versions, append  curl  commands\nto the appropriate initContainer for each version you want\nNginx to serve. For example, to configure Nginx to serve MongoDB  4.2.0 \nand  4.4.0 : Save this file with a  .yaml  file extension. Deploy Nginx by invoking the following  oc \ncommand on the Nginx resource file you created: The service in this tutorial exposes Nginx to traffic from other nodes\nin your   cluster over port  80 . This allows the MongoDB\nDatabase resource pods you deploy using the   to download\nthe installation archives from Nginx. Run the following command to create a service your Nginx deployment: Paste the following example service into a text editor: Save this file with a  .yaml  file extension. Create the service by invoking the following\n kubectl  command on the service file you created: The highlighted section uses the following   configuration\nsettings: automation.versions.source: remote  in\n spec.configuration  to enable Remote Mode. automation.versions.download.baseUrl  in\n spec.configuration  to provide the base URL of the\nHTTP resources that serve the MongoDB installation archives. Update this line to replace  <namespace>  with the namespace to\nwhich you deploy resources with the  . automation.versions.download.baseUrl.allowOnlyAvailableBuilds:\n\"false\"  in  spec.configuration  to help ensure\nenterprise builds have no issues. You must set this parameter only if you use a version of\n  before 4.4.11. Open your preferred text editor and paste the  \nspecification into the appropriate location in your resource file. Invoke the following  kubectl  command on the filename of the\n  resource  definition: To check the status of your   resource, invoke the following\ncommand: See  Troubleshoot the   for information about the\nresource deployment statuses. After the   resource completes the  Reconciling  phase, the\ncommand returns output similar to the following: Copy the value of the  status.opsManager.url  field, which states\nthe resource's connection  . You use this value when you create a\n  later in the procedure. MongoDB Agents running in MongoDB database resource containers that\nyou create with the   download the installation archives\nfrom   via Nginx instead of from the Internet. If you have not done so already, complete the following\nprerequisites: Create Credentials for the  Create One Project using a ConfigMap Deploy a  MongoDB Database resource \nin the same namespace to which you deployed  .\nEnsure that you: Match the  spec.opsManager.configMapRef.name  of the resource\nto the  metadata.name  of your ConfigMap. Match the  spec.credentials  of the resource to the name of\nthe secret you created that contains an   programmatic\nAPI key pair.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-conf\ndata:\n  nginx.conf: |\n    events {}\n    http {\n      server {\n        server_name localhost;\n        listen 80;\n        location /linux/ {\n          alias /mongodb-ops-manager/mongodb-releases/linux/;\n        }\n        location /tools/ {\n          alias /tools/;\n        }\n      }\n    }\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <nginix-configmap>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - image: nginx:1.14.2\n          imagePullPolicy: IfNotPresent\n          name: nginx\n          ports:\n            - containerPort: 80\n          volumeMounts:\n            - mountPath: /mongodb-ops-manager/mongodb-releases/linux\n              name: mongodb-versions\n            - mountPath: /tools/db/\n              name: mongodb-tools\n            - name: nginx-conf\n              mountPath: /etc/nginx/nginx.conf\n              subPath: nginx.conf\n      initContainers:\n        - name: setting-up-ubuntu-mongodb\n          image: curlimages/curl:latest\n          command:\n            - curl\n            - -L\n            - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-ubuntu1604-4.2.0.tgz\n            - -o\n            - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-ubuntu1604-4.2.0.tgz\n          volumeMounts:\n            - name: mongodb-versions\n              mountPath: /mongodb-ops-manager/mongodb-releases/linux\n        - name: setting-up-ubuntu-mongodb-tools\n          image: curlimages/curl:latest\n          command:\n            - curl\n            - -L\n            - https://fastdl.mongodb.org/tools/db/mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz\n            - -o\n            - /tools/db/mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz\n          volumeMounts:\n            - name: mongodb-tools\n              mountPath: /tools/db/\n      restartPolicy: Always\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n        - name: mongodb-versions\n          emptyDir: {}\n        - name: mongodb-tools\n          emptyDir: {}\n        - configMap:\n            name: nginx-conf\n          name: nginx-conf\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "- name: setting-up-ubuntu-mongodb\n  image: curlimages/curl:latest\n  command:\n    - curl\n    - -L\n    - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-ubuntu1604-4.2.0.tgz\n    - -o\n    - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-ubuntu1604-4.2.0.tgz\n    - &&\n    - curl\n    - -L\n    - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-ubuntu1604-4.4.0.tgz\n    - -o\n    - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-ubuntu1604-4.4.0.tgz"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <nginix>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - image: nginx:1.14.2\n          imagePullPolicy: IfNotPresent\n          name: nginx\n          ports:\n            - containerPort: 80\n          volumeMounts:\n            - mountPath: /mongodb-ops-manager/mongodb-releases/linux\n              name: mongodb-versions\n            - mountPath: /tools/db/\n              name: mongodb-tools\n            - name: nginx-conf\n              mountPath: /etc/nginx/nginx.conf\n              subPath: nginx.conf\n      initContainers:\n        - name: setting-up-rhel-mongodb\n          image: curlimages/curl:latest\n          command:\n            - curl\n            - -L\n            - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel<version>-4.2.0.tgz\n            - -o\n            - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel<version>-4.2.0.tgz\n          volumeMounts:\n            - name: mongodb-versions\n              mountPath: /mongodb-ops-manager/mongodb-releases/linux\n        - name: setting-up-rhel-mongodb-tools\n          image: curlimages/curl:latest\n          command:\n            - curl\n            - -L\n            - https://fastdl.mongodb.org/tools/db/mongodb-database-tools-rhel<version>-x86_64-100.1.0.tgz\n            - -o\n            - /tools/db/mongodb-database-tools-rhel<version>-x86_64-100.1.0.tgz\n          volumeMounts:\n            - name: mongodb-tools\n              mountPath: /tools/db/\n      restartPolicy: Always\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n        - name: mongodb-versions\n          emptyDir: {}\n        - name: mongodb-tools\n          emptyDir: {}\n        - configMap:\n            name: nginx-conf\n          name: nginx-conf\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "initContainers:\n  - name: setting-up-rhel-mongodb\n    image: curlimages/curl:latest\n    command:\n      - curl\n      - -L\n      - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel<version>-4.2.0.tgz\n      - -o\n      - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel<version>-4.2.0.tgz\n      - &&\n      - curl\n      - -L\n      - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel<version>-4.4.0.tgz\n      - -o\n      - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel<version>-4.4.0.tgz"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f <nginix>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: Service\nmetadata:\n name: nginx-svc\n labels:\n   app: nginx\nspec:\n ports:\n - port: 80\n   protocol: TCP\n selector:\n   app: nginx\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <nginix-service>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: ops-manager-localmode\nspec:\n replicas: 1\n version: \"4.4.0-rc1\"\n adminCredentials: ops-manager-admin-secret\n configuration:\n   # this enables local mode in Ops Manager\n   automation.versions.source: remote\n   automation.versions.download.baseUrl: \"http://nginx-svc.<namespace>.svc.cluster.local:80\"\n   # set the following only if you use a version before Ops Manager 4.4.11\n   automation.versions.download.baseUrl.allowOnlyAvailableBuilds: \"false\"\n\n backup:\n   enabled: false\n\n applicationDatabase:\n   members: 3\n   persistent: true\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2020-05-15T16:20:22Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: \"4.2.11-ent\"\n  backup:\n    phase: \"\"\n  opsManager:\n    lastTransition: \"2020-05-15T16:20:26Z\"\n    phase: Running\n    replicas: 1\n    url: http://ops-manager-localmode-svc.mongodb.svc.cluster.local:8080\n    version: \"4.2.12\"\n"
                }
            ],
            "preview": "In a default configuration, the MongoDB Agents and Backup Daemons\naccess MongoDB installation archives over the Internet from ",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-om-with-tls",
            "title": "Secure Application Database using TLS",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Verify your new  certificates.",
                "Create a Secret with your new  certificates.",
                "Optional: Create a ConfigMap containing the Certificate Authority.",
                "Specify the Secret with certs to the  yaml definition.",
                "Apply changes to your  deployment.",
                "Track the status of your  instance."
            ],
            "paragraphs": "The   can use   certificates to encrypt connections\nbetween members of the application database replica set. Before you secure your application database using   encryption,\ncomplete the following: Install the Kubernetes Operator . Deploy the Ops Manager application  that\nyou want to secure. Create a   certificate for each member of the Application\nDatabase's  replica set . These   certificates require two attributes: DNS Names Each certificate must include a   or Subject Name\nwith the name of the   in  . These names must\nresemble this format: Key Usages MongoDB requires the   certs to include two specific\nkey-usages ( 5280 ): \"server auth\" \"client auth\" If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Verify that each member of the Replica Set has one   certificate named with the following format: <resource-name>-db-<index>-pem Where  <index>  is a 0-based index number to the total amount of\nmembers minus one. ( 0  to  n-1 ) Create a new Secret from these files: kubectl  creates one Secret containing the three certificates. You must provide a   certificate when the   that\nsigned the certificates might be not \"recognized\" as an official\nauthority. Recognized and valid certificates can be created with\n cert-manager  or  HashiCorp Vault . If you signed the certificates using a   certificate management\ntool like  cert-manager  or\n HashiCorp Vault , you must create a\n  containing the  's certificate file. If you output the certificate as a file, name this file  ca-pem .\nThis simplifies creating the  . This creates a   named  ca . This\n  contains one entry called  ca-pem  with the\ncontents of the   file and the certificate chain for\n downloads.mongodb.com . You must concatenate your custom   file and the entire\n  certificate chain from  downloads.mongodb.com  to prevent\n  from becoming inoperable if the application database\nrestarts. Obtain the entire   certificate chain from\n downloads.mongodb.com . The following  openssl  command\noutputs each certificate in the chain to your current working\ndirectory, in  .crt  format: Concatenate your  's certificate file with the\nentire   certificate chain from  downloads.mongodb.com  that\nyou obtained in the previous step: Create the  : The   mounts the   you add using the\n spec.applicationDatabase.security.tls.ca  setting to\nboth the   and the Application Database pods. Invoke the following  kubectl  command on the filename of the\n  resource definition: To check the status of your   resource, invoke the following\ncommand: When   is running, the command returns the following\noutput under the  status  field: See  Troubleshoot the   for information about the\nresource deployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "<opsmgr-name>-db-<index>.<opsmgr-name>-db-svc.<namespace>.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic appdb-cert \\\n        --from-file=om-appdb-tls-enabled-db-0-pem \\\n        --from-file=om-appdb-tls-enabled-db-1-pem \\\n        --from-file=om-appdb-tls-enabled-db-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "openssl s_client -showcerts -verify 2 \\\n-connect downloads.mongodb.com:443 -servername downloads.mongodb.com < /dev/null \\\n| awk '/BEGIN/,/END/{ if(/BEGIN/){a++}; out=\"cert\"a\".crt\"; print >out}'"
                },
                {
                    "lang": "sh",
                    "value": "cat cert1.crt cert2.crt cert3.crt cert4.crt  >> ca-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap ca --from-file=\"ca-pem\""
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: om-appdb-tls-enabled\nspec:\n  replicas: 1\n  version: \"4.2.6\"\n  adminCredentials: ops-manager-admin-secret\n  configuration:\n    mms.fromEmailAddr: admin@example.com\n    mms.security.allowCORS: \"false\"\n  applicationDatabase:\n    members: 3\n    version: \"4.2.11-ent\"\n    persistent: true\n    security:\n      tls:\n        ca: \"appdb-ca\" # Optional. Name of the ConfigMap file\n                       # containing the certicate authority that\n                       # signs the certificates that the application\n                       # database uses.\n        secretRef:\n          name: \"appdb-certs\" # Name of the Secret object\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2019-12-06T17:46:15Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: \"4.2.11-ent\"\n  opsManager:\n    lastTransition: \"2019-12-06T17:46:32Z\"\n    phase: Running\n    replicas: 1\n    url: https://om-appdb-tls-enabled-svc.dev.svc.cluster.local:8443\n    version: \"4.2.6\""
                }
            ],
            "preview": "The  can use  certificates to encrypt connections\nbetween members of the application database replica set.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/modify-resource-image",
            "title": "Modify  or MongoDB Kubernetes Resource Containers",
            "headings": [
                "Define a Volume Mount for a MongoDB Kubernetes Resource",
                "Tune MongoDB Kubernetes Resource Docker Images with an InitContainer",
                "Build Custom Images with Dockerfile Templates",
                "MongoDB Dockerfile Templates",
                "Context Images",
                "docker build Example"
            ],
            "paragraphs": "You can modify the containers in the   in which   and\nMongoDB database resources run using the  template  or\n podTemplate  setting that applies to your deployment: To review which fields you can add to a  template  or a\n podTemplate , see the  Kubernetes documentation . When you create containers with a  template  or  podTemplate , the\n  handles container creation differently based on the\n name  you provide for each container in the  containers  array: MongoDB database:  spec.podSpec.podTemplate :  spec.statefulSet.spec.template backup-daemon :  spec.backup.statefulSet.spec.template If the  name  field  matches  the name of the applicable resource\nimage, the   updates the   or MongoDB database\ncontainer in the   to which the  template  or\n podTemplate  applies: :  mongodb-enterprise-ops-manager backup-daemon :  mongodb-backup-daemon MongoDB database:  mongodb-enterprise-database Application Database:  mongodb-enterprise-appdb If the  name  field  does not match  the name of the applicable\nresource image, the   creates a new container in each\n  to which the  template  or  podTemplate  applies. On-disk files in containers in   don't survive container\ncrashes or restarts. Using the  spec.podSpec.podTemplate \nsetting, you can add a  volume mount \nto persist data in a MongoDB database resource for the life of the\n . To create a volume mount for a MongoDB database resource: Update the MongoDB database resource definition to include a volume\nmount for containers in the database pods that the  \ncreates. Use  spec.podSpec.podTemplate  to define a volume mount: Apply the updated resource definition:  Docker images run on Ubuntu and use Ubuntu's default\nsystem configuration. To tune the underlying Ubuntu system\nconfiguration in the   containers, add a privileged\nInitContainer\n init container \nusing one of the following settings: To tune Docker images for a MongoDB database resource container:  adds a privileged InitContainer to each   that the\n  creates using the   definition. Open a shell session to a running container in your database resource\n  and verify your changes. spec.podSpec.podTemplate : add a privileged InitContainer\nto a MongoDB database resource container. spec.statefulSet.spec.template : add a privileged\nInitContainer to an   resource container. MongoDB database resource Docker images use the Ubuntu default\n keepalive  time of  7200 . MongoDB recommends a shorter\n keepalive  time of  120  for database deployments. You can tune the  keepalive  time in the database resource Docker\nimages if you experience network timeouts or socket errors in\ncommunication between clients and the database resources. Update the MongoDB database resource definition to append a\nprivileged InitContainer to the database pods that the\n  creates. Change  spec.podSpec.podTemplate  the  keepalive \nvalue to the recommended value of  120 : Apply the updated resource definition: To follow the previous  keepalive  example, invoke the following\ncommand to get the current  keepalive  value: You can modify MongoDB Dockerfile templates to create custom\n  images that suit your use case. To build a\ncustom image, you need: Your custom Dockerfile, modified from a MongoDB template. The MongoDB-provided context image for your template. The Dockerfiles used to build container images are publicly\navailable from the\n MongoDB Enterprise Kubernetes GitHub repository . The Dockerfile directory is organized by resource name, version and\ndistribution: Copy the template you want to use to your own Dockerfile and modify as\ndesired. To build an image from any MongoDB Dockerfile template, you must supply\nits context image. Each Dockerfile template has one associated context image, retrievable\nfrom the same  Quay.io  registry as the original\nimages. Context image are always tagged in the format\n quay.io/mongodb/<resource-name>:<image-version>-context . To supply a context image to  docker build , include the\n --build-arg  option with the  imagebase  variable set to a\nQuay.io tag, where  <resource-name>  and  <image-version>  match\nyour Dockerfile template. If you want to build the  mongodb-enterprise-database  version\n2.0.0 image for any distribution, include: The Ubuntu distribution for  mongodb-enterprise-operator  version\n1.9.1 is based on  ubuntu:1604  by default. In this example, that\nbase Dockerfile template is modified to use  ubuntu:1804  and\nsaved as  myDockerfile . The following command builds the custom image and gives it the tag\n 1.9.1-ubuntu-1804 : Include a hyphen ( - ) at the end of  docker build  to read\nthe output of  cat myDockerfile  instead of providing a\nlocal directory as build context. To learn more about  docker build , see the\n Docker documentation .",
            "code": [
                {
                    "lang": "yaml",
                    "value": "podSpec:\n  podTemplate:\n    spec:\n      containers:\n      - name: mongodb-enterprise-database\n        volumeMounts:\n        - mountPath: </new/mount/path>\n          name: survives-restart\n      volumes:\n      - name: survives-restart\n        emptyDir: {}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-resource-conf>.yaml -n <namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  podSpec:\n    podTemplate:\n      spec:\n        initContainers:\n        - name: \"adjust-tcp-keepalive\"\n          image: \"busybox:latest\"\n          securityContext:\n            privileged: true\n          command: [\"sysctl\", \"-w\", \"net.ipv4.tcp_keepalive_time=120\"]"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-resource-conf>.yaml -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "> kubectl exec -n <namespace> -it <pod-name> -- cat /proc/sys/net/ipv4/tcp_keepalive_time\n\n\n> 120"
                },
                {
                    "lang": "sh",
                    "value": "\u251c\u2500\u2500 <resource name>\n\u2502   \u2514\u2500\u2500 <image version>\n\u2502       \u2514\u2500\u2500 <base distribution>\n\u2502           \u2514\u2500\u2500 Dockerfile template"
                },
                {
                    "lang": "sh",
                    "value": "--build-arg imagebase=quay.io/mongodb/mongodb-enterprise-database:2.0.0-context"
                },
                {
                    "lang": "sh",
                    "value": "cat myDockerfile | docker build --build-arg imagebase=quay.io/mongodb/mongodb-enterprise-operator:1.9.1-context \\\n--tag mongodb-enterprise-operator:1.9.1-ubuntu-1804 -"
                }
            ],
            "preview": "You can modify the containers in the  in which  and\nMongoDB database resources run using the template or\npodTemplate setting that applies to your deployment:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/connect-from-outside-k8s",
            "title": "Connect to a MongoDB Database Resource from Outside Kubernetes",
            "headings": [
                "Prerequisite",
                "Compatible MongoDB Versions",
                "Procedure",
                "Open your standalone resource  file.",
                "Copy the highlighted section of this standalone resource.",
                "Paste the copied example section into your existing standalone resource.",
                "Change the highlighted settings to your preferred values.",
                "Save your standalone config file.",
                "Update and restart your standalone deployment.",
                "Discover the dynamically assigned NodePorts.",
                "Test the connection to the standalone.",
                "Deploy a replica set with the .",
                "Add Subject Alternate Names to your  certificates.",
                "Create a NodePort for each .",
                "Discover the dynamically assigned NodePorts.",
                "Open your replica set resource  file.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Change the highlighted settings to your preferred values.",
                "Confirm the external hostnames and NodePort values in your replica set resource.",
                "Save your replica set config file.",
                "Update and restart your replica set deployment.",
                "Test the connection to the replica set.",
                "Deploy a replica set with the .",
                "Configure services to ensure connectivity.",
                "Configure routes to ensure  terminination passthrough.",
                "Optional: Add Subject Alternate Names to your  certificates.",
                "Open your replica set resource  file.",
                "Configure your replica set resource  file.",
                "Change the settings to your preferred values.",
                "Save your replica set config file.",
                "Optional: Create the necessary  certificates and  secrets.",
                "Optional: Approve  requests.",
                "Update and restart your replica set deployment.",
                "Test the connection to the replica set.",
                "Open your sharded cluster resource  file.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Change the highlighted settings to your preferred values.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Discover the dynamically assigned NodePorts.",
                "Test the connection to the sharded cluster."
            ],
            "paragraphs": "The following procedure describes how to connect to a MongoDB resource\ndeployed by   from outside of the   cluster, including how to\ncontrol port mapping. For your databases to be accessed outside of  , they must run\nMongoDB 4.2.3 or later. How you connect to a MongoDB resource that the   deployed\nfrom outside of the   cluster depends on the resource. This procedure uses the following example: To connect to your  -deployed MongoDB\nstandalone resource from outside of the   cluster: Change the highlighted settings of this   file to match your\ndesired  standalone  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. Key Type Necessity Description Example spec.exposedExternally Boolean Optional Set this value to  true  to allow external services to connect\nto the MongoDB deployment. This results in   creating a\n NodePort service . true Invoke the following   command to update and restart your\n standalone : Discover the dynamically assigned NodePort: The list output should contain an entry similar to the following:  exposes   on port  27017  within the  \ncontainer. The NodePort service exposes the  mongod  via port  30994 .\nNodePorts range from 30000 to 32767, inclusive. To connect to your deployment from outside of the   cluster, run\nthe   command with the external   of a   as the\n --host  flag. If a node in the   cluster has an external   of\n ec2-54-212-23-143.us-west-2.compute.amazonaws.com , you can\nconnect to this standalone instance from outside of the  \ncluster using the following command: To obtain the external  DNS (Domain Name System)  of your\nKubernetes cluster, you can run the following command: This command displays the external DNS in the\n Addresses.ExternalDNS  section of the output. Alternatively, you can output the external DNS directly by running: To connect to your  -deployed MongoDB replica\nset resource from outside of the   cluster: This procedure explains the least complicated way to\nenable external connectivity. Other utilities can be\nused in production. If you haven't deployed a replica set, follow the instructions to\n deploy one . You must enable   for the replica set with the\n spec.security.tls.enabled  setting. The replica set must use\na custom certificate stored with  spec.security.tls.ca . Add each external   name to the certificate  . Invoke the following commands to create the NodePorts: Discover the dynamically assigned NodePorts: NodePorts range from 30000 to 32767, inclusive. Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. Key Type Necessity Description Example boolean Optional Set this value to  true  to enable   on the MongoDB\ndeployment. By default,   requires hosts to use and accept\n  encrypted connections. To connect to a replica set from outside  , set this\nvalue to  true . true collection Conditional Add this parameter and values if you need your database to be\naccessed outside of  . This setting allows you to provide\ndifferent   settings within the   cluster and to the\n  cluster. The   uses split horizon   for\nreplica set members. This feature allows communication both\nwithin the   cluster and from outside  . You may add multiple external mappings per host. Make sure that each value in this array is unique. Make sure that the number of entries in this array matches\nthe value given in  spec.members . Set the  spec.security.tls.enabled  to  true  to\nenable  . This method to use split horizons requires\nthe Server Name Indication extension of the   protocol. See Setting Confirm that the external hostnames in the\n spec.connectivity.replicaSetHorizons  setting are correct. External hostnames should match the   names of   worker nodes.\nThese can be  any  nodes in the   cluster.   do internal\nrouting if the pod is run on another node. Set the ports in  spec.connectivity.replicaSetHorizons  to\nthe NodePort values that you discovered. Invoke the following   command to update and restart your\n replica set : If the connection succeeds, you should see: Don't use the  --sslAllowInvalidCertificates  flag in production.\nIn production, share the     files with client tools\nor applications. To connect to your  -deployed MongoDB replica\nset resource from outside of the   cluster with OpenShift: If you haven't deployed a replica set, follow the instructions to\n deploy one . You must enable   for the replica set with the\n spec.security.tls.enabled  setting. The replica set must use\na custom certificate stored with  spec.security.tls.ca . Paste the following example services into a text editor: If the  spec.selector  has entries that target headless\nservices or applications, OpenShift may create a software\nfirewall rule explicitly dropping connectivity.  Review the\nselectors carefully and consider targeting the stateful set pod\nmembers directly as seen in the example.  Routes in OpenShift\noffer port 80 or port 443. This example service uses\nport 443. Change the settings to your preferred values. Save this file with a  .yaml  file extension. To create the services, invoke the following  kubectl  command\non the services file you created: Paste the following example routes into a text editor: To ensure the     negotiation with   necessary\nfor   to respond with the correct horizon replica set\ntopology for the drivers to use, you must set  \ntermination passthrough. Change the settings to your preferred values. Save this file with a  .yaml  file extension. To create the routes, invoke the following  kubectl  command on\nthe routes file you created: Add each external   name to the certificate  . Use the following example to edit your replica set resource  \nfile: OpenShift clusters require localhost horizons if you intend to use\nthe   to create each  . If you manually create\nyour   certificates, ensure you include localhost in\nthe   list. Key Type Necessity Description Example boolean Optional Set this value to  true  to enable   on the MongoDB\ndeployment. By default,   requires hosts to use and accept\n  encrypted connections. To connect to a replica set from outside  , set this\nvalue to  true . true collection Conditional Add this parameter and values if you need your database to be\naccessed outside of  . This setting allows you to provide\ndifferent   settings within the   cluster and to the\n  cluster. The   uses split horizon   for\nreplica set members. This feature allows communication both\nwithin the   cluster and from outside  . You may add multiple external mappings per host. Make sure that each value in this array is unique. Make sure that the number of entries in this array matches\nthe value given in  spec.members . Set the  spec.security.tls.enabled  to  true  to\nenable  . This method to use split horizons requires\nthe Server Name Indication extension of the   protocol. See Setting Configure TLS for your replica set . Create one secret for the MongoDB replica set\nand one for the certificate authority. The   uses these\nsecrets to place the   files in the pods for MongoDB to use. If you do not manually create the   certificates for the\ndeployment, check for pending   approval requests: When the requests come through, approve them: Invoke the following   command to update and restart your\n replica set : The   should deploy the MongoDB replica set,\nconfigured with the horizon routes created for ingress. After\nthe   completes the deployment, you may connect with the\nhorizon using   connectivity.  If the certificate authority is\nnot present on your workstation, you can view and copy it from a\nMongoDB pod using the following command: To test the connections, run the following command: If the connection succeeds, you should see: In the following example, use your replica set names and replace  {redacted}  with the domain that you manage. Don't use the  --tlsAllowInvalidCertificates  flag in production.\nIn production, share the     files with client tools\nor applications. For this procedure, you must deploy a  -enabled sharded MongoDB\ncluster in the  .\nProvide the external   names ( s) for each member of\nthe MongoDB sharded cluster. The   for each MongoDB hosts corresponds to: To connect to your  -deployed MongoDB sharded\ncluster resource from outside of the   cluster: Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. Key Type Necessity Description Example spec.exposedExternally Boolean Optional Set this value to  true  to allow external services to connect\nto the MongoDB deployment. This results in   creating a\n NodePort service . true boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true collection Optional List of every domain that should be added to   certificates\nto each pod in this deployment. When you set this parameter,\nevery   that the   transforms into a  \ncertificate includes a   in the form  <pod\nname>.<additional cert domain> . true Invoke the following   command to update and restart your\n sharded cluster : Discover the dynamically assigned NodePort: The list output should contain an entry similar to the following:  exposes   on port  27017  within the  \ncontainer. The NodePort service exposes the  mongod  via port  30078 .\nNodePorts range from 30000 to 32767, inclusive. To connect to your deployment from outside of the   cluster, run\nthe   command with the external   of a   as the\n --host  flag. If a node in the   cluster has an external   of\n ec2-54-212-23-143.us-west-2.compute.amazonaws.com , you can\nconnect to this sharded cluster instance from outside of the  \ncluster using the following command: To obtain the external  DNS (Domain Name System)  of your\nKubernetes cluster, you can run the following command: This command displays the external DNS in the\n Addresses.ExternalDNS  section of the output. Alternatively, you can output the external DNS directly by running:",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\nspec:\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true\n  exposedExternally: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\nspec:\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  exposedExternally: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <standalone-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get services -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\n\n<my-standalone>           NodePort    10.102.27.116   <none>        27017:30994/TCP   8m30s"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host ec2-54-212-23-143.us-west-2.compute.amazonaws.com --port 30994"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe nodes"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get nodes -o jsonpath='{ $.items[*].status.addresses[?(@.type==\"ExternalDNS\")].address }'"
                },
                {
                    "lang": "sh",
                    "value": "kubectl expose pod/<my-replica-set>-0 --type=\"NodePort\" --port 27017\nkubectl expose pod/<my-replica-set>-1 --type=\"NodePort\" --port 27017\nkubectl expose pod/<my-replica-set>-2 --type=\"NodePort\" --port 27017"
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl get svc | grep <my-replica-set>\n<my-replica-set>-0     NodePort   172.30.39.228   <none>  27017:30907/TCP  16m\n<my-replica-set>-1     NodePort   172.30.185.136  <none>  27017:32350/TCP  16m\n<my-replica-set>-2     NodePort   172.30.84.192   <none>  27017:31185/TCP  17m\n<my-replica-set>-svc   ClusterIP  None            <none>  27017/TCP        38m"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  type: ReplicaSet\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:30907\"\n      - \"example-website\": \"web2.example.com:32350\"\n      - \"example-website\": \"web3.example.com:31185\"\n..."
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:30907\"\n      - \"example-website\": \"web2.example.com:32350\"\n      - \"example-website\": \"web3.example.com:31185\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host <my-replica-set>/web1.example.com:30907,web2.example.com:32350,web3.example.com:31185 \\\n      --ssl \\\n      --sslAllowInvalidCertificates"
                },
                {
                    "lang": "javascript",
                    "value": "MongoDB Enterprise <my-replica-set>:PRIMARY"
                },
                {
                    "lang": "",
                    "value": "---\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-external-0\nspec:\n  ports:\n    - name: mongodb\n      protocol: TCP\n      port: 443\n      targetPort: 27017\n  selector:\n    statefulset.kubernetes.io/pod-name: my-external-0\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-external-1\nspec:\n  ports:\n    - name: mongodb\n      protocol: TCP\n      port: 443\n      targetPort: 27017\n  selector:\n    statefulset.kubernetes.io/pod-name: my-external-1\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-external-2\nspec:\n  ports:\n    - name: mongodb\n      protocol: TCP\n      port: 443\n      targetPort: 27017\n  selector:\n    statefulset.kubernetes.io/pod-name: my-external-2\n\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <my-external-services>.yaml"
                },
                {
                    "lang": "",
                    "value": "---\napiVersion: v1\nkind: Route\nmetadata:\n  name: my-external-0\nspec:\n  host: my-external-0.{redacted}\n  to:\n    kind: Service\n    name: my-external-0\n  tls:\n    termination: passthrough\n---\napiVersion: v1\nkind: Route\nmetadata:\n  name: my-external-1\nspec:\n  host: my-external-1.{redacted}\n  to:\n    kind: Service\n    name: my-external-1\n  tls:\n    termination: passthrough\n---\napiVersion: v1\nkind: Route\nmetadata:\n  name: my-external-2\nspec:\n  host: my-external-2.{redacted}\n  to:\n    kind: Service\n    name: my-external-2\n  tls:\n    termination: passthrough\n\n...\n "
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <my-external-routes>.yaml"
                },
                {
                    "lang": "",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-external\n  namespace: mongodb\nspec:\n  type: ReplicaSet\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: {redacted}\n  credentials: {redacted}\n  persistent: false\n  security:\n    tls:\n      # TLS must be enabled to allow external connectivity\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"SCRAM\",\"X509\"]\n  connectivity:\n    # The \"localhost\" routes are there just to make sure the localhost \n    # TLS SAN is created in the CSR, per OpenShift route requirements.\n    # \"ocroute\" is the configured route in openshift\n    replicaSetHorizons:\n      - \"ocroute\": \"my-external-0.{redacted}:443\"\n        \"localhost\": \"localhost:27017\"\n      - \"ocroute\": \"my-external-1.{redacted}:443\"\n        \"localhost\": \"localhost:27018\"\n      - \"ocroute\": \"my-external-2.{redacted}:443\"\n        \"localhost\": \"localhost:27019\"\n\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "oc get csr"
                },
                {
                    "lang": "sh",
                    "value": "oc adm certificate approve {certificate-0}.{namespace} ... {certificate-n}.{namespace}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "oc exec -it my-external-0 -- cat /mongodb-automation/ca.pem"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host my-external/my-external-0.{redacted}:443,my-external-1.{redacted}:443,my-external-2.{redacted}:443 \\\n      --tls \\\n      --tlsAllowInvalidCertificates"
                },
                {
                    "lang": "javascript",
                    "value": "MongoDB Enterprise <my-replica-set>:PRIMARY"
                },
                {
                    "lang": "sh",
                    "value": "<mdb-resource-name><shard><pod-index>.<external-domain>\n<mdb-resource-name><config><pod-index>.<external-domain>\n<mdb-resource-name><mongos><pod-index>.<external-domain>\n\nEach |tls| certificate requires the |fqdn| (|san-dns|) that\ncorresponds to the |fqdn| that this host has outside the\nsharded cluster deployed with the |k8s-op-short|.\n\n.. literalinclude:: /includes/code-examples/yaml-files/example-sharded-cluster.yaml\n   :language: yaml\n   :start-after: START-exposed-sharded-tls-full\n   :end-before: END-exposed-sharded-tls-full\n   :linenos:\n   :lineno-start: 1\n   :emphasize-lines: 19-24\n   :copyable: false"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  credentials: my-secret\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  exposedExternally: true\n  security:\n    tls:\n      enabled: true\n      additionalCertificateDomains:\n        - \"additional-cert-test.com\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get services -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\n\n<my-sharded cluster>      NodePort    10.106.44.30    <none>        27017:30078/TCP   10s"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host ec2-54-212-23-143.us-west-2.compute.amazonaws.com --port 30078"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe nodes"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get nodes -o jsonpath='{ $.items[*].status.addresses[?(@.type==\"ExternalDNS\")].address }'"
                }
            ],
            "preview": "The following procedure describes how to connect to a MongoDB resource\ndeployed by  from outside of the  cluster, including how to\ncontrol port mapping.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/upgrade-mdb-version",
            "title": "Upgrade MongoDB Version and FCV",
            "headings": [
                "Example"
            ],
            "paragraphs": "You can upgrade the major, minor, and/or feature compatibility versions\nof your MongoDB resource. These settings are configured in your\nresource's  . To upgrade your resource's major and/or minor versions, set the\n spec.version  setting to the desired MongoDB version. To modify your resource's\n feature compatibility version ,\nset the  spec.featureCompatibilityVersion  setting to the\ndesired version. If you update  spec.version  to a later version, consider setting\n spec.featureCompatibilityVersion  to the current working\nMongoDB version to give yourself the option to downgrade if\nnecessary. To learn more about feature compatibility, see\n setFeatureCompatibilityVersion  in the MongoDB Manual. Consider the following   for a standalone resource: This resource has a MongoDB version of \u200b. The\nfollowing steps upgrade the deployment's MongoDB version to\n 4.2.11-ent :  automatically reconfigures your deployment with the new\nspecifications. You can see these changes reflected in the   or\n Cloud Manager  application. Perform the following modifications to the resource's ConfigMap: Set  spec.version  to the desired MongoDB version. Set  spec.featureCompatibilityVersion  to the current\nworking MongoDB version: Setting  featureCompatibilityVersion  to  \"4.0\"  disables\n 4.2 features incompatible with MongoDB 4.0 . Reapply the configuration to  :",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone-downgrade\nspec:\n  version: \"4.0.14-ent\"\n  type: Standalone\n  project: my-project\n  credentials: my-credentials\n  persistent: false\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone-downgrade\nspec:\n  version: \"4.2.2-ent\"\n  featureCompatibilityVersion: \"4.0\"\n  type: Standalone\n  project: my-project\n  credentials: my-credentials\n  persistent: false\n..."
                },
                {
                    "lang": "none",
                    "value": "kubectl apply -f <standalone-config>.yaml"
                }
            ],
            "preview": "You can upgrade the major, minor, and/or feature compatibility versions\nof your MongoDB resource. These settings are configured in your\nresource's .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/create-x509-client-certs",
            "title": "Generate X.509 Client Certificates",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Generate a Private Key and Certificate Signing Request",
                "Create a new directory to complete this tutorial.",
                "Enter your newly created directory.",
                "Copy and save the following example JSON.",
                "Generate a key file.",
                "Generate the Certificate Signing Request.",
                "Submit the New CSR to the Kubernetes ",
                "Create a  in Kubernetes.",
                "View your CSRs.",
                "Approve the CSR.",
                "Verify that your certificate has been approved",
                "Obtain the Newly Issued Certificate from the Kubernetes CA",
                "Generate the X.509 certificate from the CSR (Certificate Signing Request).",
                "Concatenate the user private key and Kubernetes certificate.",
                "Connect to the X.509-Enabled MongoDB Deployment",
                "Configure kubectl to default to your namespace.",
                "Copy and save the following example .",
                "Create the X.509 MongoDB user.",
                "Verify your newly created user",
                "Use your X.509 user to connect to the MongoDB deployment"
            ],
            "paragraphs": "The   can deploy MongoDB instances with\n X.509 authentication \nenabled. If X.509 authentication has been enabled for the deployment,\nyou must generate and use an X.509 certificate to connect to the\ndeployment. This new client certificate must be signed by the\nKubernetes   to be accepted by the MongoDB deployment. Use the procedure outlined in this document to: Generate an X.509 certificate. Get that certificate signed by the Kubernetes\n CA (Certificate Authority) . Use the certificate to connect to your X.509-enabled MongoDB\ndeployment. A full description of Transport Layer Security (TLS), Public Key Infrastructure (PKI)\ncertificates, and Certificate Authorities is beyond the scope of this\ndocument. This page assumes prior knowledge of   and\nX.509 authentication. Your cluster must run Kubernetes 1.19 or later. You must use  kubectl  1.19 or later or  oc  4.6 or later. To complete this tutorial, you must have the  \ninstalled. For instructions on installing the  ,\nsee  Install the  . This tutorial assumes you have a MongoDB deployment which\nrequires X.509 authentication. For instructions on deploying\nMongoDB resources, see  Deploy a MongoDB Database Resource . This tutorial uses  CFSSL  to generate X.509 certificates.  CFSSL \nis a certificate generation tool built by\n Cloudflare . For instructions on\ninstalling  CFSSL , refer to the\n CFSSL GitHub page . If you have not done so already, add the  bin  directory in your Go\nworkspace to your  PATH  environment variable. For more information, see the  Go documentation . The user configuration files used in this tutorial are\nstrictly examples. You may need to adjust the values in the\nexamples to suit your deployment's needs. For more\ninformation on formatting user ConfigMaps,\nsee  Manage Database Users . Run the following command to create a new directory for\nthe configuration files used in this tutorial: In the  client-x509-certs-tutorial  directory, save the following\nJSON as   x509_user.json : Run the following command to pass the JSON from the previous step\nto  CFSSL  and generate a key file: You should see output similar to the following: You now have a file called  x509_user_key.json  containing\na new private key. Run the following command to use your  x509_user_key.json  key\nfile to generate a certificate signing request (CSR): This command generates two files: x509_user-key.pem , the private key for the user x509_user.csr , the CSR that represents the user Kubernetes' own certificate authority provides the trusted  \nfor the Kubernetes cluster. You need the  .csr  and  .pem  files\ngenerated in the previous section to request a new certificate from\nKubernetes. Run the following command to create a CSR\nin Kubernetes: Run the following command to view a list of CSRs: You should see an output similar to the following: The CSR remains in  Pending  condition\nuntil Kubernetes approves it. Run the following command to\napprove the certificate: You should see an output similar to the following: Run the following command to verify that the Kubernetes   has\napproved your certificate: You should see an output similar to the following: You can use the new certificate.\nThe  status.certificate  attribute of the CSR\ngenerated in the  previous section \ncontains the certificate. Run the following command to generate the certificate from the\nCSR object to a file called  client.crt : A   client can use the  client.crt \ncertificate to connect to the X.509-enabled MongoDB deployment. You need both the  x509_user-key.pem  and  client.crt  files\nto connect to the deployment. Run the following command to\nconcatenate the two files into the a new  .pem  file: With the client certificate created, you can create a MongoDB user\nand connect to the X.509-enabled deployment. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Save the following ConfigMap as  x509-mongodb-user.yaml : This ConfigMap  .yaml  file describes a  MongoDBUser  custom object. You\ncan use these custom objects to create MongoDB users. In this example, the ConfigMap describes the user as an X.509\nuser that the client can use to connect to MongoDB with the\ncorresponding X.509 certificate. Run the following command to apply the ConfigMap and create the\nX.509 MongoDB user: You should see an output similar to the following: Run the following command to check the state of the  new-x509-user : You should see an output similar to the following: Once you have created your X.509 user, try to connect to the\ndeployment using the MongoDB Shell ( mongosh ): On Kubernetes Pods, the   file is saved in\n /var/run/secrets/kubernetes.io/serviceaccount/ca.crt , which\nis the file location used for the  --sslCAFile  connection\noption.",
            "code": [
                {
                    "lang": "sh",
                    "value": "$ export PATH=$PATH:$(go env GOPATH)/bin"
                },
                {
                    "lang": "sh",
                    "value": "mkdir client-x509-certs-tutorial"
                },
                {
                    "lang": "sh",
                    "value": "cd client-x509-certs-tutorial"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"names\": [\n    {\"O\": \"organization\"},\n    {\"OU\": \"organizationalunit\"}\n  ],\n  \"CN\": \"my-x509-authenticated-user\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 4096\n  }\n}"
                },
                {
                    "lang": "sh",
                    "value": "cfssl genkey x509_user.json > x509_user_key.json"
                },
                {
                    "lang": "sh",
                    "value": "2019/06/04 18:12:38 [INFO] generate received request\n2019/06/04 18:12:38 [INFO] received CSR\n2019/06/04 18:12:38 [INFO] generating key: rsa-4096\n2019/06/04 18:12:40 [INFO] encoded CSR"
                },
                {
                    "lang": "sh",
                    "value": "cfssljson -f x509_user_key.json -bare x509_user"
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | kubectl apply -f -\napiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\n  name: x509-user.some-namespace\nspec:\n  groups:\n  - system:authenticated\n  request: $(cat x509_user.csr | base64 | tr -d '\\n')\n  usages:\n  - digital signature\n  - key encipherment\n  - client auth\n  signerName: kubernetes.io/kube-apiserver-client\nEOF"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                         AGE    REQUESTOR                                 CONDITION\nx509-user.some-namespace     1m     system:serviceaccount:some-namespace      Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve x509-user.some-namespace"
                },
                {
                    "lang": "sh",
                    "value": "certificatesigningrequest.certificates.k8s.io/x509-user.some-namespace approved"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                       AGE   REQUESTOR                              CONDITION\nx509-user.some-namespace   45m   system:serviceaccount:some-namespace   Approved,Issued"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr x509-user.some-namespace -o jsonpath='{.status.certificate}' | base64 --decode > client.crt"
                },
                {
                    "lang": "sh",
                    "value": "cat x509_user-key.pem client.crt > x509-full.pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "none",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: new-x509-user\nspec:\n  username: \"CN=my-x509-authenticated-user,OU=organizationalunit,O=organization\"\n  db: \"$external\"\n  mongodbResourceRef:\n    name: '<name of the MongoDB resource>'\n  roles:\n    - db: \"admin\"\n      name: \"readWriteAnyDatabase\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f x509-mongodb-user.yaml"
                },
                {
                    "lang": "sh",
                    "value": "mongodbuser.mongodb.com/new-x509-user created"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdbu/new-x509-user -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "NAME            CREATED AT\nnew-x509-user   8m"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host {host} --port {port} --tls --tlsCAFile /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --tlsCertificateKeyFile x509-full.pem --authenticationMechanism MONGODB-X509 --authenticationDatabase '$external'"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host {host} --port {port} --ssl --sslCAFile /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --sslPEMKeyFile x509-full.pem --authenticationMechanism MONGODB-X509 --authenticationDatabase '$external'"
                }
            ],
            "preview": "The  can deploy MongoDB instances with\nX.509 authentication\nenabled. If X.509 authentication has been enabled for the deployment,\nyou must generate and use an X.509 certificate to connect to the\ndeployment. This new client certificate must be signed by the\nKubernetes  to be accepted by the MongoDB deployment.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-tls",
            "title": "Secure Deployments using TLS",
            "headings": [
                "General Prerequisites",
                "Configure TLS for a Replica Set",
                "Prerequisites",
                "Create TLS Certificates for a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Create the  for your TLS certificates.",
                "Create the  for your agents' X.509 certificates.",
                "Create the  to link your  with your deployment.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the TLS settings for your replica set resource using a Custom Certificate Authority.",
                "Save your replica set config file.",
                "Apply your changes to your replica set deployment.",
                "Track the status of your deployment.",
                "Renew TLS Certificates for a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Renew the  for your TLS certificates.",
                "Restart the Pods in Your Deployment.",
                "Track the status of your deployment.",
                "Configure TLS for a Sharded Cluster",
                "Prerequisites",
                "Create TLS Certificates for a Sharded Cluster",
                "Configure kubectl to default to your namespace.",
                "Create the  for your Shards' TLS certificates.",
                "Create the  for your config server's TLS certificates.",
                "Create the  for your mongos server's TLS certificates.",
                "Create the  for your agents' X.509 certificates.",
                "Create the  to link your  with your deployment.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the TLS settings for your sharded cluster resource using a Custom Certificate Authority.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Track the status of your deployment.",
                "Renew TLS Certificates for a Sharded Cluster",
                "Configure kubectl to default to your namespace.",
                "Renew the  for your Shards' TLS certificates.",
                "Renew the  for your config server's TLS certificates.",
                "Renew the  for your mongos server's TLS certificates.",
                "Restart the Pods in Your Deployment.",
                "Track the status of your deployment."
            ],
            "paragraphs": "The   can use   certificates to encrypt connections\nbetween: This guide instructs you on how to configure the   to use\n  for its MongoDB instances. MongoDB hosts in a replica set or sharded cluster Client applications and MongoDB deployments Automatically generating   certificates with the  \nis deprecated and will be removed in a future release. You must provide certificates from your own CA, as described in the\nfollowing procedures, for production environments. Before you secure your MongoDB deployment using   encryption,\ncomplete the following: Install the Kubernetes Operator Create Credentials for the Kubernetes Operator Before you secure your replica set using   encryption, complete the\nfollowing: Deploy the Replica Set \nthat you want to secure Create a   file for each of the following components: For the Agent PEM files, ensure that: PEM file purpose Save File As... Your custom  ca-pem Each member of your replica set <metadata.name>-<X>-pem Your project's Automation or MongoDB Agent mms-automation-agent-pem Your project's Backup Agent (if needed) mms-backup-agent-pem Your project's Monitoring Agent (if needed) mms-monitoring-agent-pem the Common Name in each   certificate is not empty, and the combined Organization and Organizational Unit in each  \ncertificate differs from the combined Organization and\nOrganizational Unit in the   certificates for your\nreplica set members. To create the   file, concatenate the   certificate and the\nPrivate Key. An example of a   file would resemble: Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. Each certificate should include a valid Domain Name. For each replica set or sharded cluster member, the Common Name, also\nknown as the Domain Name, for that member's certificate must match\nthe   of the POD on which this cluster member\nis deployed. The   name in each certificate has the following syntax:\n pod-name.service-name.namespace.svc.cluster.local . This name is\ndifferent for each Pod hosting a member of the replica set or a\nsharded cluster. For example, for a member of a replica set deployed on a Pod with\nthe name  rs-mongos-0-0 , in the   service\nnamed  mongo-0  that is created in the default  mongodb \nnamespace, the   is: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create a new   that stores\nthe replica set's certificates: This example covers a three-member replica set. If you have more than\nthree members, you can add them to the certificate using the\n --from-file  option. Run this  kubectl  command to create a new   that stores\nthe agents' X.509 certificates: Run this  kubectl  command to link your   to your replica\nset: Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Required If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Required Add the  's name that stores the custom  \nthat you used to sign your deployment's   certificates. <custom-ca> Invoke the following   command to update your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. If you have already created certificates, we recommend that you renew\nthem periodically using the following procedure. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to renew an existing   that\nstores the replica set's certificates: This example covers a three-member replica set. If you have more than\nthree members, you can add them to the certificate using the\n --from-file  option. Run this  kubectl  command to force a  \nof the StatefulSets to restart the Pods. The Pods restart and begin watching the renewed secrets. To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. Before you secure your replica set using   encryption, complete the\nfollowing: Deploy the Sharded Cluster \nthat you want to secure Create a   file for each of the following components: For the Agent PEM files, ensure that: PEM file purpose Save File As... Your custom  ca-pem Each shard in your sharded cluster <metadata.name>-<Y>-<X>-pem Each member of your config server replica set <metadata.name>-config-<X>-pem Each  <metadata.name>-mongos-<X>-pem Your project's Automation or MongoDB Agent mms-automation-agent-pem Your project's Backup Agent (if needed) mms-backup-agent-pem Your project's Monitoring Agent (if needed) mms-monitoring-agent-pem the Common Name in each   certificate is not empty, and the combined Organization and Organizational Unit in each  \ncertificate differs from the combined Organization and\nOrganizational Unit in the   certificates for your\nsharded cluster members, config server members, and each  . To create the   file, concatenate the   certificate and the\nPrivate Key. An example of a   file would resemble: Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. Each certificate should include a valid Domain Name. For each replica set or sharded cluster member, the Common Name, also\nknown as the Domain Name, for that member's certificate must match\nthe   of the POD on which this cluster member\nis deployed. The   name in each certificate has the following syntax:\n pod-name.service-name.namespace.svc.cluster.local . This name is\ndifferent for each Pod hosting a member of the replica set or a\nsharded cluster. For example, for a member of a replica set deployed on a Pod with\nthe name  rs-mongos-0-0 , in the   service\nnamed  mongo-0  that is created in the default  mongodb \nnamespace, the   is: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create a new   that stores\nthe sharded cluster shards' certificates: This example covers a two-shard sharded cluster with five members per\nshard. If you have more than two shards or five members per shard,\nyou can add them to the certificate using the  --from-file  option. Run this  kubectl  command to create a new   that stores\nthe sharded cluster config server's certificates: Run this  kubectl  command to create a new   that stores\nthe sharded cluster   certificates: Run this  kubectl  command to create a new   that stores\nthe agents' X.509 certificates: Run this  kubectl  command to link your   to your replica\nset: Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Required If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Required Add the  's name that stores the custom  \nthat you used to sign your deployment's   certificates. <custom-ca> Invoke the following   command to update and restart your\n sharded cluster : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. If you have already created certificates, we recommend that you renew\nthem periodically using the following procedure. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to renew an existing   that\nstores the sharded cluster shards' certificates: This example covers a two-shard sharded cluster with five members per\nshard. If you have more than two shards or five members per shard,\nyou can add them to the certificate using the  --from-file  option. Run this  kubectl  command to renew an existing   that\nstores the sharded cluster config server's certificates: Run this  kubectl  command to renew an existing   that\nstores the sharded cluster   certificates: Run this  kubectl  command to force a  \nof the StatefulSets to restart the Pods. The Pods restart and begin watching the renewed secrets. To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "text",
                    "value": "-----BEGIN CERTIFICATE-----\n...\n... your TLS certificate\n...\n-----END CERTIFICATE-----\n-----BEGIN RSA PRIVATE KEY-----\n...\n... your private key\n...\n-----END RSA PRIVATE KEY----"
                },
                {
                    "lang": "sh",
                    "value": "rs-mongos-0-0.mongo-0.mongodb.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <metadata.name>-cert \\\n  --from-file=<metadata.name>-0-pem \\\n  --from-file=<metadata.name>-1-pem \\\n  --from-file=<metadata.name>-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic agent-certs \\\n  --from-file=mms-automation-agent-pem \\\n  --from-file=mms-backup-agent-pem \\\n  --from-file=mms-monitoring-agent-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap custom-ca --from-file=ca-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <metadata.name>-cert \\\n  --from-file=<metadata.name>-0-pem \\\n  --from-file=<metadata.name>-1-pem \\\n  --from-file=<metadata.name>-2-pem \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl rollout restart sts <name-of-the-resource>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "text",
                    "value": "-----BEGIN CERTIFICATE-----\n...\n... your TLS certificate\n...\n-----END CERTIFICATE-----\n-----BEGIN RSA PRIVATE KEY-----\n...\n... your private key\n...\n-----END RSA PRIVATE KEY----"
                },
                {
                    "lang": "sh",
                    "value": "rs-mongos-0-0.mongo-0.mongodb.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-0-cert \\\n       --from-file=<metadata.name>-0-0-pem \\\n       --from-file=<metadata.name>-0-1-pem \\\n       --from-file=<metadata.name>-0-2-pem \\\n       --from-file=<metadata.name>-0-3-pem \\\n       --from-file=<metadata.name>-0-4-pem\n\nkubectl -n mongodb create secret generic <metadata.name>-1-cert \\\n       --from-file=<metadata.name>-1-0-pem \\\n       --from-file=<metadata.name>-1-1-pem \\\n       --from-file=<metadata.name>-1-2-pem \\\n       --from-file=<metadata.name>-1-3-pem \\\n       --from-file=<metadata.name>-1-4-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-config-cert \\\n       --from-file=<metadata.name>-config-0-pem \\\n       --from-file=<metadata.name>-config-1-pem \\\n       --from-file=<metadata.name>-config-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-mongos-cert \\\n       --from-file=<metadata.name>-mongos-0-pem \\\n       --from-file=<metadata.name>-mongos-1-pem \\\n       --from-file=<metadata.name>-mongos-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic agent-certs \\\n  --from-file=mms-automation-agent-pem \\\n  --from-file=mms-backup-agent-pem \\\n  --from-file=mms-monitoring-agent-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap custom-ca --from-file=ca-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-0-cert \\\n       --from-file=<metadata.name>-0-0-pem \\\n       --from-file=<metadata.name>-0-1-pem \\\n       --from-file=<metadata.name>-0-2-pem \\\n       --from-file=<metadata.name>-0-3-pem \\\n       --from-file=<metadata.name>-0-4-pem \\\n       --dry-run=client \\\n        -o yaml |\nkubectl apply -f -\n\nkubectl -n mongodb create secret generic <metadata.name>-1-cert \\\n       --from-file=<metadata.name>-1-0-pem \\\n       --from-file=<metadata.name>-1-1-pem \\\n       --from-file=<metadata.name>-1-2-pem \\\n       --from-file=<metadata.name>-1-3-pem \\\n       --from-file=<metadata.name>-1-4-pem \\\n       --dry-run=client \\\n        -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-config-cert \\\n       --from-file=<metadata.name>-config-0-pem \\\n       --from-file=<metadata.name>-config-1-pem \\\n       --from-file=<metadata.name>-config-2-pem \\\n       --dry-run=client \\\n        -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-mongos-cert \\\n       --from-file=<metadata.name>-mongos-0-pem \\\n       --from-file=<metadata.name>-mongos-1-pem \\\n       --from-file=<metadata.name>-mongos-2-pem \\\n       --dry-run=client \\\n        -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl rollout restart sts <name-of-the-resource>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "The  can use  certificates to encrypt connections\nbetween:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/scale-resources",
            "title": "Scale a Deployment",
            "headings": [
                "Considerations",
                "Examples"
            ],
            "paragraphs": "You can scale your  replica set  and  sharded cluster \ndeployments up or down to match your desired configuration. Scaling up\nincreases the number of members and/or shards in the deployment,\nthereby improving your deployment's redundancy and availability. The\nscale of your deployment is configured in its corresponding\n . To scale your replica set deployment, set the  spec.members \nsetting to the desired number of replica set members. To learn more\nabout replication, see  Replication  in the\nMongoDB manual. To scale your sharded cluster deployment, set the following settings\nas desired: To learn more about sharded cluster configurations, see\n Sharded Cluster Components  in the MongoDB manual. Setting Description spec.shardCount Number of  shards  in the sharded cluster. spec.mongodsPerShardCount Number of members per shard. spec.mongosCount Number of Shard Routers. spec.configServerCount Number of members in the Config Server. The   does not support modifying deployment types.\nFor example, you cannot convert a standalone deployment to a\nreplica set. To modify the type of a deployment,\nwe recommend the following procedure: Create the new deployment with the desired configuration. Back up the data  from\nyour current deployment. Restore the data  from your current\ndeployment to the new deployment. Test your application connections to the new deployment as needed. Once you have verified that the new deployment contains the\nrequired data and can be reached by your application(s), bring\ndown the old deployment. For MongoDB 4.4 deployments using   v1.7.0 and earlier, you\ncan increase or decrease the number of members in a replica set or a\nsharded cluster by only one member at a time. To scale a replica set from three members to five members, you\nmust: Change the value of the  spec.members  setting from\n 3  to  4 . Reapply the configuration to  . Change the value of the  spec.members  setting from\n 4  to  5 . Reapply the configuration to  . Select the desired tab based on the deployment configuration you\nwant to scale: Consider a replica set resource with the following\n : To scale up this replica set and add more members: Adjust the  spec.members  setting to the desired\nnumber of members: For MongoDB 4.4 deployments using   v1.7.0 and\nearlier, you can increase or decrease the number of members\nin a replica set by only one member at a time. Reapply the configuration to  : Consider a sharded cluster resource with the following\n : To scale up this sharded cluster: Adjust the following settings to the desired values: For MongoDB 4.4 deployments, you can increase or decrease\nthe number of members in a sharded cluster by only one\nmember at a time. spec.shardCount spec.mongodsPerShardCount spec.mongosCount spec.configServerCount Reapply the configuration to  :",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-secure-replica-set>\nspec:\n  members: 4\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <repl-set-config>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-secure-sharded-cluster>\nspec:\n  shardCount: 3\n  mongodsPerShardCount: 3\n  mongosCount: 3\n  configServerCount: 4\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-config>.yaml"
                }
            ],
            "preview": "You can scale your replica set and sharded cluster\ndeployments up or down to match your desired configuration. Scaling up\nincreases the number of members and/or shards in the deployment,\nthereby improving your deployment's redundancy and availability. The\nscale of your deployment is configured in its corresponding\n.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-internal-auth",
            "title": "Secure Internal Authentication with X.509 and TLS",
            "headings": [
                "General Prerequisites",
                "Configure X.509 Internal Authentication for a Replica Set",
                "Prerequisites",
                "Create Internal Authentication X.509 Certificates for a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Create the  for your TLS certificates.",
                "Create the  for your agents' X.509 certificates.",
                "Create the  for your X.509 certificates.",
                "Create the  to link your  with your deployment.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the TLS settings for your replica set resource using a Custom Certificate Authority.",
                "Configure the general X.509 settings for your replica set resource.",
                "Configure the internal X.509 settings for your replica set resource.",
                "Save your replica set config file.",
                "Apply your changes to your replica set deployment.",
                "Track the status of your deployment.",
                "Renew Internal Authentication X.509 Certificates for a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Renew the  for your TLS certificates.",
                "Renew the  for your X.509 certificates.",
                "Renew the  for your agents' X.509 certificates.",
                "Restart the Pods in Your Deployment.",
                "Track the status of your deployment.",
                "Configure X.509 Internal Authentication for a Sharded Cluster",
                "Prerequisites",
                "Create Internal Authentication X.509 Certificates for a Sharded Cluster",
                "Configure kubectl to default to your namespace.",
                "Create the  for your Shards' TLS certificates.",
                "Create the  for your config server's TLS certificates.",
                "Create the  for your mongos server's TLS certificates.",
                "Create the  for your agents' X.509 certificates.",
                "Create the  for your Shards' X.509 certificates.",
                "Create the  for your config server's X.509 certificates.",
                "Create the  for your mongos server's X.509 certificates.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the TLS settings for your sharded cluster resource using a Custom Certificate Authority.",
                "Configure the general X.509 settings for your sharded cluster resource.",
                "Configure the internal X.509 settings for your sharded cluster resource.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Track the status of your deployment.",
                "Renew Internal Authentication X.509 Certificates for a Sharded Cluster",
                "Configure kubectl to default to your namespace.",
                "Renew the  for your Shards' TLS certificates.",
                "Renew the  for your config server's TLS certificates.",
                "Renew the  for your mongos server's TLS certificates.",
                "Renew the  for your Shards' X.509 certificates.",
                "Renew the  for your config server's X.509 certificates.",
                "Renew the  for your mongos server's X.509 certificates.",
                "Renew the  for your agents' X.509 certificates.",
                "Restart the Pods in Your Deployment.",
                "Track the status of your deployment."
            ],
            "paragraphs": "This guide instructs you on how to configure: The   doesn't support other authentication schemes between\nMongoDB nodes in a cluster. X.509 internal authentication between MongoDB nodes in a cluster. X.509 authentication from clients to your MongoDB instances.  to encrypt connections between MongoDB hosts in a replica set\nor sharded cluster.  to encrypt connections client applications and MongoDB\ndeployments. Automatically generating   certificates with the  \nis deprecated and will be removed in a future release. You must provide certificates from your own CA, as described in the\nfollowing procedures, for production environments. Before you secure any of your MongoDB deployments using  \nencryption, complete the following: Install the Kubernetes Operator Create Credentials for the Kubernetes Operator Enabling X.509 authentication at the project level configures all\nagents to use X.509 client authentication when communicating with\nMongoDB deployments. X.509 client authentication requires one of the following:  4.1.7 or later  4.0.11 or later Before you secure your replica set using   encryption, complete the\nfollowing: Deploy the Replica Set \nthat you want to secure Create a   file for each of the following components: For the Agent PEM files, ensure that: PEM file purpose Save File As... Your custom  ca-pem Each member of your replica set <metadata.name>-<X>-pem Your project's Automation or MongoDB Agent mms-automation-agent-pem Your project's Backup Agent (if needed) mms-backup-agent-pem Your project's Monitoring Agent (if needed) mms-monitoring-agent-pem the Common Name in each   certificate is not empty, and the combined Organization and Organizational Unit in each  \ncertificate differs from the combined Organization and\nOrganizational Unit in the   certificates for your\nreplica set members. Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. To create the   file, concatenate the   certificate and the\nPrivate Key. An example of a   file would resemble: Each certificate should include a valid Domain Name. For each replica set or sharded cluster member, the Common Name, also\nknown as the Domain Name, for that member's certificate must match\nthe   of the POD on which this cluster member\nis deployed. The   name in each certificate has the following syntax:\n pod-name.service-name.namespace.svc.cluster.local . This name is\ndifferent for each Pod hosting a member of the replica set or a\nsharded cluster. For example, for a member of a replica set deployed on a Pod with\nthe name  rs-mongos-0-0 , in the   service\nnamed  mongo-0  that is created in the default  mongodb \nnamespace, the   is: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create a new   that stores\nthe replica set's certificates: This example covers a three-member replica set. If you have more than\nthree members, you can add them to the certificate using the\n --from-file  option. Run this  kubectl  command to create a new   that stores\nthe agents' X.509 certificates: Run this  kubectl  command to create a new   that stores\nthe replica set's certificates: This example covers a three-member replica set. If you have more than\nthree members, you can add them to the certificate using the\n --from-file  option. Run this  kubectl  command to link your   to your replica\nset: Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Required If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Required Add the  's name that stores the custom  \nthat you used to sign your deployment's   certificates. <custom-ca> To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Required Set this value to  true  to enable authentication on the\nMongoDB deployment. true array Conditional Set this value to  [\"X509\"] . [\"X509\"] To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example string Required Use this setting to enable\n X.509 internal cluster authentication . Once internal cluster authentication is enabled, it can't\nbe disabled. X509 Invoke the following   command to update your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. If you have already created certificates, we recommend that you renew\nthem periodically using the following procedure. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to renew an existing   that\nstores the replica set's certificates: This example covers a three-member replica set. If you have more than\nthree members, you can add them to the certificate using the\n --from-file  option. Run this  kubectl  command to renew an existing   that\nstores the replica set's certificates: This example covers a three-member replica set. If you have more than\nthree members, you can add them to the certificate using the\n --from-file  option. Run this  kubectl  command to renew an existing   that\nstores the agents' X.509 certificates: Run this  kubectl  command to force a  \nof the StatefulSets to restart the Pods. The Pods restart and begin watching the renewed secrets. To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. Before you secure your sharded cluster using   encryption, complete\nthe following: Deploy the Sharded Cluster \nthat you want to secure Create a   file for each of the following components: For the Agent PEM files, ensure that: PEM file purpose Save File As... Your custom  ca-pem Each shard in your sharded cluster <metadata.name>-<Y>-<X>-pem Each member of your config server replica set <metadata.name>-config-<X>-pem Each  <metadata.name>-mongos-<X>-pem Your project's Automation or MongoDB Agent mms-automation-agent-pem Your project's Backup Agent (if needed) mms-backup-agent-pem Your project's Monitoring Agent (if needed) mms-monitoring-agent-pem the Common Name in each   certificate is not empty, and the combined Organization and Organizational Unit in each  \ncertificate differs from the combined Organization and\nOrganizational Unit in the   certificates for your\nsharded cluster members, config server members, and each  . To create the   file, concatenate the   certificate and the\nPrivate Key. An example of a   file would resemble: Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. Each certificate should include a valid Domain Name. For each replica set or sharded cluster member, the Common Name, also\nknown as the Domain Name, for that member's certificate must match\nthe   of the POD on which this cluster member\nis deployed. The   name in each certificate has the following syntax:\n pod-name.service-name.namespace.svc.cluster.local . This name is\ndifferent for each Pod hosting a member of the replica set or a\nsharded cluster. For example, for a member of a replica set deployed on a Pod with\nthe name  rs-mongos-0-0 , in the   service\nnamed  mongo-0  that is created in the default  mongodb \nnamespace, the   is: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create a new   that stores\nthe sharded cluster shards' certificates: This example covers a two-shard sharded cluster with five members per\nshard. If you have more than two shards or five members per shard,\nyou can add them to the certificate using the  --from-file  option. Run this  kubectl  command to create a new   that stores\nthe sharded cluster config server's certificates: Run this  kubectl  command to create a new   that stores\nthe sharded cluster   certificates: Run this  kubectl  command to create a new   that stores\nthe agents' X.509 certificates: Run this  kubectl  command to create a new   that stores\nthe sharded cluster shards' certificates: This example covers a two-shard sharded cluster with five members per\nshard. If you have more than two shards or five members per shard,\nyou can add them to the certificate using the  --from-file  option. Run this  kubectl  command to create a new   that stores\nthe sharded cluster config server's certificates: Run this  kubectl  command to create a new   that stores\nthe sharded cluster   certificates: Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Required If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Required Add the  's name that stores the custom  \nthat you used to sign your deployment's   certificates. <custom-ca> To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Required Set this value to  true  to enable authentication on the\nMongoDB deployment. true array Conditional Set this value to  [\"X509\"] . [\"X509\"] To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example string Required Use this setting to enable\n X.509 internal cluster authentication . Once internal cluster authentication is enabled, it can't\nbe disabled. X509 Invoke the following   command to update and restart your\n sharded cluster : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. If you have already created certificates, we recommend that you renew\nthem periodically using the following procedure. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to renew an existing   that\nstores the sharded cluster shards' certificates: This example covers a two-shard sharded cluster with five members per\nshard. If you have more than two shards or five members per shard,\nyou can add them to the certificate using the  --from-file  option. Run this  kubectl  command to renew an existing   that\nstores the sharded cluster config server's certificates: Run this  kubectl  command to renew an existing   that\nstores the sharded cluster   certificates: Run this  kubectl  command to renew an existing   that stores\nthe sharded cluster shards' certificates: This example covers a two-shard sharded cluster with five members per\nshard. If you have more than two shards or five members per shard,\nyou can add them to the certificate using the  --from-file  option. Run this  kubectl  command to renew an existing   that stores\nthe sharded cluster config server's certificates: Run this  kubectl  command to renew an existing   that stores\nthe sharded cluster   certificates: Run this  kubectl  command to renew an existing   that\nstores the agents' X.509 certificates: Run this  kubectl  command to force a  \nof the StatefulSets to restart the Pods. The Pods restart and begin watching the renewed secrets. To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "text",
                    "value": "-----BEGIN CERTIFICATE-----\n...\n... your TLS certificate\n...\n-----END CERTIFICATE-----\n-----BEGIN RSA PRIVATE KEY-----\n...\n... your private key\n...\n-----END RSA PRIVATE KEY----"
                },
                {
                    "lang": "sh",
                    "value": "rs-mongos-0-0.mongo-0.mongodb.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <metadata.name>-cert \\\n  --from-file=<metadata.name>-0-pem \\\n  --from-file=<metadata.name>-1-pem \\\n  --from-file=<metadata.name>-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic agent-certs \\\n  --from-file=mms-automation-agent-pem \\\n  --from-file=mms-backup-agent-pem \\\n  --from-file=mms-monitoring-agent-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <metadata.name>-clusterfile \\\n  --from-file=<metadata.name>-0-pem \\\n  --from-file=<metadata.name>-1-pem \\\n  --from-file=<metadata.name>-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap custom-ca --from-file=ca-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <metadata.name>-cert \\\n  --from-file=<metadata.name>-0-pem \\\n  --from-file=<metadata.name>-1-pem \\\n  --from-file=<metadata.name>-2-pem \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <metadata.name>-clusterfile \\\n  --from-file=<metadata.name>-0-pem \\\n  --from-file=<metadata.name>-1-pem \\\n  --from-file=<metadata.name>-2-pem \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic agent-certs \\\n  --from-file=mms-automation-agent-pem \\\n  --from-file=mms-backup-agent-pem \\\n  --from-file=mms-monitoring-agent-pem \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl rollout restart sts <name-of-the-resource>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "text",
                    "value": "-----BEGIN CERTIFICATE-----\n...\n... your TLS certificate\n...\n-----END CERTIFICATE-----\n-----BEGIN RSA PRIVATE KEY-----\n...\n... your private key\n...\n-----END RSA PRIVATE KEY----"
                },
                {
                    "lang": "sh",
                    "value": "rs-mongos-0-0.mongo-0.mongodb.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-0-cert \\\n       --from-file=<metadata.name>-0-0-pem \\\n       --from-file=<metadata.name>-0-1-pem \\\n       --from-file=<metadata.name>-0-2-pem \\\n       --from-file=<metadata.name>-0-3-pem \\\n       --from-file=<metadata.name>-0-4-pem\n\nkubectl -n mongodb create secret generic <metadata.name>-1-cert \\\n       --from-file=<metadata.name>-1-0-pem \\\n       --from-file=<metadata.name>-1-1-pem \\\n       --from-file=<metadata.name>-1-2-pem \\\n       --from-file=<metadata.name>-1-3-pem \\\n       --from-file=<metadata.name>-1-4-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-config-cert \\\n       --from-file=<metadata.name>-config-0-pem \\\n       --from-file=<metadata.name>-config-1-pem \\\n       --from-file=<metadata.name>-config-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-mongos-cert \\\n       --from-file=<metadata.name>-mongos-0-pem \\\n       --from-file=<metadata.name>-mongos-1-pem \\\n       --from-file=<metadata.name>-mongos-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic agent-certs \\\n  --from-file=mms-automation-agent-pem \\\n  --from-file=mms-backup-agent-pem \\\n  --from-file=mms-monitoring-agent-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-0-clusterfile \\\n       --from-file=<metadata.name>-0-0-pem \\\n       --from-file=<metadata.name>-0-1-pem \\\n       --from-file=<metadata.name>-0-2-pem \\\n       --from-file=<metadata.name>-0-3-pem \\\n       --from-file=<metadata.name>-0-4-pem\n\nkubectl -n mongodb create secret generic <metadata.name>-1-clusterfile \\\n       --from-file=<metadata.name>-1-0-pem \\\n       --from-file=<metadata.name>-1-1-pem \\\n       --from-file=<metadata.name>-1-2-pem \\\n       --from-file=<metadata.name>-1-3-pem \\\n       --from-file=<metadata.name>-1-4-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-config-clusterfile \\\n       --from-file=<metadata.name>-config-0-pem \\\n       --from-file=<metadata.name>-config-1-pem \\\n       --from-file=<metadata.name>-config-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-mongos-clusterfile \\\n       --from-file=<metadata.name>-mongos-0-pem \\\n       --from-file=<metadata.name>-mongos-1-pem \\\n       --from-file=<metadata.name>-mongos-2-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-0-cert \\\n       --from-file=<metadata.name>-0-0-pem \\\n       --from-file=<metadata.name>-0-1-pem \\\n       --from-file=<metadata.name>-0-2-pem \\\n       --from-file=<metadata.name>-0-3-pem \\\n       --from-file=<metadata.name>-0-4-pem \\\n       --dry-run=client \\\n        -o yaml |\nkubectl apply -f -\n\nkubectl -n mongodb create secret generic <metadata.name>-1-cert \\\n       --from-file=<metadata.name>-1-0-pem \\\n       --from-file=<metadata.name>-1-1-pem \\\n       --from-file=<metadata.name>-1-2-pem \\\n       --from-file=<metadata.name>-1-3-pem \\\n       --from-file=<metadata.name>-1-4-pem \\\n       --dry-run=client \\\n        -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-config-cert \\\n       --from-file=<metadata.name>-config-0-pem \\\n       --from-file=<metadata.name>-config-1-pem \\\n       --from-file=<metadata.name>-config-2-pem \\\n       --dry-run=client \\\n        -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-mongos-cert \\\n       --from-file=<metadata.name>-mongos-0-pem \\\n       --from-file=<metadata.name>-mongos-1-pem \\\n       --from-file=<metadata.name>-mongos-2-pem \\\n       --dry-run=client \\\n        -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-0-clusterfile \\\n       --from-file=<metadata.name>-0-0-pem \\\n       --from-file=<metadata.name>-0-1-pem \\\n       --from-file=<metadata.name>-0-2-pem \\\n       --from-file=<metadata.name>-0-3-pem \\\n       --from-file=<metadata.name>-0-4-pem \\\n       --dry-run=client \\\n        -o yaml |\nkubectl apply -f -\n\nkubectl -n mongodb create secret generic <metadata.name>-1-clusterfile \\\n       --from-file=<metadata.name>-1-0-pem \\\n       --from-file=<metadata.name>-1-1-pem \\\n       --from-file=<metadata.name>-1-2-pem \\\n       --from-file=<metadata.name>-1-3-pem \\\n       --from-file=<metadata.name>-1-4-pem \\\n       --dry-run=client \\\n        -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-config-clusterfile \\\n       --from-file=<metadata.name>-config-0-pem \\\n       --from-file=<metadata.name>-config-1-pem \\\n       --from-file=<metadata.name>-config-2-pem \\\n       --dry-run=client \\\n        -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-mongos-clusterfile \\\n       --from-file=<metadata.name>-mongos-0-pem \\\n       --from-file=<metadata.name>-mongos-1-pem \\\n       --from-file=<metadata.name>-mongos-2-pem \\\n       --dry-run=client \\\n        -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic agent-certs \\\n  --from-file=mms-automation-agent-pem \\\n  --from-file=mms-backup-agent-pem \\\n  --from-file=mms-monitoring-agent-pem \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl rollout restart sts <name-of-the-resource>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "This guide instructs you on how to configure:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/om-arch",
            "title": " Architecture in ",
            "headings": [
                "The MongoDBOpsManager Custom Resource Definition",
                "Application Database",
                "",
                "Backup Daemon",
                "Reconciling the MongoDBOpsManager Custom Resource"
            ],
            "paragraphs": "You can use the   to deploy   and MongoDB resources\nto a   cluster. The   manages the lifecycle of each of\nthese deployments differently. The   manages   deployments using the\n MongoDBOpsManager   . The   watches\nthe custom resource's specification for changes. When the\nspecification changes, the   validates the changes and\nmakes the appropriate updates to the resources in the   cluster. MongoDBOpsManager   s specification defines the\nfollowing   components: the Application Database, the   application, and the Backup Daemon. For the Application Database, the   deploys a MongoDB\nreplica set as a   to the   cluster.   creates\none Pod in the StatefulSet for each member\nthat comprises your Application Database replica set. Each Pod in\nthe StatefulSet runs a MongoDB Agent instance. By default, each MongoDB Agent starts the bundled   on its\nPod in the StatefulSet. If you want to use a specific MongoDB Server\nversion for the Application Database instead, specify the version that\nyou want to start using the  spec.applicationDatabase.version \nsetting. Each MongoDB Agent downloads the   version you\nspecify from the Internet and starts it on its Pod in the StatefulSet. After each MongoDB Agent starts  s on its Application Database\nPod, the MongoDB Agents add all   processes to the Application\nDatabase replica set. You configure the number of replicas in and other\nconfiguration options for the Application Database replica set in the\n spec.applicationDatabase  collection in the\n MongoDBOpsManager  custom resource. The   passes\nthis configuration to the MongoDB Agents using a   that the\n  mounts to each Pod in the Application Database StatefulSet. Each time that you update\nthe  spec.applicationDatabase  collection, the\n  applies the changes to the MongoDB Agent configuration and\nthe StatefulSet specification, if applicable. If the StatefulSet\nspecification changes,   upgrades the Pods in a rolling\nfashion and restarts each Pod. The   creates a   with  clusterIp=none  to\nprovide connectivity to each Application Database Pod from within the\n  cluster. If you set  spec.applicationDatabase.persistent  to\n true , the   creates a   for each Pod in the\nApplication Database StatefulSet. You can customize the   for the Application Database Pods using\nthe  spec.applicationDatabase.podSpec.persistence.single  or\n spec.applicationDatabase.podSpec.persistence.multiple  options. Depending on the   or the environment to which you deploy the\n ,   might create the   using\n dynamic volume provisioning . After the Application Database reaches a  Running  state, the\n  starts the  . For  , the\n  deploys a StatefulSet to the   cluster.  \ncreates one Pod in the StatefulSet for each   replica that\nyou want to deploy. Each Pod contains one   process. The   creates a   with  clusterIp=none  to\nallow clients deployed to the   cluster to connect to  . To\nallow clients external to the   cluster to connect to  ,\nconfigure the  spec.externalConnectivity  collection in the\nspecification for your   deployment. Deploy  multiple   replicas to\nmake your deployment highly available in the event of an   Pod\nfailure. If  spec.backup.enabled  is  true , the  \nstarts the Backup Daemon after the   reaches a  Running \nstage. For the Backup Daemon,   deploys a StatefulSet\nto the   cluster.   creates one pod in the\nStatefulSet for the Backup Daemon. If you enable backup, you must provide additional fields in the\n spec.backup  collection to configure:\nthe  oplog store  and a  blockstore  or an    snapshot store . If you enable backup, the   creates a   for the\nBackup Daemon's  head database . You can\nconfigure the head database using the  spec.backup.headDB \nsetting. The   invokes   APIs to ensure that the\n 's backup configuration matches the one that you define in\nthe custom resource definition. The following diagram describes how the   reconciles\nchanges to the  MongoDBOpsManager   . The   creates or updates the\n <om_resource_name>-db-config  Secret. This secret contains\nthe configurations that the MongoDB Agent uses to start the\nApplication Database replica set. The   creates or updates the  <om_resource_name>-db \nApplication Database StatefulSet. This StatefulSet contains at\nleast three  . Each Pod runs one MongoDB Agent instance. Each MongoDB Agent starts a\n  instance on its pod. The   mounts the  <om_resource_name>-db-config \nSecret to each Pod. The MongoDB Agent uses this secret to\nconfigure the Application Database replica set. The   creates or updates the  <om_resource_name> \nStatefulSet. This StatefulSet contains one Pod for each\n  replica. Each   replica connects to the Application\nDatabase. Most changes to the  MongoDBOpsManager   \ntrigger a rolling upgrade of the Pods in the\n <om_resource_name>  StatefulSet.  Enabling TLS for the\nApplication Database  also triggers a rolling\nrestart because the connection string to the Application Database\nchanges. Changes to the following  MongoDBOpsManager \n  collections don't trigger a rolling upgrade: spec.backup spec.applicationDatabase The   invokes   APIs to create an admin user.\nThe   saves this admin user's credentials in the\n <om_resource_name>-admin-key  Secret. The  \nuses these credentials for all other   API invocations. This reconciliation step happens only once: when you use the\n  to create an   resource. The\n  skips this step when it updates the resource. The   performs a rolling upgrade of the Pods in the\n <om_resource_name>-db  Application Database StatefulSet\nto enable   to monitor it. This reconciliation step happens only when you enable Monitoring\nfor an application database for the first time. This happens most\noften when you deploy a new   resource. If  spec.backup.enabled  is  true , the  \ncreates the  <om_resource_name>-backup-daemon  StatefulSet or\nverifies that it is running. The   mounts a   for\nthe head database. The Backup Daemon connects to the same Application Database as the\n  deployment. If  spec.backup.enabled  is  true , the  \ninvokes   APIs to ensure that the  's backup\nconfiguration matches the one that you define in the custom resource\ndefinition.",
            "code": [],
            "preview": "You can use the  to deploy  and MongoDB resources\nto a  cluster. The  manages the lifecycle of each of\nthese deployments differently.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-operator-install",
            "title": "Plan your  Installation",
            "headings": [],
            "paragraphs": "Use the   to deploy: To deploy MongoDB resources with the  , you need an\n  instance. Deploy this instance to   using the Operator or\noutside   using\n traditional installation methods . The\nOperator uses     methods to deploy and then manage MongoDB\nresources. Ops Manager resources MongoDB standalone, replica set, and sharded cluster resources Review the architecture of the custom resources in the  :\nthe   and the MongoDB database. Review compatible versions of  , OpenShift, MongoDB, and  . Review container image details. Set the scope for the   deployment by configuring which\ntype of namespace the   should use. Review   deployment scopes and other preparation\ninformation. Review the prerequisites before you install the  .",
            "code": [],
            "preview": "Use the  to deploy:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-om-container-local-mode",
            "title": "Configure an  Resource to use Local Mode",
            "headings": [
                "Considerations",
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Delete the  that manages your  Pods.",
                "Copy the highlighted fields of this  resource.",
                "Paste the copied example section into your existing  resource.",
                "Save your  config file.",
                "Apply changes to your  deployment.",
                "In a rolling fashion, delete your old  Pods.",
                "Track the status of your  instance.",
                "Download the MongoDB installation archive to your local machine.",
                "Copy the MongoDB installation archive to the  Persistent Volume for each  replica you deployed.",
                "Deploy a MongoDB Database Resource."
            ],
            "paragraphs": "In a default configuration, the MongoDB Agents and Backup Daemons\naccess MongoDB installation archives over the Internet from  You can configure   to run in  Local Mode  with the\n  if the nodes in your   cluster don't have access to\nthe Internet. The Backup Daemons and managed MongoDB resources download\ninstallation archives only from a   that you create for\nthe   StatefulSet. This procedure covers uploading installation archives to  . Configuring   to use Local Mode in   is not recommended.\nConsider  configuring Ops Manager to use Remote Mode  instead. When you upgrade the version of an   resource in Local\nMode, you might need to install the latest version of the  MongoDB\nDatabase Tools .  Version Action 4.4.4+ No installation necessary. 4.4.0 - 4.4.3 Install the latest version of the  MongoDB Database Tools .\n automation.versions.directory  specifies\nthe location of the Database Tools, which defaults to\n /mongodb-ops-manager/mongodb-releases/ . Deploy an   Resource . The following procedure shows you how to\nupdate your       to enable Local Mode. To avoid downtime when you enable Local Mode, ensure that you set\n spec.replicas  to a value greater than  1  in your\n  resource definition. If you updated your   resource definition to make  \nhighly available, apply your changes before you begin this tutorial: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : In this tutorial, you update the StatefulSet that manages the  \nPods in your   cluster. You must first delete the   StatefulSet so that   can apply\nthe updates that Local Mode requires. Find the name of your   StatefulSet: The entry in the response that matches the\n metadata.name  of your Your   StatefulSet is the entry in the response that matches\nthe  metadata.name  in your   resource\ndefinition. Delete the   StatefulSet: Ensure that you include the  --cascade=false  flag when you\ndelete your   StatefulSet. If you don't include this\nflag,   also deletes your   Pods. The highlighted section: Uses the   configuration setting\n automation.versions.source: local  in\n spec.configuration  to enable Local Mode. Defines a   for the   StatefulSet to store the\nMongoDB installation archive. MongoDB Agents running in MongoDB\ndatabase resource containers that you create with the  \ndownload the installation archives from   instead of from the\nInternet. Open your preferred text editor and paste the  \nspecification into the appropriate location in your resource file. Invoke the following  kubectl  command on the filename of the\n  resource definition:  creates a new   StatefulSet when you apply the changes\nto your   resource definition. Before proceeding to the next\nstep, run the following command to ensure that the   StatefulSet\nexists: The new   StatefulSet should show 0 members ready: List the   Pods in your   cluster: Delete one   Pod:  recreates the   Pod you deleted. Continue to get the\nstatus of the new Pod until it is ready: The output looks like the following when the new Pod is\ninitializing: The output looks like the following when the new Pod is ready: Repeat Steps  b  and  c  until you've deleted all of your\n  Pods and confirmed that all of the new Pods are ready. To check the status of your   resource, invoke the following\ncommand: See  Troubleshoot the   for information about the\nresource deployment statuses. After the   resource completes the  Reconciling  phase, the\ncommand returns output similar to the following: Copy the value of the  status.opsManager.url  field, which states\nthe resource's connection  . You use this value when you create a\n  later in the procedure. The installers that you download depend on the environment to which\nyou deployed the operator: The examples below provide you with a link to quickly download the\nspecified versions of MongoDB Community edition and the MongoDB\nDatabase tools. To download MongoDB Enterprise Edition, or any other version of\nMongoDB Community Edition, visit the  MongoDB Download Center . Download the Ubuntu 16.04 installation tarball for the\nMongoDB Server version you want the   to\ndeploy. For example, to download the  4.4.0  release: If you deployed a version of   from 4.4.0 up to and\nincluding 4.4.3, you must also\ndownload the Ubuntu 16.04 MongoDB Database Tools installation\ntarball. For example, to download the  100.1.0  release: Download the RHEL installation tarball for the MongoDB\ndatabase version you want the   to deploy. For\nexample, to download the  4.4.0  release: If you deployed a version of   from 4.4.0 up to and\nincluding 4.4.3, you must also\ndownload the RHEL MongoDB Database Tools installation\ntarball. For example, to download the  100.1.0  release: The commands that you use depend on the environment to which you\ndeployed the operator: Only copy the MongoDB installation tarballs to  Replica 1  and\nbeyond  if you deployed more than one    replica . To copy the MongoDB installation archive to the\n  PersistentVolume: Copy the MongoDB Server installation tarball to the\n  PersistentVolume. For example, to copy the  4.4.0 \nrelease: If you deployed a version of   from 4.4.0 up to and\nincluding 4.4.3, copy the MongoDB\nDatabase Tools installation tarball to the  \nPersistentVolume. For example, to copy the\n 100.1.0  release: To copy the MongoDB installation archive to the\n  PersistentVolume: Copy the MongoDB Server installation tarball to the\n  PersistentVolume. For example, to copy the  4.4.0 \nrelease: If you deployed a version of   from 4.4.0 up to and\nincluding 4.4.3, copy the MongoDB\nDatabase Tools installation tarball to the  \nPersistentVolume. For example, to copy the\n 100.1.0  release: MongoDB Agents running in MongoDB database resource containers that\nyou create with the   download the installation archives\nfrom   instead of from the Internet. If you have not done so already, complete the following\nprerequisites: Create Credentials for the  Create One Project using a ConfigMap Deploy a  MongoDB database resource \nin the same namespace to which you deployed  .\nEnsure that you: Match the  spec.opsManager.configMapRef.name  of the resource\nto the  metadata.name  of your ConfigMap. Match the  spec.credentials  of the resource to the name of\nthe secret you created that contains an   programmatic\nAPI key pair.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulsets"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulsets -n mongodb\nNAME                       READY   AGE\nops-manager-localmode      2/2     2m31s\nops-manager-localmode-db   3/3     4m46s"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete statefulset --cascade=false <ops-manager-statefulset>"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: ops-manager-localmode\nspec:\n replicas: 2\n version: \"4.2.12\"\n adminCredentials: ops-manager-admin-secret\n configuration:\n   # this enables local mode in Ops Manager\n   automation.versions.source: local\n\n statefulSet:\n   spec:\n     # the Persistent Volume Claim will be created for each Ops Manager Pod\n     volumeClaimTemplates:\n       - metadata:\n           name: mongodb-versions\n         spec:\n           accessModes: [ \"ReadWriteOnce\" ]\n           resources:\n             requests:\n               storage: \"20Gi\"\n     template:\n       spec:\n         containers:\n           - name: mongodb-ops-manager\n             volumeMounts:\n               - name: mongodb-versions\n                 # this is the directory in each Pod where all MongoDB\n                 # archives must be put\n                 mountPath: /mongodb-ops-manager/mongodb-releases\n\n\n backup:\n   enabled: false\n\n applicationDatabase:\n   members: 3\n   persistent: true\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulsets"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulsets -n mongodb\nNAME                       READY   AGE         ops-manager-localmode      0/2     2m31s\nops-manager-localmode-db   3/3     4m46s"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete pod <om-pod-0>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                          READY   STATUS    RESTARTS   AGE\nmongodb-enterprise-operator-5648d4c86-k5brh   1/1     Running   0          5m24s\nops-manager-localmode-0                       0/1     Running   0          0m55s\nops-manager-localmode-1                       1/1     Running   0          5m45s\nops-manager-localmode-db-0                    1/1     Running   0          5m19s\nops-manager-localmode-db-1                    1/1     Running   0          4m54s\nops-manager-localmode-db-2                    1/1     Running   0          4m12s"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                          READY   STATUS    RESTARTS   AGE\nmongodb-enterprise-operator-5648d4c86-k5brh   1/1     Running   0          5m24s\nops-manager-localmode-0                       1/1     Running   0          3m55s\nops-manager-localmode-1                       1/1     Running   0          5m45s\nops-manager-localmode-db-0                    1/1     Running   0          5m19s\nops-manager-localmode-db-1                    1/1     Running   0          4m54s\nops-manager-localmode-db-2                    1/1     Running   0          4m12s"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2020-05-15T16:20:22Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: \"4.2.11-ent\"\n  backup:\n    phase: \"\"\n  opsManager:\n    lastTransition: \"2020-05-15T16:20:26Z\"\n    phase: Running\n    replicas: 1\n    url: http://ops-manager-localmode-svc.mongodb.svc.cluster.local:8080\n    version: \"4.2.12\"\n"
                },
                {
                    "lang": "sh",
                    "value": "curl -OL https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-ubuntu1604-4.4.0.tgz"
                },
                {
                    "lang": "sh",
                    "value": "curl -OL https://fastdl.mongodb.org/tools/db/mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz"
                },
                {
                    "lang": "sh",
                    "value": "curl -OL https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel<version>-4.4.0.tgz"
                },
                {
                    "lang": "sh",
                    "value": "curl -OL https://fastdl.mongodb.org/tools/db/mongodb-database-tools-rhel<version>-x86_64-100.1.0.tgz"
                },
                {
                    "lang": "sh",
                    "value": "kubectl cp mongodb-linux-x86_64-ubuntu1604-4.4.0.tgz \\\n\"ops-manager-localmode-0:/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-ubuntu1604-4.4.0.tgz\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl cp mongodb-linux-x86_64-ubuntu1604-4.4.0.tgz \\\n\"ops-manager-localmode-1:/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-ubuntu1604-4.4.0.tgz\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl cp mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz \\\n\"ops-manager-localmode-0:/mongodb-ops-manager/mongodb-releases/mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl cp mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz \\\n\"ops-manager-localmode-1:/mongodb-ops-manager/mongodb-releases/mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz\""
                },
                {
                    "lang": "sh",
                    "value": "oc rsync  \"ops-manager-localmode-0:/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-rhel<version>-4.4.0.tgz\" \\\nmongodb-linux-x86_64-rhel<version>-4.4.0.tgz"
                },
                {
                    "lang": "sh",
                    "value": "oc rsync  \"ops-manager-localmode-1:/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-rhel<version>-4.4.0.tgz\" \\\nmongodb-linux-x86_64-rhel<version>-4.4.0.tgz"
                },
                {
                    "lang": "sh",
                    "value": "kubectl cp mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz \\\n\"ops-manager-localmode-0:/mongodb-ops-manager/mongodb-releases/mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl cp mongodb-database-tools-rhel<version>-x86_64-100.1.0.tgz \\\n\"ops-manager-localmode-1:/mongodb-ops-manager/mongodb-releases/mongodb-database-tools-rhel<version>-x86_64-100.1.0.tgz\""
                }
            ],
            "preview": "In a default configuration, the MongoDB Agents and Backup Daemons\naccess MongoDB installation archives over the Internet from ",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-sharded-cluster",
            "title": "Deploy a Sharded Cluster",
            "headings": [
                "Prerequisites",
                "Considerations",
                "Do Not Deploy Monitoring Agents inside and outside ",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example to create a new sharded cluster resource.",
                "Configure the settings highlighted in the preceding step as follows.",
                "Add any additional accepted settings for a sharded cluster deployment.",
                "Save this file with a .yaml file extension.",
                "Start your sharded cluster deployment.",
                "Track the status of your sharded cluster deployment."
            ],
            "paragraphs": "Sharded clusters  provide horizontal scaling\nfor large data sets and enable high throughput operations by\ndistributing the data set across a group of servers. To learn more about sharding, see\n Sharding Introduction  in the\nMongoDB manual. Use this procedure to deploy a new sharded cluster that   manages.\nLater, you can use   to add shards and perform other maintenance\noperations on the cluster. At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . You can use the   to deploy MongoDB resources with\n  version 4.0.11 or later and Cloud Manager. You can't use the   to deploy MongoDB resources to\n . To deploy a  sharded cluster  using an  , you need to complete the\nfollowing procedures: Install Kubernetes Operator Create One Project using a ConfigMap Create Credentials for the  Starting in   version 1.3.0, you can only have one\nMongoDB resource per project. To learn how to migrate your project to\na single-cluster configuration, see  Migrate to One Resource per Project (Required for Version 1.3.0) . Do not mix MongoDB deployments outside   with ones inside  \nin the same Project. Due to   network translation, a Monitoring Agent outside  \ncannot monitor MongoDB instances inside  . For this reason, k8s\nand non-k8s deployments in the same Project are not supported. Use\nseparate projects. The procedure for deploying a sharded cluster depends on whether you\nrequire the deployment to run with   enabled for intra-cluster\ncommunication and clients connecting to the database: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. This is a   file that you can modify to meet your desired\nconfiguration. Change the highlighted settings to match your desired\n sharded cluster  configuration. Open your preferred text editor and paste the   specification\ninto a new text file. Key Type Description Example metadata.name string Label for this    sharded cluster   . Resource names must be 44 characters or less. metadata.name  documentation on  names . myproject spec.shardCount integer Number of shards to deploy. 2 spec.mongodsPerShardCount integer Number of shard members per shard. 3 spec.mongosCount integer Number of shard routers to deploy. 2 spec.configServerCount integer Number of members of the config server replica set. 3 spec.version string Version of MongoDB that this  sharded cluster  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. 4.2.11-ent spec.opsManager.configMapRef.name string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value  must  match the namespace in which you created the\n   project ConfigMap . The   tracks any changes to the ConfigMap and\nreconciles the state of the  . <myproject> spec.credentials string Name of the     you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . This value  must  match the namespace in which you created the\nsecret and the  name  value you provided for your  \n   Secret . The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. ShardedCluster spec.persistent string Optional. Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. If this value is  true , then the following values are set\nto their default value of  16Gi : To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: spec.shardPodSpec.persistence.single spec.configSrvPodSpec.persistence.single If you want one   for each  , configure the\n spec.shardPodSpec.persistence.single  and\n spec.configSrvPodSpec.persistence.single \ncollections. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: In the  spec.configSrvPodSpec.persistence.multiple \ncollection:\n-  .data \n-  .journal \n-  .logs In the  spec.configSrvPodSpec.persistence.multiple  collection:\n-  .data \n-  .journal \n-  .logs Your containers must have permissions to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the deployment item does not fix\nissues with your  , contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n  specification file for a  sharded cluster \ndeployment: For config server For shard routers For shard members spec.backup.mode spec.clusterDomain spec.exposedExternally spec.logLevel spec.featureCompatibilityVersion spec.connectivity.replicaSetHorizons You must set  spec.clusterDomain  if your   cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n  might not function as expected. spec.configSrv.additionalMongodConfig spec.configSrvPodSpec.cpu spec.configSrvPodSpec.cpuRequests spec.configSrvPodSpec.memory spec.configSrvPodSpec.memoryRequests spec.configSrvPodSpec.persistence.single spec.configSrvPodSpec.persistence.multiple.data spec.configSrvPodSpec.persistence.multiple.journal spec.configSrvPodSpec.persistence.multiple.logs spec.configSrvPodSpec.nodeAffinity spec.configSrvPodSpec.podAffinity spec.configSrvPodSpec.podAntiAffinityTopologyKey spec.configSrvPodSpec.podTemplate.metadata spec.configSrvPodSpec.podTemplate.spec spec.mongos.additionalMongodConfig spec.mongosPodSpec.cpu spec.mongosPodSpec.cpuRequests spec.mongosPodSpec.memory spec.mongosPodSpec.memoryRequests spec.mongosPodSpec.nodeAffinity spec.mongosPodSpec.podAffinity spec.mongosPodSpec.podAntiAffinityTopologyKey spec.mongosPodSpec.podTemplate.metadata spec.mongosPodSpec.podTemplate.spec spec.shard.additionalMongodConfig spec.shardPodSpec.cpu spec.shardPodSpec.cpuRequests spec.shardPodSpec.memory spec.shardPodSpec.memoryRequests spec.shardPodSpec.nodeAffinity spec.shardPodSpec.persistence.single spec.shardPodSpec.persistence.multiple.data spec.shardPodSpec.persistence.multiple.journal spec.shardPodSpec.persistence.multiple.logs spec.shardPodSpec.podAffinity spec.shardPodSpec.podAntiAffinityTopologyKey spec.shardPodSpec.podTemplate.metadata spec.shardPodSpec.podTemplate.spec Invoke the following   command to create your\n sharded cluster : Check the log  after running this\ncommand. If the creation was successful, you should see a message\nsimilar to the following: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "2018-06-26T10:30:30.346Z INFO operator/shardedclusterkube.go:52 Created! {\"sharded cluster\": \"my-sharded-cluster\"}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "Sharded clusters provide horizontal scaling\nfor large data sets and enable high throughput operations by\ndistributing the data set across a group of servers.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-standalone",
            "title": "Deploy a Standalone MongoDB Instance",
            "headings": [
                "Prerequisites",
                "Considerations",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the following example standalone  .",
                "Open your preferred text editor and paste the  specification into a new text file.",
                "Configure the settings highlighted in the preceeding step as follows.",
                "Add any additional accepted settings for a Standalone deployment.",
                "Save this file with a .yaml file extension.",
                "Start your Standalone deployment.",
                "Track the status of your standalone deployment."
            ],
            "paragraphs": "You can deploy a  standalone  MongoDB instance for   to\nmanage. Use standalone instances for testing and development.\n Do not  use these deployments for production systems as they lack\nreplication and high availability. For all production deployments\nuse replica sets. To learn about replica sets, see\n Deploy a Replica Set . At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . You can use the   to deploy MongoDB resources with\n  version 4.0.11 or later and Cloud Manager. You can't use the   to deploy MongoDB resources to\n . To deploy a  standalone  using an  , you need to complete the\nfollowing procedures: Install Kubernetes Operator Create One Project using a ConfigMap Create Credentials for the  Starting in   version 1.3.0, you can only have one\nMongoDB resource per project. To learn how to migrate your project to\na single-cluster configuration, see  Migrate to One Resource per Project (Required for Version 1.3.0) . To troubleshoot your sharded cluster, see: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : This is a   file that you can modify to meet your desired\nconfiguration. Change the highlighted settings to match your desired\nstandalone configuration. Key Type Description Example metadata.name string Label for this   standalone  . Resource names must be 44 characters or less. metadata.name  documentation on  names . my-project spec.version string Version of MongoDB that is installed on this\nstandalone. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. 4.2.11-ent string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value  must  match the namespace in which you created the\n   project ConfigMap . <myproject> spec.credentials string Name of the     you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . This value  must  match the namespace in which you created the\nsecret and the  name  value you provided for your  \n   Secret . The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. Standalone spec.persistent string Optional. If this value is  true , then\n spec.podSpec.persistence.single  is set to its\ndefault value of  16Gi . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: If you want one   for each  , configure the\n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs Your containers must have permissions to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the deployment item does not fix\nissues with your  , contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n  specification file for a Standalone deployment: spec.additionalMongodConfig spec.exposedExternally spec.logLevel spec.featureCompatibilityVersion spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs spec.podSpec.podAffinity spec.podSpec.podTemplate spec.podSpec.nodeAffinity Invoke the following   command to create your standalone: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. Find a Specific Pod Review Logs from Specific Pod",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\nspec:\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <standalone-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "You can deploy a standalone MongoDB instance for  to\nmanage. Use standalone instances for testing and development.\nDo not use these deployments for production systems as they lack\nreplication and high availability. For all production deployments\nuse replica sets. To learn about replica sets, see\nDeploy a Replica Set.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-op-compatibility",
            "title": " Compatibility",
            "headings": [
                " and OpenShift Versions",
                "Supported Platforms and MongoDB Versions",
                " and  Versions"
            ],
            "paragraphs": "The   is compatible with the following   and OpenShift\nversions. Unless otherwise noted, each   version listed\nspans the full release series starting from the listed version. Versions in  italics  are deprecated.  Release Series  Version OpenShift Version 1.6.x 1.16, 1.17, 1.18, 1.19 3.11, 4.2, 4.3, 4.4, 4.5 1.7.x 1.16, 1.17, 1.18, 1.19 3.11, 4.2, 4.3, 4.4, 4.5 1.8.x 1.18, 1.19, 1.20 4.3, 4.4, 4.5 1.9.x 1.18, 1.19, 1.20 4.3, 4.4, 4.5, 4.6 1.10.x 1.19, 1.20, 1.21 4.6, 4.7 1.11.x 1.19, 1.20, 1.21 4.6, 4.7 The   is compatible with different versions of MongoDB\ndepending on the base image of the MongoDB database resource. To learn\nwhich MongoDB versions your base image supports, see\n prod-notes-supported-platforms  in the MongoDB Manual. Unless\notherwise noted, each   version listed spans the full\nrelease series starting from the listed version.  Release Series Base Image 1.6.x Ubuntu 16.04, Red Hat UBI 7 Base Image  1 1.7.x Ubuntu 16.04, Red Hat UBI 8 Base Image  1 1.8.x Ubuntu 16.04, Red Hat UBI 8 Base Image  1 1.9.x Ubuntu 16.04, Red Hat UBI 8 Base Image  1 1.10.x Ubuntu 18.04, Red Hat UBI 8 Base Image  1 Most commonly used for OpenShift deployments The   is compatible with   and with the\nfollowing   versions. Unless otherwise noted, each  \nversion listed spans the full release series starting from the listed\nversion.  Release Series  Version 1.6.x 4.0, 4.2, 4.4 1.7.x 4.2, 4.4 1.8.x 4.2, 4.4 1.9.x 4.2, 4.4 1.10.x 4.2, 4.4 1.11.x 4.2, 4.4",
            "code": [],
            "preview": "The  is compatible with the following  and OpenShift\nversions. Unless otherwise noted, each  version listed\nspans the full release series starting from the listed version.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/upgrade-k8s-operator",
            "title": "Upgrade the ",
            "headings": [
                "Change to the directory in which you cloned the repository.",
                "Upgrade the  for MongoDB deployments.",
                "Optional: Customize the   before upgrading it.",
                "If all of the following are true, provide the name of the ConfigMap for your   with the spec.security.tls.ca setting:",
                "Upgrade to the new version of the .",
                "Change to the directory in which you cloned the repository.",
                "Upgrade the  for MongoDB deployments.",
                "If all of the following are true, provide the name of the ConfigMap for your   with the spec.security.tls.ca setting:",
                "Optional: Customize your Helm Chart before upgrading it.",
                "Upgrade the .",
                "Change to the directory in which you cloned the repository.",
                "Upgrade the  for MongoDB deployments.",
                "If all of the following are true, provide the name of the ConfigMap for your   with the spec.security.tls.ca setting:",
                "Optional: Customize your Helm Chart before upgrading it.",
                "Upgrade to the latest version of the .",
                "Change to the directory in which you cloned the repository.",
                "Upgrade the  for MongoDB deployments.",
                "Optional: Customize the   before upgrading it.",
                "If all of the following are true, provide the name of the ConfigMap for your   with the spec.security.tls.ca setting:",
                "Upgrade to the new version of the .",
                "Change to the directory in which you cloned the repository.",
                "Upgrade the  for MongoDB deployments.",
                "If all of the following are true, provide the name of the ConfigMap for your   with the spec.security.tls.ca setting:",
                "Optional: Customize your Helm Chart before upgrading it.",
                "Upgrade the .",
                "Change to the directory in which you cloned the repository.",
                "Upgrade the  for MongoDB deployments.",
                "If all of the following are true, provide the name of the ConfigMap for your   with the spec.security.tls.ca setting:",
                "Optional: Customize your Helm Chart before upgrading it.",
                "Upgrade to the latest version of the ."
            ],
            "paragraphs": "The following procedure outlines how to upgrade the  \nto its latest version. Starting in   version 1.3.0, you can only have one\nMongoDB resource per project. To learn how to migrate your project to\na single-cluster configuration, see  Migrate to One Resource per Project (Required for Version 1.3.0) . The following steps depend on how your environment is configured: Invoke the following   command: You might need to add one or more optional settings. To learn about optional   installation settings,\nsee  Operator kubectl and oc Installation Settings . Use the following command to add the\n spec.security.tls.ca  setting to your   resource\ndefinition: You secure your   deployment using   certificates. You sign your   certificates using a custom  . You want to upgrade the   from a version earlier than\n1.7.1 to version 1.7.1 or later. Invoke the following   command: To resolve this error: You might receive the following error when you upgrade the\n : Remove the old   deployment. Removing the   deployment doesn\u2019t affect the lifecycle\nof your MongoDB resources. Repeat the  kubectl apply  command to upgrade to the new\nversion of the  . Invoke the following   command: Use the following command to add the\n spec.security.tls.ca  setting to your   resource\ndefinition: You secure your   deployment using   certificates. You sign your   certificates using a custom  . You want to upgrade the   from a version earlier than\n1.7.1 to version 1.7.1 or later. To learn about optional   installation settings, see\n Operator Helm Installation Settings . Invoke the following  helm  command: To resolve this error: You might receive the following error when you upgrade the\n : Remove the old   deployment. Removing the   deployment doesn\u2019t affect the lifecycle\nof your MongoDB resources. Repeat the  helm  command to upgrade to the new version of the  . To upgrade the   on a host not connected to the\nInternet: Invoke the following   command: Use the following command to add the\n spec.security.tls.ca  setting to your   resource\ndefinition: You secure your   deployment using   certificates. You sign your   certificates using a custom  . You want to upgrade the   from a version earlier than\n1.7.1 to version 1.7.1 or later. To learn about optional   installation settings, see\n Operator Helm Installation Settings . Invoke the following  helm  command to upgrade with modified pull\npolicy values: To resolve this error: You might receive the following error when you upgrade the\n : Remove the old   deployment. Removing the   deployment doesn\u2019t affect the lifecycle\nof your MongoDB resources. Repeat the  helm  command to upgrade to the new version of the  . Invoke the following   command: If you run OpenShift 3.11 or earlier, you must first manually edit the   to remove subresources. In each  , remove the\nfollowing option: You must add your  <openshift-pull-secret>  to the\n ServiceAccount  definitions: You might need to add one or more optional settings. To learn about optional   installation settings,\nsee  Operator kubectl and oc Installation Settings . Use the following command to add the\n spec.security.tls.ca  setting to your   resource\ndefinition: You secure your   deployment using   certificates. You sign your   certificates using a custom  . You want to upgrade the   from a version earlier than\n1.7.1 to version 1.7.1 or later. Invoke the following   command: To resolve this error: You might receive the following error when you upgrade the\n : Remove the old   deployment. Removing the   deployment doesn\u2019t affect the lifecycle\nof your MongoDB resources. Repeat the  oc apply  command to upgrade to the new\nversion of the  . Invoke the following   command: If you run OpenShift 3.11 or earlier, you must first manually edit the   to remove subresources. In each  , remove the\nfollowing option: Use the following command to add the\n spec.security.tls.ca  setting to your   resource\ndefinition: You secure your   deployment using   certificates. You sign your   certificates using a custom  . You want to upgrade the   from a version earlier than\n1.7.1 to version 1.7.1 or later. To learn about optional   installation settings, see\n Operator Helm Installation Settings . Invoke the following  helm  command: To resolve this error: You might receive the following error when you upgrade the\n : Remove the old   deployment. Removing the   deployment doesn\u2019t affect the lifecycle\nof your MongoDB resources. Repeat the  helm  command to upgrade to the new version of the  . To upgrade the   on a host not connected to the\nInternet: Invoke the following   command: If you run OpenShift 3.11 or earlier, you must first manually edit the   to remove subresources. In each  , remove the\nfollowing option: Use the following command to add the\n spec.security.tls.ca  setting to your   resource\ndefinition: You secure your   deployment using   certificates. You sign your   certificates using a custom  . You want to upgrade the   from a version earlier than\n1.7.1 to version 1.7.1 or later. To learn about optional   installation settings, see\n Operator Helm Installation Settings . Invoke the following  helm  command with\nmodified pull policy values: To resolve this error: You might receive the following error when you upgrade the\n : Remove the old   deployment. Removing the   deployment doesn\u2019t affect the lifecycle\nof your MongoDB resources. Repeat the  helm  command to upgrade to the new version of the  . To troubleshoot your  , see\n Review Logs from the  . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl replace -f crds.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl patch opsmanagers/<opsmgr-resource-metadata.name> \\\n    --patch '{\"spec\": {\"security\": {\"tls\": {\"ca\": \"<ca-configmap>\"}}}}' \\\n    --type merge"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f mongodb-enterprise.yaml"
                },
                {
                    "lang": "sh",
                    "value": "Forbidden: updates to statefulset spec for fields other than\n'replicas', 'template', and 'updateStrategy' are forbidden"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete deployment/mongodb-enterprise-operator --namespace <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl replace -f crds.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl patch opsmanagers/<opsmgr-resource-metadata.name> \\\n    --patch '{\"spec\": {\"security\": {\"tls\": {\"ca\": \"<ca-configmap>\"}}}}' \\\n    --type merge"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade <chart-name> helm_chart \\\n     --values helm_chart/values.yaml"
                },
                {
                    "lang": "sh",
                    "value": "Error: UPGRADE FAILED: cannot patch \"mongodb-enterprise-operator\"\nwith kind Deployment: Deployment.apps \"mongodb-enterprise-operator\"\nis invalid: ... field is immutable"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete deployment/mongodb-enterprise-operator --namespace <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl replace -f crds.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl patch opsmanagers/<opsmgr-resource-metadata.name> \\\n    --patch '{\"spec\": {\"security\": {\"tls\": {\"ca\": \"<ca-configmap>\"}}}}' \\\n    --type merge"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade <chart-name> helm_chart \\\n     --values helm_chart/values.yaml \\\n     --set registry.pullPolicy=IfNotPresent"
                },
                {
                    "lang": "sh",
                    "value": "Error: UPGRADE FAILED: cannot patch \"mongodb-enterprise-operator\"\nwith kind Deployment: Deployment.apps \"mongodb-enterprise-operator\"\nis invalid: ... field is immutable"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete deployment/mongodb-enterprise-operator --namespace <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "oc replace -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  subresources:\n    status: {}\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-appdb\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-database-pods\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n"
                },
                {
                    "lang": "none",
                    "value": "oc patch opsmanagers/<opsmgr-resource-metadata.name> \\\n    --patch '{\"spec\": {\"security\": {\"tls\": {\"ca\": \"<ca-configmap>\"}}}}' \\\n    --type merge"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f mongodb-enterprise-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "Forbidden: updates to statefulset spec for fields other than\n'replicas', 'template', and 'updateStrategy' are forbidden"
                },
                {
                    "lang": "sh",
                    "value": "oc delete deployment/mongodb-enterprise-operator --namespace <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "oc replace -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  subresources:\n    status: {}\n"
                },
                {
                    "lang": "none",
                    "value": "oc patch opsmanagers/<opsmgr-resource-metadata.name> \\\n    --patch '{\"spec\": {\"security\": {\"tls\": {\"ca\": \"<ca-configmap>\"}}}}' \\\n    --type merge"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade <chart-name> helm_chart \\\n      --values helm_chart/values-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "Error: UPGRADE FAILED: cannot patch \"mongodb-enterprise-operator\"\nwith kind Deployment: Deployment.apps \"mongodb-enterprise-operator\"\nis invalid: ... field is immutable"
                },
                {
                    "lang": "sh",
                    "value": "oc delete deployment/mongodb-enterprise-operator --namespace <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "oc replace -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  subresources:\n    status: {}\n"
                },
                {
                    "lang": "none",
                    "value": "oc patch opsmanagers/<opsmgr-resource-metadata.name> \\\n    --patch '{\"spec\": {\"security\": {\"tls\": {\"ca\": \"<ca-configmap>\"}}}}' \\\n    --type merge"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade <chart-name> helm_chart \\\n     --values helm_chart/values-openshift.yaml \\\n     --set registry.pullPolicy=IfNotPresent \\\n     --set registry.imagePullSecrets=<openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "Error: UPGRADE FAILED: cannot patch \"mongodb-enterprise-operator\"\nwith kind Deployment: Deployment.apps \"mongodb-enterprise-operator\"\nis invalid: ... field is immutable"
                },
                {
                    "lang": "sh",
                    "value": "oc delete deployment/mongodb-enterprise-operator --namespace <namespace>"
                }
            ],
            "preview": "The following procedure outlines how to upgrade the \nto its latest version.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/configure-om-queryable-backups",
            "title": "Configure Queryable Backups for  Resources",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Create the PEM file for backups.",
                "Create a Secret containing the PEM file.",
                "Mount the Secret as a volume that  custom objects will use.",
                "Save your  config file.",
                "Apply changes to your  deployment.",
                "Track the status of the mounted volumes and Secrets."
            ],
            "paragraphs": "You can configure  queryable backups \nfor   resources that you deploy in the  . Queryable backups allow you to  run queries \non specific backup snapsnots from your   resources. Querying\n  backups helps you compare data from different snapshots\nand identify the best snapshot to use for  restoring data . In the following procedure you: Once the   deploys the updated configuration for the\n  custom resource,   can read the Secret from the\nspecified location in the  queryable.pem  parameter in  .\nYou can now access the backup snapshots and run queries on them. In the   documentation, queryable backups are also\nreferred to as queryable snapshots, or queryable restores. Create the  queryable.pem  file that holds the\ncertificatesfor accessing the backup snapshots that you intend to query. Create the Secret containing the  queryable.pem  file. Configure a persistent volume that is attached to the  \n  Pod in the  . Specify the mount point for the Secret in  the persistent volume's\nconfiguration. Save the   custom resource configuration and apply it. Before you configure queryable backups, complete the following: Install the Kubernetes Operator . Deploy the Ops Manager application . Configure Backup Settings for the Ops Manager Resource .\nIn the linked procedures, see the steps for configuring backups. After you configure queryable backups, you can  query them  to select the best backup snapshot to use for\n restoring data . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Create the  queryable.pem \nfile that you will use for accessing and querying backups based on\nyour deployment's   requirements. The PEM file contains a public\nkey certificate and its associated private key that are needed to\naccess and run queries on   backup snapshots. To learn more about the PEM file's requirements, see\n Authorization and Authentication Requirements . Run the following command to create a Secret with the\n queryable.pem \nfile that you created in the previous step: The   must be able to access the  queryable.pem  file in the mount point\nfor the persistent volume in the Pod's container for  . To mount the Secret, use one of these methods: Configure volumes using  volumeClaimTemplates  and specify the\nlocation for the  queryable.pem  file: Configure volumes without using  volumeClaimTemplates  and specify\nthe location for the  queryable.pem  file: Invoke the following  kubectl  command on the filename of the\n  resource definition: When you apply the changes to your   resource\ndefinition,   updates the   StatefulSet,\ncreates the volumes, and mounts the Secrets. Obtain the list of persistent volume claims: Obtain the Secrets: Check the status of your   resources: The  -w  flag means \"watch\". With the \"watch\" flag set, the\noutput refreshes immediately when the configuration changes until\nthe status phase achieves the  Running  state. To learn more about the resource deployment statuses, see\n Troubleshoot the  .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic queryable-pem --from-file=./queryable.pem"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: ops-manager\nspec:\n  replicas: 1\n  version: 4.2.12\n  adminCredentials: ops-manager-admin-secret\n  configuration:\n    mms.fromEmailAddr: \"admin@thecompany.com\"\n    brs.queryable.pem: \"/certs/queryable.pem\"\n\n  statefulSet:\n    spec:\n     # the Persistent Volume Claim is created for each Ops Manager Pod\n     volumeClaimTemplates:\n       - metadata:\n           name: queryable-volume\n         spec:\n           accessModes: [ \"ReadWriteOnce\" ]\n           storageClassName: <your_storage_class_name>\n           resources:\n             requests:\n                storage: 1G\n     template:\n       spec:\n         containers:\n           - name: mongodb-ops-manager\n           volumeMounts:\n                - name: queryable-volume\n                - mountPath: /certs\n         volumes:\n           - name: queryable-pem\n           secret:\n            secretName: queryable-pem\n\n  applicationDatabase:\n     members: 3\n     version: 4.2.6-ent"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\n kind: MongoDBOpsManager\n metadata:\n   name: ops-manager\nspec:\n   replicas: 1\n   version: 4.2.12\n   adminCredentials: ops-manager-admin-secret\n   configuration:\n     brs.queryable.pem: \"/certs/queryable.pem\"\n     mms.fromEmailAddr: \"admin@thecompany.com\"\n   statefulSet:\n     template:\n       spec:\n         containers:\n           - name: mongodb-ops-manager\n           volumeMounts:\n            - name: queryable-volume\n            - mountPath: /certs/\n\n         volumes:\n           - name: queryable-pem\n           secret:\n             secretName: queryable-pem\n\napplicationDatabase:\n  members: 3\n  version: 4.2.6-ent"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pvc"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get secrets"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om <resource-name> -o yaml -w"
                }
            ],
            "preview": "You can configure queryable backups\nfor  resources that you deploy in the .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-om-container",
            "title": "Deploy an  Resource",
            "headings": [
                "Prerequisites and Considerations",
                "Considerations for  Deployments over HTTPS",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the following example   .",
                "Open your preferred text editor and paste the  specification into a new text file.",
                "Configure the settings highlighted in the prior example.",
                "Optional: Configure Backup settings.",
                "Optional: Configure any additional settings for an  deployment.",
                "Save this file with a .yaml file extension.",
                "Create your  instance.",
                "Track the status of your  instance.",
                "Access the  application.",
                "Optional: Create credentials for the Kubernetes Operator.",
                "Optional: Create a project using a .",
                "Optional: Deploy MongoDB database resources to complete the Backup configuration.",
                "Optional: Confirm that the  resource is running.",
                "Configure kubectl to default to your namespace.",
                "Concatenate your TLS certificate and Private Key.",
                "Create a Kubernetes secret for your certificates.",
                "If necessary, validate your TLS Certificate.",
                "Copy the following example   .",
                "Open your preferred text editor and paste the  specification into a new text file.",
                "Configure the settings highlighted in the prior example.",
                "Optional: Configure Backup settings",
                "Optional: Configure any additional settings for an  deployment.",
                "Save this file with a .yaml file extension.",
                "Create your  instance.",
                "Track the status of your  instance.",
                "Access the  application.",
                "Create credentials for the Kubernetes Operator.",
                "Create a project using a .",
                "Deploy MongoDB database resources to complete the Backup configuration.",
                "Confirm that the  resource is running."
            ],
            "paragraphs": "You can deploy   in a container with the  . Before you deploy an   resource, make sure you  plan for\nyour Ops Manager resource : Complete the  Prerequisites Read the  Considerations . You can configure your deployed   resource to run over  ,\nrather than  . A full description of TLS, PKI (Public Key\nInfrastructure) certificates, and Certificate Authority is beyond the\nscope of this tutorial. This tutorial assumes prior knowledge of TLS/SSL\nas well as access to valid certificates. When running over  ,   runs on port  8443  by default. Select the appropriate tab based on whether you want your  \ninstance to run over   or  : If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the highlighted settings to match your desired\n  configuration. Key Type Description Example metadata.name string Name for this      . Resource names must be 44 characters or less. metadata.name  documentation on  names . om spec.replicas number Number of   instances to run in parallel. The minimum valid value is  1 . For high availability, set this value to more than  1 . Multiple\n  instances can read from the same Application\nDatabase, ensuring failover if one instance is unavailable and\nenabling you to update the   resource without downtime. 1 spec.version string Version of   to be installed. The format should be  X.Y.Z .\nTo view available   versions, view the\n container registry . \u200b spec.adminCredentials string Name of the   you  created \nfor the   admin user. Configure the secret to use the same   as the\n  resource. om-admin-secret string Optional . The   service  ServiceType \nthat exposes   outside of  . Exclude the\n spec. externalConnectivity  setting\nand its children if you don't want the   to\ncreate a   service to route external traffic to the\n  application. LoadBalancer integer Number of members of the  mms-application-database \nreplica set. 3 string Optional . Version of MongoDB that the  mms-application-database \nshould run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the  Enterprise edition . To learn more about MongoDB versioning, see see\n release-version-numbers  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. To deploy   inside   without an Internet connection,\nomit this setting or leave the value empty. The  \ninstalls the  bundled MongoDB Enterprise  version  4.2.11-ent  by default. 4.2.11-ent boolean Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. spec.applicationDatabase.podSpec.persistence. \n spec.podSpec.persistence.single \nis set to its default value of  16Gi . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: You must set this value to  true . If you want one   for each  , configure the\n spec.applicationDatabase. \n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: Grant your containers permission to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the resource does not fix\nissues with your  , contact MongoDB support. true If you want to enable backup, you must configure all of the following\nsettings: You must also configure an  S3 snapshot store \nor a  blockstore . To configure a snapshot store, configure the following settings: To configure a blockstore, configure the following settings: Key Type Description Example boolean Flag that indicates that Backup is enabled. You must specify\n spec.backup.enabled: true  to configure settings\nfor the head database, oplog store, and snapshot store. true string Name of the oplog store. oplog1 string Name of the MongoDB database resource for the oplog store. my-oplog-db If you deploy both an    snapshot store \nand a  blockstore ,  \nrandomly choses one to use for Backup. Key Type Description Example string Name of the   snapshot store. s3store1 string Name of the   that contains the  accessKey  and\n secretKey  fields. The  backup-daemon  uses the\nvalues of these fields as credentials to access the   or\n -compatible bucket. my-s3-credentials string  of the   or  -compatible bucket that\n stores  the\ndatabase Backup snapshots. s3.us-east-1.amazonaws.com string Name of the   or  -compatible bucket that stores the\ndatabase Backup snapshots. my-bucket Key Type Description Example string Name of the blockstore. blockStore1 string Name of the MongoDB database resource that you create for the\nblockstore. You must deploy this database resource in the same\nnamespace as the   resource. my-mongodb-blockstore Add any  optional settings  that you\nwant to apply to your deployment to the   specification file. Invoke the following  kubectl  command on the filename of the\n  resource definition: To check the status of your   resource, invoke the following\ncommand: The command returns the following output under the  status  field\nwhile the resource deploys: The   reconciles the resources in the following order: The   doesn't reconcile a resource until the preceding\none enters the  Running  phase. After the   resource completes the  Reconciling  phase, the\ncommand returns the following output under the  status  field if you\nenabled backup: Backup remains in a  Pending  state until you configure the Backup\ndatabases. Application Database. . Backup. The  status.opsManager.url  field states the resource's\nconnection  . Using this  , you can reach   from\ninside the   cluster or  create a project using a\nConfigMap . The steps you take differ based on how you are routing traffic to the\n  application in  . If you configured the   to\ncreate a   service for you, or you created a   service\nmanually, use one of the following methods to access the  \napplication: To learn how to access the   application using a third-party\nservice, refer to the documentation for your solution. Query your cloud provider to get the   of the load\nbalancer service. See your cloud provider's documentation\nfor details. Open a browser window and navigate to the  \napplication using the   and port number of your load\nbalancer service. Log in to   using the  admin user credentials . Set your firewall rules to allow access from the Internet to\nthe  spec.externalConnectivity. port \non the host on which your   cluster is running. Open a browser window and navigate to the  \napplication using the   and the\n spec.externalConnectivity. port . Log in to   using the  admin user credentials . If you enabled Backup, you must create an   organization,\ngenerate programmatic API keys, and create a  . These\nactivities follow the prerequisites and procedure on the\n Create Credentials for the   page. If you enabled Backup, create a project by following the prerequisites\nand procedure on the  Create One Project using a ConfigMap  page. You must set  data.baseUrl  in the ConfigMap to the  's  . To find this  , invoke the following command: The command returns the URL of the   in the\n status.opsManager.url  field. If you deploy   with the   and   will\nmanage MongoDB database resources deployed  outside  of the  \ncluster it's deployed to, you must set  data.baseUrl  to the same\nvalue of the\n spec.configuration.mms.centralUrl \nsetting in the   resource specification. If you enabled  mms-backup-functional-overview ,\ncreate a MongoDB database resource for the oplog and snapshot stores\nto complete the configuration. Deploy a  MongoDB database resource  for the oplog store in the same\nnamespace as the   resource. Match the  metadata.name  of the resource with the\n spec.backup.opLogStores.mongodbResourceRef.name  that you specified\nin your   resource definition. Create this database as a  replica set . Choose one of the following: Deploy a  MongoDB database resource  for the blockstore in the\nsame namespace as the   resource. Match the  metadata.name  of the resource to the\n spec.backup.blockStores.mongodbResourceRef.name \nthat you specified in your   resource definition. Configure an   bucket to use as the   snapshot store. Ensure that you can access the   bucket using the details\nthat you specified in your   resource definition. If you enabled backup, check the status of your   resource by\ninvoking the following command: When   is running, the command returns the following\noutput under the  status  field: See  Troubleshoot the   for information about the\nresource deployment statuses. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : If your   certificate and Private Key are separate files, run the\nfollowing command to concatenate them: The   requires that the   instance's  \ncertificate and Private Key are concatenated into a single file\ncalled  server.pem . Once you have your   certificate and Private Key in a file called\n server.pem , run the following command to store the certificates\nin a  : If your   certificate is signed by a Custom Certificate\nAuthority, you must provide a  CA (Certificate Authority) \ncertificate to validate the   certificate. To validate the\n  certificate, create a   to hold the\n CA (Certificate Authority)  certificate: You must concatenate your custom   file and the entire\n  certificate chain from  downloads.mongodb.com  to prevent\n  from becoming inoperable if the application database\nrestarts. The   requires that the certificate is named\n mms-ca.crt  in the ConfigMap. Obtain the entire   certificate chain from\n downloads.mongodb.com . The following  openssl  command\noutputs each certificate in the chain to your current working\ndirectory, in  .crt  format: Concatenate your  's certificate file with the\nentire   certificate chain from  downloads.mongodb.com  that\nyou obtained in the previous step: Create the  : Change the highlighted settings to match your desired\n  configuration. Key Type Description Example metadata.name string Name for this      . Resource names must be 44 characters or less. metadata.name  documentation on  names . om spec.replicas number Number of   instances to run in parallel. The minimum valid value is  1 . For high availability, set this value to more than  1 . Multiple\n  instances can read from the same Application\nDatabase, ensuring failover if one instance is unavailable and\nenabling you to update the   resource without downtime. 1 spec.version string Version of   to be installed. The format should be  X.Y.Z .\nTo view available   versions, view the\n container registry . \u200b spec.adminCredentials string Name of the   you  created \nfor the   admin user. Configure the secret to use the same   as the\n  resource. om-admin-secret string Name of the   you created to verify  \ncertificates signed using a Custom Certificate Authority. This field is required if you signed your  \ncertificates using a Custom Certificate Authority. om-http-cert-ca string Name of of the   you created for the  \ncertificate. om-http-cert string The   service  ServiceType \nthat exposes   outside of  . Exclude the\n spec. externalConnectivity  setting\nand its children if you don't want the   to\ncreate a   service to route external traffic to the\n  application. LoadBalancer integer Number of members of the  mms-application-database \nreplica set. 3 string Optional . Version of MongoDB that the  mms-application-database \nshould run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the  Enterprise edition . To learn more about MongoDB versioning, see see\n release-version-numbers  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. To deploy   inside   without an Internet connection,\nomit this setting or leave the value empty. The  \ninstalls the  bundled MongoDB Enterprise  version  4.2.11-ent  by default. 4.2.11-ent boolean Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. spec.applicationDatabase.podSpec.persistence. \n spec.podSpec.persistence.single \nis set to its default value of  16Gi . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: You must set this value to  true . If you want one   for each  , configure the\n spec.applicationDatabase. \n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: Grant your containers permission to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the resource does not fix\nissues with your  , contact MongoDB support. true If you want to enable backup for your   instance, you must\nconfigure all of the following settings: You must also configure an  S3 snapshot store \nor a  blockstore . To configure a snapshot store, configure the following settings: To configure a blockstore, configure the following settings: Key Type Description Example boolean Flag that indicates that Backup is enabled for your You must\nspecify  spec.backup.enabled: true  to configure settings\nfor the head database, oplog store, and snapshot store. true string Name of the oplog store. oplog1 string Name of the MongoDB database resource for the oplog store. my-oplog-db If you deploy both an    snapshot store \nand a  blockstore ,  \nrandomly choses one to use for Backup. Key Type Description Example string Name of the   snapshot store. s3store1 string Name of the   that contains the  accessKey  and\n secretKey  fields. The  backup-daemon  uses the\nvalues of these fields as credentials to access the   or\n -compatible bucket. my-s3-credentials string  of the   or  -compatible bucket that\n stores  the\ndatabase Backup snapshots. s3.us-east-1.amazonaws.com string Name of the   or  -compatible bucket that stores the\ndatabase Backup snapshots. my-bucket Key Type Description Example string Name of the blockstore. blockStore1 string Name of the MongoDB database resource that you create for the\nblockstore. You must deploy this database resource in the same\nnamespace as the   resource. my-mongodb-blockstore Add any  optional settings  that you\nwant to apply to your deployment to the   specification file. Invoke the following  kubectl  command on the filename of the\n  resource definition: To check the status of your   resource, invoke the following\ncommand: The command returns the following output under the  status  field\nwhile the resource deploys: The   reconciles the resources in the following order: The   doesn't reconcile a resource until the preceding\none enters the  Running  phase. After the   resource completes the  Reconciling  phase, the\ncommand returns the following output under the  status  field if you\nenabled backup: Backup remains in a  Pending  state until you configure the Backup\ndatabases. After the resource completes the  Reconciling  phase, the command\nreturns the following output under the  status  field: Backup remains in a  Pending  state until you configure the Backup\ndatabases. Application Database. . Backup. The  status.opsManager.url  field states the resource's\nconnection  . Using this  , you can reach   from\ninside the   cluster or  create a project using a\nConfigMap . The  status.opsManager.url  field states the resource's\nconnection  . Using this  , you can reach   from\ninside the   cluster or  create a project using a\nConfigMap . The steps you take differ based on how you are routing traffic to the\n  application in  . If you configured the   to\ncreate a   service for you, or you created a   service\nmanually, use one of the following methods to access the  \napplication: To learn how to access the   application using a third-party\nservice, refer to the documentation for your solution. Query your cloud provider to get the   of the load\nbalancer service. See your cloud provider's documentation\nfor details. Open a browser window and navigate to the  \napplication using the   and port number of your load\nbalancer service. Log in to   using the  admin user credentials . Set your firewall rules to allow access from the Internet to\nthe  spec.externalConnectivity. port \non the host on which your   cluster is running. Open a browser window and navigate to the  \napplication using the   and the\n spec.externalConnectivity. port . Log in to   using the  admin user credentials . To configure credentials, you must create an   organization,\ngenerate programmatic API keys, and create a  . These\nactivities follow the prerequisites and procedure on the\n Create Credentials for the   page. To create a project, follow the prerequisites and procedure on the\n Create One Project using a ConfigMap  page. Set the following fields in your project ConfigMap: Set  data.baseUrl  in the ConfigMap to the  's  .\nTo find this  , invoke the following command: The command returns the URL of the   in the\n status.opsManager.url  field. If you deploy   with the   and   will\nmanage MongoDB database resources deployed  outside  of the  \ncluster it's deployed to, you must set  data.baseUrl  to the same\nvalue of the\n spec.configuration.mms.centralUrl \nsetting in the   resource specification. Set  data.sslMMSCAConfigMap  to the name of your\n  containing the root  CA (Certificate\nAuthority)  certificate used to sign the   host's\ncertificate. The   requires this name to be\n mms-ca.crt . By default,   enables  mms-backup-functional-overview .\nCreate a MongoDB database resource for the oplog and snapshot stores\nto complete the configuration. Deploy a  MongoDB database resource  for the oplog store in the same\nnamespace as the   resource. Match the  metadata.name  of the resource with the\n spec.backup.opLogStores.mongodbResourceRef.name  that you specified\nin your   resource definition. Create this database as a three-member  replica set . Deploy a  MongoDB database resource  for the   snapshot store in the\nsame namespace as the   resource. Match the  metadata.name  of the resource to the\n spec.backup.s3Stores.mongodbResourceRef.name \nthat you specified in your   resource definition. Create the   snapshot store as a replica set. To check the status of your   resource, invoke the following\ncommand: When   is running, the command returns the following\noutput under the  status  field: See  Troubleshoot the   for information about the\nresource deployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: <myopsmanager>\nspec:\n  replicas: 1\n  version: <opsmanagerversion>\n  adminCredentials: <adminusercredentials> # Should match metadata.name\n                                           # in the Kubernetes secret\n                                           # for the admin user\n  externalConnectivity:\n    type: LoadBalancer\n\n  applicationDatabase:\n    members: 3\n    version: <mongodbversion>\n    persistent: true\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n applicationDatabase:\n  lastTransition: \"2020-04-01T09:49:22Z\"\n  message: AppDB Statefulset is not ready yet\n  phase: Reconciling\n  type: \"\"\n  version: \"\"\n backup:\n  phase: \"\"\n opsManager:\n  phase: \"\""
                },
                {
                    "lang": "yaml",
                    "value": " status:\n   applicationDatabase:\n     lastTransition: \"2020-04-01T09:50:20Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: \"4.2.11-ent\"\n  backup:\n   lastTransition: \"2020-04-01T09:57:42Z\"\n   message: The MongoDB object <namespace>/<oplogresourcename>\n     doesn't exist\n   phase: Pending\n   opsManager:\n     lastTransition: \"2020-04-01T09:57:40Z\"\n     phase: Running\n     replicas: 1\n     url: http://om-svc.cloudqa.svc.cluster.local:8080\n     version: \"4.2.8\""
                },
                {
                    "lang": "sh",
                    "value": "http://ops.example.com:8080"
                },
                {
                    "lang": "sh",
                    "value": "http://ops.example.com:30036"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": " status:\n   applicationDatabase:\n     lastTransition: \"2020-04-01T10:00:32Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: \"4.2.11-ent\"\n  backup:\n   lastTransition: \"2020-04-01T09:57:42Z\"\n   message: The MongoDB object <namespace>/<oplogresourcename>\n     doesn't exist\n   phase: Pending\n   opsManager:\n     lastTransition: \"2020-04-01T09:57:40Z\"\n     phase: Running\n     replicas: 1\n     url: http://om-svc.cloudqa.svc.cluster.local:8080\n     version: \"4.2.8\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2020-04-01T10:00:32Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: \"4.2.11-ent\"\n  backup:\n    lastTransition: \"2020-04-01T10:00:53Z\"\n    phase: Running\n    version: \"4.2.8\"\n  opsManager:\n    lastTransition: \"2020-04-01T10:00:34Z\"\n    phase: Running\n    replicas: 1\n    url: http://om-svc.cloudqa.svc.cluster.local:8080\n    version: \"4.2.8\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "cat <private-key>.key <tls-certificate>.crt > server.pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic om-http-cert --from-file=\"server.pem\" -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "openssl s_client -showcerts -verify 2 \\\n-connect downloads.mongodb.com:443 -servername downloads.mongodb.com < /dev/null \\\n| awk '/BEGIN/,/END/{ if(/BEGIN/){a++}; out=\"cert\"a\".crt\"; print >out}'"
                },
                {
                    "lang": "sh",
                    "value": "cat cert1.crt cert2.crt cert3.crt cert4.crt  >> ca-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap om-http-cert-ca --from-file=\"mms-ca.crt\""
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: <myopsmanager>\nspec:\n  replicas: 1\n  version: <opsmanagerversion>\n  adminCredentials: <adminusercredentials> # Should match metadata.name\n                                           # in the Kubernetes secret\n                                           # for the admin user\n  security:\n    tls:\n      ca: <om-http-cert-ca>\n      secretRef:\n        name: <tlscertificate> # Should match metadata.name\n                               # in the Kubernetes secret\n                               # for the TLS Certificate / Private Key\n  externalConnectivity:\n    type: LoadBalancer\n\n  applicationDatabase:\n    members: 3\n    version: <mongodbversion>\n    persistent: true\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n applicationDatabase:\n  lastTransition: \"2020-04-01T09:49:22Z\"\n  message: AppDB Statefulset is not ready yet\n  phase: Reconciling\n  type: \"\"\n  version: \"\"\n backup:\n  phase: \"\"\n opsManager:\n  phase: \"\""
                },
                {
                    "lang": "yaml",
                    "value": " status:\n   applicationDatabase:\n     lastTransition: \"2020-04-01T09:50:20Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: \"4.2.0\"\n  backup:\n   lastTransition: \"2020-04-01T09:57:42Z\"\n   message: The MongoDB object <namespace>/<oplogresourcename>\n     doesn't exist\n   phase: Pending\n   opsManager:\n     lastTransition: \"2020-04-01T09:57:40Z\"\n     phase: Running\n     replicas: 1\n     url: http://om-svc.cloudqa.svc.cluster.local:8443\n     version: \"4.2.8\""
                },
                {
                    "lang": "yaml",
                    "value": " status:\n   applicationDatabase:\n     lastTransition: \"2019-12-06T18:23:22Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: \"4.2.11-ent\"\n   opsManager:\n     lastTransition: \"2019-12-06T18:23:26Z\"\n     message: The MongoDB object namespace/oplogdbname doesn't exist\n     phase: Pending\n     url: http://om-svc.dev.svc.cluster.local:8443\n     version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "http://ops.example.com:8443"
                },
                {
                    "lang": "sh",
                    "value": "http://ops.example.com:30036"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "  status:\n    applicationDatabase:\n      lastTransition: \"2019-12-06T18:23:22Z\"\n      members: 3\n      phase: Running\n      type: ReplicaSet\n      version: \"4.2.11-ent\"\n    opsManager:\n      lastTransition: \"2019-12-06T18:23:26Z\"\n      message: The MongoDB object namespace/oplogdbname doesn't exist\n      phase: Pending\n      url: http://om-svc.dev.svc.cluster.local:8443\n      version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2019-12-06T17:46:15Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: \"4.2.11-ent\"\n  opsManager:\n    lastTransition: \"2019-12-06T17:46:32Z\"\n    phase: Running\n    replicas: 1\n    url: http://om-backup-svc.dev.svc.cluster.local:8443\n    version: \"4.2.6\""
                }
            ],
            "preview": "You can deploy  in a container with the .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/connect-from-inside-k8s",
            "title": "Connect to a MongoDB Database Resource from Inside Kubernetes",
            "headings": [
                "Considerations",
                "Procedure",
                "Open the Topology view for your deployment.",
                "Click  for the deployment to which you want to connect.",
                "Click Connect to this instance.",
                "Copy the connection command displayed in the Connect to your Deployment dialog.",
                "Run the connection command in a terminal to connect to the deployment."
            ],
            "paragraphs": "The following procedure describes how to connect to a MongoDB resource\ndeployed by   from inside of the   cluster. You must be able to connect to the host and port where you deployed your\n  resource. To learn more about connecting to your deployment, see\n Connect to a MongoDB Process . Perform the following steps in the   or\n Cloud Manager \napplication, depending on where your clusters are hosted: When connecting to a resource from inside of  , the\nhostname to which you connect has the following form: Click  Deployment  in the left navigation. To connect to a sharded cluster resource named\n shardedcluster , you might use the following connection\nstring:",
            "code": [
                {
                    "lang": "sh",
                    "value": "<k8s-pod-name>.<k8s-internal-service-name>.<k8s-namespace>.<cluster-name>"
                },
                {
                    "lang": "none",
                    "value": "mongo --host shardedcluster-mongos-0.shardedcluster-svc.mongodb.svc.cluster.local --port 27017"
                }
            ],
            "preview": "The following procedure describes how to connect to a MongoDB resource\ndeployed by  from inside of the  cluster.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-x509-auth",
            "title": "Secure Client Authentication with X.509",
            "headings": [
                "General Prerequisites",
                "Configure X.509 Client Authentication for a Replica Set",
                "Prerequisites",
                "Create X.509 Certificates for a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Create the  for your TLS certificates.",
                "Create the  for your agents' X.509 certificates.",
                "Create the  to link your  with your deployment.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the TLS settings for your replica set resource using a Custom Certificate Authority.",
                "Configure the general X.509 settings for your replica set resource.",
                "Save your replica set config file.",
                "Apply your changes to your replica set deployment.",
                "Track the status of your deployment.",
                "Renew X.509 Certificates for a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Renew the  for your TLS certificates.",
                "Renew the  for your agents' X.509 certificates.",
                "Restart the Pods in Your Deployment.",
                "Track the status of your deployment.",
                "Configure X.509 Client Authentication for a Sharded Cluster",
                "Prerequisites",
                "Create X.509 Certificates for a Sharded Cluster",
                "Configure kubectl to default to your namespace.",
                "Create the  for your Shards' TLS certificates.",
                "Create the  for your config server's TLS certificates.",
                "Create the  for your mongos server's TLS certificates.",
                "Create the  for your agents' X.509 certificates.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the TLS settings for your sharded cluster resource using a Custom Certificate Authority.",
                "Configure the general X.509 settings for your sharded cluster resource.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Track the status of your deployment.",
                "Renew X.509 Certificates for a Sharded Cluster",
                "Configure kubectl to default to your namespace.",
                "Renew the  for your Shards' TLS certificates.",
                "Renew the  for your config server's TLS certificates.",
                "Renew the  for your mongos server's TLS certificates.",
                "Renew the  for your agents' X.509 certificates.",
                "Restart the Pods in Your Deployment.",
                "Track the status of your deployment."
            ],
            "paragraphs": "The   can use X.509 certificates to authenticate your\nclient applications to your MongoDB deployments. This guide instructs you on how to configure: X.509 authentication from clients to your MongoDB instances.  to encrypt connections between MongoDB hosts in a replica set\nor sharded cluster.  to encrypt connections client applications and MongoDB\ndeployments. Automatically generating   certificates with the  \nis deprecated and will be removed in a future release. You must provide certificates from your own CA, as described in the\nfollowing procedures, for production environments. Before you secure your MongoDB deployment using   encryption,\ncomplete the following: Install the Kubernetes Operator Create Credentials for the Kubernetes Operator Enabling X.509 authentication at the project level configures all\nagents to use X.509 client authentication when communicating with\nMongoDB deployments. X.509 client authentication requires one of the following:  4.1.7 or later  4.0.11 or later Before you secure your replica set using   encryption, complete the\nfollowing: Deploy the Replica Set \nthat you want to secure Create a   file for each of the following components: For the Agent PEM files, ensure that: PEM file purpose Save File As... Your custom  ca-pem Each member of your replica set <metadata.name>-<X>-pem Your project's Automation or MongoDB Agent mms-automation-agent-pem Your project's Backup Agent (if needed) mms-backup-agent-pem Your project's Monitoring Agent (if needed) mms-monitoring-agent-pem the Common Name in each   certificate is not empty, and the combined Organization and Organizational Unit in each  \ncertificate differs from the combined Organization and\nOrganizational Unit in the   certificates for your\nreplica set members. To create the   file, concatenate the   certificate and the\nPrivate Key. An example of a   file would resemble: Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. Each certificate should include a valid Domain Name. For each replica set or sharded cluster member, the Common Name, also\nknown as the Domain Name, for that member's certificate must match\nthe   of the POD on which this cluster member\nis deployed. The   name in each certificate has the following syntax:\n pod-name.service-name.namespace.svc.cluster.local . This name is\ndifferent for each Pod hosting a member of the replica set or a\nsharded cluster. For example, for a member of a replica set deployed on a Pod with\nthe name  rs-mongos-0-0 , in the   service\nnamed  mongo-0  that is created in the default  mongodb \nnamespace, the   is: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create a new   that stores\nthe replica set's certificates: This example covers a three-member replica set. If you have more than\nthree members, you can add them to the certificate using the\n --from-file  option. Run this  kubectl  command to create a new   that stores\nthe agents' X.509 certificates: Run this  kubectl  command to link your   to your replica\nset: Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Required If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Required Add the  's name that stores the custom  \nthat you used to sign your deployment's   certificates. <custom-ca> To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Required Set this value to  true  to enable authentication on the\nMongoDB deployment. true array Conditional Set this value to  [\"X509\"] . [\"X509\"] Invoke the following   command to update your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. If you have already created certificates, we recommend that you renew\nthem periodically using the following procedure. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to renew an existing   that\nstores the replica set's certificates: This example covers a three-member replica set. If you have more than\nthree members, you can add them to the certificate using the\n --from-file  option. Run this  kubectl  command to renew an existing   that\nstores the agents' X.509 certificates: Run this  kubectl  command to force a  \nof the StatefulSets to restart the Pods. The Pods restart and begin watching the renewed secrets. To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. Before you secure your replica set using   encryption, complete the\nfollowing: Deploy the Sharded Cluster \nthat you want to secure Create a   file for each of the following components: For the Agent PEM files, ensure that: PEM file purpose Save File As... Your custom  ca-pem Each shard in your sharded cluster <metadata.name>-<Y>-<X>-pem Each member of your config server replica set <metadata.name>-config-<X>-pem Each  <metadata.name>-mongos-<X>-pem Your project's Automation or MongoDB Agent mms-automation-agent-pem Your project's Backup Agent (if needed) mms-backup-agent-pem Your project's Monitoring Agent (if needed) mms-monitoring-agent-pem the Common Name in each   certificate is not empty, and the combined Organization and Organizational Unit in each  \ncertificate differs from the combined Organization and\nOrganizational Unit in the   certificates for your\nsharded cluster members, config server members, and each  . To create the   file, concatenate the   certificate and the\nPrivate Key. An example of a   file would resemble: Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. Each certificate should include a valid Domain Name. For each replica set or sharded cluster member, the Common Name, also\nknown as the Domain Name, for that member's certificate must match\nthe   of the POD on which this cluster member\nis deployed. The   name in each certificate has the following syntax:\n pod-name.service-name.namespace.svc.cluster.local . This name is\ndifferent for each Pod hosting a member of the replica set or a\nsharded cluster. For example, for a member of a replica set deployed on a Pod with\nthe name  rs-mongos-0-0 , in the   service\nnamed  mongo-0  that is created in the default  mongodb \nnamespace, the   is: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create a new   that stores\nthe sharded cluster shards' certificates: This example covers a two-shard sharded cluster with five members per\nshard. If you have more than two shards or five members per shard,\nyou can add them to the certificate using the  --from-file  option. Run this  kubectl  command to create a new   that stores\nthe sharded cluster config server's certificates: Run this  kubectl  command to create a new   that stores\nthe sharded cluster   certificates: Run this  kubectl  command to create a new   that stores\nthe agents' X.509 certificates: Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Required If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Required Add the  's name that stores the custom  \nthat you used to sign your deployment's   certificates. <custom-ca> To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Required Set this value to  true  to enable authentication on the\nMongoDB deployment. true array Conditional Set this value to  [\"X509\"] . [\"X509\"] Invoke the following   command to update and restart your\n sharded cluster : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses. If you have already created certificates, we recommend that you renew\nthem periodically using the following procedure. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to renew an existing   that\nstores the sharded cluster shards' certificates: This example covers a two-shard sharded cluster with five members per\nshard. If you have more than two shards or five members per shard,\nyou can add them to the certificate using the  --from-file  option. Run this  kubectl  command to renew an existing   that\nstores the sharded cluster config server's certificates: Run this  kubectl  command to renew an existing   that\nstores the sharded cluster   certificates: Run this  kubectl  command to renew an existing   that\nstores the agents' X.509 certificates: Run this  kubectl  command to force a  \nof the StatefulSets to restart the Pods. The Pods restart and begin watching the renewed secrets. To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when the configuration changes until the status phase\nachieves the  Running  state. See  Troubleshoot the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "text",
                    "value": "-----BEGIN CERTIFICATE-----\n...\n... your TLS certificate\n...\n-----END CERTIFICATE-----\n-----BEGIN RSA PRIVATE KEY-----\n...\n... your private key\n...\n-----END RSA PRIVATE KEY----"
                },
                {
                    "lang": "sh",
                    "value": "rs-mongos-0-0.mongo-0.mongodb.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <metadata.name>-cert \\\n  --from-file=<metadata.name>-0-pem \\\n  --from-file=<metadata.name>-1-pem \\\n  --from-file=<metadata.name>-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic agent-certs \\\n  --from-file=mms-automation-agent-pem \\\n  --from-file=mms-backup-agent-pem \\\n  --from-file=mms-monitoring-agent-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap custom-ca --from-file=ca-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <metadata.name>-cert \\\n  --from-file=<metadata.name>-0-pem \\\n  --from-file=<metadata.name>-1-pem \\\n  --from-file=<metadata.name>-2-pem \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic agent-certs \\\n  --from-file=mms-automation-agent-pem \\\n  --from-file=mms-backup-agent-pem \\\n  --from-file=mms-monitoring-agent-pem \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl rollout restart sts <name-of-the-resource>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "text",
                    "value": "-----BEGIN CERTIFICATE-----\n...\n... your TLS certificate\n...\n-----END CERTIFICATE-----\n-----BEGIN RSA PRIVATE KEY-----\n...\n... your private key\n...\n-----END RSA PRIVATE KEY----"
                },
                {
                    "lang": "sh",
                    "value": "rs-mongos-0-0.mongo-0.mongodb.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-0-cert \\\n       --from-file=<metadata.name>-0-0-pem \\\n       --from-file=<metadata.name>-0-1-pem \\\n       --from-file=<metadata.name>-0-2-pem \\\n       --from-file=<metadata.name>-0-3-pem \\\n       --from-file=<metadata.name>-0-4-pem\n\nkubectl -n mongodb create secret generic <metadata.name>-1-cert \\\n       --from-file=<metadata.name>-1-0-pem \\\n       --from-file=<metadata.name>-1-1-pem \\\n       --from-file=<metadata.name>-1-2-pem \\\n       --from-file=<metadata.name>-1-3-pem \\\n       --from-file=<metadata.name>-1-4-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-config-cert \\\n       --from-file=<metadata.name>-config-0-pem \\\n       --from-file=<metadata.name>-config-1-pem \\\n       --from-file=<metadata.name>-config-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-mongos-cert \\\n       --from-file=<metadata.name>-mongos-0-pem \\\n       --from-file=<metadata.name>-mongos-1-pem \\\n       --from-file=<metadata.name>-mongos-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic agent-certs \\\n  --from-file=mms-automation-agent-pem \\\n  --from-file=mms-backup-agent-pem \\\n  --from-file=mms-monitoring-agent-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-0-cert \\\n       --from-file=<metadata.name>-0-0-pem \\\n       --from-file=<metadata.name>-0-1-pem \\\n       --from-file=<metadata.name>-0-2-pem \\\n       --from-file=<metadata.name>-0-3-pem \\\n       --from-file=<metadata.name>-0-4-pem \\\n       --dry-run=client \\\n        -o yaml |\nkubectl apply -f -\n\nkubectl -n mongodb create secret generic <metadata.name>-1-cert \\\n       --from-file=<metadata.name>-1-0-pem \\\n       --from-file=<metadata.name>-1-1-pem \\\n       --from-file=<metadata.name>-1-2-pem \\\n       --from-file=<metadata.name>-1-3-pem \\\n       --from-file=<metadata.name>-1-4-pem \\\n       --dry-run=client \\\n        -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-config-cert \\\n       --from-file=<metadata.name>-config-0-pem \\\n       --from-file=<metadata.name>-config-1-pem \\\n       --from-file=<metadata.name>-config-2-pem \\\n       --dry-run=client \\\n        -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-mongos-cert \\\n       --from-file=<metadata.name>-mongos-0-pem \\\n       --from-file=<metadata.name>-mongos-1-pem \\\n       --from-file=<metadata.name>-mongos-2-pem \\\n       --dry-run=client \\\n        -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic agent-certs \\\n  --from-file=mms-automation-agent-pem \\\n  --from-file=mms-backup-agent-pem \\\n  --from-file=mms-monitoring-agent-pem \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl rollout restart sts <name-of-the-resource>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "The  can use X.509 certificates to authenticate your\nclient applications to your MongoDB deployments.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/set-scope-k8s-operator",
            "title": "Set Scope for  Deployment",
            "headings": [
                " Deployment Scopes",
                "Operator Uses Same Namespace as Resources",
                "Operator Uses Different Namespace than Resources",
                "Operator Uses Cluster-Wide Scope",
                "Next Steps"
            ],
            "paragraphs": "Before you install the  , you can set the scope of the\n  deployment. The scopes depend on the namespaces in\nwhich you choose to deploy   and  . You can set one of these scopes: Operator Uses Same Namespace as Resources   (Default) Operator Uses Different Namespace than Resources Operator Uses Cluster-Wide Scope You can set the scope for the   to use the same   as\nresources. In this case, the   watches   and\n  in that same  . When you  install  the  , it\nuses the default namespace. You can set the scope for the   to use a different  \nthan its resources. In this case, the   watches  \nand   in a   that you specify. You can use  helm  to install the   with this scope.\nFollow the relevant  installation instructions  for  helm , but use the following command to\nset the namespace for the   to watch: Setting the namespace ensures that: The namespace you want the   to watch has the correct\n  and  . The  clusterRole  and  clusterRoleBinding \nare included in the default configuration files that you apply during\nthe installation. To create the  clusterRole  and\n clusterRoleBinding , you must have\n cluster-admin privileges . The   can watch and create resources in this namespace. You can set the scope for the   to the   cluster.\nIn this case, the   watches   and  \nin all   in the   cluster. To set a cluster-wide scope for the  , follow the\ninstructions for your preferred installation method. You can deploy only one instance of the   with a\ncluster-wide scope per   cluster. Before you deploy the  , configure the following items: Use the  mongodb-enterprise.yaml \nsample   file from the  MongoDB Enterprise Kubernetes Operator GitHub repository . Set the  spec.template.spec.containers.name.env.name:WATCH_NAMESPACE  in\n mongodb-enterprise.yaml \nto  * . In  mongodb-enterprise.yaml ,\nchange: to: Add the following code to the  ClusterRole  that you\nhave just modified: In  mongodb-enterprise.yaml ,\nchange: to: Create all required local   service accounts and secrets. In the following sample   file, replace  <namespace>  with the\nnamespace in which you want the   to deploy resources. Before you deploy the  , configure the following items: Configure the   to watch all namespaces: Create the required service accounts for each namespace where you\nwant to deploy   and  : Before you deploy the  , configure the following items: Use the  mongodb-enterprise-openshift.yaml \nsample   file from the  MongoDB Enterprise Kubernetes Operator GitHub repository . Set the  spec.template.spec.containers.name.env.name:WATCH_NAMESPACE  in\n mongodb-enterprise-openshift.yaml \nto  * . In  mongodb-enterprise-openshift.yaml ,\nchange: to: Add the following code to the  ClusterRole  that you\nhave just modified: In  mongodb-enterprise-openshift.yaml ,\nchange: to: Create all required local   service accounts and secrets. In the following sample   file, replace  <namespace>  with the\nnamespace in which you want the   to deploy\nresources. Use   or the OpenShift Container Platform user\ninterface to apply the resulting   file. Before you deploy the  , configure the following items: Configure the   to watch all namespaces: Create the required service accounts for each namespace where you\nwant to deploy   and  : After setting up the scope for the  , you can: Read the  Considerations . Complete the  Prerequisites . Install the Kubernetes Operator .",
            "code": [
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --set operator.watchNamespace=<namespace> \\"
                },
                {
                    "lang": "sh",
                    "value": "kind:  Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: mongodb-enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "kind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: mongodb-enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "- apiGroups:\n  - \"\"\n  resources:\n  - namespaces\n  verbs:\n  - list\n  - watch"
                },
                {
                    "lang": "sh",
                    "value": "kind:  RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: mongodb-enterprise-operator\n namespace: mongodb\nroleRef:\n apiGroup: rbac.authorization.k8s.io\n kind: Role\n name: mongodb-enterprise-operator\nsubjects:\n - kind: ServiceAccount\n name: mongodb-enterprise-operator\n namespace: mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kind:  ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: mongodb-enterprise-operator\n namespace: mongodb\nroleRef:\n apiGroup: rbac.authorization.k8s.io\n kind: ClusterRole\n name: mongodb-enterprise-operator\nsubjects:\n - kind: ServiceAccount\n name: mongodb-enterprise-operator\n namespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-database-pods\n  namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-ops-manager\n  namespace: <namespace>\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - secrets\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - patch\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: mongodb-enterprise-appdb\nsubjects:\n  - kind: ServiceAccount\n    name: mongodb-enterprise-appdb\n    namespace: <namespace>\n...\n\n"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n--set operator.watchNamespace=*"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set namespace=<namespace> \\\nhelm_chart --show-only templates/database-roles.yaml | kubectl\napply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "kind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "- apiGroups:\n  - \"\"\n  resources:\n  - namespaces\n  verbs:\n  - list\n  - watch"
                },
                {
                    "lang": "sh",
                    "value": "kind:  RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: enterprise-operator\n namespace: mongodb\nroleRef:\n apiGroup: rbac.authorization.k8s.io\n kind: Role\n name: enterprise-operator\nsubjects:\n - kind: ServiceAccount\n name: enterprise-operator\n namespace: mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kind:  ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: enterprise-operator\n namespace: mongodb\nroleRef:\n apiGroup: rbac.authorization.k8s.io\n kind: ClusterRole\n name: enterprise-operator\nsubjects:\n - kind: ServiceAccount\n name: enterprise-operator\n namespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-database-pods\n  namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-ops-manager\n  namespace: <namespace>\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - secrets\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - patch\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: mongodb-enterprise-appdb\nsubjects:\n  - kind: ServiceAccount\n    name: mongodb-enterprise-appdb\n    namespace: <namespace>\n...\n\n"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n--set operator.watchNamespace=* \\\n--values helm_chart/values-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set namespace=<namespace> \\\nhelm_chart --show-only templates/database-roles.yaml | oc\napply -f -"
                }
            ],
            "preview": "Before you install the , you can set the scope of the\n deployment. The scopes depend on the namespaces in\nwhich you choose to deploy  and .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/manage-database-users-scram",
            "title": "Manage Database Users using SCRAM Authentication",
            "headings": [
                "Considerations",
                "Supported SCRAM Implementations",
                "Supported Authentication Mechanisms",
                "Prerequisites",
                "Add a Database User",
                "Create User Secret",
                "Configure kubectl to default to your namespace.",
                "Copy the following example .",
                "Create a new User Secret YAML file.",
                "Change the highlighted lines.",
                "Save the User Secret file with a .yaml extension.",
                "Create MongoDBUser",
                "Copy the following example MongoDBUser.",
                "Create a new MongoDBUser file.",
                "Change the highlighted lines.",
                "Add any additional roles for the user to the MongoDBUser.",
                "Save the MongoDBUser file with a .yaml extension.",
                "Create the user.",
                "View the newly created user in .",
                "Delete a Database User",
                "Change Authentication Mechanism"
            ],
            "paragraphs": "The   supports managing database users using SCRAM\nauthentication on MongoDB deployments. When you specify  SCRAM  as the authentication mechanism, the\nimplementation of SCRAM used depends upon: The version of MongoDB and If the database is the Application Database or another database. MongoDB Version Database SCRAM Implementation 3.6 or earlier Any except Application Database SCRAM-SHA-1 4.0 or later Any except Application Database SCRAM-SHA-256 Any Application Database SCRAM-SHA-1 The   supports only SCRAM and X.509 authentication\nmechanisms in deployments it creates. In an Operator-created\ndeployment, you cannot use   to: After enabling SCRAM authentication, you can add SCRAM users using the\n  interface or the MongoDBUser  . Configure other authentication mechanisms for deployments. Manage users  not  using SCRAM or X.509 authentication. Before managing database users, you must deploy a\n standalone ,\n replica set , or\n sharded cluster . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : You can choose to use a cleartext password: or you can choose to use a Base64-encoded password: Make sure to copy the desired password configuration. Plaintext\npasswords use  stringData.password  and Base64-encoded\npasswords use  data.password Open your preferred text editor. Paste this User Secret into a new text file. Use the following table to guide you through changing the highlighted\nlines in the Secret: Key Type Description Example metadata.name string Name of the database password secret. Resource names must be 44 characters or less. mms-scram-user-1-password stringData.password string Plaintext password for the desired user. Use this option and value  or   data.password . You\ncan't use both. <my-plain-text-password> data.password string Base64-encoded password for the desired user. Use this option and value  or   stringData.password .\nYou can't use both. You must encode your password into Base64 yourself then\npaste the resulting value with this option. There are\ntools for most every platform and multiple web-based\ntools as well. <my-base64-encoded-password> Open your preferred text editor. Paste this MongoDBUser into a new YAML file. Use the following table to guide you through changing the highlighted\nlines in the MongoDBUser YAML file: Key Type Description Example metadata.name string Name of the database user resource. Resource names must be 44 characters or less. mms-scram-user-1 spec.username string Name of the database user. mms-scram-user-1 spec.passwordSecretKeyRef.name string metadata.name  value of the   that stores the\nuser's password. my-resource spec.mongodbResourceRef.name string Name of the  MongoDB resource  to\nwhich this user is associated. my-resource spec.roles.db string Database on which the  role  can act. admin spec.roles.name string Name of the  role  to grant the database\nuser. The role name can be any\n built-in MongoDB role  or\n custom role  that\nexists in  . readWriteAnyDatabase You may grant additional roles to this user. Invoke the following   command to create your database user: You can view the newly-created user in  : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. To delete a database user, pass the  metadata.name  from the user\nMongoDBUser to the following command: To change your user authenication from SCRAM to X.509: Disable authentication. Under  spec.security.authentication , change  enabled  to\n false . Reapply the user's resource definition. Wait for the MongoDBResource to reach the  running  state. Enable SCRAM authentication. Under  spec.security.authentication , change  enabled  to\n true  and set  modes  to  [\"SCRAM\"] . Reapply the MongoDBUser resource. Wait for the MongoDBResource to reach the  running  state.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: <mms-scram-user-1>\nspec:\n  passwordSecretKeyRef:\n    name: <mms-user-1-password>\n    # Match to metadata.name of the User Secret\n    key: password\n  username: \"<mms-scram-user-1>\"\n  db: \"admin\" #\n  mongodbResourceRef:\n    name: \"<my-replica-set>\"\n    # Match to MongoDB resource using authenticaiton\n  roles:\n    - db: \"admin\"\n      name: \"clusterAdmin\"\n    - db: \"admin\"\n      name: \"userAdminAnyDatabase\"\n    - db: \"admin\"\n      name: \"readWrite\"\n    - db: \"admin\"\n      name: \"userAdminAnyDatabase\"\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mdbu <metadata.name>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  security:\n    authentication:\n      enabled : false"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  security:\n    authentication:\n      enabled : true\n      modes: [\"SCRAM\"]"
                }
            ],
            "preview": "The  supports managing database users using SCRAM\nauthentication on MongoDB deployments.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-op-architecture",
            "title": " Architecture",
            "headings": [],
            "paragraphs": "The   provides a container image for the MongoDB Agent in  . This\nallows you to manage and deploy MongoDB database clusters with full monitoring,\nbackups, and automation provided by  . The   container serves as a host on which   orchestrates the\ninstallation of   processes and deploys the cluster configuration. As part of deployment, the   creates   for\nthe   StatefulSets. The   container uses   to maintain the cluster state\nbetween restarts. The   architecture consists of: An Ops Manager custom resource . Through this resource, the  \ndeploys   components: the application database, the  \napplication, and the Backup Daemon in the   containers. After the deployment\nis operational, the   components reconcile updates that you make to\nthe MongoDB cluster configuration. To learn more, see   Architecture in  . MongoDB database custom resources . The   deploys the  MongoDB \ndatabase and the  MongoDBUser   . After the deployment is\noperational, these resources reconcile updates that you make to the\nuser or the MongoDB cluster configuration. To learn more, see  MongoDB Database Architecture in  .",
            "code": [],
            "preview": "The  provides a container image for the MongoDB Agent in . This\nallows you to manage and deploy MongoDB database clusters with full monitoring,\nbackups, and automation provided by .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-om-resource",
            "title": "Plan Your Ops Manager Resource",
            "headings": [
                "Architecture",
                "Considerations",
                "Encryption Key",
                "Application Database",
                "Topology",
                "Monitoring",
                "Authentication",
                "Offline Deployments",
                "Streamlined Configuration",
                "Backup",
                "Oplog Store",
                "Blockstore",
                "S3 Snapshot Store",
                "Disable Backup",
                "Configure  to Run over HTTPS",
                "Ops Manager Application Access",
                "Deploying  in Remote or Local Mode",
                "Managing External MongoDB Deployments",
                "Prerequisites"
            ],
            "paragraphs": "MongoDB   is an enterprise application that manages, backs\nup, and monitors MongoDB deployments. With  , you can scale and\nupgrade MongoDB, optimize queries, perform point-in-time restores,\nreceive performance alerts, and more. To easily manage and maintain\n  and its underlying database, you can use the   to run\n  as a container on  . Before you deploy an   resource, make sure you read the\n considerations  and complete\nthe  prerequisites . For   resource architecture details, see   Architecture in  . The   generates an encryption key to protect sensitive\ninformation in the  mms-application-database . The  \nsaves this key in a   in the same namespace as the  \nresource. The   names the secret\n <om-resource-name>-gen-key . If you remove the   resource, the key remains stored in the\nsecret on the   cluster. If you stored the Application Database in\na   and you create another   resource with the same name,\nthe   reuses the secret. If you create an  \nresource with a different name, then   creates a new\nsecret and Application Database, and the old secret isn't reused. When you create an instance of   through the  , the\n mms-application-database  is deployed as a  replica set .\nYou can't configure the Application Database as a  standalone \ndatabase or  sharded cluster . If you have concerns about\nperformance or size requirements for the Application Database, contact\n MongoDB Support . The Application Database must use  . When you\n Deploy an   Resource , ensure that you set\n spec.applicationDatabase. \n spec.persistent  to  true . The   automatically configures   to monitor the\nApplication Database that backs the  . The  \ncreates a project named  <ops-manager-deployment-name>-db  for you to\nmonitor the Application Database deployment.  monitors the Application Database deployment, but   does\nnot manage it. You cannot change the Application Database's\nconfiguration in the  . The   UI might display warnings in the\n <ops-manager-deployment-name>-db  project stating that the\nagents for the Application Database are out of date. You can safely\nignore these warnings. The   enforces  SCRAM-SHA-1 \n authentication  on\nthe Application Database. The   creates the database user which   uses to\nconnect to the Application Database. This database user has the\nfollowing attributes: You can't modify the   database user's name and roles. You\n create a secret  to set the database user's\npassword. You edit the secret to update the password. If you don't\ncreate a secret or delete an existing secret, the  \ngenerates a password and stores it. Username mongodb-ops-manager Authentication Database admin Roles readWriteAnyDatabase dbAdminAnyDatabase clusterMonitor The   bundles MongoDB Enterprise version\n 4.2.11-ent  with the  Application Database  image to enable offline\ndeployments of   resources. To deploy   inside   without an Internet connection, omit\nthe  spec.applicationDatabase.version  setting or leave the\nvalue empty. If the new release of the   uses the new image of\nApplication Database, where the bundled mongodb has changed, and if\n spec.applicationDatabase.version  is omitted, then the\nApplication Database is automatically upgraded to the new version. After you deploy  , you need to configure it. The regular\nprocedure involves setting up   through the\n configuration wizard . If you\nset some essential settings in your object specification before you\ndeploy, you can bypass the configuration wizard. In the  spec.configuration  block of your   object\nspecification, you need to: Add  mms.ignoreInitialUiSetup  and set to\n true . Add the  minimum configuration settings  to\nallow the   instance to start without errors. To disable the   configuration wizard, configure the\nfollowing settings in your  spec.configuration  block: Replace the example values with the values you want your   to\nuse.  enables  mms-backup-functional-overview  by\ndefault. The   deploys a   comprised of\none pod to host the  backup-daemon , and then creates a  \nand   for the Backup Daemon's  head database . The\n  uses the  Ops Manager API  to\nenable the Backup Daemon and configure the head database. To configure Backup, you must create MongoDB database resources for\nthe  oplog store  and for one of the\nfollowing: If you deploy both an  \n snapshot store  and a\n blockstore ,   chooses\none to use for Backup at random. The   resource remains in a  Pending  state until you configure these Backup resources.   snapshot store . blockstore . You must deploy a three-member replica set to store your\n oplog slices . The Oplog database only supports the  SCRAM  authentication mechanism.\nYou cannot enable other authentication mechanisms. If you enable  SCRAM  authentication on the oplog database, you\nmust: Create a MongoDB user resource to connect   to the oplog\ndatabase. Specify the  name \nof the user in the   resource definition. If you deploy   4.2 with  SCRAM  authentication enabled, you\nmust specify a MongoDB version earlier than than 4.0 in the oplog\n database resource definition . To configure a  blockstore , you\nmust deploy a replica set to store snapshots. To configure an    snapshot store , you\nmust create an     or  -compatible bucket to store your\ndatabase Backup  snapshots . The default configuration stores snapshot metadata in the Application\nDatabase. You can also deploy a replica set to store snapshot metadata,\nthen configure it using the\n spec.backup.s3Stores.mongodbResourceRef.name  and\n spec.backup.s3Stores.mongodbResourceRef.user  settings in\nthe   resource definition. You can update any additional  \n configuration settings \nthat   doesn't manage through the  . To disable backup after you enabled it: Set the        spec.backup.enabled \nsetting to  false . Disable backups  in the\n . Delete the  backup-daemon   : The   and   for the Backup Daemon's  head\ndatabase  are not deleted when you delete the  backup-daemon \n . You can retrieve stored data before you delete\nthese   resources. To learn about reclaiming  , see the\n Kubernetes documentation . You can configure your   instance created through the  \nto run over   instead of  . To configure your   instance to run over  , provide a  \ncertificate and Private Key in the   configuration object. For detailed instructions, see  Deploy an   Resource . If you have existing deployments, you must restart them manually\nafter enabling  . To avoid restarting your deployments,\nconfigure   before deploying your managed resources. To learn more, see  HTTPS Enabled After Deployment . By default, the   doesn't create a   service to route\ntraffic originating from outside of the   cluster to the  \napplication. To access the   application, you can: The simplest method is configuring the   to create a  \nservice that routes external traffic to the   application. The\n  deployment procedure instructs you to add the following\nsettings to the   specification that configures the\n  to create a service: Configure the   to create a   service. Create a   service manually. MongoDB recommends using a\n LoadBalancer    service if your cloud provider supports it. If you're using OpenShift, use\n routes . Use a third-party service, such as Istio. spec. externalConnectivity spec.externalConnectivity. type You can use the   to configure   to operate in\n Local  or  Remote  mode if your environment prevents granting hosts\nin your   cluster access to the Internet. In these modes, the Backup\nDaemons and managed MongoDB resources download installation archives\nfrom   instead of from the Internet: Configure an   Resource to use Remote Mode :   reads the\ninstallation archives from HTTP endpoints on a web server\nor S3-compatible file store deployed to your   cluster. Configure an   Resource to use Local Mode :   reads the instalation\narchives from a   that you create for the   StatefulSet. When you deploy   with the  ,   can manage\nMongoDB database resources deployed: If   manages MongoDB database resources deployed to different\n  clusters than   or outside of   clusters, you must: To the same   cluster as  . Outside of   clusters. Add the  mms.centralUrl  setting to  spec.configuration  in the\n  resource specification. Set the value to the URL by which   is exposed outside of the\n  cluster: Update the ConfigMaps  referenced by\nall MongoDB database resources inside the   cluster that you\ndeployed with the  . Set  data.baseUrl  to the same value of the\n spec.configuration.mms.centralUrl \nsetting in the   resource specification. This includes the ConfigMaps that the  MongoDB database resources\nfor the oplog and snapshot stores reference . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Install  the   1.4.1\nor newer. Ensure that the host on which you want to deploy   has a\nminimum of five gigabytes of memory. Create a     for an admin user in the same  \nas the   resource. When you deploy the   resource,   creates a user with\nthese credentials and grants it the  Global Owner  role.\nUse these credentials to log in to   for the first time. Once\nyou deploy  , change the password or remove this secret. The admin user's password must adhere to the  \n password complexity requirements . ( Optional ) To set the password for the   database user,\ncreate a   in the same   as the   resource. The   creates the database user that   uses to\nconnect to the  mms-application-database . You can set the\npassword for this database user by invoking the following command to\ncreate a secret: If you don't create a secret, then the   automatically\ngenerates a password and stores it internally. To learn more,\nsee  Authentication . If you choose to create a secret for the   database user,\nyou must specify the secret's\n name \nin the   resource definition. By default, the\n  looks for the password value in the  password \nkey. If you stored the password value in a different key, you\nmust also specify that\n key \nname in the   resource definition. ( Optional ). To configure Backup to an   snapshot store, create\na   in the same namespace as the   resource. This secret stores your   credentials so that the  \ncan connect   to your     or  -compatible bucket.\nThe secret must contain the following key-value pairs: To create the secret, invoke the following command: To learn more about managing   snapshot storage, see the\n Prerequisites . Key Value accessKey Unique identifer of the   user who owns the   or\n -compatible bucket. secretKey Secret key of the   user who owns the   or\n -compatible bucket.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "spec:\n  configuration:\n    mms.ignoreInitialUiSetup: \"true\"\n    automation.versions.source: \"remote\"\n    mms.adminEmailAddr: cloud-manager-support@mongodb.com\n    mms.fromEmailAddr: cloud-manager-support@mongodb.com\n    mms.mail.hostname: email-smtp.us-east-1.amazonaws.com\n    mms.mail.port: \"465\"\n    mms.mail.ssl: \"true\"\n    mms.mail.transport: smtp\n    mms.minimumTLSVersion: TLSv1.2\n    mms.replyToEmailAddr: cloud-manager-support@mongodb.com\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete statefulset <metadata.name>-backup-daemon -n <namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  configuration:\n    mms.centralUrl: https://a9a8f8566e0094380b5c257746627b82-1037623671.us-east-1.elb.example.com:8080/"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <adminusercredentials> \\\n  --from-literal=Username=\"<username>\" \\\n  --from-literal=Password=\"<password>\" \\\n  --from-literal=FirstName=\"<firstname>\" \\\n  --from-literal=LastName=\"<lastname>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <om-db-user-secret-name> \\\n  --from-literal=password=\"<om-db-user-password>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <my-aws-s3-credentials> \\\n  --from-literal=accessKey=\"<AKIAIOSFODNN7EXAMPLE>\" \\\n  --from-literal=secretKey=\"<wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY>\""
                }
            ],
            "preview": "MongoDB  is an enterprise application that manages, backs\nup, and monitors MongoDB deployments. With , you can scale and\nupgrade MongoDB, optimize queries, perform point-in-time restores,\nreceive performance alerts, and more. To easily manage and maintain\n and its underlying database, you can use the  to run\n as a container on .",
            "tags": null,
            "facets": null
        }
    ]
}