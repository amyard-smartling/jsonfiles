{
    "url": "http://mongodb.com/docs/kafka-connector/v1.10",
    "includeInGlobalSearch": false,
    "documents": [
        {
            "slug": "compatibility",
            "title": "Compatibility",
            "headings": [
                "MongoDB Compatibility",
                "MongoDB Kafka Sink Connector",
                "MongoDB Kafka Source Connector",
                "Kafka Compatibility"
            ],
            "paragraphs": "The MongoDB Kafka sink connector requires MongoDB v3.6 or later. The MongoDB Kafka source connector requires MongoDB v3.6 or later. If you are using MongoDB v3.6, the connector can only listen for\nchanges on  collections . If you need the connector to listen for\nchanges on a  database  or  deployment , you must use\nMongoDB v4.0 or later. We recommend that you only use the MongoDB Kafka Connector with an official version\nof the MongoDB server. We cannot guarantee it functions correctly\nwith any other version. The MongoDB Kafka Connector requires Confluent Kafka Connect v2.1.0 or later. It's\ntested against Apache Kafka v2.3 and later. You can use the\nKafka Connect service with several Apache Kafka compatible platforms\nincluding the following: The connector works directly with Kafka Connect. The connector does not\nconnect directly to a Kafka cluster which means it's compatible with any\nApache Kafka platform that supports Kafka Connect. If you have any questions about the connector, ask them on the\n MongoDB Community Forums . Confluent Platform v5.3 or later Microsoft Azure Event Hubs Red Hat AMQ Streams",
            "code": [],
            "preview": "The MongoDB Kafka Connector requires Confluent Kafka Connect v2.1.0 or later. It's\ntested against Apache Kafka v2.3 and later. You can use the\nKafka Connect service with several Apache Kafka compatible platforms\nincluding the following:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "security-and-authentication",
            "title": "Security and Authentication",
            "headings": [],
            "paragraphs": "Read the following sections to learn how to secure communications between MongoDB\nand the MongoDB Kafka Connector: Encrypt the Messages Your Connector Sends with SSL/TLS Authenticate Your Connector with MongoDB using Amazon Web Services",
            "code": [],
            "preview": "Read the following sections to learn how to secure communications between MongoDB\nand the MongoDB Kafka Connector:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "issues-and-help",
            "title": "Issues & Help",
            "headings": [
                "Bugs / Feature Requests"
            ],
            "paragraphs": "Often, the quickest way to get support for general questions is through the\n MongoDB Community Forums . Refer to our  support channels  documentation for more information. If you think you've found a bug or want to see a new feature in the\nMongoDB Kafka Connector, please open a case in our issue management tool, JIRA: Bug reports in JIRA for the Kafka Connector project are  public . If you've identified a security vulnerability in the connector or any other\nMongoDB project, please report it according to the instructions found in the\n Create a Vulnerability Report . Create an account and login . Navigate to  the KAFKA project . Click  Create . Please provide as much information as possible about the\nissue and the steps to reproduce it.",
            "code": [],
            "preview": "Often, the quickest way to get support for general questions is through the\nMongoDB Community Forums.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "troubleshooting",
            "title": "Troubleshooting",
            "headings": [],
            "paragraphs": "Learn how to address issues you may encounter while running the MongoDB Kafka Connector. Invalid Resume Token",
            "code": [],
            "preview": "Learn how to address issues you may encounter while running the MongoDB Kafka Connector.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "migrate-from-kafka-connect-mongodb",
            "title": "Migrate from Kafka Connect MongoDB",
            "headings": [
                "Update Configuration Settings",
                "Update Custom Classes",
                "Update Post Processor Subclasses"
            ],
            "paragraphs": "Use this guide to migrate your Kafka deployments from the community-created\n Kafka Connect MongoDB  sink connector\nto the  official MongoDB Kafka connector . The following sections list the changes you must make to your Kafka\nConnect sink connector configuration settings and custom classes to transition\nto the MongoDB Kafka sink connector. Make the following changes to the configuration settings of your Kafka Connect\ndeployment before using them with your MongoDB Kafka connector deployment: Replace values that include the package  at.grahsl.kafka.connect.mongodb \nwith the package  com.mongodb.kafka.connect . Replace your  connector.class  setting with the MongoDB Kafka sink\nconnector class. Remove the  mongodb.  prefix from your Kafka Connect property names.\nFor example, change  mongodb.connection.uri  to  connection.uri . Remove the  document.id.strategies  setting if it exists. If the value of\nthis setting reference custom strategies, move them to the\n document.id.strategy  setting. Read the  Update Custom Classes \nsection to discover what changes you must make to your custom classes. Replace any property names you use to specify per-topic or collection\noverrides that contain the  mongodb.collection  prefix with the equivalent\nkey in\n sink connector Kafka topic configuration topic properties . If you use any custom classes in your Kafka Connect sink connector deployment,\nmake the following changes to them before adding them to your MongoDB Kafka\nconnector deployment: Replace imports that include  at.grahsl.kafka.connect.mongodb  with\n com.mongodb.kafka.connect . Replace references to the  MongoDbSinkConnector  class with the\n MongoSinkConnector  class. Update custom sink connector strategy classes to implement the\n com.mongodb.kafka.connect.sink.processor.id.strategy.IdStrategy \ninterface. Update references to the  MongoDbSinkConnectorConfig  class. In the\nMongoDB Kafka connector, the logic from that class is split into the\nfollowing classes: MongoSinkConfig MongoSinkTopicConfig If you have classes that subclass a post processor in your Kafka Connect\nconnector deployment, update methods that override ones in the Kafka Connect\n PostProcessor  class to match the method signatures of the MongoDB Kafka\nconnector  PostProcessor class .",
            "code": [
                {
                    "lang": "properties",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSinkConnector"
                }
            ],
            "preview": "Use this guide to migrate your Kafka deployments from the community-created\nKafka Connect MongoDB sink connector\nto the official MongoDB Kafka connector.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector",
            "title": "Sink Connector",
            "headings": [
                "Overview",
                "Configuration Properties",
                "Fundamentals"
            ],
            "paragraphs": "This section focuses on the  MongoDB Kafka sink connector .\nThe sink connector is a Kafka Connect connector that reads data from Apache Kafka and\nwrites data to MongoDB. To learn about configuration options for your sink connector, see the\n Configuration Properties  section. To learn how features of the sink connector work and how to configure them, see the\n Fundamentals  section.",
            "code": [],
            "preview": "This section focuses on the MongoDB Kafka sink connector.\nThe sink connector is a Kafka Connect connector that reads data from Apache Kafka and\nwrites data to MongoDB.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "quick-start",
            "title": "Quick Start",
            "headings": [
                "Overview",
                "Install the Required Packages",
                "Download the Sandbox",
                "Start the Sandbox",
                "Add Connectors",
                "Add a Source Connector",
                "Add a Sink Connector",
                "Send the Contents of a Document through Your Connectors",
                "Remove the Sandbox",
                "Next Steps"
            ],
            "paragraphs": "This guide shows you how to configure the MongoDB Kafka Connector to send data between MongoDB\nand Apache Kafka. After completing this guide, you should understand how to use the Kafka Connect\nREST API to configure MongoDB Kafka Connectors to read data from MongoDB and write it to a\nKafka topic, and to read data from a Kafka topic and write it to MongoDB. To complete the steps in this guide, you must download and work in a\n sandbox , a containerized development environment that includes services\nyou need to build a sample  data pipeline . Read the following sections to set up your sandbox and sample data pipeline. After you complete this guide, you can remove the environment by\nfollowing the instructions in the  Remove the Sandbox \nsection. Download and install the following packages: The sandbox uses Docker for convenience and consistency. To learn more about\ndeployment options for Apache Kafka, see the following resources: Docker Git This guide uses the following Docker-specific terminology: Learn more about Docker from the Docker official\n Get Started Guide . Container Image Official Apache Kafka Quick Start Official Install Confluent Platform Guide We created a sandbox that includes the services you need in this tutorial\nto build your sample data pipeline. To download the sandbox, clone the tutorial repository to your development\nenvironment. Then navigate to the directory that corresponds to the quickstart\ntutorial. If you use bash or a similar shell, use the following commands: The sandbox starts the following services in Docker containers: To start the sandbox run the following command from the tutorial directory: When you start the sandbox, Docker downloads any images it needs to run. After Docker downloads and builds the images, you should see the following\noutput in your development environment: MongoDB, configured as a replica set Apache Kafka Kafka Connect with the MongoDB Kafka Connector installed Apache Zookeeper which manages the configuration for Apache Kafka In total, the Docker images for this tutorial require about 2.4 GB\nof space. The following list shows how long it takes to download the images with\ndifferent internet speeds: 40 megabits per second: 8 minutes 20 megabits per second: 16 minutes 10 megabits per second: 32 minutes The sandbox maps the following services to ports on your host\nmachine: These ports must be free to start the sandbox. The sandbox MongoDB server maps to port  35001  on your host machine The sandbox Kafka Connect JMX server maps to port  35000  on your host machine To complete the sample data pipeline, you must add connectors to Kafka Connect\nto transfer data between Kafka Connect and MongoDB. Add a  source connector  to\ntransfer data from MongoDB to Apache Kafka. Add a  sink connector  to transfer\ndata from Apache Kafka to MongoDB. To add connectors in the sandbox, first start an interactive bash shell in\nyour Docker container using the following command: After your shell session starts, you should see the following prompt: Use the shell in your Docker container to add a source connector using the\nKafka Connect REST API. The following API request adds a source connector configured with the\nfollowing properties: To confirm that you added the source connector, run the following command: The preceding command should output the names of the running connectors: To learn more about source connector properties, see the page on\n Source Connector Configuration Properties . To learn more about aggregation pipelines, see the MongoDB manual page on\n Aggregation Pipelines . The class Kafka Connect uses to instantiate the connector The connection URI, database, and collection of the MongoDB replica set from\nwhich the connector reads data An aggregation pipeline that adds a field  travel  with the value\n \"MongoDB Kafka Connector\"  to inserted documents the connector reads from MongoDB It takes up to three minutes for the Kafka Connect REST API to start. If\nyou receive the following error, wait three minutes and run the preceding\ncommand again: Use the shell in your Docker container to add a sink connector using the\nKafka Connect REST API. The following API request adds a sink connector configured with the\nfollowing properties: To confirm that you added both source and sink connector, run the following\ncommand: The preceding command should output the names of the running connectors: To learn more about sink connector properties, see the page on\n Sink Connector Configuration Properties . To learn more about change data capture events, see the\n Change Data Capture Handlers  guide. The class Kafka Connect uses to instantiate the connector The connection URI, database, and collection of the MongoDB replica set to\nwhich the connector writes data The Apache Kafka topic from which the connector reads data A change data capture handler for MongoDB change event documents To send the contents of a document through your connectors, insert a\ndocument into the MongoDB collection from which your source connector reads\ndata. To insert a new document into your collection, enter the MongoDB shell from\nthe shell in your Docker container using the following command: After you run the preceding command, you should see the following prompt: From the MongoDB shell, insert a document into the  sampleData \ncollection of the  quickstart  database using the following commands: After you insert a document into the  sampleData  collection, confirm that\nyour connectors processed the change. Check the contents of the  topicData \ncollection using the following command: You should see output that resembles the following: Exit the MongoDB shell with the following command: To conserve resources in your development environment, remove the sandbox. Before you remove the sandbox, exit from the shell session in your Docker\ncontainer by running the following command: You can choose to remove both the Docker containers and images, or exclusively\nthe containers. If you remove the containers and images, you must download\nthem again to restart your sandbox which is approximately 2.4 GB\nin size. If you exclusively remove the containers, you can reuse the\nimages and avoid downloading most of the large files in the sample data\npipeline. Select the tab that corresponds to the removal task you want to run. Run the following shell command to remove the Docker containers and\nimages from the sandbox: Run the following shell command to remove the Docker containers but\nkeep the images for the sandbox: To learn how to install the MongoDB Kafka Connector, see the  Install the MongoDB Kafka Connector  guide. To learn more about how to process and move data from Apache Kafka to MongoDB, see\nthe  Sink Connector  guide. To learn more about how to process and move data from MongoDB to Apache Kafka, see\nthe  Source Connector  guide.",
            "code": [
                {
                    "lang": "bash",
                    "value": "git clone https://github.com/mongodb-university/kafka-edu.git\ncd kafka-edu/docs-examples/mongodb-kafka-base/"
                },
                {
                    "lang": "shell",
                    "value": "docker-compose -p mongo-kafka up -d --force-recreate"
                },
                {
                    "lang": "text",
                    "value": "...\nCreating zookeeper ... done\nCreating broker    ... done\nCreating schema-registry ... done\nCreating connect         ... done\nCreating rest-proxy      ... done\nCreating mongo1          ... done\nCreating mongo1-setup    ... done"
                },
                {
                    "lang": "shell",
                    "value": "docker exec -it mongo1 /bin/bash"
                },
                {
                    "lang": "none",
                    "value": "MongoDB Kafka Connector Sandbox $"
                },
                {
                    "lang": "bash",
                    "value": "curl -X POST \\\n     -H \"Content-Type: application/json\" \\\n     --data '\n     {\"name\": \"mongo-source\",\n      \"config\": {\n         \"connector.class\":\"com.mongodb.kafka.connect.MongoSourceConnector\",\n         \"connection.uri\":\"mongodb://mongo1:27017/?replicaSet=rs0\",\n         \"database\":\"quickstart\",\n         \"collection\":\"sampleData\",\n         \"pipeline\":\"[{\\\"$match\\\": {\\\"operationType\\\": \\\"insert\\\"}}, {$addFields : {\\\"fullDocument.travel\\\":\\\"MongoDB Kafka Connector\\\"}}]\"\n         }\n     }\n     ' \\\n     http://connect:8083/connectors -w \"\\n\""
                },
                {
                    "lang": "shell",
                    "value": "curl -X GET http://connect:8083/connectors"
                },
                {
                    "lang": "none",
                    "value": "[\"mongo-source\"]"
                },
                {
                    "lang": "none",
                    "value": "...\ncurl: (7) Failed to connect to connect port 8083: Connection refused"
                },
                {
                    "lang": "bash",
                    "value": "curl -X POST \\\n     -H \"Content-Type: application/json\" \\\n     --data '\n     {\"name\": \"mongo-sink\",\n      \"config\": {\n         \"connector.class\":\"com.mongodb.kafka.connect.MongoSinkConnector\",\n         \"connection.uri\":\"mongodb://mongo1:27017/?replicaSet=rs0\",\n         \"database\":\"quickstart\",\n         \"collection\":\"topicData\",\n         \"topics\":\"quickstart.sampleData\",\n         \"change.data.capture.handler\": \"com.mongodb.kafka.connect.sink.cdc.mongodb.ChangeStreamHandler\"\n         }\n     }\n     ' \\\n     http://connect:8083/connectors -w \"\\n\""
                },
                {
                    "lang": "shell",
                    "value": "curl -X GET http://connect:8083/connectors"
                },
                {
                    "lang": "none",
                    "value": "[\"mongo-source\", \"mongo-sink\"]"
                },
                {
                    "lang": "shell",
                    "value": "mongosh mongodb://mongo1:27017/?replicaSet=rs0"
                },
                {
                    "lang": "shell",
                    "value": "rs0 [primary] test>"
                },
                {
                    "lang": "javascript",
                    "value": "use quickstart\ndb.sampleData.insertOne({\"hello\":\"world\"})"
                },
                {
                    "lang": "javascript",
                    "value": "db.topicData.find()"
                },
                {
                    "lang": "json",
                    "value": "[\n    {\n      _id: ObjectId(...),\n      hello: 'world',\n      travel: 'MongoDB Kafka Connector'\n    }\n]"
                },
                {
                    "lang": "shell",
                    "value": "exit"
                },
                {
                    "lang": "shell",
                    "value": "exit"
                },
                {
                    "lang": "shell",
                    "value": "docker-compose -p mongo-kafka down --rmi all"
                },
                {
                    "lang": "shell",
                    "value": "docker-compose -p mongo-kafka down"
                }
            ],
            "preview": "This guide shows you how to configure the MongoDB Kafka Connector to send data between MongoDB\nand Apache Kafka.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "introduction",
            "title": "Introduction",
            "headings": [],
            "paragraphs": "Read the following sections to learn about the MongoDB Kafka Connector, Kafka Connect, and Apache Kafka: Kafka and Kafka Connect Install the Connector Connect to MongoDB Data Formats Converters",
            "code": [],
            "preview": "Read the following sections to learn about the MongoDB Kafka Connector, Kafka Connect, and Apache Kafka:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector",
            "title": "Source Connector",
            "headings": [
                "Overview",
                "Configuration Properties",
                "Usage Examples",
                "Fundamentals"
            ],
            "paragraphs": "This section focuses on the  MongoDB Kafka source connector .\nThe source connector is a Kafka Connect connector that reads data from MongoDB and\nwrites data to Apache Kafka. The source connector works by opening a single change stream with\nMongoDB and sending data from that change stream to Kafka Connect. Your source\nconnector maintains its change stream for the duration of its runtime, and your\nconnector closes its change stream when you stop it. To learn about configuration options for your source connector, see the\n Configuration Properties  section. To view examples of source connector configurations, see the\n Usage Examples  section. To learn how features of the source connector work and how to configure them, see the\n Fundamentals  section.",
            "code": [],
            "preview": "This section focuses on the MongoDB Kafka source connector.\nThe source connector is a Kafka Connect connector that reads data from MongoDB and\nwrites data to Apache Kafka.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorials",
            "title": "Tutorials",
            "headings": [],
            "paragraphs": "The tutorials in this section show you how to perform specific tasks with\nthe MongoDB Kafka Connector. For instructions on setting up your environment for the following tutorials,\nread the  Tutorial Setup  first: Explore Change Streams Getting Started with the MongoDB Kafka Source Connector Getting Started with the MongoDB Kafka Sink Connector Replicate Data with a Change Data Capture Handler Migrate an Existing Collection to a Time Series Collection",
            "code": [],
            "preview": "The tutorials in this section show you how to perform specific tasks with\nthe MongoDB Kafka Connector.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "",
            "title": "MongoDB Kafka Connector",
            "headings": [
                "Overview",
                "What's New",
                "Quick Start",
                "Introduction",
                "Tutorials",
                "Sink Connector",
                "Source Connector",
                "Security and Authentication",
                "Monitoring",
                "Migrate from the Community Connector",
                "Troubleshooting",
                "How to Contribute",
                "Issues & Help",
                "Compatibility"
            ],
            "paragraphs": "The  MongoDB Connector for Apache Kafka  is\na Confluent-verified connector that persists data from Apache Kafka topics as a\ndata sink into MongoDB and publishes changes from MongoDB into Kafka\ntopics as a data source. We recommend that you use one of the following MongoDB partner service\nofferings to host your Apache Kafka cluster and MongoDB Kafka Connector: Confluent Cloud Register to learn more about the  MongoDB Source Connector ,\nor read the  documentation . Register to learn more about the  MongoDB Sink Connector ,\nor read the  documentation . Amazon Managed Streaming for Apache Kafka (MSK) Redpanda Cloud To learn more about the MongoDB Source Connector, read the  documentation . To learn more about the MongoDB Sink Connector, read the  documentation . You can also use Atlas Stream Processing, which is a MongoDB-native way to\nprocess streaming data by using the MongoDB Query API. It transforms the way\nthat developers build modern applications. Use Atlas Stream Processing to continuously process streaming data,\nvalidate schemas, and materialize views into either Atlas database\ncollections or Apache Kafka topics. To learn more about Atlas Stream Processing, see the\n Atlas Stream Processing \nproduct page or read the  docs . For a list of new features and changes in each version, see the\n What's New  section. Learn how to get started with the MongoDB Kafka Connector and begin working with data\nin the  Quick Start  section. Learn about and how the MongoDB Kafka Connector passes data between Apache Kafka\nand MongoDB in the  Introduction  section. In this\nsection, you can also learn about how to install the connector and\nconfigure it to connect to your MongoDB deployment. Follow  tutorials  to learn how to set\nup the MongoDB Kafka Connector to read and write data for several use cases. Learn how to configure how the MongoDB Kafka sink connector writes data from\nApache Kafka into MongoDB in the  Sink Connector  section. Learn how to configure how the MongoDB Kafka source connector writes data from\nMongoDB into Apache Kafka in the  Source Connector  section. Learn how to secure communications between MongoDB and the\nMongoDB Kafka Connector in the  Security and Authentication  section. In this\nsection, you can also learn how to configure the MongoDB Kafka Connector to\nauthenticate to MongoDB with your AWS Identity and IAM credentials. Learn how to monitor your MongoDB Kafka source and sink connectors in\nthe  Monitoring  section. Learn how to to migrate from the legacy Kafka Connect MongoDB\nsink connector to the official MongoDB Kafka Connector in the\n Migration Guide . Learn how to resolve issues you may encounter while running the\nMongoDB Kafka Connector in the  Troubleshooting  section. Learn how to contribute to the MongoDB Kafka Connector codebase in\nthe  How to Contribute  section. Learn how to report bugs and request features in the\n Issues & Help  section. For information about compatibility between the MongoDB Kafka Connector and\nMongoDB, see the  Compatibility  section. In this section,\nyou can also learn about compatibility between the MongoDB Kafka Connector and\nConfluent Kafka Connect.",
            "code": [],
            "preview": "The MongoDB Connector for Apache Kafka is\na Confluent-verified connector that persists data from Apache Kafka topics as a\ndata sink into MongoDB and publishes changes from MongoDB into Kafka\ntopics as a data source.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "contribute",
            "title": "How to Contribute",
            "headings": [],
            "paragraphs": "We are happy to accept contributions to help improve the MongoDB Kafka Connector. We guide\ncommunity contributions to ensure they meet the standards of the codebase. Please\nensure that your pull request meets the following criteria: To get started, check out the source code and create a branch by running the\nfollowing commands in your shell: Once you push your changes to your feature branch, make sure it passes the\nGradle checks. You can run the checks with the following command: To learn more about the connector source code, see the  GitHub repository . To learn more about Gradle, see the official\n Gradle website . Includes documentation comments describing your feature Includes unit tests that cover the functionality of your feature Passes the Gradle  check  task, which includes the following tasks: test  task integrationTest  task spotlessCheck  task You must have a local MongoDB replica set running to perform Gradle integration\ntests. To learn how to set up a MongoDB replica set, see\n Deploy a Replica Set  in the MongoDB manual. You can skip tests in the  integrationTest  task related to\nthe following areas unless your code specifically modifies connector behavior\nrelated to these areas: You can run the authentication tests by enabling authentication in your\nlocal MongoDB replica set and specifying your credentials in your connection URI. To learn how\nto enable authentication in a replica set, see\n Deploy Replica Set With Keyfile Authentication \nin the MongoDB manual. You can run tests related to a specific MongoDB version by deploying a local replica set\nwith that version of MongoDB. Specific versions of MongoDB Authentication",
            "code": [
                {
                    "lang": "shell",
                    "value": "git clone https://github.com/mongodb/mongo-kafka.git\ncd mongo-kafka\ngit checkout -b <your branch name>"
                },
                {
                    "lang": "shell",
                    "value": "./gradlew clean check --continue -Dorg.mongodb.test.uri=<your local mongodb replica set connection uri>"
                }
            ],
            "preview": "We are happy to accept contributions to help improve the MongoDB Kafka Connector. We guide\ncommunity contributions to ensure they meet the standards of the codebase. Please\nensure that your pull request meets the following criteria:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "whats-new",
            "title": "What's New",
            "headings": [
                "What's New in 1.10.1",
                "What's New in 1.10",
                "What's New in 1.9.1",
                "What's New in 1.9",
                "What's New in 1.8.1",
                "What's New in 1.8",
                "Sink Connector",
                "What's New in 1.7",
                "Sink Connector",
                "Source Connector",
                "Bug Fixes",
                "What's New in 1.6.1",
                "Bug Fixes",
                "What's New in 1.6",
                "Sink Connector",
                "Source Connector",
                "Bug Fixes",
                "What's New in 1.5",
                "Sink Connector",
                "Source Connector",
                "Bug Fixes",
                "What's New in 1.4",
                "Sink Connector",
                "Source Connector",
                "Bug Fixes",
                "What's New in 1.3",
                "Sink Connector",
                "Source Connector",
                "Bug Fixes",
                "What's New in 1.2",
                "Sink Connector",
                "Source Connector",
                "Bug Fixes",
                "What's New in 1.1",
                "Sink Connector",
                "Source Connector",
                "Bug Fixes",
                "What's New in 1.0"
            ],
            "paragraphs": "Learn what's new by version: Version 1.10.1 Version 1.10 Version 1.9.1 Version 1.9 Version 1.8.1 Version 1.8 Version 1.7 Version 1.6.1 Version 1.6 Version 1.5 Version 1.4 Version 1.3 Version 1.2 Version 1.1 Version 1.0 Version 1.9 introduced a bug related to  MongoSourceTask.start  that can cause a resource leak on\nboth the connector side and the server side . Upgrade to version 1.10.1 if you are using version 1.9 or 1.10 of the connector. Fixed a resource leak related to  MongoSourceTask.start  that was introduced in version 1.9. Added the connector name to JMX monitoring metrics. Added support for SSL by creating the following configuration options: connection.ssl.truststore connection.ssl.truststorePassword connection.ssl.keystore connection.ssl.keystorePassword Ensured the driver parses config values from config providers before\nvalidating them. Corrected the behavior of schema inference for documents in nested arrays. Introduced the  startup.mode=timestamp  setting that allows you to\nstart a Change Stream at a specific timestamp by setting the new\n startup.mode.timestamp.start.at.operation.time  property. Deprecated the  copy.existing  property and all  copy.existing.* \nproperties. You should use  startup.mode=copy_existing  and\n startup.mode.copy.existing.*  properties to configure the copy existing\nfeature. Introduced the  change.stream.full.document.before.change  setting that\nallows you to access and configure the pre-image of an update\noperation in the change stream event document. Improved  schema inference  for nested\ndocuments contained in arrays. Introduced the  publish.full.document.only.tombstones.on.delete  setting\nthat configures the connector to send tombstone events when documents\nare deleted. This setting only applies when  publish.full.document.only \nis  true . Added MongoDB server exception information to dead letter queue messages. Corrected the type returned by  getAttribute()  and\n getAttributes()  method calls in JMX MBeans to  Attribute . Updated the MongoDB Java driver dependency to version 4.7. Added several logger events and details in source and sink connectors\nto help with debugging. For a complete list\nof updates, see the\n KAFKA-302  issue in JIRA. Added JMX monitoring support for the source and sink connectors.\nTo learn more about monitoring connectors, see the\n Monitoring  page. Added support for the Debezium MongoDB change stream CDC handler. You\ncan now configure the connector to listen for events produced by this\nhandler. Updated the MongoDB Java driver dependency to version 4.5 Added dead letter queue error reports in the event\n the connector experiences bulk write errors Added support for unordered bulk writes with the  bulk.write.ordered   configuration property Added warning when attempting to use a Change Data Capture (CDC)\nhandler with a post processor Removed support for the  max.num.retries  configuration property Removed support for the  retries.defer.timeout  configuration property To disable retries, specify the  retryWrites=false  option\nin your MongoDB connection URI. The following configuration, which contains a placeholder MongoDB connection\nURI, disables retries: To learn more about connecting the MongoDB Kafka Connector to MongoDB, see the\n Connect to MongoDB  guide. To learn more about connection URI options, see the\n Connection Options \nguide in the MongoDB Java driver documentation. Added support for user-defined topic separators with the\n topic.separator  configuration property Added support for the\n allow disk use \nfield of the MongoDB Query API in the copy existing aggregation with the\n copy.existing.allow.disk.use   configuration property Added support for  Avro schema namespaces \nin the  output.schema.value  and  output.schema.key \n configuration properties Fixed Avro schema union validation Updated MongoDB Java driver dependency to 4.3.1 in the combined JARs Fixed connection validator user privilege check Fixed a bug in  UuidProvidedIn[Key|Value]Strategy  classes that prevented\nthem from loading Added support for  Stable API  to\nforce the server to run operations with behavior compatible with the\nspecified API version Starting from February 2022, the  Versioned API  is known the  Stable API .\nAll concepts and features remain the same with this naming change. Added error handling properties for the\n sink connector \nand  source connector \nthat can override the Kafka Connect framework's error handling behavior Added  mongo-kafka-connect-<version>-confluent.jar , which contains\nthe connector and all dependencies needed to run it on the Confluent Platform Added support for  automatic time-series collection creation \nin MongoDB 5.0 to efficiently store sequences of measurements over a period\nof time. Learn how to configure connectors to  Migrate an Existing Collection to a Time Series Collection . Improved the error logging for bulk write exceptions No new changes, additions or improvements Corrected the behavior of  LazyBsonDocument#clone  to respond to any\nchanges made once unwrapped Fixed the timestamp integer overflow in the Source Connector Updated to enable recovery when calling the  getMore()  method in the\nSource Connector Updated to enable recovery from broken change stream due to event sizes that\nare greater than 16 MB in the Source Connector Updated the MongoDB Java driver dependency to version 4.2 Added the  DeleteOneBusinessKeyStrategy  write strategy to remove records\nfrom a topic Added support for handling errant records that cause problems when\nprocessing them Added support for Qlik Replicate Change Data Capture (CDC) to process event\nstreams Replaced  BsonDocument  with  RawBsonDocument Improved the  copy.existing  namespace handling Improved the error messages for invalid pipeline operators Improved the efficiency of heartbeats by making them tombstone messages Corrected the inferred schema naming conventions Updated to ensure that schemas can be backwards compatible Fixed the Sink validation issue with  topics.regex Fixed the Sink NPE issue when using with Confluent Connect 6.1.0 Updated to ensure that the change stream cursor closes so it only reports\nerrors that exist Changed to include or exclude the  _id  field for a projection only if\nit's explicitly added Updated the MongoDB Java Driver to version 4.1 Added support for Change Data Capture (CDC) based on MongoDB change stream\nevents Added the  NamespaceMapper  interface to allow for dynamic namespace\nmapping Added the  TopicMapper  interface to allow topic mapping Changed the top-level inferred schema to be mandatory Fixed a validation issue and synthetic configuration property in the Sink\nConnector Corrected general exception logging Updated to clone the  LazyBsonDocument  instead of the unwrapped\n BsonDocument Added automated integration testing for the latest Kafka Connector and\nConfluent Platform versions to ensure compatibility Added support for records that contain  Bson  byte types Added support for the  errors.tolerance  property Changed  max.num.retries  default to  1 Improved the error messages for business key errors Improved the error handling for  List  and JSON array configuration options Updated to use the dot notation for filters in key update strategies Added support to output a key or value as a  Bson  byte type Added support for schema and custom Avro schema definitions Added support for dead letter queue and the  errors.tolerance  property Added configurations for the following formatters: DefaultJson ExtendedJson SimplifiedJson Added configuration for  copy.existing.pipeline  to allow you to use\nindexes during the copying process Added configuration for  copy.existing.namespace.regex  to allow you to\ncopy the filtering of namespaces Added configuration for  offset.partition.name  to allow for custom\npartitioning naming strategies Updated to validate that the  fullDocument  field is a document Updated to sanitize the connection string in the offset partition map to\nimprove maintenance of the  connection.uri ,  database , and\n collection  parameters Updated to disable publishing a source record without a topic name Stopped MongoDB 3.6 from copying existing issues when the collection\ndidn't exist in the Source Connector We deprecated the following post processors: If you are using one of these post processors, use the respective one\ninstead for future compatibility: BlacklistKeyProjector BlacklistValueProjector WhitelistKeyProjector WhitelistValueProjector BlockListKeyProjector BlockListValueProjector , AllowListKeyProjector AllowListValueProjector Added configurations for the following properties: document.id.strategy.overwrite.existing UuidStrategy  output types document.id.strategy.partial.value.projection.type document.id.strategy.partial.value.projection.list document.id.strategy.partial.key.projection.type document.id.strategy.partial.key.projection.list UuidProvidedInKeyStrategy UuidProvidedInValueStrategy Added the  UpdateOneBusinessKeyTimestampStrategy  post processor Added built-in support for parallelism and scalable data copying by\nassigning topic partitions to tasks Improved the error messaging for missing resume tokens Removed failures with the  MongoCopyDataManager  when the source database\ndoes not exist Fixed the copying the existing resumability error in the Source Connector Added support for the  topics.regex  property Updated to ignore unused source record key or value fields Added validation for the connection using  MongoSinkConnector.validate Added validation for the connection using  MongoSourceConnector.validate Removed the  \"Unrecognized field: startAfter\"  error for resuming a change\nstream in the Source Connector The initial GA release.",
            "code": [
                {
                    "lang": "properties",
                    "value": "connection.uri=mongodb://mongodb0.example.com:27017,mongodb1.example.com:27017,mongodb2.example.com:27017/?replicaSet=myRepl&retryWrites=false"
                }
            ],
            "preview": "Learn what's new by version:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorials/tutorial-setup",
            "title": "Kafka Connector Tutorial Setup",
            "headings": [
                "Requirements",
                "Set Up Your Development Environment with Docker",
                "Clone or Download the Tutorial Repository",
                "Run the Development Environment",
                "Verify the Successful Setup"
            ],
            "paragraphs": "The tutorials in this section run on a development environment using Docker to\npackage the dependencies and configurations you need to run the\nMongoDB Connector for Apache Kafka. Make sure you complete the development environment setup\nsteps before proceeding to the tutorials. Create or sign into your Docker account to download the Docker containers\nyou need for this tutorial. To learn how to sign up for an account and\ninstall Docker Desktop, read the signup and download steps in the\n Docker Hub Quickstart . A terminal app and shell. For MacOS users, use Terminal or a similar app.\nFor Windows users, use PowerShell. Optional. Install  git  to download the setup files. To learn how to\ninstall git, read the  Git Downloads \npage. Next, clone the tutorial git repository with the following command: If you do not have git installed, you can download the\n zip archive \ninstead. Select the tab that matches your OS for instructions on how to run the\ncommands in this guide: Start the Docker image with the following command: The \"mongo-kafka-base\" image creates a Docker container that includes\nall the services you need in the tutorial and runs them on a shared\nnetwork called \"mongodb-kafka-base_localnet\" as shown in the following\ndiagram: When the command completes successfully, it outputs the following\ntext: Navigate to the tutorial directory \"mongodb-kafka-base\" within\nthe repository or unzipped archive using bash shell. If you\ncloned the repository with git, your command resembles the\nfollowing: Navigate to the tutorial directory \"mongodb-kafka-base\" within\nthe repository or unzipped archive using PowerShell.  If you\ncloned the repository with git, your command resembles the\nfollowing: The sandbox maps the following services to ports on your host\nmachine: These ports must be free to start the sandbox. The sandbox MongoDB server maps to port  35001  on your host machine The sandbox Kafka Connect JMX server maps to port  35000  on your host machine Confirm the development environment started normally by running the\nfollowing commands: This command should output the following information if the Docker\ndevelopment environment was set up successfully: Since you have not started the connectors, the status and configured\nlist are empty. Your development environment setup is complete and you can proceed to\nthe next step of the tutorial. You can connect to the MongoDB server running in your\ndevelopment environment with the following connection string:",
            "code": [
                {
                    "lang": "bash",
                    "value": "git clone https://github.com/mongodb-university/kafka-edu.git"
                },
                {
                    "lang": "bash",
                    "value": "docker-compose -p mongo-kafka up -d --force-recreate"
                },
                {
                    "lang": "bash",
                    "value": "cd kafka-edu/docs-examples/mongodb-kafka-base/"
                },
                {
                    "lang": "none",
                    "value": "cd kafka-edu\\docs-examples\\mongodb-kafka-base\\"
                },
                {
                    "lang": "text",
                    "value": "...\nCreating zookeeper ... done\nCreating broker    ... done\nCreating schema-registry ... done\nCreating connect         ... done\nCreating rest-proxy      ... done\nCreating mongo1          ... done\nCreating mongo1-setup    ... done"
                },
                {
                    "lang": "bash",
                    "value": "docker exec mongo1 status"
                },
                {
                    "lang": "text",
                    "value": "Kafka topics:\n\n    \"topic\": \"docker-connect-status\",\n    \"topic\": \"docker-connect-offsets\",\n    \"topic\": \"docker-connect-configs\",\n    \"topic\": \"__consumer_offsets\",\n\nThe status of the connectors:\n\n\nCurrently configured connectors\n\n[]\n\n\nVersion of MongoDB Connector for Apache Kafka installed:\n\n{\"class\":\"com.mongodb.kafka.connect.MongoSinkConnector\",\"type\":\"sink\",\"version\":\"1.8.0\"}\n{\"class\":\"com.mongodb.kafka.connect.MongoSourceConnector\",\"type\":\"source\",\"version\":\"1.8.0\"}"
                },
                {
                    "lang": "text",
                    "value": "mongodb://localhost:35001/?directConnection=true"
                }
            ],
            "preview": "The tutorials in this section run on a development environment using Docker to\npackage the dependencies and configurations you need to run the\nMongoDB Connector for Apache Kafka. Make sure you complete the development environment setup\nsteps before proceeding to the tutorials.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorials/sink-connector",
            "title": "Getting Started with the MongoDB Kafka Sink Connector",
            "headings": [
                "Get Started with the MongoDB Kafka Sink Connector",
                "Complete the Tutorial Setup",
                "Configure the Sink Connector",
                "Write Data to a Kafka Topic",
                "View the Data in the MongoDB Collection",
                "(Optional) Stop the Docker Containers",
                "Summary",
                "Learn More"
            ],
            "paragraphs": "Follow this tutorial to learn how to configure a MongoDB Kafka sink connector\nto read data from an Apache Kafka topic and write it to a MongoDB\ncollection. :style:connected Complete the steps in the  Kafka Connector Tutorial Setup  to start the\nthe Confluent Kafka Connect and MongoDB environment. Create an interactive shell session on the tutorial Docker Container\nusing the following command: Create a source configuration file called  simplesink.json  with\nthe following command: Paste the following configuration information into the file and save\nyour changes: Run the following command in the shell to start the sink connector\nusing the configuration file you created: Run the following command in the shell to check the status of the\nconnectors: If your sink connector started successfully, you should see the\nfollowing output: The highlighted lines in the configuration properties specify\n converters  which instruct the connector how to translate the data\nfrom Kafka. The  cx  command is a custom script included in the tutorial\ndevelopment environment. This script runs the following\nequivalent request to the Kafka Connect REST API to create a new\nconnector: In the same shell, create a Python script to write data to a Kafka\ntopic. Paste the following code into the file and save your changes: Run the Python script: In the same shell, connect to MongoDB using  mongosh , the MongoDB\nshell by running the following command: After you connect successfully, you should see the following\nMongoDB shell prompt: At the prompt, type the following commands to retrieve all the\ndocuments in the  Tutorial2.pets  MongoDB namespace: You should see the following document returned as the result: Exit the MongoDB shell by entering the command  exit . After you complete this tutorial, free resources on your computer\nby stopping or removing Docker assets. You can choose to remove\nboth the Docker containers and images, or exclusively the\ncontainers. If you remove the containers and images, you must\ndownload them again to restart your MongoDB Kafka Connector development environment,\nwhich is approximately 2.4 GB in size. If you\nexclusively remove the containers, you can reuse the images and avoid\ndownloading most of the large files in the sample data pipeline. Select the tab that corresponds to the removal task you want to run. To restart the containers, follow the same steps required to start them\nin the  Tutorial Setup . If you plan to complete any more MongoDB Kafka Connector tutorials,\nconsider removing only containers. If you don't plan\nto complete any more MongoDB Kafka Connector tutorials, consider\nremoving containers and images. Run the following shell command to remove the Docker containers and\nimages for the development environment: Run the following shell command to remove the Docker containers but\nkeep the images for the development environment: In this tutorial, you configured a sink connector to save data from\na Kafka topic to a collection in a MongoDB cluster. Read the following resources to learn more about concepts mentioned in\nthis tutorial: Sink Connector Configuration Properties Introduction to Kafka Connector Converters Kafka Connect REST API",
            "code": [
                {
                    "lang": "bash",
                    "value": "docker exec -it mongo1 /bin/bash"
                },
                {
                    "lang": "bash",
                    "value": "nano simplesink.json"
                },
                {
                    "lang": "bash",
                    "value": "cx simplesink.json"
                },
                {
                    "lang": "bash",
                    "value": "status"
                },
                {
                    "lang": "",
                    "value": "{\n  \"name\": \"mongo-tutorial-sink\",\n  \"config\": {\n    \"connector.class\": \"com.mongodb.kafka.connect.MongoSinkConnector\",\n    \"topics\": \"Tutorial2.pets\",\n    \"connection.uri\": \"mongodb://mongo1\",\n    \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n    \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n    \"value.converter.schemas.enable\": false,\n    \"database\": \"Tutorial2\",\n    \"collection\": \"pets\"\n  }\n}\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -X POST -H \"Content-Type: application/json\" -d @simplesink.json http://connect:8083/connectors -w \"\\n\""
                },
                {
                    "lang": "text",
                    "value": "Kafka topics:\n...\nThe status of the connectors:\n\nsink  |  mongo-tutorial-sink  |  RUNNING  |  RUNNING  |  com.mongodb.kafka.connect.MongoSinkConnector\n\nCurrently configured connectors\n\n[\n\"mongo-tutorial-sink\"\n]\n...\n"
                },
                {
                    "lang": "bash",
                    "value": "nano kafkawrite.py"
                },
                {
                    "lang": "bash",
                    "value": "python3 kafkawrite.py"
                },
                {
                    "lang": "python",
                    "value": "from kafka import KafkaProducer\nimport json\nfrom json import dumps\n\np = KafkaProducer(bootstrap_servers = ['broker:29092'], value_serializer = lambda x:dumps(x).encode('utf-8'))\n\ndata = {'name': 'roscoe'}\n\np.send('Tutorial2.pets', value = data)\n\np.flush()"
                },
                {
                    "lang": "bash",
                    "value": "mongosh \"mongodb://mongo1\""
                },
                {
                    "lang": "none",
                    "value": "rs0 [direct: primary] test>"
                },
                {
                    "lang": "javascript",
                    "value": "use Tutorial2\ndb.pets.find()"
                },
                {
                    "lang": "json",
                    "value": "{ _id: ObjectId(\"62659...\"), name: 'roscoe' }"
                },
                {
                    "lang": "shell",
                    "value": "docker-compose -p mongo-kafka down --rmi all"
                },
                {
                    "lang": "shell",
                    "value": "docker-compose -p mongo-kafka down"
                }
            ],
            "preview": "Follow this tutorial to learn how to configure a MongoDB Kafka sink connector\nto read data from an Apache Kafka topic and write it to a MongoDB\ncollection.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorials/replicate-with-cdc",
            "title": "Replicate Data with a Change Data Capture Handler",
            "headings": [
                "Overview",
                "Replicate Data with a CDC Handler",
                "Complete the Tutorial Setup",
                "Start Interactive Shells",
                "Configure the Source Connector",
                "Configure the Sink Connector",
                "Monitor the Kafka Topic",
                "Write Data into the Source and Watch the Data Flow",
                "(Optional) Generate Additional Changes",
                "Summary",
                "Learn More"
            ],
            "paragraphs": "Follow this tutorial to learn how to use a\n change data capture (CDC) handler  to replicate data with the MongoDB Kafka Connector.\nA CDC handler is an application that translates CDC events into MongoDB\nwrite operations. Use a CDC handler when you need to reproduce the changes\nin one datastore into another datastore. In this tutorial, you configure and run MongoDB Kafka source and sink\nconnectors to make two MongoDB collections contain the same documents using\nCDC. The source connector writes change stream data from the original\ncollection to a Kafka topic and the sink connector writes the Kafka topic\ndata to the target MongoDB collection. If you want to learn more about how CDC handlers work, see the\n Change Data Capture Handlers  guide. Complete the steps in the  Kafka Connector Tutorial Setup  to start the\nthe Confluent Kafka Connect and MongoDB environment. Start two interactive shells on the Docker container in separate\nwindows. In the tutorial, you can use the shells to run and observe\ndifferent tasks. Run the following command from a terminal to start an interactive shell. We will refer to this interactive shell as  CDCShell1  throughout this tutorial. Run the following command in a second terminal to start an interactive shell: We will refer to this interactive shell as  CDCShell2  throughout this tutorial. Arrange the two windows on your screen to keep both of them visible to\nsee real-time updates. Use  CDCShell1  to configure your connectors and monitor your Kafka\ntopic. Use  CDCShell2  to perform write operations in MongoDB. In  CDCShell1 , configure a source connector to read from the\n CDCTutorial.Source  MongoDB namespace and write to the\n CDCTutorial.Source  Kafka topic. Create a configuration file called  cdc-source.json  using the\nfollowing command: Paste the following configuration information into the file and save\nyour changes: Run the following command in  CDCShell1  to start the source connector\nusing the configuration file you created: Run the following command in the shell to check the status of the\nconnectors: If your source connector started successfully, you should see the\nfollowing output: The  cx  command is a custom script included in the tutorial\ndevelopment environment. This script runs the following\nequivalent request to the Kafka Connect REST API to create a new\nconnector: In  CDCShell1 , configure a sink connector to copy data from the\n CDCTutorial.Source  Kafka topic to  CDCTutorial.Destination \nMongoDB namespace. Create a configuration file called  cdc-sink.json  using the\nfollowing command: Paste the following configuration information into the file and save\nyour changes: Run the following command in the shell to start the sink connector\nusing the configuration file you created: Run the following command in the shell to check the status of the\nconnectors: If your sink connector started successfully, you should see the\nfollowing output: In  CDCShell1 , monitor the Kafka topic for incoming events. Run the\nfollowing command to start the  kafkacat  application which outputs\ndata published to the topic: Once started, you should see the following output that indicates there\nis currently no data to read: The  kc  command is a custom script included in the tutorial\ndevelopment environment that calls the  kafkacat  application\nwith options to connect to Kafka and format the output of the\nspecified topic. In  CDCShell2 , connect to MongoDB using  mongosh , the MongoDB\nshell by running the following command: After you connect successfully, you should see the following\nMongoDB shell prompt: At the prompt, type the following commands to insert a new document\ninto the  CDCTutorial.Source  MongoDB namespace: Once MongoDB completes the insert command, you should receive an\nacknowledgment that resembles the following text: The source connector picks up the change and publishes it to the\nKafka topic. You should see the following topic message in your\n CDCShell1  window: The sink connector picks up the Kafka message and sinks the data\ninto MongoDB. You can retrieve the document from the\n CDCTutorial.Destination  namespace in MongoDB by running the\nfollowing command in the MongoDB shell you started in  CDCShell2 : You should see the following document returned as the result: Try removing documents from the  CDCTutorial.Source  namespace\nby running the following command from the MongoDB shell: You should see the following topic message in your  CDCShell1 \nwindow: Run the following command to retrieve the current number of documents\nin the collection: This returns the following output, indicating the collection is empty: Run the following command to exit the MongoDB shell: In this tutorial, you set up a source connector to capture changes to a\nMongoDB collection and send them to Apache Kafka. You also configured a\nsink connector with a MongoDB CDC Handler to move the data from Apache\nKafka to a MongoDB collection. Read the following resources to learn more about concepts mentioned in\nthis tutorial: Change Data Capture Handlers Change Streams Sink Connector Source Connector",
            "code": [
                {
                    "lang": "bash",
                    "value": "docker exec -it mongo1 /bin/bash"
                },
                {
                    "lang": "bash",
                    "value": "docker exec -it mongo1 /bin/bash"
                },
                {
                    "lang": "bash",
                    "value": "nano cdc-source.json"
                },
                {
                    "lang": "bash",
                    "value": "cx cdc-source.json"
                },
                {
                    "lang": "bash",
                    "value": "status"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"mongo-cdc-source\",\n  \"config\": {\n    \"connector.class\": \"com.mongodb.kafka.connect.MongoSourceConnector\",\n    \"connection.uri\": \"mongodb://mongo1\",\n    \"database\": \"CDCTutorial\",\n    \"collection\": \"Source\"\n  }\n}\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -X POST -H \"Content-Type: application/json\" -d @cdc-source.json http://connect:8083/connectors -w \"\\n\""
                },
                {
                    "lang": "text",
                    "value": "Kafka topics:\n...\nThe status of the connectors:\n\nsource  |  mongo-cdc-source  |  RUNNING  |  RUNNING  |  com.mongodb.kafka.connect.MongoSourceConnector\n\nCurrently configured connectors\n\n[\n\"mongo-cdc-source\"\n]\n...\n"
                },
                {
                    "lang": "bash",
                    "value": "nano cdc-sink.json"
                },
                {
                    "lang": "bash",
                    "value": "cx cdc-sink.json"
                },
                {
                    "lang": "bash",
                    "value": "status"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"mongo-cdc-sink\",\n  \"config\": {\n    \"connector.class\": \"com.mongodb.kafka.connect.MongoSinkConnector\",\n    \"topics\": \"CDCTutorial.Source\",\n    \"change.data.capture.handler\": \"com.mongodb.kafka.connect.sink.cdc.mongodb.ChangeStreamHandler\",\n    \"connection.uri\": \"mongodb://mongo1\",\n    \"database\": \"CDCTutorial\",\n    \"collection\": \"Destination\"\n  }\n}\n"
                },
                {
                    "lang": "text",
                    "value": "Kafka topics:\n...\nThe status of the connectors:\n\nsink    |  mongo-cdc-sink    |  RUNNING  |  RUNNING  |  com.mongodb.kafka.connect.MongoSinkConnector\nsource  |  mongo-cdc-source  |  RUNNING  |  RUNNING  |  com.mongodb.kafka.connect.MongoSourceConnector\n\nCurrently configured connectors\n\n[\n\"mongo-cdc-sink\"\n\"mongo-cdc-source\"\n]\n...\n"
                },
                {
                    "lang": "bash",
                    "value": "kc CDCTutorial.Source"
                },
                {
                    "lang": "none",
                    "value": "% Reached end of topic CDCTutorial.Source [0] at offset 0"
                },
                {
                    "lang": "bash",
                    "value": "mongosh \"mongodb://mongo1\""
                },
                {
                    "lang": null,
                    "value": "rs0 [direct: primary] test>"
                },
                {
                    "lang": "json",
                    "value": "use CDCTutorial\ndb.Source.insertOne({ proclaim: \"Hello World!\" });"
                },
                {
                    "lang": "json",
                    "value": "{\n  acknowledged: true,\n  insertedId: ObjectId(\"600b38ad...\")\n}"
                },
                {
                    "lang": "json",
                    "value": "db.Destination.find()"
                },
                {
                    "lang": "json",
                    "value": "[\n  {\n    _id: ObjectId(\"600b38a...\"),\n    proclaim: 'Hello World'\n  }\n]"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"schema\": { \"type\": \"string\", \"optional\": false },\n  \"payload\": {\n    \"_id\": { \"_data\": \"8260...\" },\n    \"operationType\": \"insert\",\n    \"clusterTime\": { \"$timestamp\": { \"t\": 1611..., \"i\": 2 } },\n    \"wallTime\": { \"$date\": \"...\" },\n    \"fullDocument\": {\n      \"_id\": { \"$oid\": \"600b38ad...\" },\n      \"proclaim\": \"Hello World!\"\n    },\n    \"ns\": { \"db\": \"CDCTutorial\", \"coll\": \"Source\" },\n    \"documentKey\": { \"_id\": { \"$oid\": \"600b38a...\" } }\n  }\n}\n"
                },
                {
                    "lang": "json",
                    "value": "db.Source.deleteMany({})"
                },
                {
                    "lang": "json",
                    "value": "db.Destination.count()"
                },
                {
                    "lang": "none",
                    "value": "0"
                },
                {
                    "lang": "none",
                    "value": "exit"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"schema\": { \"type\": \"string\", \"optional\": false },\n  \"payload\": {\n    \"_id\": { \"_data\": \"8261....\" },\n    ...\n    \"operationType\": \"delete\",\n    \"clusterTime\": { \"$timestamp\": { \"t\": 1631108282, \"i\": 1 } },\n    \"ns\": { \"db\": \"CDCTutorial\", \"coll\": \"Source\" },\n    \"documentKey\": { \"_id\": { \"$oid\": \"6138...\" } }\n  }\n}\n"
                }
            ],
            "preview": "Follow this tutorial to learn how to use a\nchange data capture (CDC) handler to replicate data with the MongoDB Kafka Connector.\nA CDC handler is an application that translates CDC events into MongoDB\nwrite operations. Use a CDC handler when you need to reproduce the changes\nin one datastore into another datastore.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorials/source-connector",
            "title": "Getting Started with the MongoDB Kafka Source Connector",
            "headings": [
                "Get Started with the MongoDB Kafka Source Connector",
                "Complete the Tutorial Setup",
                "Configure the Source Connector",
                "Create Change Events",
                "Reconfigure the Change Stream",
                "(Optional) Stop the Docker Containers",
                "Summary",
                "Learn More"
            ],
            "paragraphs": "Follow this tutorial to learn how to configure a MongoDB Kafka source connector\nto read data from a change stream and publish it to an Apache Kafka topic. Complete the steps in the  Kafka Connector Tutorial Setup  to start the\nthe Confluent Kafka Connect and MongoDB environment. Create an interactive shell session on the tutorial Docker container\ndownloaded for the Tutorial Setup using the following command: Create a source configuration file called  simplesource.json  with\nthe following command: Paste the following configuration information into the file and save\nyour changes: Run the following command in the shell to start the source connector\nusing the configuration file you created: Run the following command in the shell to check the status of the\nconnectors: If your source connector started successfully, you should see the\nfollowing output: The  cx  command is a custom script included in the tutorial\ndevelopment environment. This script runs the following\nequivalent request to the Kafka Connect REST API to create a new\nconnector: In the same shell, connect to MongoDB using  mongosh , the MongoDB\nshell by running the following command: After you connect successfully, you should see the following\nMongoDB shell prompt: At the prompt, type the following commands to insert a new document: Once MongoDB completes the insert command, you should receive an\nacknowledgment that resembles the following text: Exit the MongoDB shell by entering the command  exit . Check the status of your Kafka environment using the following\ncommand: In the output of the preceding command, you should see the new topic\nthat the source connector created after receiving the change event: Confirm the content of data on the new Kafka topic by running the\nfollowing command: You should see the following Kafka topic data, organized by \"Key\"\nand \"Value\" sections when you run the preceding command: From the \"Value\" section of the output, you can find the part of the\n payload  that includes the  fullDocument  data as highlighted in\nthe following formatted JSON document: The  kc  command is a helper script that outputs the content of\na Kafka topic. You can omit the metadata from the events created by the change\nstream by configuring it to only return the  fullDocument  field. Stop the connector using the following command: Edit the source configuration file called  simplesource.json  with\nthe following command: Remove the existing configuration, add the following configuration,\nand save the file: Run the following command in the shell to start the source connector\nusing the configuration file you updated: Connect to MongoDB using  mongosh  using the following command: At the prompt, type the following commands to insert a new document: Exit  mongosh  by running the following command: Confirm the content of data on the new Kafka topic by running the\nfollowing command: The  payload  field in the \"Value\" document should contain only the\nfollowing document data: The  del  command is a helper script that calls the Kafka\nConnect REST API to stop the connector and is equivalent to the\nfollowing command: After you complete this tutorial, free resources on your computer\nby stopping or removing Docker assets. You can choose to remove\nboth the Docker containers and images, or exclusively the\ncontainers. If you remove the containers and images, you must\ndownload them again to restart your MongoDB Kafka Connector development environment,\nwhich is approximately 2.4 GB in size. If you\nexclusively remove the containers, you can reuse the images and avoid\ndownloading most of the large files in the sample data pipeline. Select the tab that corresponds to the removal task you want to run. To restart the containers, follow the same steps required to start them\nin the  Tutorial Setup . If you plan to complete any more MongoDB Kafka Connector tutorials,\nconsider removing only containers. If you don't plan\nto complete any more MongoDB Kafka Connector tutorials, consider\nremoving containers and images. Run the following shell command to remove the Docker containers and\nimages for the development environment: Run the following shell command to remove the Docker containers but\nkeep the images for the development environment: In this tutorial, you started a source connector using different\nconfigurations to alter the change stream event data published to a Kafka\ntopic. Read the following resources to learn more about concepts mentioned in\nthis tutorial: Source Connector Configuration Properties Kafka Connect REST API",
            "code": [
                {
                    "lang": "bash",
                    "value": "docker exec -it mongo1 /bin/bash"
                },
                {
                    "lang": "bash",
                    "value": "nano simplesource.json"
                },
                {
                    "lang": "bash",
                    "value": "cx simplesource.json"
                },
                {
                    "lang": "bash",
                    "value": "status"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"mongo-simple-source\",\n  \"config\": {\n    \"connector.class\": \"com.mongodb.kafka.connect.MongoSourceConnector\",\n    \"connection.uri\": \"mongodb://mongo1\",\n    \"database\": \"Tutorial1\",\n    \"collection\": \"orders\"\n  }\n}\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -X POST -H \"Content-Type: application/json\" -d @simplesource.json http://connect:8083/connectors -w \"\\n\""
                },
                {
                    "lang": "text",
                    "value": "Kafka topics:\n...\n\nThe status of the connectors:\n\nsource  |  mongo-simple-source  |  RUNNING  |  RUNNING  |  com.mongodb.kafka.connect.MongoSourceConnector\n\nCurrently configured connectors\n\n[\n  \"mongo-simple-source\"\n]\n...\n"
                },
                {
                    "lang": "bash",
                    "value": "mongosh \"mongodb://mongo1\""
                },
                {
                    "lang": null,
                    "value": "rs0 [direct: primary] test>"
                },
                {
                    "lang": "javascript",
                    "value": "use Tutorial1\ndb.orders.insertOne( { 'order_id' : 1, 'item' : 'coffee' } )"
                },
                {
                    "lang": "json",
                    "value": "{\n  acknowledged: true,\n  insertedId: ObjectId(\"627e7e...\")\n}"
                },
                {
                    "lang": "bash",
                    "value": "status"
                },
                {
                    "lang": null,
                    "value": "...\n\"topic\": \"Tutorial1.orders\",\n..."
                },
                {
                    "lang": "bash",
                    "value": "kc Tutorial1.orders"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": {\n    \"_data\": \"8262655A...\"\n  },\n  \"operationType\": \"insert\",\n  \"clusterTime\": {\n    \"$timestamp\": {\n      \"t\": 1650809557,\n      \"i\": 2\n    }\n  },\n  \"wallTime\": {\n    \"$date\": \"2022-10-13T17:06:23.409Z\"\n  },\n  \"fullDocument\": {\n    \"_id\": {\n      \"$oid\": \"62655a...\"\n    },\n    \"order_id\": 1,\n    \"item\": \"coffee\"\n  },\n  \"ns\": {\n    \"db\": \"Tutorial1\",\n    \"coll\": \"orders\"\n  },\n  \"documentKey\": {\n    \"_id\": {\n      \"$oid\": \"62655a...\"\n    }\n  }\n}\n"
                },
                {
                    "lang": "bash",
                    "value": "del mongo-simple-source"
                },
                {
                    "lang": "bash",
                    "value": "nano simplesource.json"
                },
                {
                    "lang": "bash",
                    "value": "cx simplesource.json"
                },
                {
                    "lang": "bash",
                    "value": "mongosh \"mongodb://mongo1\""
                },
                {
                    "lang": "bash",
                    "value": "use Tutorial1\ndb.orders.insertOne( { 'order_id' : 2, 'item' : 'oatmeal' } )"
                },
                {
                    "lang": "bash",
                    "value": "exit"
                },
                {
                    "lang": "bash",
                    "value": "kc Tutorial1.orders"
                },
                {
                    "lang": "bash",
                    "value": "curl -X DELETE connect:8083/connectors/<parameter>"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"mongo-simple-source\",\n  \"config\": {\n    \"connector.class\": \"com.mongodb.kafka.connect.MongoSourceConnector\",\n    \"connection.uri\": \"mongodb://mongo1\",\n    \"publish.full.document.only\": true,\n    \"database\": \"Tutorial1\",\n    \"collection\": \"orders\"\n  }\n}\n"
                },
                {
                    "lang": "json",
                    "value": "{ \"_id\": { \"$oid\": \"<your _id value>\" }, \"order_id\": 2, \"item\": \"oatmeal\" }\n"
                },
                {
                    "lang": "shell",
                    "value": "docker-compose -p mongo-kafka down --rmi all"
                },
                {
                    "lang": "shell",
                    "value": "docker-compose -p mongo-kafka down"
                }
            ],
            "preview": "Follow this tutorial to learn how to configure a MongoDB Kafka source connector\nto read data from a change stream and publish it to an Apache Kafka topic.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorials/explore-change-streams",
            "title": "Explore MongoDB Change Streams",
            "headings": [
                "Explore Change Streams",
                "Complete the Tutorial Setup",
                "Connect to the Docker Container",
                "Open a Change Stream",
                "Trigger a Change Event",
                "Open a Filtered Change Stream",
                "Observe the Filtered Change Stream",
                "(Optional) Stop the Docker Containers",
                "Summary",
                "Learn More"
            ],
            "paragraphs": "Follow this tutorial to learn how to create a change stream on a MongoDB\ncollection and observe the change events it creates. Complete the steps in the  Kafka Connector Tutorial Setup  to start the\nthe Confluent Kafka Connect and MongoDB environment. Create two interactive shell sessions on the tutorial Docker\nContainer, each in a separate window. Run the following command from a terminal to start an interactive shell. We will refer to this interactive shell as  ChangeStreamShell1  throughout this tutorial. Run the following command in a second terminal to start an interactive shell: We will refer to this interactive shell as  ChangeStreamShell2  throughout this tutorial. In  ChangeStreamShell1 , create a Python script to open a change\nstream using the PyMongo driver. Paste the following code into the file and save the changes: Run the Python script: The script outputs the following message after it starts successfully: In  ChangeStreamShell2 , connect to MongoDB using  mongosh , the MongoDB\nshell, using the following command: After you connect successfully, you should see the following\nMongoDB shell prompt: At the prompt, type the following commands: After entering the preceding commands, switch to  ChangeStreamShell1  to view\nthe change stream output, which should resemble the following: To stop the script, press  Ctrl+C . By the end of this step, you've successfully triggered and observed a\nchange stream event. You can apply a filter to a change stream by passing it an aggregation\npipeline. In  ChangeStreamShell1 , create a new Python script to open a filtered change\nstream using the PyMongo driver. Paste the following code into the file and save the changes: Run the Python script: The script outputs the following message after it starts successfully: Return to your  ChangeStreamShell2  session which should be connected to\nMongoDB using  mongosh . At the prompt, type the following commands: As indicated by the script output, the change stream creates a change\nevent because it matches the following pipeline: Try inserting the following documents in in  ChangeStreamShell2  to verify the\nchange stream only produces events when the documents match the filter: After you complete this tutorial, free resources on your computer\nby stopping or removing Docker assets. You can choose to remove\nboth the Docker containers and images, or exclusively the\ncontainers. If you remove the containers and images, you must\ndownload them again to restart your MongoDB Kafka Connector development environment,\nwhich is approximately 2.4 GB in size. If you\nexclusively remove the containers, you can reuse the images and avoid\ndownloading most of the large files in the sample data pipeline. Select the tab that corresponds to the removal task you want to run. To restart the containers, follow the same steps required to start them\nin the  Tutorial Setup . If you plan to complete any more MongoDB Kafka Connector tutorials,\nconsider removing only containers. If you don't plan\nto complete any more MongoDB Kafka Connector tutorials, consider\nremoving containers and images. Run the following shell command to remove the Docker containers and\nimages for the development environment: Run the following shell command to remove the Docker containers but\nkeep the images for the development environment: In this tutorial, you created a change stream on MongoDB and observed the\noutput. The MongoDB Kafka source connector reads the change events from a change\nstream that you configure, and writes them to a Kafka topic. To learn how to configure a change stream and Kafka topic for a source\nconnector, proceed to the  Getting Started with the MongoDB Kafka Source Connector \ntutorial. Read the following resources to learn more about concepts mentioned in\nthis tutorial: Change Streams and the Source Connector Modify Change Stream Output MongoDB Shell (mongosh)",
            "code": [
                {
                    "lang": "bash",
                    "value": "docker exec -it mongo1 /bin/bash"
                },
                {
                    "lang": "bash",
                    "value": "docker exec -it mongo1 /bin/bash"
                },
                {
                    "lang": "bash",
                    "value": "nano openchangestream.py"
                },
                {
                    "lang": "bash",
                    "value": "python3 openchangestream.py"
                },
                {
                    "lang": "bash",
                    "value": "Change Stream is opened on the Tutorial1.orders namespace.  Currently watching ..."
                },
                {
                    "lang": "python",
                    "value": "import pymongo\nfrom bson.json_util import dumps\n\nclient = pymongo.MongoClient('mongodb://mongo1')\ndb = client.get_database(name='Tutorial1')\nwith db.orders.watch() as stream:\n    print('\\nA change stream is open on the Tutorial1.orders namespace.  Currently watching ...\\n\\n')\n    for change in stream:\n        print(dumps(change, indent = 2))"
                },
                {
                    "lang": "bash",
                    "value": "mongosh \"mongodb://mongo1\""
                },
                {
                    "lang": "none",
                    "value": "rs0 [direct: primary] test>"
                },
                {
                    "lang": "javascript",
                    "value": "use Tutorial1\ndb.orders.insertOne( { 'test' : 1 } )"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": {\n    \"_data\": \"826264...\"\n  },\n  \"operationType\": \"insert\",\n  \"clusterTime\": {\n    \"$timestamp\": {\n      \"t\": 1650754657,\n      \"i\": 1\n    }\n  },\n  \"wallTime\": {\n    \"$date\": \"2022-10-13T17:06:23.409Z\"\n  },\n  \"fullDocument\": {\n    \"_id\": {\n      \"$oid\": \"<_id value of document>\"\n    },\n    \"test\": 1\n  },\n  \"ns\": {\n    \"db\": \"Tutorial1\",\n    \"coll\": \"orders\"\n  },\n  \"documentKey\": {\n    \"_id\": {\n      \"$oid\": \"<_id value of document>\"\n    }\n  }\n}\n"
                },
                {
                    "lang": "bash",
                    "value": "nano pipeline.py"
                },
                {
                    "lang": "python",
                    "value": "python3 pipeline.py"
                },
                {
                    "lang": null,
                    "value": "Change Stream is opened on the Tutorial1.sensors namespace.  Currently watching for values > 100..."
                },
                {
                    "lang": "python",
                    "value": "import pymongo\nfrom bson.json_util import dumps\nclient = pymongo.MongoClient('mongodb://mongo1')\ndb = client.get_database(name='Tutorial1')\npipeline = [ { \"$match\": { \"$and\": [ { \"fullDocument.type\": \"temp\" }, { \"fullDocument.value\": { \"$gte\": 100 } } ] } } ]\nwith db.sensors.watch(pipeline=pipeline) as stream:\n    print('\\nChange Stream is opened on the Tutorial1.sensors namespace.  Currently watching for values > 100...\\n\\n')\n    for change in stream:\n        print(dumps(change, indent = 2))\n"
                },
                {
                    "lang": "javascript",
                    "value": "use Tutorial1\ndb.sensors.insertOne( { 'type' : 'temp', 'value':101 } )"
                },
                {
                    "lang": "json",
                    "value": "[ { \"$match\": { \"$and\": [ { \"fullDocument.type\": \"temp\" }, { \"fullDocument.value\": { \"$gte\": 100 } } ] } } ]"
                },
                {
                    "lang": "javascript",
                    "value": "db.sensors.insertOne( { 'type' : 'temp', 'value': 99 } )\ndb.sensors.insertOne( { 'type' : 'pressure', 'value': 22 } )"
                },
                {
                    "lang": "shell",
                    "value": "docker-compose -p mongo-kafka down --rmi all"
                },
                {
                    "lang": "shell",
                    "value": "docker-compose -p mongo-kafka down"
                }
            ],
            "preview": "Follow this tutorial to learn how to create a change stream on a MongoDB\ncollection and observe the change events it creates.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "monitoring",
            "title": "Monitoring",
            "headings": [
                "Overview",
                "Use Cases",
                "Sink Connector",
                "Source Connector",
                "Monitor the Connector",
                "Enable Monitoring",
                "Types of Metrics",
                "JMX Paths",
                "Example",
                "Example - Monitor the Quick Start",
                "Download JConsole.",
                "Start the Quick Start pipeline and add connectors.",
                "Start JConsole.",
                "Connect to the Kafka Connect JMX server.",
                "Enter your JMX Server URI.",
                "Click Connect.",
                "In the dialog box, click Insecure Connection.",
                "Explore your connectors' metrics.",
                "Navigate to the MBeans tab in JConsole.",
                "Inspect connector metrics.",
                "Continue the Quick Start.",
                "Return to JConsole.",
                "Stop and remove the Quick Start environment.",
                "Available Metrics",
                "Sink Connector JMX Metrics",
                "Source Connector JMX Metrics"
            ],
            "paragraphs": "Learn how to observe the behavior of your MongoDB Kafka source connector or\nMongoDB Kafka sink connector through  monitoring .\nMonitoring is the process of getting information about the\nactivities a running program performs for use in an application\nor an application performance management library. To learn how monitoring works in the connector and how to use\nit, see the  Use Cases  section. To view an example that shows how to monitor a running connector,\nsee the  Example - Monitor the Quick Start  section. To view a list of all metrics produced by MongoDB source and sink\nconnectors, see the  Available Metrics  section. This section describes use cases for monitoring MongoDB source and sink\nconnectors, and how you can use the metrics your connector provides\nto satisfy those use cases. To learn what types of metrics the connector provides and when\nyou must implement logic to compute a value, see\n Types of Metrics . The following table describes some use cases for monitoring the MongoDB\nsink connector and the metrics the sink connector provides\nto satisfy those use cases: You can find descriptions of all MongoDB sink connector metrics in the\n Sink Connector JMX Metrics  section. Use Case Metrics to Use You need to know if a component of your pipeline is falling behind. Use the  latest-kafka-time-difference-ms \nmetric. This metric indicates the interval of time between\nwhen a record arrived in a Kafka topic and when your connector\nreceived that record. If the value of this metric is increasing,\nit signals that there may be a problem with Apache Kafka or MongoDB. You need to know the total number of records your connector\nwrote to MongoDB. Use the  records  metric. You need to know the total number of write errors your connector\nencountered when attempting to write to MongoDB. Use the  batch-writes-failed  metric. You need to know if your MongoDB performance is getting slower\nover time. Use the  in-task-put-duration-ms  metric to initially diagnose\na slowdown. Use the following metrics to further diagnose any issues: batch-writes-successful-duration-over-<number>-ms batch-writes-failed-duration-over-<number>-ms processing-phase-duration-over-<number>-ms You need to find a bottleneck in how Kafka Connect and your MongoDB sink\nconnector write Apache Kafka records to MongoDB. Compare the values of the following metrics: in-task-put-duration-ms in-connect-framework-duration-ms The following table describes some use cases for monitoring the MongoDB\nsource connector and the metrics the source connector provides\nto satisfy those use cases: You can find descriptions of all MongoDB source connector metrics in the\n Source Connector JMX Metrics  section. Use Case Metrics to Use You need to know if a component of your pipeline is falling behind. Use the  latest-mongodb-time-difference-secs \nmetric. This metric indicates how old the most recent change\nstream event your connector processed is. If this metric is increasing,\nit signals that there may be a problem with Apache Kafka or MongoDB. You need to know the total number of change stream events your source connector\nhas processed. Use the  records  metric. You need to know the percentage of records your connector\nreceived but failed to write to Apache Kafka. Perform the following calculation with the  records ,\n records-filtered , and  records-acknowledged  metrics: You need to know the average size of the documents your connector\nhas processed. Perform the following calculation with the  mongodb-bytes-read  and\n records  metrics: To learn how to calculate the average size of records over a span of\ntime, see  mongodb-bytes-read . You need to find a bottleneck in how Kafka Connect and your MongoDB source\nconnector write MongoDB documents to Apache Kafka. Compare the values of the following metrics: in-task-poll-duration-ms in-connect-framework-duration-ms You need to know if your MongoDB performance is getting slower\nover time. Use the  in-task-poll-duration-ms  metric to initially diagnose\na slowdown. Use the following metrics to further diagnose any issues: initial-commands-successful-duration-over-<number>-ms initial-commands-failed-duration-over-<number>-ms getmore-commands-successful-duration-over-<number>-ms getmore-commands-failed-duration-over-<number>-ms The MongoDB Kafka Connector uses  Java Management Extensions (JMX)  to enable monitoring.\nJMX is a technology included in the Java Platform, Standard Edition that provides\ntools to monitor applications and devices. You can view the\nmetrics produced by the connector with any JMX console, such\nas JConsole. The MongoDB Kafka Connector provides metrics for individual  tasks .\nTasks are classes instantiated by Kafka Connect that copy\ndata to and from datastores and Apache Kafka. The names and\nresponsibilities of the two types of tasks in Kafka Connect are as\nfollows: A sink connector configures one or more sink tasks.\nA source connector configures one or more source tasks. To learn more about JMX, see the following resources from Oracle: To learn more about tasks and connectors in Kafka Connect, see the following resources: A source task copies data from a data store to Apache Kafka. A sink task copies data from Apache Kafka to a data store. Java Management Extensions Guide Using JConsole Kafka Connect Concepts from Confluent Kafka Connect API documentation for the Task interface Kafka Connect API documentation for the Connector abstract class The MongoDB Kafka Connector uses Kafka Connect's metrics infrastructure to\nserve metrics. To read the metrics produced by your connector,\nenable JMX in your Kafka Connect deployment. To learn how to enable JMX for a Kafka Connect instance\nrunning on your host machine, see the\n Official Kafka Documentation . To learn how to enable JMX for a containerized Kafka Connect\ndeployment, see\n Kafka Monitoring and Metrics Using JMX with Docker . The connector provides metrics related to the following\ntypes of quantities: For some use cases, you must perform extra computations with the\nmetrics the connector provides. For example, you must compute the\nfollowing values from provided metrics: To view some examples of computed metrics, see the\n Use Cases  section. The number of times an event has occurred in total for a connector task The value related to the most recent occurrence of an event The rate of change of a metric The value of a metric over a span of time The difference between one metric and another metric The MongoDB Kafka Connector and Kafka Connect both produce metrics for MongoDB connector\ntasks. Both sets of metrics provide information about how your tasks\ninteract with Kafka Connect, but only the MongoDB Kafka Connector\nmetrics provide information about how your tasks interact with\nMongoDB. The MongoDB Kafka Connector produces metrics under the following JMX\npaths: Kafka Connect produces metrics under the following JMX paths: To relate Kafka Connect metrics to MongoDB Kafka Connector metrics, you must remember the\norder in which you added your connectors to Kafka Connect. com.mongodb.kafka.connect.sink-task-metrics.sink-task-<monitonically increasing number> com.mongodb.kafka.connect.source-task-metrics.source-task-<monitonically increasing number> com.mongodb.kafka.connect.source-task-metrics.source-task-change-stream-<monitonically increasing number> com.mongodb.kafka.connect.source-task-metrics.source-task-copy-existing-<monitonically increasing number> kafka.connect.sink-task-metrics.<connector-name> kafka.connect.source-task-metrics.<connector-name> kafka.connect.connector-task-metrics.<connector-name> If the MongoDB Kafka Connector ever encounters a naming conflict when it attempts\nto register an  MBean  on a JMX path, the MongoDB Kafka Connector adds a version suffix\nto the  MBean . For example, if the connector tries to register an  MBean  under the path\n com.mongodb.kafka.connect.sink-task-metrics.sink-task-0  and is unable to do\nso, it attempts to register the  MBean  under\n com.mongodb.kafka.connect.sink-task-metrics.sink-task-0-v1 . Assume you add a single MongoDB source connector\nnamed  my-source-connector  to your deployment. The MongoDB source connector writes metrics to the following\nJMX path: Kafka Connect writes metrics for this task under the following\npath: com.mongodb.kafka.connect.sink-task-metrics.sink-task-0 kafka.connect.sink-task-metrics.my-source-connector The sample environment provided in the Quick Start exposes metrics\non your host machine at the URI  localhost:35000 . To view these metrics with JConsole, perform the following actions: JConsole is part of the Java Platform, Standard Edition. To download JConsole, download the\n Java SE Development Kit \nfrom Oracle. Follow the Quick Start until the  Send the Contents of a Document through Your Connectors \nstep. Run the following command from your command line to start JConsole: Enter the URI  localhost:35000  into the  Remote Process \ntext input box in the JConsole interface. Notice that the  com.mongodb.kafka.connect.sink-task-metrics.sink-task-0.records \nattribute has a value of  0 . This value indicates that\nyour sink task has not received any records from Apache Kafka. Continue the Quick Start until, but not through, the\n Remove the Sandbox  step. Navigate back to the  MBeans  tab in JConsole.\nThe  com.mongodb.kafka.connect.sink-task-metrics.sink-task-0.records \nattribute should now have a value of  1 . To stop and remove the Quick Start environment, follow the\n Remove the Sandbox  step of the Quick Start. Use the attributes in the tables in this section to monitor the behavior of your source and\nsink connectors through Java Management Extensions (JMX). JMX (Java Management Extensions)  represents an individual metric as an attribute of an  MBean .\nTo learn more about attributes and  MBeans , see the\n Standard MBeans Tutorial \nfrom Oracle. A MongoDB source connector task has a  poll()  method to retrieve\ndocuments from MongoDB and send them to Apache Kafka. A MongoDB sink\nconnector task has a  put()  method to retrieve documents from Apache Kafka\nand send them to MongoDB. To learn more about  poll()  and  put()  methods, see the following\nresources: Kafka Connect API documentation for SourceTask interface Kafka Connect API documentation for SinkTask interface Attribute Name Description records The total number of Kafka records a MongoDB sink task received. records-successful The total number of Kafka records a MongoDB sink task successfully\nwrote to MongoDB. records-failed The total number of Kafka records a MongoDB sink task failed to write\nto MongoDB. latest-kafka-time-difference-ms The number of milliseconds of the most recent time difference recorded\nbetween a MongoDB sink task and Kafka. This value is calculated by\nsubtracting the current time of the connector's clock and the timestamp\nof the last record the task received. in-task-put The total number of times the Kafka Connect framework executed the\n put()  method of the MongoDB sink task. in-task-put-duration-ms The total number of milliseconds the Kafka Connect framework spent\nexecuting the  put()  method of a MongoDB sink task. in-task-put-duration-over-1-ms The total number of MongoDB sink task  put()  method executions\nwith a duration that exceeded 1 millisecond. in-task-put-duration-over-10-ms The total number of MongoDB sink task  put()  method executions\nwith a duration that exceeded 10 milliseconds. in-task-put-duration-over-100-ms The total number of MongoDB sink task  put()  method executions\nwith a duration that exceeded 100 milliseconds. in-task-put-duration-over-1000-ms The total number of MongoDB sink task  put()  method executions\nwith a duration that exceeded 1000 milliseconds. in-task-put-duration-over-10000-ms The total number of MongoDB sink task  put()  method executions\nwith a duration that exceeded 10000 milliseconds. in-connect-framework The total number of times code in the Kafka Connect framework\nexecuted after the first invocation of the  put()  method of the\nMongoDB sink task. in-connect-framework-duration-ms The total number of milliseconds spent executing code in the Kafka\nConnect framework since the framework first invoked the  put() \nmethod of the MongoDB sink task. This metric does not count time\nexecuting code in the MongoDB sink task towards the total. in-connect-framework-duration-over-1-ms The total number of times code in the Kafka Connect framework\nexecuted for a duration that exceeded 1 millisecond. in-connect-framework-duration-over-10-ms The total number of times code in the Kafka Connect framework\nexecuted for a duration that exceeded 10 milliseconds. in-connect-framework-duration-over-100-ms The total number of times code in the Kafka Connect framework\nexecuted for a duration that exceeded 100 milliseconds. in-connect-framework-duration-over-1000-ms The total number of times code in the Kafka Connect framework executed\nfor a duration that exceeded 1000 milliseconds. in-connect-framework-duration-over-10000-ms The total number of times code in the Kafka Connect framework executed\nfor a duration that exceeded 10000 milliseconds. processing-phases The total number of times a MongoDB sink task executed the processing\nphase on a batch of records from Kafka. The processing phase of a\nMongoDB sink task is the set of actions that starts after records are\nobtained from Kafka and ends before records are written to MongoDB. processing-phases-duration-ms The total number of milliseconds a MongoDB sink task spent processing\nrecords before writing them to MongoDB. processing-phases-duration-over-1-ms The total number of MongoDB sink task processing phase executions with\na duration that exceeded 1 millisecond. processing-phases-duration-over-10-ms The total number of MongoDB sink task processing phase executions with\na duration that exceeded 10 milliseconds. processing-phases-duration-over-100-ms The total number of MongoDB sink task processing phase executions with\na duration that exceeded 100 milliseconds. processing-phases-duration-over-1000-ms The total number of MongoDB sink task processing phase executions with\na duration that exceeded 1000 milliseconds. processing-phases-duration-over-10000-ms The total number of MongoDB sink task processing phase executions with\na duration that exceeded 10000 milliseconds. batch-writes-successful The total number of batches a MongoDB sink task successfully wrote\nto the MongoDB server. batch-writes-successful-duration-ms The total number of milliseconds a MongoDB sink task spent successfully\nwriting to the MongoDB server. batch-writes-successful-duration-over-1-ms The total number of successful batch writes performed by the MongoDB\nsink task with a duration that exceeded 1 millisecond. batch-writes-successful-duration-over-10-ms The total number of successful batch writes performed by the MongoDB\nsink task with a duration that exceeded 10 milliseconds. batch-writes-successful-duration-over-100-ms The total number of successful batch writes performed by the MongoDB\nsink task with a duration that exceeded 100 milliseconds. batch-writes-successful-duration-over-1000-ms The total number of successful batch writes performed by the MongoDB\nsink task with a duration that exceeded 1000 milliseconds. batch-writes-successful-duration-over-10000-ms The total number of successful batch writes performed by the MongoDB\nsink task with a duration that exceeded 10000 milliseconds. batch-writes-failed The total number of batches a MongoDB sink task failed to write\nto the MongoDB server. batch-writes-failed-duration-ms The total number of milliseconds a MongoDB sink task spent\nunsuccessfully attempting to write batches to the MongoDB server. batch-writes-failed-duration-over-1-ms The total number of failed batch writes attempted by the MongoDB\nsink task with a duration that exceeded 1 millisecond. batch-writes-failed-duration-over-10-ms The total number of failed batch writes attempted by the MongoDB\nsink task with a duration that exceeded 10 milliseconds. batch-writes-failed-duration-over-100-ms The total number of failed batch writes attempted by the MongoDB sink\ntask with a duration that exceeded 100 milliseconds. batch-writes-failed-duration-over-1000-ms The total number of failed batch writes attempted by the MongoDB sink\ntask with a duration that exceeded 1000 milliseconds. batch-writes-failed-duration-over-10000-ms The total number of failed batch writes attempted by the MongoDB sink\ntask with a duration that exceeded 10000 milliseconds. Some metrics for source connector tasks distinguish between\n initial commands  and  getMore  commands. An initial\ncommand is a  find  or  aggregate  command sent to a MongoDB\ninstance that retrieves the first set of documents in a client-side\nMongoDB cursor. The  getMore  command is the MongoDB command that fetches\nthe subsequent sets of documents in a cursor. To learn more about  getMore  commands, see the\n getMore  page. Attribute Name Description records The total number of records a MongoDB source task passed to the\nKafka Connect framework. records-filtered The number of records a MongoDB source task passed to the\nKafka Connect framework that were then filtered by the framework.\nA filtered record is not written to Kafka. records-acknowledged The total number of records a MongoDB source task passed to the\nKafka Connect framework that were then successfully\nwritten to Kafka. mongodb-bytes-read The total number of bytes a MongoDB source task read from the\nMongoDB server. To calculate the average size of the records your sink connector\nprocessed over a span of time, perform the following actions: Determine the change in the value of the  mongodb-bytes-read \nattribute for a span of time. Determine the change in the value of the  records  attribute\nfor the same span of time you used for the preceding step. Divide the change in the value of the  mongodb-bytes-read  attribute\nby the change in the value of the  records  attribute. latest-mongodb-time-difference-secs The number of seconds of the most recent time difference recorded between\na MongoDB server and the post-batch resume token held by a MongoDB\nsource task. This value is calculated by subtracting the timestamp\nof the task's post-batch resume token from the  operationTime  value of\nthe most recent successful MongoDB command executed by the task. in-task-poll The total number of times the Kafka Connect framework executed\nthe  poll()  method of a MongoDB source task. in-task-poll-duration-ms The total number of milliseconds the Kafka Connect framework\nspent executing the  poll()  method of a MongoDB source task. in-task-poll-duration-over-1-ms The total number of MongoDB source task  poll()  method executions\nwith a duration that exceeded 1 millisecond. in-task-poll-duration-over-10-ms The total number of MongoDB source task  poll()  method executions\nwith a duration that exceeded 10 milliseconds. in-task-poll-duration-over-100-ms The total number of MongoDB source task  poll()  method executions\nwith a duration that exceeded 100 milliseconds. in-task-poll-duration-over-1000-ms The total number of MongoDB source task  poll()  method executions\nwith a duration that exceeded 1000 milliseconds. in-task-poll-duration-over-10000-ms The total number of MongoDB source task  poll()  method executions\nwith a duration that exceeded 10000 milliseconds. in-connect-framework The total number of times code in the Kafka Connect framework\nexecuted after the first invocation of the  poll()  method of the\nMongoDB source task. in-connect-framework-duration-ms The total number of milliseconds spent executing code in the\nKafka Connect framework since the framework first invoked the\n poll()  method of the MongoDB source task. This metric does\nnot count time executing code in the MongoDB sink task towards the total. in-connect-framework-duration-over-1-ms The total number of times code in the Kafka Connect framework\nexecuted for a duration that exceeded 1 millisecond. in-connect-framework-duration-over-10-ms The total number of times code in the Kafka Connect framework\nexecuted for a duration that exceeded 10 milliseconds. in-connect-framework-duration-over-100-ms The total number of times code in the Kafka Connect framework\nexecuted for a duration that exceeded 100 milliseconds. in-connect-framework-duration-over-1000-ms The total number of times code in the Kafka Connect framework\nexecuted for a duration that exceeded 1000 milliseconds. in-connect-framework-duration-over-10000-ms The total number of times code in the Kafka Connect framework\nexecuted for a duration that exceeded 10000 milliseconds. initial-commands-successful The total number of initial commands issued by a MongoDB source\ntask that succeeded. An initial command is a find or aggregate command\nsent to a MongoDB server that retrieves the first set of documents in a\ncursor. A  getMore  command is not an initial command. initial-commands-successful-duration-ms The total number of milliseconds a MongoDB source task spent executing\ninitial commands that succeeded. initial-commands-successful-duration-over-1-ms The total number of successful initial commands issued by a MongoDB\nsource task with a duration that exceeded 1 millisecond. initial-commands-successful-duration-over-10-ms The total number of successful initial commands issued by a MongoDB\nsource task with a duration that exceeded 10 milliseconds. initial-commands-successful-duration-over-100-ms The total number of successful initial commands issued by a MongoDB\nsource task with a duration that exceeded 100 milliseconds. initial-commands-successful-duration-over-1000-ms The total number of successful initial commands issued by a MongoDB\nsource task with a duration that exceeded 1000 milliseconds. initial-commands-successful-duration-over-10000-ms The total number of successful initial commands issued by a MongoDB\nsource task with a duration that exceeded 10000 milliseconds. getmore-commands-successful The total number of  getMore  commands issued by a MongoDB source\ntask that succeeded. getmore-commands-successful-duration-ms The total number of milliseconds a MongoDB source task spent executing\n getMore  commands that succeeded. getmore-commands-successful-duration-over-1-ms The total number of successful  getMore  commands issued by a\nMongoDB source task with a duration that exceeded 1 millisecond. getmore-commands-successful-duration-over-10-ms The total number of successful  getMore  commands issued by a MongoDB\nsource task with a duration that exceeded 10 milliseconds. getmore-commands-successful-duration-over-100-ms The total number of successful  getMore  commands issued by a MongoDB\nsource task with a duration that exceeded 100 milliseconds. getmore-commands-successful-duration-over-1000-ms The total number of successful  getMore  commands issued by a\nMongoDB source task with a duration that exceeded 1000 milliseconds. getmore-commands-successful-duration-over-10000-ms The total number of successful  getMore  commands issued by a MongoDB\nsource task with a duration that exceeded 10000 milliseconds. initial-commands-failed The total number of initial commands issued by a MongoDB source\ntask that failed. An initial command is a find or aggregate command\nsent to a MongoDB server that retrieves the first set of documents in a\ncursor. A  getMore  command is not an initial command. initial-commands-failed-duration-ms The total number of milliseconds a MongoDB source task spent\nunsuccessfully attempting to issue initial commands to the MongoDB\nserver. initial-commands-failed-duration-over-1-ms The total number of failed initial commands issued by a MongoDB\nsource task with a duration that exceeded 1 millisecond. initial-commands-failed-duration-over-10-ms The total number of failed initial commands issued by a\nMongoDB source task with a duration that exceeded 10 milliseconds. initial-commands-failed-duration-over-100-ms The total number of failed initial commands issued by a MongoDB source\ntask with a duration that exceeded 100 milliseconds. initial-commands-failed-duration-over-1000-ms The total number of failed initial commands issued by a MongoDB source\ntask with a duration that exceeded 1000 milliseconds. initial-commands-failed-duration-over-10000-ms The total number of failed initial commands issued by a MongoDB\nsource task with a duration that exceeded 10000 milliseconds. getmore-commands-failed The total number of  getMore  commands issued by a MongoDB source\ntask that failed. getmore-commands-failed-duration-ms The total number of milliseconds a MongoDB source task spent\nunsuccessfully attempting to issue  getMore  commands to the MongoDB\nserver. getmore-commands-failed-duration-over-1-ms The total number of failed  getMore  commands issued by a MongoDB source\ntask with a duration that exceeded 1 millisecond. getmore-commands-failed-duration-over-10-ms The total number of failed  getMore  commands issued by a MongoDB source\ntask with a duration that exceeded 10 milliseconds. getmore-commands-failed-duration-over-100-ms The total number of failed  getMore  commands issued by a MongoDB source\ntask with a duration that exceeded 100 milliseconds. getmore-commands-failed-duration-over-1000-ms The total number of failed  getMore  commands issued by a MongoDB source\ntask with a duration that exceeded 1000 milliseconds. getmore-commands-failed-duration-over-10000-ms The total number of failed  getMore  commands issued by a MongoDB\nsource task with a duration that exceeded 10000 milliseconds.",
            "code": [
                {
                    "lang": "text",
                    "value": "(records - (records-acknowledged + records-filtered)) / records"
                },
                {
                    "lang": "text",
                    "value": "mongodb-bytes-read / records"
                },
                {
                    "lang": "shell",
                    "value": "jconsole"
                }
            ],
            "preview": "Learn how to observe the behavior of your MongoDB Kafka source connector or\nMongoDB Kafka sink connector through monitoring.\nMonitoring is the process of getting information about the\nactivities a running program performs for use in an application\nor an application performance management library.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "introduction/connect",
            "title": "Connect to MongoDB",
            "headings": [
                "Overview",
                "How to Connect",
                "How to Configure Your Connection",
                "Authentication"
            ],
            "paragraphs": "Learn how to connect the MongoDB Kafka Connector to MongoDB using a\n connection Uniform Resource Identifier (URI) . A connection URI is a\nstring that contains the following information: The following is an example of a connection URI for a MongoDB replica set: To learn more about the connection URI format, see\n Connection String URI Format  in the MongoDB Manual. The address of your MongoDB deployment  required Connection settings  optional Authentication settings  optional Authentication credentials  optional Specify a connection URI with the following configuration option in both\na source and sink connector: To learn more about this configuration option, see the following resources: Source connector configuration options Sink connector configuration options The MongoDB Kafka Connector uses the  MongoDB Java driver  to parse your connection URI.\nThe MongoDB Java driver is an artifact that enables Java applications like Kafka Connect\nto interact with MongoDB. To learn what connection URI options are available in the connector, see\n the MongoDB Java driver Connection guide . Version 1.10 of the MongoDB Kafka Connector uses version\n4.7 of the MongoDB Java driver. All authentication mechanisms available in the MongoDB Java driver are available\nin the MongoDB Kafka Connector. The following is an example of a connection URI that authenticates with MongoDB using\n SCRAM-SHA-256  authentication: To learn what authentication mechanisms are available, see\n the MongoDB Java driver Authentication Mechanisms guide . To learn more about authentication in the connector, see the\n Security and Authentication guide . To avoid storing your authentication secrets as plain text in your  connection.uri \nsetting, load your secrets from a secure location as your connector starts.\nTo learn how to load your secrets as your connector starts, see\n the Externalize Secrets guide from Confluent .",
            "code": [
                {
                    "lang": "text",
                    "value": "mongodb://mongodb0.example.com:27017,mongodb1.example.com:27017,mongodb2.example.com:27017/?replicaSet=myRepl"
                },
                {
                    "lang": "properties",
                    "value": "connection.uri=<your connection uri>"
                },
                {
                    "lang": "text",
                    "value": "mongodb://<username>:<password>@<hostname>:<port>/?authSource=<authenticationDb>&authMechanism=SCRAM-SHA-256"
                }
            ],
            "preview": "Learn how to connect the MongoDB Kafka Connector to MongoDB using a\nconnection Uniform Resource Identifier (URI). A connection URI is a\nstring that contains the following information:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorials/migrate-time-series",
            "title": "Migrate an Existing Collection to a Time Series Collection",
            "headings": [
                "Migrate a Collection to a Time Series Collection",
                "Complete the Tutorial Setup",
                "Generate Sample Data",
                "Configure the Source Connector",
                "Configure the Sink Connector",
                "Verify the Time Series Collection Data",
                "Summary",
                "Learn More"
            ],
            "paragraphs": "Follow this tutorial to learn how to convert an existing MongoDB\ncollection to a  time series collection  using the MongoDB Kafka Connector. Time series collections efficiently store time series data. Time series\ndata consists of measurements taken at time intervals, metadata that describes\nthe measurement, and the time of the measurement. To convert data from a MongoDB collection to a time series collection using\nthe connector, you need to perform the following tasks: In this tutorial, you perform these preceding tasks to migrate stock data\nfrom a collection to a time series collection. The time series collection\nstores and indexes the data more efficiently and retains the ability to analyze\nstock performance over time using aggregation operators. Identify the time field common to all documents in the collection. Configure a source connector to copy the existing collection data to a\nKafka topic. Configure a sink connector to copy the Kafka topic data to the time series\ncollection. Complete the steps in the  Kafka Connector Tutorial Setup  to start the\nthe Confluent Kafka Connect and MongoDB environment. Run the following command to start a script in your Docker environment that generates\na sample collection containing fabricated stock symbols and their prices\nin your tutorial MongoDB replica set: Once the data generator starts running, you should see the generated\ndata that resembles the following: In a separate terminal window, create an interactive shell session on the\ntutorial Docker container downloaded for the Tutorial Setup using\nthe following command: Create a source configuration file called  stock-source.json  with the\nfollowing command: Paste the following configuration information into the file and save\nyour changes: This configuration instructs the connector to copy existing data from\nthe  PriceData  MongoDB collection to the\n marketdata.Stocks.PriceData  Kafka topic, and once complete, any\nfuture data inserted in that collection. Run the following command in the shell to start the source connector\nusing the configuration file you created: Run the following command in the shell to check the status of the\nconnectors: If your source connector started successfully, you should see the\nfollowing output: Once the source connector starts up, confirm the Kafka topic received\nthe collection data by running the following command: The output should show topic data as it is published by the source\nconnector that resembles the following: You can exit  kafkacat  by typing  CTRL+C . The  cx  command is a custom script included in the tutorial\ndevelopment environment. This script runs the following\nequivalent request to the Kafka Connect REST API to create a new\nconnector: Configure a sink connector to read data from the Kafka topic and write\nit to a time series collection named  StockDataMigrate  in a database\nnamed  Stocks . Create a sink configuration file called  stock-sink.json  with the\nfollowing command: Paste the following configuration information into the file and save\nyour changes: Run the following command in the shell to start the sink connector\nusing the configuration file you updated: After your sink connector finishes processing the topic data, the\ndocuments in the  StockDataMigrate  time series collection contain\nthe  tx_time  field with an  ISODate  type value. The sink connector configuration above uses the time field date\nformat converter. Alternatively, you can use the  TimestampConverter \nSingle Message Transform (SMT) to convert the  tx_time  field from a\n String  to an  ISODate . When using the  TimestampConverter  SMT,\nyou must define a schema for the data in the Kafka topic. For information on how to use the  TimestampConverter  SMT, see the\n TimestampConverter \nConfluent documentation. Once the sink connector completes processing the topic data, the\n StockDataMigrate  time series collection should contain all the\nmarket data from your  PriceData  collection. To view the data in MongoDB, run the following command to connect to\nyour replica set using  mongosh : At the prompt, type the following commands to retrieve all the\ndocuments in the  Stocks.StockDataMigrate  MongoDB namespace: You should see a list of documents returned from the command that\nresemble the following document: In this tutorial, you created a stock ticker data generator that periodically\nwrote data into a MongoDB collection.\nYou configured a source connector to copy the data into a Kafka topic and\nconfigured a sink connector to write that data into a new MongoDB time\nseries collection. Read the following resources to learn more about concepts mentioned in\nthis tutorial: Kafka Time Series Properties Time Series Collections",
            "code": [
                {
                    "lang": "bash",
                    "value": "docker exec -ti mongo1 /bin/bash -c \"cd /stockgenmongo/ && python3 stockgen.py -db Stocks -col PriceData\""
                },
                {
                    "lang": "bash",
                    "value": "...\n1 _id=528e9... MSP MASSIVE SUBMARINE PARTNERS traded at 31.08 2022-05-25 21:15:15\n2 _id=528e9... RWH RESPONSIVE_WHOLESALER HOLDINGS traded at 18.42 2022-05-25 21:15:15\n3 _id=528e9... FAV FUZZY ATTACK VENTURES traded at 31.08 2022-05-25 21:15:15\n..."
                },
                {
                    "lang": "bash",
                    "value": "docker exec -it mongo1 /bin/bash"
                },
                {
                    "lang": "bash",
                    "value": "nano stock-source.json"
                },
                {
                    "lang": "bash",
                    "value": "cx stock-source.json"
                },
                {
                    "lang": "bash",
                    "value": "status"
                },
                {
                    "lang": null,
                    "value": "kafkacat -b broker:29092 -C -t marketdata.Stocks.PriceData"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"mongo-source-marketdata\",\n  \"config\": {\n    \"connector.class\": \"com.mongodb.kafka.connect.MongoSourceConnector\",\n    \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n    \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n    \"publish.full.document.only\": \"true\",\n    \"connection.uri\": \"mongodb://mongo1\",\n    \"topic.prefix\": \"marketdata\",\n    \"database\": \"Stocks\",\n    \"collection\": \"PriceData\",\n    \"copy.existing\": \"true\"\n  }\n}\n"
                },
                {
                    "lang": "bash",
                    "value": "curl -X POST -H \"Content-Type: application/json\" -d @stock-source.json http://connect:8083/connectors -w \"\\n\""
                },
                {
                    "lang": "text",
                    "value": "Kafka topics:\n...\n\nThe status of the connectors:\n\nsource  |  mongo-source-marketdata  |  RUNNING  |  RUNNING  |  com.mongodb.kafka.connect.MongoSourceConnector\n\nCurrently configured connectors\n\n[\n\"mongo-source-marketdata\"\n]\n...\n"
                },
                {
                    "lang": "text",
                    "value": "{\"schema\":{ ... }, \"payload\": \"{ \"_id\": { \"$oid\": \"628e9...\"}, \"company_symbol\": \"MSP\", \"Company_name\": \"MASSIVE SUBMARINE PARTNERS\", \"price\": 309.98, \"tx_time\": { \"$date\": 16535...\" }\"}\n"
                },
                {
                    "lang": "bash",
                    "value": "nano stock-sink.json"
                },
                {
                    "lang": "bash",
                    "value": "cx stock-sink.json"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"mongo-sink-marketdata\",\n  \"config\": {\n    \"connector.class\": \"com.mongodb.kafka.connect.MongoSinkConnector\",\n    \"topics\": \"marketdata.Stocks.PriceData\",\n    \"connection.uri\": \"mongodb://mongo1\",\n    \"database\": \"Stocks\",\n    \"collection\": \"StockDataMigrate\",\n    \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n    \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n    \"timeseries.timefield\": \"tx_time\",\n    \"timeseries.timefield.auto.convert\": \"true\",\n    \"timeseries.timefield.auto.convert.date.format\": \"yyyy-MM-dd'T'HH:mm:ss'Z'\"\n  }\n}\n"
                },
                {
                    "lang": "shell",
                    "value": "mongosh \"mongodb://mongo1\""
                },
                {
                    "lang": "json",
                    "value": "use Stocks\ndb.StockDataMigrate.find()"
                },
                {
                    "lang": "javascript",
                    "value": "{\n    tx_time: ISODate(\"2022-05-25T21:16:35.983Z\"),\n    _id: ObjectId(\"628e9...\"),\n    symbol: 'FAV',\n    price: 18.43,\n    company_name: 'FUZZY ATTACK VENTURES'\n}\n"
                }
            ],
            "preview": "Follow this tutorial to learn how to convert an existing MongoDB\ncollection to a time series collection using the MongoDB Kafka Connector.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "introduction/install",
            "title": "Install the MongoDB Kafka Connector",
            "headings": [
                "Overview",
                "Install the Connector on Confluent Platform",
                "Install the Connector on Apache Kafka",
                "Download a Connector JAR File"
            ],
            "paragraphs": "Learn how to install the MongoDB Kafka Connector. The connector is available for Confluent Platform and\nApache Kafka deployments. To see installation instructions for your deployment type,\nnavigate to one of the following sections: Install the Connector on Confluent Platform Install the Connector on Apache Kafka Click the following tabs to see instructions for how to install the\nconnector on Confluent Platform: Install the Confluent Hub Client . Follow the installation instructions for the\n MongoDB Connector for Apache Kafka \nusing the Confluent Hub Client. Follow the directions on the Confluent page for\n Manually Installing Community Connectors . Use the connector GitHub URL and uber JAR locations in the\n reference table \nwhen appropriate in the Confluent manual installation instructions. Locate and download the uber JAR to get all the dependencies required\nfor the connector. Check the\n reference table \nto find the uber JAR. If you are unable to use the uber JAR or prefer to manage your own\ndependencies, download the JAR that contains the minimum required\ndependencies and resolve any runtime dependencies. You can use\na plugin such as  Maven dependency:tree \nto generate the dependency tree. Copy the JAR and any dependencies into the Kafka plugins directory\nwhich you can specify in your\n plugin.path \nconfiguration setting (e.g.  plugin.path=/usr/local/share/kafka/plugins ). If you intend to run the connector as distributed worker processes, you\nmust repeat this process for each server or virtual machine. You can download the connector source and JAR files from the following locations: You can identify the contents of the JAR files by the suffix in the\nfilename. Consult the following table for a description of each suffix: For example,\n mongo-kafka-connect-1.10.1-all.jar \nis the uber JAR for the version 1.10.1 connector. Kafka Connector GitHub repository (source code) mongodb/mongo-kafka Maven Central repository (JAR files) mongo-kafka-connect Suffix Description all The uber JAR that contains the connector, MongoDB dependencies, and\nApache Avro confluent Contains the minimum requirements for the connector and Confluent\nPlatform javadoc Contains the Javadoc documentation for the connector classes sources Contains the source code that corresponds to the compiled connector\nclasses",
            "code": [],
            "preview": "Learn how to install the MongoDB Kafka Connector. The connector is available for Confluent Platform and\nApache Kafka deployments. To see installation instructions for your deployment type,\nnavigate to one of the following sections:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "introduction/kafka-connect",
            "title": "Kafka and Kafka Connect",
            "headings": [
                "Overview",
                "Apache Kafka",
                "Kafka Connect",
                "Diagram"
            ],
            "paragraphs": "In this guide, you can learn the following foundational information about Apache\nKafka and Kafka Connect: What Apache Kafka and Kafka Connect are What problems Apache Kafka and Kafka Connect solve Why Apache Kafka and Kafka Connect are useful How data moves through an Apache Kafka and Kafka Connect pipeline Apache Kafka is an open source publish/subscribe messaging system. Apache Kafka\nprovides a flexible,  fault tolerant , and  horizontally scalable  system to\nmove data throughout datastores and applications. A system is fault tolerant\nif the system can continue operating even if certain components of the\nsystem stop working. A system is horizontally scalable if the system can be\nexpanded to handle larger workloads by adding more machines rather than by\nimproving a machine's hardware. For more information on Apache Kafka, see the following resources: Confluent \"What is Apache Kafka?\" Page Apache Kafka Official Documentation Kafka Connect is a component of Apache Kafka that solves the problem of\nconnecting Apache Kafka to datastores such as MongoDB. Kafka Connect solves this\nproblem by providing the following resources: The Kafka Connect framework defines an API for developers to write reusable\n connectors . Connectors enable Kafka Connect deployments\nto interact with a specific datastore as a data source or a data sink. The\nMongoDB Kafka Connector is one of these connectors. For more information on Kafka Connect, see the following resources: A fault tolerant runtime for transferring data to and from datastores. A framework for the Apache Kafka community to share solutions for\nconnecting Apache Kafka to different datastores. Confluent Kafka Connect Page Apache Kafka Official Documentation, Kafka Connect Guide Building your First Connector for Kafka Connect  from the Apache Software Foundation While you could write your own application to connect Apache Kafka to a\nspecific datastore using producer and consumer clients, Kafka Connect may be\na better fit for you. Here are some reasons to use Kafka Connect: Kafka Connect has a fault tolerant distributed architecture to ensure a\nreliable pipeline. There are a large number of community maintained connectors for connecting\nApache Kafka to popular datastores like MongoDB, PostgreSQL, and MySQL using the\nKafka Connect framework. This reduces the amount of boilerplate code you need to\nwrite and maintain to manage database connections, error handling,\ndead letter queue integration, and other problems involved in connecting Apache Kafka\nwith a datastore. You have the option to use a managed Kafka Connect cluster from Confluent. The following diagram shows how information flows through an example data pipeline\nbuilt with Apache Kafka and Kafka Connect. The example pipeline uses a MongoDB\ncluster as a data source, and a MongoDB cluster as a data sink. All connectors and datastores in the example pipeline are optional, and you can\nswap them out for the connectors and datastores you need for your deployment.",
            "code": [],
            "preview": "In this guide, you can learn the following foundational information about Apache\nKafka and Kafka Connect:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/configuration-properties",
            "title": "Source Connector Configuration Properties",
            "headings": [
                "Overview",
                "Settings"
            ],
            "paragraphs": "In this section, you can read descriptions of MongoDB Kafka source connector properties,\nincluding essential Confluent Kafka Connect settings and connector-specific\nsettings. For an example source connector configuration file, see\n MongoSourceConnector.properties . For source connector performance recommendations, see  Tuning the Source Connector . See the following categories for a list of related configuration properties: See the  Confluent Source Connector configuration documentation \nfor more information on these settings. Category Description MongoDB Connection Properties Specify how to connect to your MongoDB cluster. Kafka Topic Properties Specify which topics to publish change stream data. Change Stream Properties Specify your change stream pipelines and cursor settings. Output Format Properties Specify the format of the data the connector publishes to your\nKafka topic. Startup Properties Specify what data the connector should convert to Change Stream\nevents. Error Handling and Resuming from Interruption Properties Specify how the connector handles errors and resumes reading after an\ninterruption. All Properties View all preceding categories of configuration properties on one page.",
            "code": [],
            "preview": "In this section, you can read descriptions of MongoDB Kafka source connector properties,\nincluding essential Confluent Kafka Connect settings and connector-specific\nsettings.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/fundamentals",
            "title": "Fundamentals",
            "headings": [],
            "paragraphs": "Read the following sections to learn how MongoDB Kafka source connector features work and\nhow to configure them: Receive Real-time Updates on Data Changes in MongoDB Apply Schemas to Documents Specify a JSON Formatter for Output",
            "code": [],
            "preview": "Read the following sections to learn how MongoDB Kafka source connector features work and\nhow to configure them:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "introduction/data-formats",
            "title": "Data Formats",
            "headings": [
                "Overview",
                "JSON",
                "Raw JSON",
                "BSON",
                "JSON Schema",
                "Avro",
                "Avro Schema",
                "Avro Binary Encoding",
                "Byte Arrays"
            ],
            "paragraphs": "In this guide, you can learn about the data formats you use when working with the\nMongoDB Kafka Connector and your pipeline. This guide uses the following sample document to show the behavior of the\ndifferent formats: JSON is a data-interchange format based on JavaScript object notation. You\nrepresent the  sample document  in JSON like this: You may encounter the following data formats related to JSON when working with the connector: For more information on JSON,\nsee the  official JSON website . Raw JSON BSON JSON Schema Raw JSON is a data format that consists of JSON objects written as strings. You represent the\n sample document  in Raw JSON like this: You use Raw JSON when you specify a String converter on a\nsource or sink connector. To view connector configurations that specify a\nString converter, see the  Converters  guide. BSON is a binary serialization encoding for JSON-like objects. BSON encodes\nthe  sample document  like this: Your connectors use the BSON format to send and receive documents from your\nMongoDB deployment. For more information on BSON, see  the BSON specification . JSON Schema is a syntax for specifying  schemas  for JSON objects. A schema is\na definition attached to an Apache Kafka Topic that defines valid values for that topic. You can specify a schema for the  sample document \nwith JSON Schema like this: You use JSON Schema when you apply JSON Schema converters to your connectors.\nTo view connector configurations that specify a\nJSON Schema converter, see the  Converters \nguide. For more information, see the official\n JSON Schema website . Apache Avro is an open-source framework for serializing and transporting\ndata described by schemas. Avro defines two data formats relevant to the connector: For more information on Apache Avro, see the\n Apache Avro Documentation . Avro schema Avro binary encoding Avro schema is a JSON-based schema definition syntax. Avro schema supports the\nspecification of the following groups of data types: You can construct an Avro schema for the  sample document \nlike this: You use Avro schema when you\n define a schema for a MongoDB Kafka source connector . For a list of all Avro schema types, see the\n Apache Avro specification . Primitive Types Complex Types Logical Types The connector does not support the following Avro types: enum  types. Use  string  instead. fixed  types. Use  bytes  instead. null  as a primitive type. However,  null  as an element in a  union  is supported. union  types with more than 2 elements. union  types with more than one  null  element. The MongoDB Kafka sink connector supports all Avro schema primitive and complex types,\nhowever sink connectors support only the following logical types: decimal date time-millis time-micros timestamp-millis timestamp-micros Avro specifies a binary serialization encoding for JSON objects defined by an\nAvro schema. If you use the\n preceding Avro schema , you can represent the\n sample document  with Avro binary encoding\nlike this: You use Avro binary encoding when you specify an Avro converter on a\nsource or sink connector. To view connector configurations that specify an\nAvro converter, see the  Converters \nguide. To learn more about Avro binary encoding, see\n this section of the Avro specification . A byte array is a consecutive sequence of unstructured bytes. You can represent the sample document as a byte array using any of the encodings\nmentioned above. You use byte arrays when your converters send data to or receive data\nfrom Apache Kafka. For more information on converters, see the\n Converters  guide.",
            "code": [
                {
                    "lang": "json",
                    "value": "{company:\"MongoDB\"}"
                },
                {
                    "lang": "json",
                    "value": "{\"company\":\"MongoDB\"}"
                },
                {
                    "lang": "text",
                    "value": "\"{\\\"company\\\":\\\"MongoDB\\\"}\""
                },
                {
                    "lang": "text",
                    "value": "\\x1a\\x00\\x00\\x00\\x02company\\x00\\x08\\x00\\x00\\x00MongoDB\\x00\\x00"
                },
                {
                    "lang": "json",
                    "value": "{\n   \"$schema\":\"http://json-schema.org/draft-07/schema\",\n   \"$id\":\"unique id\",\n   \"type\":\"object\",\n   \"title\":\"Example Schema\",\n   \"description\":\"JSON Schema for the sample document.\",\n   \"required\":[\n      \"company\"\n   ],\n   \"properties\":{\n      \"company\":{\n         \"$id\":\"another unique id\",\n         \"type\":\"string\",\n         \"title\":\"Company\",\n         \"description\":\"A field to hold the name of a company\"\n      }\n   },\n   \"additionalProperties\":false\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n \u00a0\"type\": \"record\",\n \u00a0\"name\": \"example\",\n \u00a0\"doc\": \"example documents have a company field\",\n \u00a0\"fields\": [\n \u00a0\u00a0\u00a0{\n \u00a0\u00a0\u00a0\u00a0\u00a0\"name\": \"company\",\n \u00a0\u00a0\u00a0\u00a0\u00a0\"type\": \"string\"\n \u00a0\u00a0\u00a0}\n \u00a0]\n}"
                },
                {
                    "lang": "text",
                    "value": "\\x0eMongoDB"
                }
            ],
            "preview": "In this guide, you can learn about the data formats you use when working with the\nMongoDB Kafka Connector and your pipeline.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/usage-examples",
            "title": "Usage Examples",
            "headings": [],
            "paragraphs": "Read the following sections to learn how to configure your MongoDB Kafka source connector to\nperform specific tasks: Filter and Transform Your MongoDB Change Stream with an Aggregation Pipeline Listen for Changes in Multiple MongoDB Collections Customize the Name of the Topic to which your Source Connector Publishes Records Copy Data From a MongoDB Collection onto an Apache Kafka Topic Ensure Documents Processed by Your Source Connector Conform to a Schema",
            "code": [],
            "preview": "Read the following sections to learn how to configure your MongoDB Kafka source connector to\nperform specific tasks:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "introduction/converters",
            "title": "Converters",
            "headings": [
                "Overview",
                "Available Converters",
                "Converters with Schemas",
                "Connector Configuration",
                "Avro Converter",
                "Protobuf Converter",
                "JSON Schema Converter",
                "JSON Converter",
                "String Converter (Raw JSON)"
            ],
            "paragraphs": "This guide describes how to use  converters  with the MongoDB Kafka Connector.\nConverters are programs that translate between bytes and\nKafka Connect's runtime data format. Converters pass data between Kafka Connect and Apache Kafka. The connector passes data\nbetween MongoDB and Kafka Connect. The following diagram shows these relationships: To learn more about converters, see the following resources: Article from Confluent . Confluent Article on Kafka Connect Concepts Converter Interface API Documentation As the connector converts your MongoDB data into Kafka Connect's runtime data\nformat, the connector works with all available converters. To learn what converter to use,  see this page from Confluent . You must use the same converter in your MongoDB Kafka source connector and MongoDB Kafka sink connector.\nFor example, if your source connector writes to a topic using Protobuf, your\nsink connector must use Protobuf to read from the topic. If you use a schema-based converter such as the Kafka Connect Avro Converter (Avro Converter),\nKafka Connect Protobuf Converter, or Kafka Connect JSON Schema Converter, you should define a schema\nin your source connector. To learn how to specify a schema, see the\n Apply Schemas  guide. This section provides templates for properties files to configure the following\nconverters in a connector pipeline: Avro Converter Protobuf Converter JSON Schema Converter JSON Converter String Converter Click the following tabs to view properties files that work with the Avro converter: To use the preceding properties file, replace the placeholder text in angle\nbrackets with your information. The following properties file defines a source connector. This connector\nuses the default schema and an Avro converter to write to an Apache Kafka topic: Avro converters are a great fit for data with a static structure but are not\na good fit for dynamic or changing data. MongoDB's schemaless document\nmodel supports dynamic data, so ensure your MongoDB data source has a static\nstructure before specifying an Avro converter. The following properties file defines a sink connector. This connector\nuses an Avro converter to read from an Apache Kafka topic: Click the following tabs to view properties files that work with the Protobuf converter: To use the preceding properties file, replace the placeholder text in angle\nbrackets with your information. The following properties file defines a source connector. This connector\nuses the default schema and a Protobuf converter to write to an Apache Kafka topic: The following properties file defines a sink connector. This connector\nuses a Protobuf converter to read from an Apache Kafka topic: Click the following tabs to view properties files that work with the JSON Schema converter: To use the preceding properties file, replace the placeholder text in angle\nbrackets with your information. The following properties files configure your connector to manage JSON Schemas\nusing Confluent Schema Registry: The following properties file defines a source connector. This connector\nuses the default schema and a JSON Schema converter to write to an Apache Kafka topic: The following properties file defines a sink connector. This connector\nuses a JSON Schema converter to read from an Apache Kafka topic: The following properties files configure your connector to embed JSON Schemas\nin messages: Embedding a JSON Schema in your message increases the size of your\nmessage. To decrease the size of your messages while using JSON\nSchema, use Schema Registry. The following properties file defines a source connector. This connector\nuses the default schema and a JSON Schema converter to write to an Apache Kafka topic: The following properties file defines a sink connector. This connector\nuses a JSON Schema converter to read from an Apache Kafka topic: Click the following tabs to view properties files that work with the JSON converter: To use the preceding properties file, replace the placeholder text in angle\nbrackets with your information. The following properties file defines a source connector. This connector\nuses a JSON converter to write to an Apache Kafka topic: The following properties file defines a sink connector. This connector\nuses a JSON converter to read from an Apache Kafka topic: Click the following tabs to view properties files that work with the String converter: To use the preceding properties file, replace the placeholder text in angle\nbrackets with your information. The following properties file defines a source connector. This connector\nuses a String converter to write to an Apache Kafka topic: The following properties file defines a sink connector. This connector\nuses a String converter to read from an Apache Kafka topic: Your sink connector must receive valid JSON strings from your\nApache Kafka topic even when using a String converter.",
            "code": [
                {
                    "lang": "java",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSourceConnector\nconnection.uri=<your mongodb uri> \ndatabase=<your database to read from>\ncollection=<your collection to read from>\noutput.format.value=schema\noutput.format.key=schema\nkey.converter=io.confluent.connect.avro.AvroConverter\nkey.converter.schema.registry.url=<your schema registry uri>\nvalue.converter=io.confluent.connect.avro.AvroConverter\nvalue.converter.schema.registry.url=<your schema registry uri>\n"
                },
                {
                    "lang": "java",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSinkConnector\nconnection.uri=<your mongodb uri> \ndatabase=<your database to write to>\ncollection=<your collection to write to>\ntopics=<your topic to read from>\nkey.converter=io.confluent.connect.avro.AvroConverter\nkey.converter.schema.registry.url=<your schema registry uri>\nvalue.converter=io.confluent.connect.avro.AvroConverter\nvalue.converter.schema.registry.url=<your schema registry uri>\n"
                },
                {
                    "lang": "java",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSourceConnector\nconnection.uri=<your mongodb uri>\ndatabase=<your database to read from>\ncollection=<your collection to read from>\noutput.format.value=schema\noutput.format.key=schema\nkey.converter=io.confluent.connect.protobuf.ProtobufConverter\nkey.converter.schema.registry.url=<your schema registry uri>\nvalue.converter=io.confluent.connect.protobuf.ProtobufConverter\nvalue.converter.schema.registry.url=<your schema registry uri>\n"
                },
                {
                    "lang": "java",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSinkConnector\nconnection.uri=<your mongodb uri> \ndatabase=<your database to write to>\ncollection=<your collection to write to>\ntopics=<your topic to read from>\nkey.converter=io.confluent.connect.protobuf.ProtobufConverter\nkey.converter.schema.registry.url=<your schema registry uri>\nvalue.converter=io.confluent.connect.protobuf.ProtobufConverter\nvalue.converter.schema.registry.url=<your schema registry uri>\n"
                },
                {
                    "lang": "java",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSourceConnector\nconnection.uri=<your mongodb uri> \ndatabase=<your database to read from>\ncollection=<your collection to read from>\noutput.format.value=schema\noutput.format.key=schema\nkey.converter=io.confluent.connect.json.JsonSchemaConverter\nkey.converter.schema.registry.url=<your schema registry uri>\nvalue.converter=io.confluent.connect.json.JsonSchemaConverter\nvalue.converter.schema.registry.url=<your schema registry uri>\n"
                },
                {
                    "lang": "java",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSinkConnector\nconnection.uri=<your mongodb uri> \ndatabase=<your database to write to>\ncollection=<your collection to write to>\ntopics=<your topic to read from>\nkey.converter=io.confluent.connect.json.JsonSchemaConverter\nkey.converter.schema.registry.url=<your schema registry uri>\nvalue.converter=io.confluent.connect.json.JsonSchemaConverter\nvalue.converter.schema.registry.url=<your schema registry uri>\n"
                },
                {
                    "lang": "java",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSourceConnector\nconnection.uri=<your mongodb uri> \ndatabase=<your database to read from>\ncollection=<your collection to read from>\noutput.format.value=schema\noutput.format.key=schema\noutput.schema.infer.value=true\nkey.converter.schemas.enable=true\nvalue.converter.schemas.enable=true\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n"
                },
                {
                    "lang": "java",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSinkConnector\nconnection.uri=<your mongodb uri> \ndatabase=<your database to write to>\ncollection=<your collection to write to>\ntopics=<your topic to read from>\nkey.converter.schemas.enable=true\nvalue.converter.schemas.enable=true\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n"
                },
                {
                    "lang": "java",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSourceConnector\nconnection.uri=<your mongodb uri>\ndatabase=<your database to read from>\ncollection=<your collection to read from>\noutput.format.value=json\noutput.format.key=json\nkey.converter.schemas.enable=false\nvalue.converter.schemas.enable=false\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n"
                },
                {
                    "lang": "java",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSinkConnector\nconnection.uri=<your mongodb uri>\ndatabase=<your database to write to>\ncollection=<your collection to write to>\ntopics=<your topic to read from>\nkey.converter.schemas.enable=false\nvalue.converter.schemas.enable=false\nkey.converter=org.apache.kafka.connect.json.JsonConverter\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\n"
                },
                {
                    "lang": "java",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSourceConnector\nconnection.uri=<your mongodb uri>\ndatabase=<your database to read from>\ncollection=<your collection to read from>\noutput.format.value=json\noutput.format.key=json\nkey.converter.schemas.enable=false\nvalue.converter.schemas.enable=false\nkey.converter=org.apache.kafka.connect.storage.StringConverter\nvalue.converter=org.apache.kafka.connect.storage.StringConverter\n"
                },
                {
                    "lang": "java",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSinkConnector\nconnection.uri=<your mongodb uri>\ndatabase=<your database to write to>\ncollection=<your collection to write to>\ntopics=<your topic to read from>\nkey.converter=org.apache.kafka.connect.storage.StringConverter\nvalue.converter=org.apache.kafka.connect.storage.StringConverter\n"
                }
            ],
            "preview": "This guide describes how to use converters with the MongoDB Kafka Connector.\nConverters are programs that translate between bytes and\nKafka Connect's runtime data format.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/configuration-properties/copy-existing",
            "title": "Copy Existing Properties",
            "headings": [
                "Overview",
                "Settings"
            ],
            "paragraphs": "Use the following configuration settings to enable the copy existing\nfeature which converts MongoDB collections into Change Stream events. Starting in Version 1.9 of the MongoDB Kafka Connector,  copy.existing*  properties\nare deprecated and may be removed in a future release. You should use\n startup.mode*  properties to configure the copy existing feature.\nTo learn about  startup.mode*  settings, see\n Startup Properties . For an example of the copy existing feature, see the\n Copy Existing Data  Usage Example. For a list of source connector configuration settings organized by category, see\nthe guide on  Source Connector Configuration Properties . Name Description If any system changes the data in the database while the source connector\nconverts existing data from it, MongoDB may produce duplicate change\nstream events to reflect the latest changes.  Since the change stream\nevents on which the data copy relies are idempotent, the copied data is\neventually consistent. In the following example, the regular-expression setting matches\ncollections that start with \"page\" in the  stats  database. The \"\" character in the example above escapes the \".\" character\nthat follows it in the regular expression. For more information on\nhow to build regular expressions, see the Java API documentation on\n Patterns . The following example shows how you can use the  $match \naggregation operator to instruct the connector to copy only\ndocuments that contain a  closed  field with a value of  false .",
            "code": [
                {
                    "lang": "none",
                    "value": "copy.existing.namespace.regex=stats\\.page.*"
                },
                {
                    "lang": "none",
                    "value": "copy.existing.pipeline=[ { \"$match\": { \"closed\": \"false\" } } ]"
                }
            ],
            "preview": "Use the following configuration settings to enable the copy existing\nfeature which converts MongoDB collections into Change Stream events.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/configuration-properties/error-handling",
            "title": "Error Handling and Resuming from Interruption Properties",
            "headings": [
                "Overview",
                "Settings",
                "Heartbeats with Single Message Transforms"
            ],
            "paragraphs": "Use the following configuration settings to specify how the MongoDB Kafka source connector\nbehaves when it encounters errors and to specify settings related to resuming\ninterrupted reads. Name Description This property overrides the  errors.tolerance \nConnect Framework property. This property overrides the  errors.log.enable \nConnect Framework property. You must set  errors.tolerance  or  mongo.errors.tolerance \nsetting to  \"all\"  to enable this property. If you enable heartbeats and specify  Single Message Transforms (SMTs)  in your\nKafka Connect deployment, you must exclude your heartbeat messages from\nyour SMTs. SMTs are a feature of Kafka Connect that enables you to specify transformations on\nthe messages that pass through your source connector without having to deploy a\nstream processing application. To exclude heartbeat messages from your SMTs, you must create and apply a\n predicate  to your SMTs. Predicates are a feature of SMTs that\nenables you to check if a message matches a conditional statement before\napplying a transformation. The following configuration defines the  IsHeartbeat  predicate which matches\nheartbeat messages sent to the default heartbeat topic: The following configuration uses the preceding predicate to exclude heartbeat\nmessages from an  ExtractField  transformation: If you do not exclude your heartbeat messages from the preceding transformation,\nyour connector raises the following error once it processes a heartbeat message: To learn more about SMTs, see\n How to Use Single Message Transforms in Kafka Connect \nfrom Confluent. To learn more about predicates, see\n Filter (Apache Kafka) \nfrom Confluent. To learn more about the  ExtractField  transformation, see\n ExtractField \nfrom Confluent. To learn more about the default key schema, see the\n Default Schemas  page.",
            "code": [
                {
                    "lang": "properties",
                    "value": "predicates=IsHeartbeat\npredicates.IsHeartbeat.type=org.apache.kafka.connect.transforms.predicates.TopicNameMatches\npredicates.IsHeartbeat.pattern=__mongodb_heartbeats"
                },
                {
                    "lang": "properties",
                    "value": "transforms=Extract\ntransforms.Extract.type=org.apache.kafka.connect.transforms.ExtractField$Key\ntransforms.Extract.field=<the field to extract from your Apache Kafka key>\ntransforms.Extract.predicate=IsHeartbeat\ntransforms.Extract.negate=true\n\n# apply the default key schema as the extract transformation requires a struct object\noutput.format.key=schema"
                },
                {
                    "lang": "none",
                    "value": "ERROR WorkerSourceTask{id=mongo-source-0} Task threw an uncaught and unrecoverable exception. Task is being killed ...\n...\nOnly Struct objects supported for [field extraction], found: java.lang.String"
                }
            ],
            "preview": "Use the following configuration settings to specify how the MongoDB Kafka source connector\nbehaves when it encounters errors and to specify settings related to resuming\ninterrupted reads.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/configuration-properties/change-stream",
            "title": "Change Stream Properties",
            "headings": [
                "Overview",
                "Settings"
            ],
            "paragraphs": "Use the following configuration settings to specify aggregation pipelines\nfor change streams and read preferences for change stream cursors when working\nwith the MongoDB Kafka source connector. For a list of source connector configuration settings organized by category, see\nthe guide on  Source Connector Configuration Properties . Name Description Customize a Pipeline to Filter Change Events Listen for Changes on Multiple Sources For more information on how this change stream option works, see\nthe MongoDB server manual guide on  Lookup Full Document for\nUpdate Operations . To learn how to configure a collection to enable\npre-images, see the  Server manual entry on pre- and\npost-images .",
            "code": [
                {
                    "lang": "none",
                    "value": "[{\"$match\": { \"$and\": [{\"operationType\": \"insert\"}, {\"fullDocument.eventId\": 1404 }] } }]"
                }
            ],
            "preview": "Use the following configuration settings to specify aggregation pipelines\nfor change streams and read preferences for change stream cursors when working\nwith the MongoDB Kafka source connector.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/configuration-properties/kafka-topic",
            "title": "Kafka Topic Properties",
            "headings": [
                "Overview",
                "Settings"
            ],
            "paragraphs": "Use the following configuration settings to specify which Kafka topics the\nMongoDB Kafka source connector should publish data to. For a list of source connector configuration settings organized by category, see\nthe guide on  Source Connector Configuration Properties . Name Description Topic Naming Prefix Usage Example Topic Naming Suffix Usage Example The following mapping instructs the connector to perform the following\nactions: The following examples show which topic the connector sends\ndifferent change stream documents to using the preceding mapping: Publish change stream documents originating from the\n myDb.myColl  MongoDB collection to the  topicOne  Kafka topic. Publish change stream documents originating from the\n myDb  MongoDB database and a collection other than  myColl \nto the Kafka topic  topicTwo.<collectionName> . Publish change stream documents originating from the  myDb \nMongoDB database that do not bear a collection name to\nthe  topicTwo  Kafka topic. A change stream document originating from the  myDb  database\nand the  myColl  collection publishes to the\n topicOne  topic. A change stream document originating from the  myDb  database\nand the  newColl  collection publishes to the\n topicTwo.newColl  topic. A change stream document originating from dropping the\n myDb  database, which does not bear a collection name,\npublishes to the  topicTwo  topic. The following mapping instructs the connector to publish all change\nstream documents to the  topicThree  topic: topic.prefix database collection topic.suffix The following configuration instructs the connector to publish\nchange stream documents from the  coll  collection of the\n db  database to the  prefix-db-coll \ntopic: When you use the  topic.separator  property, keep in mind that it\ndoes not affect how you define the  topic.namespace.map  property.\nThe  topic.namespace.map  property uses MongoDB\n namespaces \nwhich you must always specify with a  \".\"  character to separate\nthe database and collection name.",
            "code": [
                {
                    "lang": "properties",
                    "value": "topic.namespace.map={\"myDb.myColl\": \"topicOne\", \"myDb\": \"topicTwo\"}"
                },
                {
                    "lang": "properties",
                    "value": "topic.namespace.map={\"*\": \"topicThree\"}"
                },
                {
                    "lang": "properties",
                    "value": "   topic.prefix=prefix\n   database=db\n   collection=coll\n   topic.separator=-"
                }
            ],
            "preview": "Use the following configuration settings to specify which Kafka topics the\nMongoDB Kafka source connector should publish data to.",
            "tags": "generated topic, customize topic",
            "facets": {
                "genre": [
                    "reference"
                ]
            }
        },
        {
            "slug": "source-connector/configuration-properties/mongodb-connection",
            "title": "MongoDB Source Connection Properties",
            "headings": [
                "Overview",
                "Settings"
            ],
            "paragraphs": "Use the following configuration settings to specify how your MongoDB Kafka source connector\nestablishes a connection and communicates with your MongoDB cluster. For a list of source connector configuration settings organized by category, see\nthe guide on  Source Connector Configuration Properties . Name Description connection.uri To avoid exposing your authentication credentials in your\n connection.uri  setting, use a\n ConfigProvider \nand set the appropriate configuration parameters. If your  database  configuration is set to  \"\" , the connector\nignores the  collection  setting. server.api.version server.api.deprecationErrors server.api.strict",
            "code": [],
            "preview": "Use the following configuration settings to specify how your MongoDB Kafka source connector\nestablishes a connection and communicates with your MongoDB cluster.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/configuration-properties/all-properties",
            "title": "All Source Connector Configuration Properties",
            "headings": [
                "Overview",
                "MongoDB Connection",
                "Kafka Topic",
                "Change Streams",
                "Output Format",
                "Startup",
                "Error Handling and Resuming from Interruption"
            ],
            "paragraphs": "On this page, you can view all available configuration properties\nfor your MongoDB Kafka source connector. This page duplicates the content of the\nother source connector configuration properties pages. To view a list of all source connector configuration properties pages,\nsee the  Source Connector Configuration Properties  page. To view only the options related to your MongoDB connection, see the\n MongoDB Source Connection Properties  page. Use the following configuration settings to specify how your MongoDB Kafka source connector\nestablishes a connection and communicates with your MongoDB cluster. Name Description connection.uri To avoid exposing your authentication credentials in your\n connection.uri  setting, use a\n ConfigProvider \nand set the appropriate configuration parameters. If your  database  configuration is set to  \"\" , the connector\nignores the  collection  setting. server.api.version server.api.deprecationErrors server.api.strict To view only the options related to your Kafka topic, see the\n Kafka Topic Properties  page. Use the following configuration settings to specify which Kafka topics the\nMongoDB Kafka source connector should publish data to. Name Description Topic Naming Prefix Usage Example Topic Naming Suffix Usage Example The following mapping instructs the connector to perform the following\nactions: The following examples show which topic the connector sends\ndifferent change stream documents to using the preceding mapping: Publish change stream documents originating from the\n myDb.myColl  MongoDB collection to the  topicOne  Kafka topic. Publish change stream documents originating from the\n myDb  MongoDB database and a collection other than  myColl \nto the Kafka topic  topicTwo.<collectionName> . Publish change stream documents originating from the  myDb \nMongoDB database that do not bear a collection name to\nthe  topicTwo  Kafka topic. A change stream document originating from the  myDb  database\nand the  myColl  collection publishes to the\n topicOne  topic. A change stream document originating from the  myDb  database\nand the  newColl  collection publishes to the\n topicTwo.newColl  topic. A change stream document originating from dropping the\n myDb  database, which does not bear a collection name,\npublishes to the  topicTwo  topic. The following mapping instructs the connector to publish all change\nstream documents to the  topicThree  topic: topic.prefix database collection topic.suffix The following configuration instructs the connector to publish\nchange stream documents from the  coll  collection of the\n db  database to the  prefix-db-coll \ntopic: When you use the  topic.separator  property, keep in mind that it\ndoes not affect how you define the  topic.namespace.map  property.\nThe  topic.namespace.map  property uses MongoDB\n namespaces \nwhich you must always specify with a  \".\"  character to separate\nthe database and collection name. To view only the options related to change streams, see the\n Change Stream Properties  page. Use the following configuration settings to specify aggregation pipelines\nfor change streams and read preferences for change stream cursors when working\nwith the MongoDB Kafka source connector. Name Description Customize a Pipeline to Filter Change Events Listen for Changes on Multiple Sources For more information on how this change stream option works, see\nthe MongoDB server manual guide on  Lookup Full Document for\nUpdate Operations . To learn how to configure a collection to enable\npre-images, see the  Server manual entry on pre- and\npost-images . To view only the options related to the format of your output, see the\n Output Format Properties  page. Use the following configuration settings to specify the format of data the\nMongoDB Kafka source connector publishes to Kafka topics. Name Description The connector supports Protobuf as an\noutput data format. You can enable this format by specifying the\n schema  value and installing and  configuring  the  Kafka Connect Protobuf\nConverter . For more information on AVRO schema, see the\n Data Formats  guide. For more information on AVRO schema, see the\n Data Formats  guide. The connector only reads this setting when you set your\n output.format.value  setting to  schema . To view only the options related to startup, see the\n Startup Properties  page. Use the following configuration settings to configure startup of the\nMongoDB Kafka source connector to convert MongoDB collections into Change Stream\nevents. Name Description If any system changes the data in the database while the source connector\nconverts existing data from it, MongoDB may produce duplicate change\nstream events to reflect the latest changes.  Since the change stream\nevents on which the data copy relies are idempotent, the copied data is\neventually consistent. An integer number of seconds since the\nEpoch in decimal format (for example,  30 ) An instant in the ISO-8601 format with one second precision (for example,\n 1970-01-01T00:00:30Z ) A BSON Timestamp in the canonical extended JSON (v2) format\n(for example,  {\"$timestamp\": {\"t\": 30, \"i\": 0}} ) In the following example, the regular-expression setting matches\ncollections that start with \"page\" in the  stats  database. The \"\" character in the example above escapes the \".\" character\nthat follows it in the regular expression. For more information on\nhow to build regular expressions, see the Java API documentation on\n Patterns . The following example shows how you can use the  $match \naggregation operator to instruct the connector to copy only\ndocuments that contain a  closed  field with a value of  false . To view only the options related to handling errors, see the\n Error Handling and Resuming from Interruption Properties  page. Use the following configuration settings to specify how the MongoDB Kafka source connector\nbehaves when it encounters errors and to specify settings related to resuming\ninterrupted reads. Name Description This property overrides the  errors.tolerance \nConnect Framework property. This property overrides the  errors.log.enable \nConnect Framework property. You must set  errors.tolerance  or  mongo.errors.tolerance \nsetting to  \"all\"  to enable this property.",
            "code": [
                {
                    "lang": "properties",
                    "value": "topic.namespace.map={\"myDb.myColl\": \"topicOne\", \"myDb\": \"topicTwo\"}"
                },
                {
                    "lang": "properties",
                    "value": "topic.namespace.map={\"*\": \"topicThree\"}"
                },
                {
                    "lang": "properties",
                    "value": "   topic.prefix=prefix\n   database=db\n   collection=coll\n   topic.separator=-"
                },
                {
                    "lang": "none",
                    "value": "[{\"$match\": { \"$and\": [{\"operationType\": \"insert\"}, {\"fullDocument.eventId\": 1404 }] } }]"
                },
                {
                    "lang": null,
                    "value": "com.mongodb.kafka.connect.source.json.formatter.DefaultJson"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.source.json.formatter.DefaultJson\ncom.mongodb.kafka.connect.source.json.formatter.ExtendedJson\ncom.mongodb.kafka.connect.source.json.formatter.SimplifiedJson"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"record\",\n  \"name\": \"keySchema\",\n  \"fields\" : [ { \"name\": \"_id\", \"type\": \"string\" } ]\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"ChangeStream\",\n  \"type\": \"record\",\n  \"fields\": [\n    { \"name\": \"_id\", \"type\": \"string\" },\n    { \"name\": \"operationType\", \"type\": [\"string\", \"null\"] },\n    { \"name\": \"fullDocument\", \"type\": [\"string\", \"null\"] },\n    { \"name\": \"ns\",\n      \"type\": [{\"name\": \"ns\", \"type\": \"record\", \"fields\": [\n                {\"name\": \"db\", \"type\": \"string\"},\n                {\"name\": \"coll\", \"type\": [\"string\", \"null\"] } ]\n               }, \"null\" ] },\n    { \"name\": \"to\",\n      \"type\": [{\"name\": \"to\", \"type\": \"record\",  \"fields\": [\n                {\"name\": \"db\", \"type\": \"string\"},\n                {\"name\": \"coll\", \"type\": [\"string\", \"null\"] } ]\n               }, \"null\" ] },\n    { \"name\": \"documentKey\", \"type\": [\"string\", \"null\"] },\n    { \"name\": \"updateDescription\",\n      \"type\": [{\"name\": \"updateDescription\",  \"type\": \"record\", \"fields\": [\n                 {\"name\": \"updatedFields\", \"type\": [\"string\", \"null\"]},\n                 {\"name\": \"removedFields\",\n                  \"type\": [{\"type\": \"array\", \"items\": \"string\"}, \"null\"]\n                  }] }, \"null\"] },\n    { \"name\": \"clusterTime\", \"type\": [\"string\", \"null\"] },\n    { \"name\": \"txnNumber\", \"type\": [\"long\", \"null\"]},\n    { \"name\": \"lsid\", \"type\": [{\"name\": \"lsid\", \"type\": \"record\",\n               \"fields\": [ {\"name\": \"id\", \"type\": \"string\"},\n                             {\"name\": \"uid\", \"type\": \"string\"}] }, \"null\"] }\n  ]\n}"
                },
                {
                    "lang": "none",
                    "value": "startup.mode.copy.existing.namespace.regex=stats\\.page.*"
                },
                {
                    "lang": "none",
                    "value": "startup.mode.copy.existing.pipeline=[ { \"$match\": { \"closed\": \"false\" } } ]"
                }
            ],
            "preview": "On this page, you can view all available configuration properties\nfor your MongoDB Kafka source connector. This page duplicates the content of the\nother source connector configuration properties pages.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/configuration-properties/startup",
            "title": "Startup Properties",
            "headings": [
                "Overview",
                "Settings"
            ],
            "paragraphs": "Use the following configuration settings to configure startup of the\nMongoDB Kafka source connector to convert MongoDB collections into Change Stream\nevents. For an example using the copy existing feature, see the\n Copy Existing Data  Usage Example. For a list of source connector configuration settings organized by category, see\nthe guide on  Source Connector Configuration Properties . Name Description If any system changes the data in the database while the source connector\nconverts existing data from it, MongoDB may produce duplicate change\nstream events to reflect the latest changes.  Since the change stream\nevents on which the data copy relies are idempotent, the copied data is\neventually consistent. An integer number of seconds since the\nEpoch in decimal format (for example,  30 ) An instant in the ISO-8601 format with one second precision (for example,\n 1970-01-01T00:00:30Z ) A BSON Timestamp in the canonical extended JSON (v2) format\n(for example,  {\"$timestamp\": {\"t\": 30, \"i\": 0}} ) In the following example, the regular-expression setting matches\ncollections that start with \"page\" in the  stats  database. The \"\" character in the example above escapes the \".\" character\nthat follows it in the regular expression. For more information on\nhow to build regular expressions, see the Java API documentation on\n Patterns . The following example shows how you can use the  $match \naggregation operator to instruct the connector to copy only\ndocuments that contain a  closed  field with a value of  false .",
            "code": [
                {
                    "lang": "none",
                    "value": "startup.mode.copy.existing.namespace.regex=stats\\.page.*"
                },
                {
                    "lang": "none",
                    "value": "startup.mode.copy.existing.pipeline=[ { \"$match\": { \"closed\": \"false\" } } ]"
                }
            ],
            "preview": "Use the following configuration settings to configure startup of the\nMongoDB Kafka source connector to convert MongoDB collections into Change Stream\nevents.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/usage-examples/topic-naming",
            "title": "Topic Naming",
            "headings": [
                "Topic Prefix Example",
                "Topic Suffix Example",
                "Topic Namespace Map Example",
                "Topic Namespace Map with Wildcard Example"
            ],
            "paragraphs": "The examples on this page show how to configure your MongoDB Kafka source connector\nto customize the name of the topic to which it publishes records. By default, the MongoDB Kafka source connector publishes change event data\nto a Kafka topic with the same name as the MongoDB  namespace  from which\nthe change events originated. A namespace is a string that's composed of the\ndatabase and collection name concatenated with a dot \".\" character. The following examples show different ways that you can customize the\nKafka topics to which the connector publishes change event data: Topic Prefix Topic Suffix Topic Namespace Map Topic Namespace Map with Wildcard You can configure your source connector to prepend a string to the\nnamespace of the change event data, and publish records to that Kafka\ntopic. This setting automatically concatenates your prefix with your\nnamespace with the \".\" character. To specify the topic prefix, use the  topic.prefix  configuration\nsetting as shown in the following example: Once set, your connector publishes any changes to the  data  collection\nin the  test  database to the Kafka topic named  myPrefix.test.data . You can configure your source connector to append a string to the\nnamespace of the change event data, and publish records to that Kafka\ntopic. This setting automatically concatenates your namespace with your\nsuffix with the \".\" character. To specify the topic suffix, use the  topic.suffix  configuration\nsetting as shown in the following example: Once set, your connector publishes any changes to the  data  collection\nin the  test  database to the Kafka topic named  test.data.mySuffix . You can configure your source connector to map namespace values to Kafka\ntopic names for incoming change event data. If the database name or namespace of the change event matches one of the\nfields in the map, the connector publishes the record to the value that\ncorresponds to that mapping. If the database name or namespace of the change event do not match any\nmapping, the connector publishes the record using the default topic naming\nscheme unless otherwise specified by a different topic naming setting. Any mapping that includes both database and collection takes precedence\nover mappings that only specify the source database name. The following example shows how to specify the  topic.namespace.map \nsetting to define a topic namespace mappings from the  carDb  database\nto the  automobiles  topic and the  carDb.ev  namespace to the\n electricVehicles  topic: Since the  carDb.ev  namespace mapping takes precedence over the  carDb \nmapping, the connector performs the following actions: The namespace map matching occurs before the connector applies any other\ntopic naming setting. If defined, the connector applies the\n topic.prefix  and the  topic.suffix  settings to the topic name\nafter the mapping. If the change event came from the database  carDb  and collection  ev ,\nthe connector sets the destination to the   electricVehicles  topic. If the change event came from the database  carDb  and a collection\nother than  ev , the connector sets the destination to the\n automobiles.<collectionName> \ntopic. If the change document came from any database other than  carDb , the\nconnector sets the destination topic to the default namespace naming\nscheme. If defined, the connector applies the  topic.prefix  and\n topic.suffix  settings to the destination topic name after it\nperforms namespace mapping. In addition to specifying database name and namespace in your topic\nnamespace map as shown in  Topic Namespace Map Example ,\nyou can use a wildcard  *  to match change events from all databases and\nnamespaces without mappings. In the preceding wildcard example, the connector publishes change documents\nthat originated from all databases other than  carDb  to the\n otherVehicles  topic.",
            "code": [
                {
                    "lang": "ini",
                    "value": "topic.prefix=myPrefix\ndatabase=test\ncollection=data"
                },
                {
                    "lang": "ini",
                    "value": "topic.suffix=mySuffix\ndatabase=test\ncollection=data"
                },
                {
                    "lang": "ini",
                    "value": "topic.namespace.map={\"carDb\": \"automobiles\", \"carDb.ev\": \"electricVehicles\"}"
                },
                {
                    "lang": "ini",
                    "value": "topic.namespace.map={\"carDb\": \"automobiles\", \"carDb.ev\": \"electricVehicles\", \"*\": \"otherVehicles\"}"
                }
            ],
            "preview": "The examples on this page show how to configure your MongoDB Kafka source connector\nto customize the name of the topic to which it publishes records.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/usage-examples/copy-existing-data",
            "title": "Copy Existing Data",
            "headings": [
                "Example",
                "Copy Data",
                "Filter Data",
                "Specify the Configuration"
            ],
            "paragraphs": "This usage example demonstrates how to copy data from a MongoDB collection to an\nApache Kafka topic using the MongoDB Kafka source connector. Suppose you need to copy a MongoDB collection to Apache Kafka and filter some of the data. Your requirements and your solutions are as follows: The  customers  collection contains the following documents: Requirement Solution Copy the  customers  collection of the  shopping  database in your\nMongoDB deployment onto an Apache Kafka topic. Only copy documents that have the value \"Mexico\" in the  country  field. Copy the contents of the  customers  collection of the  shopping  database by\nspecifying the following configuration options in your source connector: Your source connector copies your collection by creating change event documents\nthat describe inserting each document into your collection. To learn more about change event documents, see the\n Change Streams  guide. To learn more about the  startup.mode  option, see\n Startup Properties . If any system changes the data in the database while the source connector\nconverts existing data from it, MongoDB may produce duplicate change\nstream events to reflect the latest changes.  Since the change stream\nevents on which the data copy relies are idempotent, the copied data is\neventually consistent. You can filter data by specifying an aggregation pipeline in the\n startup.mode.copy.existing.pipeline  option of your source connector configuration. The\nfollowing configuration specifies an aggregation pipeline that matches all\ndocuments with \"Mexico\" in the  country  field: To learn more about  the  startup.mode.copy.existing.pipeline  option, see\n Startup Properties . To learn more about aggregation pipelines, see the following resources: Customize a Pipeline to Filter Change Events  Usage Example Aggregation  in the MongoDB manual. Your final source connector configuration to copy the  customers  collection should\nlook like this: Once your connector copies your data, you see the following change event\ndocument corresponding to the\n preceding sample collection \nin the  shopping.customers  Apache Kafka topic: Use a change data capture handler to convert change event documents in an\nApache Kafka topic into MongoDB write operations. To learn more, see the\n Change Data Capture Handlers  guide.",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": 1,\n  \"country\": \"Mexico\",\n  \"purchases\": 2,\n  \"last_viewed\": { \"$date\": \"2021-10-31T20:30:00.245Z\" }\n}\n{\n  \"_id\": 2,\n  \"country\": \"Iceland\",\n  \"purchases\": 8,\n  \"last_viewed\": { \"$date\": \"2015-07-20T10:00:00.135Z\" }\n}"
                },
                {
                    "lang": "properties",
                    "value": "database=shopping\ncollection=customers\nstartup.mode=copy_existing"
                },
                {
                    "lang": "properties",
                    "value": "startup.mode.copy.existing.pipeline=[{ \"$match\": { \"country\": \"Mexico\" } }]"
                },
                {
                    "lang": "properties",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSourceConnector\nconnection.uri=<your production MongoDB connection uri>\ndatabase=shopping\ncollection=customers\nstartup.mode=copy_existing\nstartup.mode.copy.existing.pipeline=[{ \"$match\": { \"country\": \"Mexico\" } }]"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": { \"_id\": 1, \"copyingData\": true },\n  \"operationType\": \"insert\",\n  \"documentKey\": { \"_id\": 1 },\n  \"fullDocument\": {\n    \"_id\": 1,\n    \"country\": \"Mexico\",\n    \"purchases\": 2,\n    \"last_viewed\": { \"$date\": \"2021-10-31T20:30:00.245Z\" }\n  },\n  \"ns\": { \"db\": \"shopping\", \"coll\": \"customers\" }\n}\n"
                }
            ],
            "preview": "This usage example demonstrates how to copy data from a MongoDB collection to an\nApache Kafka topic using the MongoDB Kafka source connector.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/usage-examples/schema",
            "title": "Specify a Schema",
            "headings": [
                "Example",
                "Receive Data from a Collection",
                "Create a Custom Schema",
                "Omit Metadata from Published Records",
                "Specify the Configuration"
            ],
            "paragraphs": "This usage example demonstrates how you can configure your MongoDB Kafka source connector\nto apply a custom  schema  to your data. A schema is a\ndefinition that specifies the structure and type information about data in an\nApache Kafka topic. Use a schema when you need to ensure the data on the topic populated\nby your source connector has a consistent structure. To learn more about using schemas with the connector, see the\n Apply Schemas  guide. Suppose your application keeps track of customer data in a MongoDB\ncollection, and you need to publish this data to a Kafka topic. You want\nthe subscribers of the customer data to receive consistently formatted data.\nYou choose to apply a schema to your data. Your requirements and your solutions are as follows: For the full configuration file that meets the requirements above, see\n Specify the Configuration . Requirement Solution Receive customer data from a MongoDB collection Provide the customer data schema Omit Kafka metadata from the customer data To configure your source connector to receive data from a MongoDB collection,\nspecify the database and collection name. For this example, you can\nconfigure the connector to read from the  purchases  collection in the\n customers  database as follows: A sample customer data document from your collection contains the following\ninformation: From the sample document, you decide your schema should present the fields\nusing the following data types: You can describe your data using the Apache Avro schema format as shown in\nthe example schema below: Field name Data types Description name string visits array \nof  timestamps Dates the customer visited goods_purchased map \nof string (the assumed type) to\n integer \nvalues Names of goods and quantity of each item the customer purchased If you want to send your data through Apache Kafka with Avro binary encoding,\nyou must use an Avro converter. For more information, see the guide on\n Converters . The connector publishes the customer data documents and metadata\nthat describes the document to a Kafka topic. You can set the connector to\ninclude only the document data contained in the  fullDocument  field of the\nrecord using the following setting: For more information on the  fullDocument  field, see the\n Change Streams  guide. Your custom schema connector configuration should resemble the following: For more information on specifying schemas, see the  Apply\nSchemas  guide. In the preceding configuration, the Kafka Connect JSON Schema Converter embeds the custom\nschema in your messages. To learn more about the JSON Schema converter, see the\n Converters  guide.",
            "code": [
                {
                    "lang": "ini",
                    "value": "database=customers\ncollection=purchases"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"Zola\",\n  \"visits\": [\n    {\n      \"$date\": \"2021-07-25T17:30:00.000Z\"\n    },\n    {\n      \"$date\": \"2021-10-03T14:06:00.000Z\"\n    }\n  ],\n  \"goods_purchased\": {\n    \"apples\": 1,\n    \"bananas\": 10\n  }\n}\n"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"record\",\n  \"name\": \"Customer\",\n  \"fields\": [{\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },{\n      \"name\": \"visits\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"long\",\n          \"logicalType\": \"timestamp-millis\"\n        }\n      }\n    },{\n      \"name\": \"goods_purchased\",\n      \"type\": {\n        \"type\": \"map\",\n        \"values\": \"int\"\n      }\n    }\n  ]\n}\n"
                },
                {
                    "lang": "ini",
                    "value": "publish.full.document.only=true"
                },
                {
                    "lang": "properties",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSourceConnector\nconnection.uri=<your MongoDB connection URI>\ndatabase=customers\ncollection=purchases\npublish.full.document.only=true\noutput.format.value=schema\noutput.schema.value={\\\"type\\\": \\\"record\\\", \\\"name\\\": \\\"Customer\\\", \\\"fields\\\": [{\\\"name\\\": \\\"name\\\", \\\"type\\\": \\\"string\\\"}, {\\\"name\\\": \\\"visits\\\", \\\"type\\\": {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"long\\\", \\\"logicalType\\\": \\\"timestamp-millis\\\"}}}, {\\\"name\\\": \\\"goods_purchased\\\", \\\"type\\\": {\\\"type\\\": \\\"map\\\", \\\"values\\\": \\\"int\\\"}}]}\nvalue.converter.schemas.enable=true\nvalue.converter=org.apache.kafka.connect.json.JsonConverter\nkey.converter=org.apache.kafka.connect.storage.StringConverter\n"
                }
            ],
            "preview": "This usage example demonstrates how you can configure your MongoDB Kafka source connector\nto apply a custom schema to your data. A schema is a\ndefinition that specifies the structure and type information about data in an\nApache Kafka topic. Use a schema when you need to ensure the data on the topic populated\nby your source connector has a consistent structure.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/usage-examples/multiple-sources",
            "title": "Listen for Changes on Multiple Sources",
            "headings": [
                "Examples",
                "Include Change Events from Multiple Databases",
                "Exclude Change Events from Multiple Collections",
                "Additional Information"
            ],
            "paragraphs": "This usage example demonstrates how to configure a MongoDB Kafka source connector\nto listen for change events on multiple MongoDB collections, and\npublish them to a Kafka topic. If you need your connector to listen for change events on a more particular\nset of databases and collections, you can use a  pipeline . A pipeline is a\nMongoDB aggregation pipeline composed of instructions to the database to\nfilter or transform data. See the next section for examples of how to\nconfigure your connector  pipeline  setting to match multiple database and\ncollection names using a regular expression. The  database  and  collection  configuration settings also affect\nwhich databases and collections on which the connector listens for change\nevents. To learn more about these settings, see the\n MongoDB Source Connection Properties  guide. The following examples show you how to use an aggregation pipeline to select\nspecific database or collection names on which to listen for change events. You can define an aggregation pipeline to select only change events on\nmultiple databases by specifying the following in the  pipeline \nsetting: The following sample configuration shows how you can set your source connector\nto listen for change events on the  sandbox  and  firewall  databases: A  $match  aggregation operator The  ns.db , field which identifies the database part of the namespace The  $regex  operator and a regular expression that matches the database\nnames You can define an aggregation pipeline to ignore change events on\nmultiple collections by specifying the following in the  pipeline \nsetting: The following sample configuration shows how you can set your source connector\nto filter out change events that originate from all collections named\n\"hyperspace\" in any database: A  $match  aggregation operator The  ns.coll  field, which identifies the collection part of the namespace The  $regex  operator and a regular expression that matches the\ncollection names The  $not  operator which instructs the enclosing  $regex  operator to\nmatch everything the regular expression does not match The  $match aggregation operator MongoDB change events MongoDB namespace Regular expression syntax using the  Patterns class $not logical query operator and regular expressions",
            "code": [
                {
                    "lang": "ini",
                    "value": "pipeline=[{\"$match\": {\"ns.db\": {\"$regex\": \"/^(sandbox|firewall)$/\"}}}]"
                },
                {
                    "lang": "ini",
                    "value": "pipeline=[{\"$match\": {\"ns.coll\": {\"$regex\": {\"$not\": \"/^hyperspace$/\"}}}}]"
                }
            ],
            "preview": "This usage example demonstrates how to configure a MongoDB Kafka source connector\nto listen for change events on multiple MongoDB collections, and\npublish them to a Kafka topic.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/usage-examples/custom-pipeline",
            "title": "Customize a Pipeline to Filter Change Events",
            "headings": [
                "Example"
            ],
            "paragraphs": "This usage example demonstrates how to configure a  pipeline  to\ncustomize the data that your MongoDB Kafka source connector consumes. A pipeline is a\nMongoDB aggregation pipeline composed of instructions to the database to\nfilter or transform data. MongoDB notifies the connector of data changes that match your aggregation\npipeline on a  change stream . A change stream is a sequence of events that\ndescribe data changes a client made to a MongoDB deployment in real-time.\nFor more information, see the MongoDB Server manual entry on\n Change Streams . Suppose you're an event coordinator who needs to collect names and arrival times\nof each guest at a specific event. Whenever a guest checks into the event,\nan application inserts a new document that contains the following details: You can define your connector  pipeline  setting to instruct the change\nstream to filter the change event information as follows: To apply these transformations, assign the following aggregation pipeline\nto your  pipeline  setting: When the application inserts the sample document, your configured\nconnector publishes the following record to your Kafka topic: For more information on managing change streams with the source connector, see\nthe connector documentation on  Change Streams . Create change events for insert operations and omit events for all other\ntypes of operations. Create change events only for documents that match the  fullDocument.eventId \nvalue \"321\" and omit all other documents. Omit the  _id  and  eventId  fields from the  fullDocument  object\nusing a projection. Make sure that the results of the pipeline contain the top-level  _id \nfield of the  payload  object, which MongoDB uses as the value of the\n resume token .",
            "code": [
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": ObjectId(...),\n  \"eventId\": 321,\n  \"name\": \"Dorothy Gale\",\n  \"arrivalTime\": 2021-10-31T20:30:00.245Z\n}"
                },
                {
                    "lang": "properties",
                    "value": "pipeline=[{\"$match\": { \"$and\": [{\"operationType\": \"insert\"}, { \"fullDocument.eventId\": 321 }] } }, {\"$project\": { \"fullDocument._id\": 0, \"fullDocument.eventId\": 0 } } ]"
                },
                {
                    "lang": "json",
                    "value": "{\n  ...\n  \"payload\": {\n    _id: { _data: ... },\n    \"operationType\": \"insert\",\n    \"fullDocument\": {\n      \"name\": \"Dorothy Gale\",\n      \"arrivalTime\": \"2021-10-31T20:30:00.245Z\",\n    },\n    \"ns\": { ... },\n    \"documentKey\": {\n      _id: {\"$oid\": ... }\n    }\n  }\n}"
                }
            ],
            "preview": "This usage example demonstrates how to configure a pipeline to\ncustomize the data that your MongoDB Kafka source connector consumes. A pipeline is a\nMongoDB aggregation pipeline composed of instructions to the database to\nfilter or transform data.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/configuration-properties/output-format",
            "title": "Output Format Properties",
            "headings": [
                "Overview",
                "Settings"
            ],
            "paragraphs": "Use the following configuration settings to specify the format of data the\nMongoDB Kafka source connector publishes to Kafka topics. For a list of source connector configuration settings organized by category, see\nthe guide on  Source Connector Configuration Properties . Name Description The connector supports Protobuf as an\noutput data format. You can enable this format by specifying the\n schema  value and installing and  configuring  the  Kafka Connect Protobuf\nConverter . For more information on AVRO schema, see the\n Data Formats  guide. For more information on AVRO schema, see the\n Data Formats  guide. The connector only reads this setting when you set your\n output.format.value  setting to  schema .",
            "code": [
                {
                    "lang": null,
                    "value": "com.mongodb.kafka.connect.source.json.formatter.DefaultJson"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.source.json.formatter.DefaultJson\ncom.mongodb.kafka.connect.source.json.formatter.ExtendedJson\ncom.mongodb.kafka.connect.source.json.formatter.SimplifiedJson"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"type\": \"record\",\n  \"name\": \"keySchema\",\n  \"fields\" : [ { \"name\": \"_id\", \"type\": \"string\" } ]\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"ChangeStream\",\n  \"type\": \"record\",\n  \"fields\": [\n    { \"name\": \"_id\", \"type\": \"string\" },\n    { \"name\": \"operationType\", \"type\": [\"string\", \"null\"] },\n    { \"name\": \"fullDocument\", \"type\": [\"string\", \"null\"] },\n    { \"name\": \"ns\",\n      \"type\": [{\"name\": \"ns\", \"type\": \"record\", \"fields\": [\n                {\"name\": \"db\", \"type\": \"string\"},\n                {\"name\": \"coll\", \"type\": [\"string\", \"null\"] } ]\n               }, \"null\" ] },\n    { \"name\": \"to\",\n      \"type\": [{\"name\": \"to\", \"type\": \"record\",  \"fields\": [\n                {\"name\": \"db\", \"type\": \"string\"},\n                {\"name\": \"coll\", \"type\": [\"string\", \"null\"] } ]\n               }, \"null\" ] },\n    { \"name\": \"documentKey\", \"type\": [\"string\", \"null\"] },\n    { \"name\": \"updateDescription\",\n      \"type\": [{\"name\": \"updateDescription\",  \"type\": \"record\", \"fields\": [\n                 {\"name\": \"updatedFields\", \"type\": [\"string\", \"null\"]},\n                 {\"name\": \"removedFields\",\n                  \"type\": [{\"type\": \"array\", \"items\": \"string\"}, \"null\"]\n                  }] }, \"null\"] },\n    { \"name\": \"clusterTime\", \"type\": [\"string\", \"null\"] },\n    { \"name\": \"txnNumber\", \"type\": [\"long\", \"null\"]},\n    { \"name\": \"lsid\", \"type\": [{\"name\": \"lsid\", \"type\": \"record\",\n               \"fields\": [ {\"name\": \"id\", \"type\": \"string\"},\n                             {\"name\": \"uid\", \"type\": \"string\"}] }, \"null\"] }\n  ]\n}"
                }
            ],
            "preview": "Use the following configuration settings to specify the format of data the\nMongoDB Kafka source connector publishes to Kafka topics.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "security-and-authentication/mongodb-aws-auth",
            "title": "MongoDB AWS-based Authentication",
            "headings": [
                "Overview",
                "Connector Connection Settings"
            ],
            "paragraphs": "In this guide, you can learn how to authenticate your MongoDB Kafka Connector with your\nMongoDB replica set using the  MONGODB-AWS  authentication mechanism.\nThe  MONGODB-AWS  authentication mechanism uses your Amazon Web\nServices Identity and Access Management (AWS IAM) credentials to authenticate\nyour user. To learn how to set up your MongoDB replica set in MongoDB Atlas to use\nAWS IAM credentials, see the guide on  How to Set Up Unified AWS Access . You need to use MongoDB Kafka Connector version 1.5 of later to connect to a MongoDB\nserver set up to authenticate using your AWS IAM credentials. AWS IAM\ncredential authentication is available in MongoDB server version 4.4\nand later. You can specify your  MONGODB-AWS  authentication credentials in your\nconnection URI connector property as shown in the following example: The preceding example uses the following placeholders which you need to\nreplace: Placeholder Description AWS access key id Value of your  AWS_ACCESS_KEY_ID . AWS secret access key Value of your  AWS_SECRET_KEY . hostname Network address of your MongoDB server. port Port number of your MongoDB server. authentication database MongoDB database that contains your user's authentication data. If\nyou omit the  authSource  parameter and placeholder value, the\ndriver uses the default value  admin . AWS session token",
            "code": [
                {
                    "lang": "ini",
                    "value": "connection.uri=mongodb://<AWS access key id>:<AWS secret access key>@<hostname>:<port>/?authSource=<authentication database>&authMechanism=MONGODB-AWS&authMechanismProperties=AWS_SESSION_TOKEN:<AWS session token>"
                }
            ],
            "preview": "In this guide, you can learn how to authenticate your MongoDB Kafka Connector with your\nMongoDB replica set using the MONGODB-AWS authentication mechanism.\nThe MONGODB-AWS authentication mechanism uses your Amazon Web\nServices Identity and Access Management (AWS IAM) credentials to authenticate\nyour user.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "security-and-authentication/tls-and-x509",
            "title": "SSL/TLS and X.509 Certificates",
            "headings": [
                "Overview",
                "Prerequisites",
                "Store Certificates on the Worker",
                "Keystore",
                "Truststore",
                "Add Credentials to the Connector"
            ],
            "paragraphs": "In this guide, you can learn how to secure communications between your\nMongoDB Kafka Connector worker and your MongoDB cluster. To secure your connection, you must perform the following tasks: Create the certificates Store the certificates on the worker host machine Supply the certificates' credentials to the connector If you host your MongoDB cluster on  MongoDB Atlas  or\nyour cluster does not explicitly require certificates, you can\nalready communicate securely and do not need to follow the steps in\nthis guide. This guide requires prior knowledge of the following concepts: Transport Layer Security X.509 Certificate Authorities (CA) PKCS 12 OpenSSL keytool Store your certificates in a  keystore  and  truststore  to secure\nyour certificate credentials for each server you run your connector worker\ninstance on. You can use a keystore to store private keys and identity certificates.\nThe keystore uses the key and certificate to verify the client's\nidentity to external hosts. If your SSL/TLS configuration requires a client certificate to connect\nto your worker instance, generate a secure private key and include the\nclient certificate bundled with the intermediate CA. Then, store this\ninformation in your keystore by using the following  openssl  command\nto generate a PKCS 12 file: You can use a truststore to store certificates from a CA. The truststore\nuses the certificates to identify parties the client trusts. Some\nexamples of these certificates are a root CA, intermediate CA and your\nMongoDB cluster's end entity certificate. Import the certificates of parties that you trust into your truststore\nby using the following  keytool  command: If your SSL/TLS configuration requires the end entity certificate for your\nMongoDB cluster, import it into your truststore with the following\ncommand: For more information on how to set up a client keystore and truststore for\ntesting purposes, see\n OpenSSL Client Certificates for Testing . The connector worker processes JVM options from your  KAFKA_OPTS \nenvironment variable. The environment variable contains the path and\npassword to your keystore and truststore. Export the following JVM options in your  KAFKA_OPTS  variable: When the worker processes the JVM options, the connector attempts to\nconnect by using the SSL/TLS protocol and certificates in your keystore\nand truststore.",
            "code": [
                {
                    "lang": "bash",
                    "value": "openssl pkcs12 -export -inkey <your private key> \\\n               -in <your bundled certificate> \\\n               -out <your output pkcs12 file>"
                },
                {
                    "lang": "bash",
                    "value": "keytool -import -trustcacerts -import -file <your root or intermediate CA>"
                },
                {
                    "lang": "bash",
                    "value": "keytool -import -file <your server bundled certificate> -keystore <your keystore name>"
                },
                {
                    "lang": "bash",
                    "value": "export KAFKA_OPTS=\"\\\n-Djavax.net.ssl.trustStore=<your path to truststore> \\\n-Djavax.net.ssl.trustStorePassword=<your truststore password> \\\n-Djavax.net.ssl.keyStore=<your path to keystore> \\\n-Djavax.net.ssl.keyStorePassword=<your keystore password>\""
                }
            ],
            "preview": "In this guide, you can learn how to secure communications between your\nMongoDB Kafka Connector worker and your MongoDB cluster.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/fundamentals/change-streams",
            "title": "Change Streams",
            "headings": [
                "Overview",
                "Change Streams",
                "Aggregation",
                "Change Event Structure",
                "Performance",
                "Source Connectors",
                "Resume Tokens"
            ],
            "paragraphs": "In this guide, you can learn about  change streams  and how they are\nused in a MongoDB Kafka source connector. Change streams are a MongoDB feature that allow you to receive real-time\nupdates on data changes. Change streams return  change event documents . A change event document contains idempotent instructions to describe a change\nthat occurred in your MongoDB deployment and metadata related to that change.\nChange event documents are generated from data in the  oplog . To view a list of all configuration options for change streams, see the\n Change Stream Properties  page. To learn more about change streams, see the following resources: To learn more about the oplog, see the MongoDB manual entry on the\n Replica Set Oplog . A standalone MongoDB instance cannot produce a change stream. Change Streams  in the MongoDB manual An Introduction to Change Streams \nblog post Use an aggregation pipeline to configure your source connector's change stream.\nSome of the ways you can configure your connector's change stream are as follows: To learn which aggregation operators you can use with a change stream, see\nthe  Modify Change Stream Output \nguide in the MongoDB manual. To view examples that use an aggregation pipeline to modify a change stream,\nsee the following pages: Filter change events by operation type Project specific fields Update the value of fields Add fields Trim the amount of data generated by the change stream Customize a Pipeline to Filter Change Events  Usage Example Copy Existing Data  Usage Example Find the complete structure of change event documents, including\ndescriptions of all fields,\n in the MongoDB manual . If you want Kafka Connect to receive just the document created or modified\nfrom your change operation, use the  publish.full.document.only=true \noption. For more information, see the  Change Stream Properties \npage. The oplog is a special capped collection which cannot use indexes.  For more\ninformation on this limitation, see\n Change Streams Production Recommendations . If you need to improve change stream performance, use a faster disk for\nyour MongoDB cluster and increase the size of your WiredTiger cache. To\nlearn how to set your WiredTiger cache, see the guide on the\n WiredTiger Storage Engine . To view the available options to configure your source connector's change stream,\nsee the  Change Stream Properties  page. The source connector works by opening a single change stream with\nMongoDB and sending data from that change stream to Kafka Connect. Your source\nconnector maintains its change stream for the duration of its runtime, and your\nconnector closes its change stream when you stop it. Your connector uses a  resume token  as its  offset . An offset is a value\nyour connector stores in an Apache Kafka topic to keep track of what source data it\nhas processed. Your connector uses its offset value when it must recover from\na restart or crash. A resume token is a piece of data that references the\n _id  field of a change event document in your MongoDB oplog. If your source connector does not have an offset, such as when you start\nthe connector for the first time, your connector starts a new change stream.\nOnce your connector receives its first change event document and publishes that\ndocument to Apache Kafka, your connector stores the resume token of that document as\nits offset. If the resume token value of your source connector's offset does not correspond to\nany entry in your MongoDB deployment's oplog, your connector has an invalid resume\ntoken. To learn how to recover from an invalid resume token, see the\n invalid token troubleshooting guide . To learn more about resume tokens, see the following resources: To learn more about offsets, see the following resources: Resume a Change Stream \nin the MongoDB manual Change Events \nin the MongoDB manual Kafka Connect  offset.storage.topic \n configuration option documentation Kafka Connect  OffsetStorageReader \n API documentation",
            "code": [],
            "preview": "In this guide, you can learn about change streams and how they are\nused in a MongoDB Kafka source connector.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "source-connector/fundamentals/specify-schema",
            "title": "Apply Schemas",
            "headings": [
                "Overview",
                "Default Schemas",
                "Key Schema",
                "Value Schema",
                "Schemas For Transformed Documents",
                "Specify Schemas",
                "Infer a Schema"
            ],
            "paragraphs": "In this guide, you can learn how to apply schemas to incoming\ndocuments in a MongoDB Kafka source connector. There are two types of schema in Kafka Connect,  key schema  and\n value schema . Kafka Connect sends messages to Apache Kafka containing both\nyour value and a key. A key schema enforces a structure for keys in messages\nsent to Apache Kafka. A value schema enforces a structure for values in messages\nsent to Apache Kafka. Specifying schemas in the connector is optional, and you can specify any of the\nfollowing combinations of schemas: If you want to send data through Apache Kafka with a specific data format, such as Apache Avro\nor JSON Schema, see the  Converters  guide. To learn more about keys and values in Apache Kafka, see the\n official Apache Kafka introduction . This guide uses the Apache Kafka definition of the word \"key\", which differs\nslightly from the BSON definition. In BSON, a \"key\" is a unique string identifier\nfor a field in a document. In Apache Kafka, a \"key\" is a byte array sent in a message used to determine\nwhat partition of a topic to write the message to. Kafka keys can be\nduplicates of other keys or  null . Only a value schema Only a key schema Both a value and key schema No schemas To see a discussion on the benefits of using schemas with Kafka Connect,\nsee  this article from Confluent . The connector provides two default schemas: To learn more about change events, see our\n guide on change streams . To learn more about default schemas, see the default schemas\n here in the MongoDB Kafka Connector source code . A key schema for the _id field of MongoDB change event documents. A value schema for MongoDB change event documents. The connector provides a default key schema for the  _id  field of change\nevent documents. You should use the default key schema unless you remove the\n _id  field from your change event document using either of the transformations\n described in this guide here . If you specify either of these transformations and want to use a key\nschema for your incoming documents, you must specify a key schema\n as described in the specify a schema section of this guide . You can enable the default key schema with the following option: The connector provides a default value schema for change event documents. You\nshould use the default value schema unless you transform your change event\ndocuments\n as described in this guide here . If you specify either of these transformations and want to use a value schema for your\nincoming documents, you must use one of the mechanisms described in the\n schemas for transformed documents section of this guide . You can enable the default value schema with the following option: There are two ways you can transform your change event documents in a\nsource connector: If you transform your MongoDB change event documents,\nyou must do the following to apply schemas: To learn more about the preceding configuration options, see the\n Change Stream Properties  page. The  publish.full.document.only=true  option An aggregation pipeline that modifies the structure of change event documents Specify schemas Have the connector infer a value schema You can specify schemas for incoming documents using Avro schema syntax. Click on\nthe following tabs to see how to specify a schema for document values and keys: To view an example that demonstrates how to specify a schema, see the\n Specify a Schema  usage example. To learn more about Avro Schema, see the\n Data Formats  guide. If you want to send your data through Apache Kafka with Avro binary encoding,\nyou must use an Avro converter. For more information, see the guide on\n Converters . You can have your source connector infer a schema for incoming documents. This\noption works well for development and for data sources that do not\nfrequently change structure, but for most production deployments we recommend that you\n specify a schema . You can have the connector infer a schema by specifying the\nfollowing options: The source connector can infer schemas for incoming documents that\ncontain nested documents stored in arrays. Starting in Version 1.9 of the\nconnector, schema inference will gather the appropriate data type\nfor fields instead of defaulting to a  string  type assignment if there are\ndifferences between nested documents described by the following cases: If field types conflict between nested documents, the connector\npushes the conflict down to the schema for the field and defaults to a\n string  type assignment. A field is present in one document but missing in another. A field is present in one document but  null  in another. A field is an array with elements of any type in one document but\nhas additional elements or elements of other data types in another. A field is an array with elements of any type in one document but an\nempty array in another. The connector does not support key schema inference. If you want to use a key\nschema and transform your MongoDB change event documents, you must specify a\nkey schema as described in\n the specify schemas section of this guide .",
            "code": [
                {
                    "lang": "properties",
                    "value": "output.format.key=schema"
                },
                {
                    "lang": "properties",
                    "value": "output.format.value=schema"
                },
                {
                    "lang": "properties",
                    "value": "output.format.key=schema\noutput.schema.key=<your avro schema>"
                },
                {
                    "lang": "properties",
                    "value": "output.format.value=schema\noutput.schema.value=<your avro schema>"
                },
                {
                    "lang": "properties",
                    "value": "output.format.value=schema\noutput.schema.infer.value=true"
                }
            ],
            "preview": "In this guide, you can learn how to apply schemas to incoming\ndocuments in a MongoDB Kafka source connector.",
            "tags": "structure, code example",
            "facets": {
                "genre": [
                    "reference"
                ]
            }
        },
        {
            "slug": "source-connector/fundamentals/json-formatters",
            "title": "JSON Formatters",
            "headings": [
                "Overview",
                "JSON Formatters",
                "Built-In Formatters",
                "Examples",
                "Default JSON Formatter",
                "Extended JSON Formatter",
                "Simplified JSON Formatter"
            ],
            "paragraphs": "In this guide, you can learn how to specify built-in JSON formatter\nclasses to use in a MongoDB Kafka source connector. JSON formatters  specify how JSON data appears. They provide instructions\nfor how the connector represents different types when it outputs data. The following table describes the formatter classes available in the\nsource connector: Class Description com.mongodb.kafka.connect.source.json.formatter.DefaultJson The legacy strict JSON formatter. com.mongodb.kafka.connect.source.json.formatter.ExtendedJson The fully type-safe extended JSON formatter. This formatter\nemphasizes type preservation and represents most values with\ntheir BSON type. com.mongodb.kafka.connect.source.json.formatter.SimplifiedJson The simplified JSON formatter. This formatter represents\n ObjectId ,  Decimal ,  Date , and  Binary  values as\nstrings. The following table describes the fields of the sample document that the\nexamples in this guide use to demonstrate each output format. The\ncolumns describe the field name, value, and type for each field or\nnested field. Name Value Type _id ObjectID  ( $oid ) w x Binary  ( $binary ) y Date  ( $date ) z In the configuration properties for your source connector, set the\nfollowing property to specify the default JSON formatter: When you output the sample document contents from your Kafka topic, the\noutput shows the following type representations: ObjectId  value with its BSON type Decimal  value with its BSON type Binary  value with its buffer string and binary type Date  value as milliseconds since the UNIX epoch In the configuration properties for your source connector, set the\nfollowing property to specify the extended JSON formatter: When you output the sample document contents from your Kafka topic, the\noutput shows the following type representations: ObjectId  value with its BSON type Decimal  value with its BSON type Double  value with its BSON type Binary  value with its buffer string and binary type Date  value as milliseconds since the UNIX epoch Int32  value with its BSON type In the configuration properties for your source connector, set the\nfollowing property to specify the simplified JSON formatter: When you output the sample document contents from your Kafka topic, the\noutput shows the following type representations: ObjectId  value as its hexadecimal string Decimal  value as a string Binary  value as its buffer string Date  value as a string",
            "code": [
                {
                    "lang": "none",
                    "value": "\"5f15aab12435743f9bd126a4\""
                },
                {
                    "lang": "none",
                    "value": "[ 12345.6789, 23.53 ]"
                },
                {
                    "lang": "none",
                    "value": "\"SSBsb3ZlIGZvcm1hdHRpbmch\" of type \"00\""
                },
                {
                    "lang": "none",
                    "value": "\"2023-05-11T08:27:07.000Z\""
                },
                {
                    "lang": "none",
                    "value": "{ a: false, b: 87, c: \"hello world\" }"
                },
                {
                    "lang": "json",
                    "value": "\"output.json.formatter\": \"com.mongodb.kafka.connect.source.json.formatter.DefaultJson\""
                },
                {
                    "lang": "json",
                    "value": "{\n   \"_id\": {\"$oid\": \"5f15aab12435743f9bd126a4\"},\n   \"w\": [{\"$numberDecimal\": \"12345.6789\"}, 23.53],\n   \"x\": {\"$binary\": \"SSBsb3ZlIGZvcm1hdHRpbmch\", \"$type\": \"00\"},\n   \"y\": {\"$date\": 1683793627000},\n   \"z\": {\"a\": false, \"b\": 87, \"c\": \"hello world\"}\n}"
                },
                {
                    "lang": "json",
                    "value": "\"output.json.formatter\": \"com.mongodb.kafka.connect.source.json.formatter.ExtendedJson\""
                },
                {
                    "lang": "json",
                    "value": "{\n   \"_id\": {\"$oid\": \"5f15aab12435743f9bd126a4\"},\n   \"w\": [{\"$numberDecimal\": \"12345.6789\"}, {\"$numberDouble\": \"23.53\"}],\n   \"x\": {\"$binary\": \"SSBsb3ZlIGZvcm1hdHRpbmch\", \"$type\": \"00\"},\n   \"y\": {\"$date\": 1683793627000},\n   \"z\": {\"a\": false, \"b\": {\"$numberInt\": \"87\"}, \"c\": \"hello world\"}\n}"
                },
                {
                    "lang": "json",
                    "value": "\"output.json.formatter\": \"com.mongodb.kafka.connect.source.json.formatter.SimplifiedJson\""
                },
                {
                    "lang": "json",
                    "value": "{\n   \"_id\": \"5f15aab12435743f9bd126a4\",\n   \"w\": [\"12345.6789\", 23.53],\n   \"x\": \"SSBsb3ZlIGZvcm1hdHRpbmch\",\n   \"y\": \"2023-05-11T08:27:07Z\",\n   \"z\": {\"a\": false, \"b\": 87, \"c\": \"hello world\"}\n}"
                }
            ],
            "preview": "In this guide, you can learn how to specify built-in JSON formatter\nclasses to use in a MongoDB Kafka source connector.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/fundamentals",
            "title": "Fundamentals",
            "headings": [],
            "paragraphs": "Read the following sections to learn how MongoDB Kafka sink connector features work and\nhow to configure them: Specify How the Connector Writes Data to MongoDB Modify Sink Records Handle Errors Convert Change Data Capture Events to Write Operations",
            "code": [],
            "preview": "Read the following sections to learn how MongoDB Kafka sink connector features work and\nhow to configure them:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/configuration-properties",
            "title": "Sink Connector Configuration Properties",
            "headings": [
                "Overview",
                "Settings"
            ],
            "paragraphs": "In this section, you can read descriptions of the MongoDB Kafka sink connector properties,\nincluding essential Confluent Kafka Connect settings and MongoDB Kafka Connector-specific\nsettings. For an example sink connector configuration file, see\n MongoSinkConnector.properties . For sink connector performance recommendations, see  Tuning the Sink Connector . See the following categories for a list of related configuration properties: See the  Confluent Sink Connector documentation \nfor more information on these settings. Category Description MongoDB Connection Configuration Properties Specify how to connect to your MongoDB cluster. MongoDB Namespace Mapping Configuration Properties Specify where to sink your data. Kafka Topic Properties Specify the Kafka topics to which the connector should subscribe. Connector Message Processing Properties Set batch size, rate limiting, and number of parallel tasks. Connector Error Handling Properties Specify how to respond to errors and configure the dead letter queue. Sink Connector Post-processor Properties Specify transformations of Kafka topic data. Sink Connector Id Strategy Properties Specify how the connector generates document ids. Sink Connector Write Model Strategies Specify how the connector writes data to MongoDB. Topic Override Properties Override how the connector processes data on specific Kafka topics. Change Data Capture Properties Specify how the connector captures CDC events from a Kafka topic. Kafka Time Series Properties Configure the connector to sink data to a MongoDB time series\ncollection. All Sink Connector Configuration Properties View all preceding categories of configuration properties on one page.",
            "code": [],
            "preview": "In this section, you can read descriptions of the MongoDB Kafka sink connector properties,\nincluding essential Confluent Kafka Connect settings and MongoDB Kafka Connector-specific\nsettings.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "troubleshooting/recover-from-invalid-resume-token",
            "title": "Invalid Resume Token",
            "headings": [
                "Overview",
                "Stack Trace",
                "Cause",
                "Solutions",
                "Temporarily Tolerate Errors",
                "Reset Stored Offsets",
                "Prevention"
            ],
            "paragraphs": "Learn how to recover from an invalid resume token\nin a MongoDB Kafka source connector. The following stack trace indicates that the source connector has an invalid resume token: When the ID of your source connector's resume token does not correspond to any\nentry in your MongoDB deployment's  oplog ,\nyour connector has no way to determine where to begin to process your\nMongoDB change stream. Click the following tabs to see scenarios\nin which you can experience this issue: For more information on the oplog, see the\n MongoDB Manual . For more information on change streams, see the\n Change Streams  guide. In this scenario, you pause your source connector and you fill your MongoDB\ndeployment's oplog: You start a Kafka deployment with a MongoDB Kafka source connector. You produce a change event in your source MongoDB namespace, and your\nconnector stores a resume token corresponding to this event. You pause your source connector. While your connector pauses, you fill your MongoDB oplog such that MongoDB\ndeletes the oplog entry corresponding to your resume token. You restart your source connector and it is unable to resume\nprocessing as its resume token does not exist in your MongoDB oplog. In this scenario, your source connector listens for changes on an infrequently\nupdated MongoDB namespace and the\n heartbeat feature  is not enabled: You start a Kafka deployment with a MongoDB Kafka source connector. You produce a change event in your source MongoDB namespace, and your\nconnector stores a resume token corresponding to this event. Your source MongoDB namespace is not updated in the time it takes for your MongoDB\ndeployment to rotate the change event corresponding to your resume token out of its\noplog. You produce a change event in your source MongoDB namespace and your\nsource connector is unable to resume processing as its resume token does\nnot exist in your MongoDB oplog. You can recover from an invalid resume token using one of the following\nstrategies: Temporarily Tolerate Errors Reset Stored Offsets You can configure your source connector to tolerate errors\nwhile you produce a change stream event that updates the\nconnector's resume token. This recovery strategy is the\nsimplest, but there is a risk that your connector briefly\nignores errors unrelated to the invalid resume token. If you\naren't comfortable briefly tolerating errors\nin your deployment, you can\n delete stored offsets  instead. To configure your source connector to temporarily tolerate errors: For more information on the  errors.tolerance  option, see the\n Error Handling and Resuming from Interruption Properties  page. Set the  errors.tolerance  option to tolerate all errors: Insert, update, or delete a document in the collection referenced by your source connector to\nproduce a change stream event that updates your connector's resume token. Once you produce a change stream event, set the  errors.tolerance \noption to no longer tolerate errors: You can reset your Kafka Connect offset data, which contains your resume token,\nto allow your connector to resume processing your change stream. To reset your offset data, change the value of the\n offset.partition.name  configuration property to a partition name that does\nnot exist on your Kafka deployment. You can set your  offset.partition.name \nproperty like this: To learn more about the  offset.partition.name  configuration property, see\nthe  Error Handling and Resuming from Interruption Properties  page. To learn about naming your connector, see the official\n Apache Kafka \ndocumentation. Consider using the following pattern to name your offset partitions: This pattern provides the following benefits: Records the number of times you reset your connector Documents to which connector an offset partition belongs Assume you named your source connector  \"source-values\"  and you are\nsetting the  offset.partition.name  property for the first time.\nYou would configure your connector as follows: The next time you reset your connector's offset data, configure\nyour connector as follows: To prevent invalid resume token errors caused by an\n infrequently updated namespace , enable\n heartbeats . Heartbeats is a feature of your source connector that causes\nyour connector to update its resume token at regular intervals as well as when\nthe contents of your source MongoDB namespace changes. Specify the following option in your source connector configuration to enable\nheartbeats: To learn more about heartbeats, see the\n Error Handling and Resuming from Interruption Properties  guide.",
            "code": [
                {
                    "lang": "text",
                    "value": "...\norg.apache.kafka.connect.errors.ConnectException: ResumeToken not found.\nCannot create a change stream cursor\n...\nCommand failed with error 286 (ChangeStreamHistoryLost): 'PlanExecutor\nerror during aggregation :: caused by :: Resume of change stream was not\npossible, as the resume point may no longer be in the oplog\n..."
                },
                {
                    "lang": "java",
                    "value": "errors.tolerance=all"
                },
                {
                    "lang": "java",
                    "value": "errors.tolerance=none"
                },
                {
                    "lang": "properties",
                    "value": "offset.partition.name=<a string>"
                },
                {
                    "lang": "properties",
                    "value": "offset.partition.name=<source connector name>.<monotonically increasing number>"
                },
                {
                    "lang": "properties",
                    "value": "offset.partition.name=source-values.1"
                },
                {
                    "lang": "properties",
                    "value": "offset.partition.name=source-values.2"
                },
                {
                    "lang": "properties",
                    "value": "heartbeat.interval.ms=<a positive integer>"
                }
            ],
            "preview": "Learn how to recover from an invalid resume token\nin a MongoDB Kafka source connector.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/configuration-properties/kafka-topic",
            "title": "Kafka Topic Properties",
            "headings": [
                "Overview",
                "Settings"
            ],
            "paragraphs": "Use the following configuration settings to specify which Kafka topics the\nMongoDB Kafka sink connector should watch for data. For a list of sink connector configuration settings organized by category, see\nthe guide on  Sink Connector Configuration Properties . Name Description You can define either the  topics  or the  topics.regex \nsetting, but not both. This regex matches topic names such as \"activity.landing.clicks\"\nand \"activity.support.clicks\". It does not match the topic names\n\"activity.landing.views\" and \"activity.clicks\". You can define either the  topics  or the  topics.regex \nsetting, but not both.",
            "code": [
                {
                    "lang": "properties",
                    "value": "topics.regex=activity\\\\.\\\\w+\\\\.clicks$"
                }
            ],
            "preview": "Use the following configuration settings to specify which Kafka topics the\nMongoDB Kafka sink connector should watch for data.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/configuration-properties/post-processors",
            "title": "Sink Connector Post-processor Properties",
            "headings": [
                "Overview",
                "Settings"
            ],
            "paragraphs": "Use the following configuration settings to specify how the MongoDB Kafka sink connector\nshould transform Kafka data before inserting it into MongoDB. For a list of sink connector configuration settings organized by category, see\nthe guide on  Sink Connector Configuration Properties . Name Description post.processor.chain For more information on post-processors and examples of\ntheir usage, see the section on\n Post-processors . For information on how to create your own strategy, see\n Custom Write Model Strategies .",
            "code": [
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.DocumentIdAdder"
                },
                {
                    "lang": "none",
                    "value": "[ { \"oldName\":\"key.fieldA\", \"newName\":\"field1\" }, { \"oldName\":\"value.xyz\", \"newName\":\"abc\" } ]"
                },
                {
                    "lang": "none",
                    "value": "[ {\"regexp\":\"^key\\\\\\\\..*my.*$\", \"pattern\":\"my\", \"replace\":\"\"}, {\"regexp\":\"^value\\\\\\\\..*$\", \"pattern\":\"\\\\\\\\.\", \"replace\":\"_\"} ]"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.writemodel.strategy.DefaultWriteModelStrategy"
                }
            ],
            "preview": "Use the following configuration settings to specify how the MongoDB Kafka sink connector\nshould transform Kafka data before inserting it into MongoDB.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/configuration-properties/cdc",
            "title": "Change Data Capture Properties",
            "headings": [
                "Overview",
                "Settings"
            ],
            "paragraphs": "Use the following configuration settings to specify a class the MongoDB Kafka sink connector\nuses to process change data capture (CDC) events. See the guide on  Sink Connector Change Data Capture \nfor examples using the built-in  ChangeStreamHandler  and handlers for the\nDebezium and Qlik Replicate event producers. For a list of sink connector configuration settings organized by category, see\nthe guide on  Sink Connector Configuration Properties . Name Description",
            "code": [],
            "preview": "Use the following configuration settings to specify a class the MongoDB Kafka sink connector\nuses to process change data capture (CDC) events.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/configuration-properties/id-strategy",
            "title": "Sink Connector Id Strategy Properties",
            "headings": [
                "Overview",
                "Settings"
            ],
            "paragraphs": "Use the following configuration settings to specify how the MongoDB Kafka sink connector\nshould determine the  _id  value for each document it writes to MongoDB. For a list of sink connector configuration settings organized by category, see\nthe guide on  Sink Connector Configuration Properties . Name Description",
            "code": [
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy"
                }
            ],
            "preview": "Use the following configuration settings to specify how the MongoDB Kafka sink connector\nshould determine the _id value for each document it writes to MongoDB.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/configuration-properties/topic-override",
            "title": "Topic Override Properties",
            "headings": [
                "Overview",
                "Settings",
                "Example"
            ],
            "paragraphs": "Use the following MongoDB Kafka sink connector configuration settings to override global or\ndefault property settings for specific topics. For a list of sink connector configuration settings organized by category, see\nthe guide on  Sink Connector Configuration Properties . Name Description The  topic.override.foo.collection=bar  setting instructs the\nsink connector to store data from the  foo  topic in the  bar \ncollection. You can specify any valid configuration setting in the\n <propertyName>  segment on a per-topic basis except\n connection.uri  and  topics . You can override the sink connector to sink data from specific topics. The\nfollowing example configuration shows how you can define configuration\nsettings for a topic named  topicA : After applying these configuration settings, the sink connector performs\nthe following for data consumed from  topicA : For an example of how to configure the Block List Projector, see the\n Post Processors  guide. Write documents to the MongoDB collection  collectionA  in batches of\nup to 100. Generate a UUID value for each new document and write it to the  _id \nfield. Omit fields  k2  and  k4  from the value projection using the\n BlockList  projection type.",
            "code": [
                {
                    "lang": "properties",
                    "value": "topic.override.topicA.collection=collectionA\ntopic.override.topicA.max.batch.size=100\ntopic.override.topicA.document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.UuidStrategy\ntopic.override.topicA.post.processor.chain=com.mongodb.kafka.connect.sink.processor.DocumentIdAdder,com.mongodb.kafka.connect.sink.processor.BlockListValueProjector\ntopic.override.topicA.value.projection.type=BlockList\ntopic.override.topicA.value.projection.list=k2,k4"
                }
            ],
            "preview": "Use the following MongoDB Kafka sink connector configuration settings to override global or\ndefault property settings for specific topics.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/configuration-properties/error-handling",
            "title": "Connector Error Handling Properties",
            "headings": [
                "Overview",
                "Settings",
                "Bulk Write Exceptions",
                "Dead Letter Queue Configuration Example"
            ],
            "paragraphs": "Use the following configuration settings to specify how the MongoDB Kafka sink connector\nhandles errors and to configure the dead letter queue. For a list of sink connector configuration settings organized by category, see\nthe guide on  Sink Connector Configuration Properties . Name Description This property overrides the  errors.tolerance \nproperty of the Connect Framework. This property overrides the  errors.log.enable \nproperty of the Connect Framework. The connector can report the following exceptions to your dead letter queue\nas context headers when performing bulk writes: To enable bulk write exception reporting to the dead letter queue, use the\nfollowing connector configuration: Name Description This class outputs the error in the following format: The fields in the preceding message contain the following information: v : The version of the  WriteException  message format.\nThis field helps parse the messages produced by this exception.\nFor version 1.10 of the connector, the version of the\nmessage format is 1. code : The code associated with the error. To learn more see the\n getCode() \nmethod documentation. message : The message associated with the error. To learn more, see the\n getMessage() \nmethod documentation. details : The details associated with the error in JSON format. To\nlearn more, see the following method documentation: getDetails() toJson() This class outputs the error in the following format: The fields in the preceding message contain the following information: v : The version of the  WriteConcernException  message format.\nThis field helps parse the messages produced by this exception.\nFor version 1.10 of the\nconnector, the version of the message format is\n1. code : The code associated with the error. To learn more see the\n getCode() \nmethod documentation. codeName : The code name associated with the error. To learn more, see the\n getCodeName() \nmethod documentation. message : The message associated with the error. To learn more, see the\n getMessage() \nmethod documentation. details : The details associated with the error in JSON format. To\nlearn more, see the following method documentation: getDetails() toJson() This exception produces no message. The connector sends an ordered bulk write operation to MongoDB MongoDB fails to process a write operation in the ordered bulk write MongoDB does not attempt to perform all subsequent write operations in the ordered bulk write Apache Kafka version 2.6 added support for handling errant records. The\nKafka connector automatically sends messages that it cannot process to the\n dead letter queue . Once on the dead letter queue, you can inspect the\nerrant records, update them, and resubmit them for processing. The following is an example configuration for enabling the dead letter queue\ntopic  example.deadletterqueue . This configuration specifies that the\ndead letter queue and log file should record invalid messages, and that\nthe dead letter queue messages should include context headers. To learn more about dead letter queues, see  Write Errors and Errant Messages to a Topic .",
            "code": [
                {
                    "lang": "properties",
                    "value": "errors.tolerance=all\nerrors.deadletterqueue.topic.name=<name of topic to use as dead letter queue>\nerrors.deadletterqueue.context.headers.enable=true"
                },
                {
                    "lang": "none",
                    "value": "v=%d, code=%d, message=%s, details=%s"
                },
                {
                    "lang": "none",
                    "value": "v=%d, code=%d, codeName=%d, message=%s, details=%s"
                },
                {
                    "lang": "properties",
                    "value": "mongo.errors.tolerance=all\nmongo.errors.log.enable=true\nerrors.log.include.messages=true\nerrors.deadletterqueue.topic.name=example.deadletterqueue\nerrors.deadletterqueue.context.headers.enable=true"
                }
            ],
            "preview": "Use the following configuration settings to specify how the MongoDB Kafka sink connector\nhandles errors and to configure the dead letter queue.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/configuration-properties/time-series",
            "title": "Kafka Time Series Properties",
            "headings": [
                "Overview",
                "Settings"
            ],
            "paragraphs": "Use the following configuration settings to specify how the MongoDB Kafka sink connector\nshould sink data to a MongoDB time series collection. For a list of sink connector configuration settings organized by category, see\nthe guide on  Sink Connector Configuration Properties . For an example on how to convert an existing collection to a time series\ncollection, see the tutorial on how to  Migrate an Existing Collection to a Time Series Collection . Name Description This field must not be the  _id  field nor the field you specified\nin the  timeseries.timefield  setting.",
            "code": [
                {
                    "lang": "none",
                    "value": "yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]"
                },
                {
                    "lang": "none",
                    "value": "timeseries.timefield.auto.convert.date.format"
                }
            ],
            "preview": "Use the following configuration settings to specify how the MongoDB Kafka sink connector\nshould sink data to a MongoDB time series collection.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/configuration-properties/mongodb-connection",
            "title": "MongoDB Connection Configuration Properties",
            "headings": [
                "Overview",
                "Settings"
            ],
            "paragraphs": "Use the following configuration settings to specify how your MongoDB Kafka sink connector\nconnects and communicates with your MongoDB cluster. For a list of sink connector configuration settings organized by category, see\nthe guide on  Sink Connector Configuration Properties . Name Description connection.uri To avoid exposing your authentication credentials in your\n connection.uri  setting, use a\n ConfigProvider \nand set the appropriate configuration parameters. server.api.version server.api.deprecationErrors server.api.strict",
            "code": [],
            "preview": "Use the following configuration settings to specify how your MongoDB Kafka sink connector\nconnects and communicates with your MongoDB cluster.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/configuration-properties/connector-message",
            "title": "Connector Message Processing Properties",
            "headings": [
                "Overview",
                "Settings"
            ],
            "paragraphs": "Use the settings on this page to configure the message processing behavior of\nthe MongoDB Kafka sink connector including the following: Message batch size Rate limits Number of parallel tasks For a list of sink connector configuration settings organized by category, see\nthe guide on  Sink Connector Configuration Properties . Name Description If you specify a value greater than  1 , the connector enables\nparallel processing of the tasks. If your topic has multiple\npartition logs, which enables the connector to read from the\ntopic in parallel, the tasks may process the messages out of\norder.",
            "code": [
                {
                    "lang": "none",
                    "value": "[ 1, 2, 3, 4, 5 ]"
                },
                {
                    "lang": "none",
                    "value": "[1], [2], [3], [4], [5]"
                }
            ],
            "preview": "Use the settings on this page to configure the message processing behavior of\nthe MongoDB Kafka sink connector including the following:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/configuration-properties/write-strategies",
            "title": "Sink Connector Write Model Strategies",
            "headings": [
                "Overview",
                "Strategies"
            ],
            "paragraphs": "Use the strategies in the following table to specify how the MongoDB Kafka sink connector\nwrites data into MongoDB. You can specify a write strategy with the following\nconfiguration: For a list of sink connector configuration settings organized by category, see\nthe guide on  Sink Connector Configuration Properties . Name Description",
            "code": [
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=<a writemodel strategy>"
                },
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.InsertOneDefaultStrategy"
                },
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy"
                },
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy"
                },
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy"
                },
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneBusinessKeyStrategy"
                },
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.UpdateOneTimestampsStrategy"
                },
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.UpdateOneBusinessKeyTimestampStrategy"
                }
            ],
            "preview": "Use the strategies in the following table to specify how the MongoDB Kafka sink connector\nwrites data into MongoDB. You can specify a write strategy with the following\nconfiguration:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/configuration-properties/all-properties",
            "title": "All Sink Connector Configuration Properties",
            "headings": [
                "Overview",
                "MongoDB Connection",
                "MongoDB Namespace",
                "FieldPathNamespaceMapper Settings",
                "Connector Topic",
                "Connector Message Processing",
                "Connector Error Handling",
                "Post Processors",
                "ID Strategy",
                "Write Model Strategy",
                "Topic Override",
                "Change Data Capture",
                "Time Series"
            ],
            "paragraphs": "On this page, you can view all available configuration properties\nfor your MongoDB Kafka sink connector. This page duplicates the content of the\nother sink connector configuration properties pages. To view a list of all sink connector configuration properties pages,\nsee the  Sink Connector Configuration Properties  page. To view only the options related to configuring your MongoDB connection,\nsee the  MongoDB Connection Configuration Properties  page. Use the following configuration settings to specify how your MongoDB Kafka sink connector\nconnects and communicates with your MongoDB cluster. Name Description connection.uri To avoid exposing your authentication credentials in your\n connection.uri  setting, use a\n ConfigProvider \nand set the appropriate configuration parameters. server.api.version server.api.deprecationErrors server.api.strict To view only the options related to specifying where the connector writes data,\nsee the  MongoDB Namespace Mapping Configuration Properties  page. Use the following configuration settings to specify which MongoDB database\nand collection that your MongoDB Kafka sink connector writes data to. You can use the\ndefault  DefaultNamespaceMapper  or specify a custom class. Name Description The connector includes an alternative class for specifying the\ndatabase and collection called  FieldPathNamespaceMapper . See\nthe  FieldPathNamespaceMapper settings \nfor more information. database collection If you configure your sink connector to use the  FieldPathNamespaceMapper ,\nyou can specify which database and collection to sink a document based on the\ndata's field values. To enable this mapping behavior, set your sink connector  namespace.mapper \nconfiguration property to the fully-qualified class name as shown below: The  FieldPathNamespaceMapper  requires you to specify the following\nsettings: You can use the following settings to customize the behavior of the\n FieldPathNamespaceMapper : One or both mapping properties to a database and collection One of the  key  or  value  mappings to a database One of the  key  or  value  mappings to a collection Name Description To view only the options related to specifying Kafka topics, see the\n Kafka Topic Properties  page. Use the following configuration settings to specify which Kafka topics the\nMongoDB Kafka sink connector should watch for data. Name Description You can define either the  topics  or the  topics.regex \nsetting, but not both. This regex matches topic names such as \"activity.landing.clicks\"\nand \"activity.support.clicks\". It does not match the topic names\n\"activity.landing.views\" and \"activity.clicks\". You can define either the  topics  or the  topics.regex \nsetting, but not both. To view only the options related to change data capture handlers, see the\n Connector Message Processing Properties  page. Use the settings on this page to configure the message processing behavior of\nthe MongoDB Kafka sink connector including the following: Message batch size Rate limits Number of parallel tasks Name Description If you specify a value greater than  1 , the connector enables\nparallel processing of the tasks. If your topic has multiple\npartition logs, which enables the connector to read from the\ntopic in parallel, the tasks may process the messages out of\norder. To view only the options related to handling errors, see the\n Connector Error Handling Properties  page. Use the following configuration settings to specify how the MongoDB Kafka sink connector\nhandles errors and to configure the dead letter queue. Name Description This property overrides the  errors.tolerance \nproperty of the Connect Framework. This property overrides the  errors.log.enable \nproperty of the Connect Framework. To view only the options related to post-processors, see the\n Sink Connector Post-processor Properties  page. Use the following configuration settings to specify how the MongoDB Kafka sink connector\nshould transform Kafka data before inserting it into MongoDB. Name Description post.processor.chain For more information on post-processors and examples of\ntheir usage, see the section on\n Post-processors . For information on how to create your own strategy, see\n Custom Write Model Strategies . To view only the options related to determining the  _id  field of your\ndocuments, see the  Sink Connector Id Strategy Properties  page. Use the following configuration settings to specify how the MongoDB Kafka sink connector\nshould determine the  _id  value for each document it writes to MongoDB. Name Description To view only the options related to write model strategies, see the\n Sink Connector Write Model Strategies  page. Use the strategies in the following table to specify how the MongoDB Kafka sink connector\nwrites data into MongoDB. You can specify a write strategy with the following\nconfiguration: Name Description To view only the options related to overriding topic settings, see the\n Topic Override Properties  page. Use the following MongoDB Kafka sink connector configuration settings to override global or\ndefault property settings for specific topics. Name Description The  topic.override.foo.collection=bar  setting instructs the\nsink connector to store data from the  foo  topic in the  bar \ncollection. You can specify any valid configuration setting in the\n <propertyName>  segment on a per-topic basis except\n connection.uri  and  topics . To view only the options related to change data capture handlers, see the\n Change Data Capture Properties  page. Use the following configuration settings to specify a class the MongoDB Kafka sink connector\nuses to process change data capture (CDC) events. See the guide on  Sink Connector Change Data Capture \nfor examples using the built-in  ChangeStreamHandler  and handlers for the\nDebezium and Qlik Replicate event producers. Name Description To view only the options related to time series collections, see the\n Kafka Time Series Properties  page. Use the following configuration settings to specify how the MongoDB Kafka sink connector\nshould sink data to a MongoDB time series collection. For an example on how to convert an existing collection to a time series\ncollection, see the tutorial on how to  Migrate an Existing Collection to a Time Series Collection . Name Description This field must not be the  _id  field nor the field you specified\nin the  timeseries.timefield  setting.",
            "code": [
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper"
                },
                {
                    "lang": "properties",
                    "value": "namespace.mapper=com.mongodb.kafka.connect.sink.namespace.mapping.FieldPathNamespaceMapper"
                },
                {
                    "lang": "properties",
                    "value": "topics.regex=activity\\\\.\\\\w+\\\\.clicks$"
                },
                {
                    "lang": "none",
                    "value": "[ 1, 2, 3, 4, 5 ]"
                },
                {
                    "lang": "none",
                    "value": "[1], [2], [3], [4], [5]"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.DocumentIdAdder"
                },
                {
                    "lang": "none",
                    "value": "[ { \"oldName\":\"key.fieldA\", \"newName\":\"field1\" }, { \"oldName\":\"value.xyz\", \"newName\":\"abc\" } ]"
                },
                {
                    "lang": "none",
                    "value": "[ {\"regexp\":\"^key\\\\\\\\..*my.*$\", \"pattern\":\"my\", \"replace\":\"\"}, {\"regexp\":\"^value\\\\\\\\..*$\", \"pattern\":\"\\\\\\\\.\", \"replace\":\"_\"} ]"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.writemodel.strategy.DefaultWriteModelStrategy"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy"
                },
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=<a writemodel strategy>"
                },
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.InsertOneDefaultStrategy"
                },
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy"
                },
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy"
                },
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy"
                },
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneBusinessKeyStrategy"
                },
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.UpdateOneTimestampsStrategy"
                },
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.UpdateOneBusinessKeyTimestampStrategy"
                },
                {
                    "lang": "none",
                    "value": "yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]"
                },
                {
                    "lang": "none",
                    "value": "timeseries.timefield.auto.convert.date.format"
                }
            ],
            "preview": "On this page, you can view all available configuration properties\nfor your MongoDB Kafka sink connector. This page duplicates the content of the\nother sink connector configuration properties pages.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/configuration-properties/mongodb-namespace",
            "title": "MongoDB Namespace Mapping Configuration Properties",
            "headings": [
                "Overview",
                "Settings",
                "FieldPathNamespaceMapper Settings"
            ],
            "paragraphs": "Use the following configuration settings to specify which MongoDB database\nand collection that your MongoDB Kafka sink connector writes data to. You can use the\ndefault  DefaultNamespaceMapper  or specify a custom class. For a list of sink connector configuration settings organized by category, see\nthe guide on  Sink Connector Configuration Properties . Name Description The connector includes an alternative class for specifying the\ndatabase and collection called  FieldPathNamespaceMapper . See\nthe  FieldPathNamespaceMapper settings \nfor more information. database collection If you configure your sink connector to use the  FieldPathNamespaceMapper ,\nyou can specify which database and collection to sink a document based on the\ndata's field values. To enable this mapping behavior, set your sink connector  namespace.mapper \nconfiguration property to the fully-qualified class name as shown below: The  FieldPathNamespaceMapper  requires you to specify the following\nsettings: You can use the following settings to customize the behavior of the\n FieldPathNamespaceMapper : One or both mapping properties to a database and collection One of the  key  or  value  mappings to a database One of the  key  or  value  mappings to a collection Name Description",
            "code": [
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper"
                },
                {
                    "lang": "properties",
                    "value": "namespace.mapper=com.mongodb.kafka.connect.sink.namespace.mapping.FieldPathNamespaceMapper"
                }
            ],
            "preview": "Use the following configuration settings to specify which MongoDB database\nand collection that your MongoDB Kafka sink connector writes data to. You can use the\ndefault DefaultNamespaceMapper or specify a custom class.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/fundamentals/change-data-capture",
            "title": "Change Data Capture Handlers",
            "headings": [
                "Overview",
                "Specify a CDC Handler",
                "Available CDC Handlers",
                "Create Your Own CDC Handler",
                "How to Use Your CDC Handler"
            ],
            "paragraphs": "Learn how to  replicate  your  change data capture (CDC)  events with a\nMongoDB Kafka sink connector. CDC is a software architecture that converts changes in a datastore\ninto a stream of  CDC events . A CDC event is a message containing a\nreproducible representation of a change performed on a datastore. Replicating\ndata is the process of applying the changes contained in CDC events from one data\nstore onto a different datastore so that the changes occur in both datastores. Use a  CDC handler  to replicate CDC events stored on an Apache Kafka topic into MongoDB.\nA CDC handler is a program that translates CDC events from a specific\n CDC event producer  into MongoDB write operations. A CDC event producer is an application that generates CDC events. CDC event\nproducers can be datastores, or applications that watch datastores and generate\nCDC events corresponding to changes in the datastores. If you would like to view a tutorial demonstrating how to replicate data, see the\n Replicate Data With a Change Data Capture Handler tutorial . MongoDB change streams is an example of a CDC architecture. To learn more about\nchange streams, see\n the MongoDB Kafka Connector guide on Change Streams . You cannot apply a  post processor \nto CDC event data. If you attempt to specify both, the connector logs a warning. You can specify a CDC handler on your sink connector with the following configuration option: To learn more, see\n change data capture configuration options . The sink connector provides CDC handlers for the following CDC event producers: Click the following tabs to learn how to configure\nCDC handlers for the preceding event producers: MongoDB Debezium Qlik Replicate The following properties file configures a sink connector to replicate\nMongoDB change event documents: To view the source code for the MongoDB CDC handler, see\n the MongoDB Kafka Connector source code . Your sink connector can replicate Debezium CDC events originating from these datastores: Click the following tabs to see how to configure the Debezium CDC handler to replicate\nCDC events from each of the preceding datastores: MongoDB Postgres MySQL The following properties file configures a sink connector to replicate\nDebezium CDC events corresponding to changes in a MongoDB instance: To view the source code for the Debezium CDC handler, see\n the MongoDB Kafka Connector source code . The following properties file configures a sink connector to replicate\nDebezium CDC events corresponding to changes in a Postgres instance: To view the source code for the Debezium CDC handler, see\n the MongoDB Kafka Connector source code . The following properties file configures a sink connector to replicate\nDebezium CDC events corresponding to changes in a MySQL instance: To view the source code for the Debezium CDC handler, see\n the MongoDB Kafka Connector source code . If the Debezium CDC handler is unable to replicate CDC events\nfrom your datastore, you can customize the handler by extending the\n DebeziumCdcHandler \nclass. For more information on custom CDC handlers, see the\n Create your Own CDC Handler section  of this guide. Your sink connector can replicate Qlik Replicate CDC events originating from these\ndatastores: The following properties file configures a sink connector to replicate\nQlik Replicate CDC events: To view the source code for the Qlik Replicate CDC handler, see\n the MongoDB Kafka Connector source code . OracleDB MySQL Postgres If the Qlik Replicate CDC handler is unable to replicate CDC events\nfrom your datastore, you can customize the handler by extending the\n QlikCdcHandler \nclass. For more information on custom CDC handlers, see the\n Create your Own CDC Handler section  of this guide. If none of the prebuilt CDC handlers fit your use case, you can create your own.\nYour custom CDC handler is a Java class that implements the  CdcHandler  interface. To learn more, see the\n source code for the CdcHandler interface . To view examples of CDC handler implementations, see\n the source code for the prebuilt CDC handlers . To configure your sink connector to use your custom CDC Handler, you must perform the\nfollowing actions: To learn how to compile a class to a JAR file,\n see this guide from Oracle . Compile your custom CDC handler class to a JAR file. Add the compiled JAR to the classpath/plugin path for your Kafka workers.\nFor more information about plugin paths, see the  Confluent documentation . Kafka Connect loads plugins in isolation. When you deploy a custom write\nstrategy, both the connector JAR and the CDC handler\nJAR should be on the same path. Your paths should resemble the following: To learn more about Kafka Connect plugins, see\n this guide from Confluent . Specify your custom class in the  change.data.capture.handler \n configuration setting .",
            "code": [
                {
                    "lang": "properties",
                    "value": "change.data.capture.handler=<cdc handler class>"
                },
                {
                    "lang": "properties",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSinkConnector\nconnection.uri=<your connection uri>\ndatabase=<your database>\ncollection=<your collection>\ntopics=<topic containing mongodb change event documents>\nchange.data.capture.handler=com.mongodb.kafka.connect.sink.cdc.mongodb.ChangeStreamHandler"
                },
                {
                    "lang": "properties",
                    "value": "connector.class=com.mongodb.kafka.connect.sink.MongoSinkConnector\nconnection.uri=<your connection uri>\ndatabase=<your mongodb database>\ncollection=<your mongodb collection>\ntopics=<topic containing debezium cdc events>\nchange.data.capture.handler=com.mongodb.kafka.connect.sink.cdc.debezium.mongodb.MongoDbHandler"
                },
                {
                    "lang": "properties",
                    "value": "connector.class=com.mongodb.kafka.connect.sink.MongoSinkConnector\nconnection.uri=<your connection uri>\ndatabase=<your mongodb database>\ncollection=<your mongodb collection>\ntopics=<topic containing debezium cdc events>\nchange.data.capture.handler=com.mongodb.kafka.connect.sink.cdc.debezium.rdbms.postgres.PostgresHandler"
                },
                {
                    "lang": "properties",
                    "value": "connector.class=com.mongodb.kafka.connect.sink.MongoSinkConnector\nconnection.uri=<your connection uri>\ndatabase=<your mongodb database>\ncollection=<your mongodb collection>\ntopics=<topic containing debezium cdc events>\nchange.data.capture.handler=com.mongodb.kafka.connect.sink.cdc.debezium.rdbms.mysql.MysqlHandler"
                },
                {
                    "lang": "properties",
                    "value": "connector.class=com.mongodb.kafka.connect.MongoSinkConnector\nconnection.uri=<your connection uri>\ndatabase=<your database>\ncollection=<your collection>\ntopics=<topic containing qlik replicate cdc events>\nchange.data.capture.handler=com.mongodb.kafka.connect.sink.cdc.qlik.rdbms.RdbmsHandler"
                }
            ],
            "preview": "Learn how to replicate your change data capture (CDC) events with a\nMongoDB Kafka sink connector. CDC is a software architecture that converts changes in a datastore\ninto a stream of CDC events. A CDC event is a message containing a\nreproducible representation of a change performed on a datastore. Replicating\ndata is the process of applying the changes contained in CDC events from one data\nstore onto a different datastore so that the changes occur in both datastores.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/fundamentals/error-handling-strategies",
            "title": "Error Handling",
            "headings": [
                "Overview",
                "Handle Errors",
                "Stop For All Errors",
                "Tolerate All Errors",
                "Write Errors and Errant Messages to a Topic",
                "Log Errors",
                "Handle Errors at the Connector Level"
            ],
            "paragraphs": "In this guide, you can learn how to handle errors in your MongoDB Kafka sink connector.\nThe following list shows some common scenarios that cause your sink\nconnector to experience an error: When your sink connector encounters an error it does two actions: You write to a topic using Avro serialization and try to decode your messages from\nthat topic using Protobuf deserialization You use a change data capture handler on a message that does not contain change\nevent documents You apply an invalid single message transform to incoming documents Handles the Error Logs the Error When your connector encounters an error, it needs to handle it in some way.\nYour sink connector can do the following in response to an error: Stop For All Errors   default Tolerate All Errors Write Errors and Errant Messages to a Topic By default, your sink connector terminates and stops processing messages\nwhen it encounters an error. This is a good option for you if any error in\nyour sink connector indicates a serious problem. When your sink connector crashes, you must do one of the\nfollowing actions and then restart your connector to resume processing messages: You can have your sink connector stop when it encounters an error by either not\nspecifying any value for the  errors.tolerance  option, or by\nadding the following to your connector configuration: Allow your sink connector to temporarily  tolerate errors Update your sink connector's configuration to allow it to process the message Remove the errant message from your topic You can configure your sink connector to tolerate all errors and never stop\nprocessing messages. This is a good option for getting your sink connector up and\nrunning quickly, but you run the risk of missing problems in your connector\nas you do not receive any feedback if something goes wrong. You can have your sink connector tolerate all errors by specifying the following\noption: If you set your connector to tolerate errors and use ordered bulk writes, you\nmay lose data. If you set your connector to tolerate errors and use unordered bulk writes,\nyou lose less data. To learn more about bulk write operations, see\nthe  Write Model Strategies page . You can configure your sink connector to write errors and errant messages to a\ntopic, called a  dead letter queue , for you to inspect or process further.\nA dead letter queue is a location in message queueing\nsystems such as Apache Kafka where the system routes errant messages instead of\ncrashing or ignoring the error. Dead letter queues combine the feedback of\nstopping the program with the durability of tolerating all errors, and are a\ngood error handling starting point for most deployments. You can have your sink connector route all errant messages to a dead\nletter queue by specifying the following options: If you want to include the specific reason for the error as well as the\nerrant message, use the following option: To learn more about dead letter queues, see Confluent's guide on\n Dead Letter Queues . To view another dead letter queue configuration example, see  Dead Letter Queue Configuration Example . To learn about the exceptions your connector defines and writes as context\nheaders to the dead letter queue,\nsee  Bulk Write Exceptions . You can record tolerated and untolerated errors to a log file. Click on the tabs\nto see how to log errors: If you would like to log metadata about your message, such as your message's\ntopic and offset, use the following option: For more information, see Confluent's guide on\n logging with Kafka Connect . The following default option makes Kafka Connect write only untolerated errors to its application log: The following option makes Kafka Connect write both tolerated and untolerated errors to its\napplication log: The sink connector provides options that allow you to configure error\nhandling at the connector level. The options are as follows: You want to use these options if you want your connector to respond differently\nto errors related to MongoDB than to errors related to the Kafka Connect framework. For more information, see the following resources: Kafka Connect Option MongoDB Kafka Connector Option errors.tolerance mongo.errors.tolerance errors.log.enable mongo.errors.log.enable Connector Error Handling Properties New Names for Error Tolerance Options JIRA Ticket",
            "code": [
                {
                    "lang": "properties",
                    "value": "errors.tolerance=none"
                },
                {
                    "lang": "properties",
                    "value": "errors.tolerance=all"
                },
                {
                    "lang": "properties",
                    "value": "errors.tolerance=all\nerrors.deadletterqueue.topic.name=<name of topic to use as dead letter queue>"
                },
                {
                    "lang": "properties",
                    "value": "errors.deadletterqueue.context.headers.enable=true"
                },
                {
                    "lang": "properties",
                    "value": "errors.log.include.messages=true"
                },
                {
                    "lang": "properties",
                    "value": "errors.log.enable=false"
                },
                {
                    "lang": "properties",
                    "value": "errors.log.enable=true"
                }
            ],
            "preview": "In this guide, you can learn how to handle errors in your MongoDB Kafka sink connector.\nThe following list shows some common scenarios that cause your sink\nconnector to experience an error:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/fundamentals/write-strategies",
            "title": "Write Model Strategies",
            "headings": [
                "Overview",
                "Bulk Write Operations",
                "How to Specify Write Model Strategies",
                "Specify a Business Key",
                "Examples",
                "Update One Timestamps Strategy",
                "Replace One Business Key Strategy",
                "Delete One Business Key Strategy",
                "Custom Write Model Strategies",
                "Sample Write Model Strategy",
                "How to Install Your Strategy"
            ],
            "paragraphs": "This guide shows you how to change the way your MongoDB Kafka sink connector writes data to\nMongoDB. You can change how your connector writes data to MongoDB for use cases\nincluding the following: You can configure how your connector writes data to MongoDB by specifying a\n write model strategy . A write model strategy is a class that defines\nhow your sink connector should write data using  write models . A write\nmodel is a MongoDB Java driver interface that defines the\nstructure of a write operation. To learn how to modify the sink records your connector receives before your\nconnector writes them to MongoDB, read the guide on\n Sink Connector Post Processors . To see a write model strategy implementation, see the source code of the\n InsertOneDefaultStrategy class . Insert documents instead of upserting them Replace documents that match a filter other than the  _id  field Delete documents that match a filter The sink connector writes data to MongoDB using bulk write operations.\nBulk writes group multiple write operations, such as inserts,\nupdates, or deletes, together. By default, the sink connector performs ordered bulk writes, which\nguarantee the order of data changes. In an ordered bulk write, if any\nwrite operation results in an error, the connector skips the remaining\nwrites in that batch. If you don't need to guarantee the order of data changes, you can\nset the  bulk.write.ordered  setting to  false  so that the\nconnector performs unordered bulk writes. The sink connector performs\nunordered bulk writes in parallel, which can improve performance. In addition, when you enable unordered bulk writes and set the\n errors.tolerance  setting to  all , even if any write\noperation in your bulk write fails, the connector continues to\nperform the remaining write operations in the batch that do not return\nerrors. To learn more about the  bulk.write.ordered  setting, see the\n Connector Message Processing Properties . To learn more about bulk write operations, see the following\ndocumentation: Server manual entry on ordered and unordered bulk operations . Bulk write operations in Java To specify a write model strategy, use the following setting: For a list of the pre-built write model strategies included in the connector,\nsee the guide on  write model strategy configurations . A business key is a value composed of one or more fields in your sink record\nthat identifies it as unique. By default, the sink connector uses the  _id \nfield of the sink record to retrieve the business key. To specify a\ndifferent business key, configure the Document Id Adder post processor to use\na custom value. You can configure the Document Id Adder to set the  _id  field from the\nsink record key as shown in the following example properties: Alternatively, you can configure it to set the  _id  field from the sink\nrecord value as shown in the following example properties: The following write model strategies require a business key: For more information on the Document Id Adder post processor, see\n Configure the Document Id Adder Post Processor . Create a unique index in your target collection that corresponds to the\nfields of your business key. This improves the performance of write\noperations from your sink connector. See the guide on\n unique indexes  for more information. ReplaceOneBusinessKeyStrategy DeleteOneBusinessKeyStrategy UpdateOneBusinessKeyTimestampStrategy This section shows examples of configuration and output of the following write\nmodel strategies: Update One Timestamps Strategy Replace One Business Key Strategy Delete One Business Key Strategy You can configure the Update One Timestamps strategy to add and update\ntimestamps when writing documents to MongoDB. This strategy performs the\nfollowing actions: Suppose you want to track the position of a train along a route, and your\nsink connector receives messages with the following structure: Use the  ProvidedInValueStrategy  to specify that your connector should use\nthe  _id  value of the message to assign the  _id  field in your MongoDB\ndocument. Specify your id and write model strategy properties as follows: After your sink connector processes the preceding example record, it inserts a\ndocument that contains the  _insertedTS  and  _modifiedTS  fields as shown\nin the following document: After one hour, the train reports its new location along its route with\na new position as shown in the following record: Once your sink connector processes the preceding record, it inserts a document\nthat contains the following data: For more information on the  ProvidedInValueStrategy , see the section\non how to  Configure the Document Id Adder Post Processor . When the connector inserts a new MongoDB document, it sets the\n _insertedTS  and  _modifiedTS  fields to the current time on the\nconnector's server. When the connector updates an existing MongoDB document, it updates the\n _modifiedTS  field to the current time on the connector's server. You can configure the Replace One Business Key strategy to replace documents\nthat match the value of the business key. To define a business key on\nmultiple fields of a record and configure the connector to replace\ndocuments that contain matching business keys, perform the following\ntasks: Suppose you want to track airplane capacity by the flight number and airport\nlocation represented by  flight_no  and  airport_code , respectively. An\nexample message contains the following information: To implement the strategy, using  flight_no  and  airport_code  as the\nbusiness key, first create a unique index on these fields in the MongoDB\nshell: Next, specify the  PartialValueStrategy  strategy and business key\nfields in the a projection list. Specify the id and write model strategy\nconfiguration as follows: The sample data inserted into the collection contains the following: When the connector processes sink data that matches the business key of\nthe existing document, it replaces the document with the new values\nwithout changing the business key fields: After the connector processes the sink data, it replaces the original sample\ndocument in MongoDB with the preceding one. Create a  unique index  in your collection\nthat corresponds to your business key fields. Specify the  PartialValueStrategy  id strategy to identify the\nfields that belong to the business key in the connector configuration. Specify the  ReplaceOneBusinessKeyStrategy  write model strategy in the\nconnector configuration. You can configure the connector to remove a document when it receives\nmessages that match a business key using the Delete One Business Key\nstrategy. To set a business key from multiple fields of a record and\nconfigure the connector to delete a document that contains a matching\nbusiness key, perform the following tasks: Suppose you need to delete a calendar event from a specific year from\na collection that contains a document that resembles the following: To implement the strategy, using  year  as the business key, first create\na unique index on these fields in the MongoDB shell: Next, specify your business key and write model strategy in your\nconfiguration as follows: If your connector processes a sink record that contains the business key\n year , it deletes the first document with a matching field value\nreturned by MongoDB. Suppose your connector processes a sink record that\ncontains the following value data: When the connector processes the preceding record, it deletes the first\ndocument from the collection that contains a  year  field with a value of\n\"2005\" such as the original\n \"Dentist Appointment\" sample document . Create a  unique index  in your MongoDB\ncollection that corresponds to your business key fields. Specify the  PartialValueStrategy  as the id strategy to identify the\nfields that belong to the business key in the connector configuration. Specify the  DeleteOneBusinessKeyStrategy  write model strategy in the\nconnector configuration. If none of the write model strategies included with the connector fit your use\ncase, you can create your own. A write model strategy is a Java class that implements the\n WriteModelStrategy  interface and must override the  createWriteModel() \nmethod. See the\n source code for the WriteModelStrategy interface \nfor the required method signature. The following custom write model strategy returns a write operation that\nreplaces a MongoDB document that matches the  _id  field of your sink\nrecord with the value of the  fullDocument  field of your sink record: For another example of a custom write model strategy, see the\n UpsertAsPartOfDocumentStrategy \nexample strategy on GitHub. To configure your sink connector to use a custom write strategy, you must\ncomplete the following actions: To learn how to compile a class to a JAR file, see the\n JAR deployment guide \nfrom the Java SE documentation. Compile the custom write strategy class to a JAR file. Add the compiled JAR to the classpath/plugin path for your Kafka workers.\nFor more information about plugin paths, see the\n Confluent documentation . Kafka Connect loads plugins in isolation. When you deploy a custom write\nstrategy, both the connector JAR and the write model strategy\nJAR must be on the same path. Your paths should resemble the following: To learn more about Kafka Connect plugins, see\n this guide from Confluent . Specify your custom class in the\n writemodel.strategy \nconfiguration setting.",
            "code": [
                {
                    "lang": "properties",
                    "value": "writemodel.strategy=<write model strategy classname>"
                },
                {
                    "lang": "properties",
                    "value": "document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.PartialKeyStrategy\ndocument.id.strategy.partial.key.projection.list=<comma-separated field names>\ndocument.id.strategy.partial.key.projection.type=AllowList"
                },
                {
                    "lang": "properties",
                    "value": "document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy\ndocument.id.strategy.partial.value.projection.list=<comma-separated field names>\ndocument.id.strategy.partial.value.projection.type=AllowList"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": \"MN-1234\",\n  \"start\": \"Beacon\",\n  \"destination\": \"Grand Central\"\n  \"position\": [ 40, -73 ]\n}"
                },
                {
                    "lang": "properties",
                    "value": "document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInValueStrategy\nwritemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.UpdateOneTimestampsStrategy"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": \"MN-1234\",\n  \"_insertedTS\": ISODate(\"2021-09-20T15:08:000Z\"),\n  \"_modifiedTS\": ISODate(\"2021-09-20T15:08:000Z\"),\n  \"start\": \"Beacon\",\n  \"destination\": \"Grand Central\"\n  \"position\": [ 40, -73 ]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": \"MN-1234\",\n  \"start\": \"Beacon\",\n  \"destination\": \"Grand Central\"\n  \"position\": [ 42, -75 ]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"_id\": \"MN-1234\",\n  \"_insertedTS\": ISODate(\"2021-09-20T15:08:000Z\"),\n  \"_modifiedTS\": ISODate(\"2021-09-20T16:08:000Z\"),\n  \"start\": \"Beacon\",\n  \"destination\": \"Grand Central\"\n  \"position\": [ 42, -75 ]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"flight_no\": \"Z342\",\n  \"airport_code\": \"LAX\",\n  \"seats\": {\n    \"capacity\": 180,\n    \"occupied\": 152\n  }\n}"
                },
                {
                    "lang": "javascript",
                    "value": "db.collection.createIndex({ \"flight_no\": 1, \"airport_code\": 1}, { unique: true })"
                },
                {
                    "lang": "properties",
                    "value": "document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy\ndocument.id.strategy.partial.value.projection.list=flight_no,airport_code\ndocument.id.strategy.partial.value.projection.type=AllowList\nwritemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"flight_no\": \"Z342\"\n  \"airport_code\": \"LAX\",\n  \"seats\": {\n    \"capacity\": 180,\n    \"occupied\": 152\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"flight_no\": \"Z342\"\n  \"airport_code\": \"LAX\",\n  \"status\": \"canceled\"\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"year\": 2005,\n  \"month\": 3,\n  \"day\": 15,\n  \"event\": \"Dentist Appointment\"\n}"
                },
                {
                    "lang": "javascript",
                    "value": "db.collection.createIndex({ \"year\": 1 }, { unique: true })"
                },
                {
                    "lang": "properties",
                    "value": "document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy\ndocument.id.strategy.partial.value.projection.list=year\ndocument.id.strategy.partial.value.projection.type=AllowList\nwritemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneBusinessKeyStrategy"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"year\": 2005,\n  ...\n}"
                },
                {
                    "lang": "java",
                    "value": "/**\n * Custom write model strategy\n *\n * This class reads the 'fullDocument' field from a change stream and\n * returns a ReplaceOne operation.\n */\n\npublic class CustomWriteModelStrategy implements WriteModelStrategy {\n\n  private static String ID = \"_id\";\n  @Override\n  public WriteModel<BsonDocument> createWriteModel(final SinkDocument document) {\n    BsonDocument changeStreamDocument = document.getValueDoc()\n        .orElseThrow(() -> new DataException(\"Missing value document\"));\n\n    BsonDocument fullDocument = changeStreamDocument.getDocument(\"fullDocument\", new BsonDocument());\n    if (fullDocument.isEmpty()) {\n      return null; // Return null to indicate no op.\n    }\n\n    return new ReplaceOneModel<>(Filters.eq(ID, fullDocument.get(ID)), fullDocument);\n  }\n}"
                }
            ],
            "preview": "This guide shows you how to change the way your MongoDB Kafka sink connector writes data to\nMongoDB.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "sink-connector/fundamentals/post-processors",
            "title": "Sink Connector Post Processors",
            "headings": [
                "Overview",
                "How Post Processors Modify Data",
                "How to Specify Post Processors",
                "Prebuilt Post Processors",
                "Configure the Document Id Adder Post Processor",
                "Create a Custom Document Id Strategy",
                "Post Processor Examples",
                "Allow List and Block List Examples",
                "Allow List Projector Example",
                "Block List Projector Example",
                "Projection Wildcard Pattern Matching Examples",
                "Allow List Wildcard Examples",
                "Block List Wildcard Example",
                "Field Renaming Examples",
                "Rename by Mapping Example",
                "Rename by Regular Expression",
                "How to Create a Custom Post Processor"
            ],
            "paragraphs": "On this page, you can learn how to configure  post processors  in your\nMongoDB Kafka sink connector. Post processors modify sink records that the connector reads from\na Kafka topic before the connector stores it in your MongoDB collection. A few\nexamples of data modifications post processors can make include: You can use the prebuilt post processors included in the connector or\nimplement your own. See the following sections for more information on post processors: Set the document  _id  field to a custom value Include or exclude message key or value fields Rename fields How Post Processors Modify Data How to Specify Post Processors Prebuilt Post Processors Configure the Document Id Adder Post Processor Post Processor Configuration Examples Create a Custom Post Processor Post processors modify data read from a Kafka topic. The connector stores\nthe message in a  SinkDocument  class which contains a representation of\nthe Kafka  SinkRecord  key and value fields. The connector sequentially\napplies any post processors specified in the configuration and stores the\nresult in a MongoDB collection. Post processors perform data modification tasks such as generating\nthe document  _id  field, projecting message key or value fields, and\nrenaming fields. You can use the prebuilt post processors included in the\nconnector, or you can implement your own by extending the\n PostProcessor \nclass. You cannot apply a post processor to  CDC handler \nevent data. If you specify both, the connector logs a warning. You can specify one or more post processors in the  post.processor.chain \nconfiguration setting as a comma-separated list. If you specify more than one,\nthe connector applies them sequentially in which each post processor modifies\nthe data output by the prior one. To ensure the documents the connector writes to MongoDB contain unique  _id \nfields, it automatically adds the  DocumentIdAdder  post processor in the\nfirst position of the chain if you do not otherwise include it. The following example setting specifies that the connector should run the\n KafkaMetaAdder  post processor first and then the\n AllowListValueProjector  post processor on the output. The following table contains a list of all the post processors included in the\nsink connector. Post Processor Name Description The  DocumentIdAdder  post processor uses a  strategy  to determine how it\nshould format the  _id  field in the MongoDB document. A strategy defines\npreset behavior that you can customize for your use case. You can specify a strategy for this post processor in the\n document.id.strategy  setting as shown in the following example: The following table shows a list of the strategies you can use to\nconfigure the  DocumentIdAdder  post processor: Strategy Name Description If the built-in document id adder strategies do not cover your use case,\nyou can define a custom document id strategy by following the steps below: For example implementations of the  IdStrategy  interface, see the\nsource code directory that contains\n id strategy implementations \npackaged with the connector. Create a Java class that implements the interface\n IdStrategy \nand contains your custom configuration logic. Compile the class to a JAR file. Add the compiled JAR to the class path / plugin path for all your\nKafka workers. For more information about plugin paths, see the\n Confluent documentation . Update the  document.id.strategy  setting to the full class name of\nyour custom class in all your Kafka workers. BSON ObjectId or UUID strategies can only guarantee at-least-once\ndelivery since the connector generates new ids on retries or when\nprocessing records again. Other strategies permit exactly-once delivery\nif you can guarantee the fields that form the document id are unique. This section shows examples of configuration and sample output of the\nfollowing types of post processors: Allow List and Block List Projection Wildcard Pattern Matching Field Renaming The  allow list  and  block list  projector post processors determine which\nfields to include and exclude from the output. When you use the  allow list  projector, the post processor only outputs data\nfrom the fields that you specify. When you use the  block list  projector, the post process only omits data\nfrom the fields that you specify. When you add a projector to your post processor chain, you must specify\nthe projector type and whether to apply it to the key or value portion of the\nsink document. See the following sections for example projector configurations and\noutput. You can use the \".\" (dot) notation to reference nested fields in the\nrecord. You can also use the notation to reference fields of documents\nin an array. Suppose your Kafka record value documents resembled the following user\nprofile data: You can configure the  AllowList  value projector to store select data\nsuch as the \"name\", \"address.city\", and \"hobbies\" fields from your value\ndocuments using the following settings: After the post processor applies the projection, it outputs the following\nrecord: Suppose your Kafka record key documents resembled the following user\nidentification data: You can configure the  BlockList  key projector to omit the \"authToken\"\nand \"registration.source\" fields before storing the data with the\nfollowing settings: After the post processor applies the projection, it outputs the following\nrecord: This section shows how you can configure the projector post processors to\nmatch wildcard patterns to match field names. For the allow list and block list wildcard pattern matching examples in\nthis section, refer to the following value document that contains weather\nmeasurements: Pattern Description * Matches any number of characters in the current level. ** Matches any characters in the current level and all nested levels. You can use the  *  wildcard to match multiple field names. The following\nexample configuration matches the following fields: After the post processor applies the allow list projection, it outputs the\nfollowing record: You can use the  **  wildcard which matches objects at any level\nstarting from the one at which you specify the wildcard. The following\nwildcard matching example projects any document that contains the field\nnamed \"low\". The post processor that applies the projection outputs the following record: The top-level field named \"city\" The fields named \"average\" that are subdocuments of any top-level field\nthat starts with the name \"wind_speed\". You can use the wildcard patterns to match fields at a specific document\nlevel as shown in the following block list configuration example: This section shows how you can configure the  RenameByMapping \nand  RenameByRegex  field renamer post processors to update field names\nin a sink record. The field renaming settings specify the following: You must specify  RenameByMapping  and  RenameByRegex  settings in a\nJSON array. You can specify nested fields by using either dot notation or\npattern matching. The field renamer post processor examples use the following example sink\nrecord: Key Document Value Document Whether to update the key or value document in the record The field names to update The new field names The  RenameByMapping  post processor setting specifies one or more\nJSON objects that assign fields matching a string to a new name. Each\nobject contains the text to match in the  oldName  element and the\nreplacement text in the  newName  element as described in the table\nbelow. The following example property matches the \"location\" field of a key\ndocument and renames it to \"country\": This setting instructs the  RenameByMapping  post processor to transform\nthe  original key document  to the\nfollowing document: You can perform a similar field name assignment for value documents by\nspecifying the value document with the appended field name in the  oldName \nfield as follows: This setting instructs the  RenameByMapping  post processor to transform\nthe  original value document  to the\nfollowing document: You can also specify one or more mappings in the  field.renamer.mapping \nproperty by using a JSON array in string format as shown in the following\nsetting: Key Name Description oldName Specifies whether to match fields in the key or value document and\nthe field name to replace. The setting uses a \".\" character to\nseparate the two values. newName Specifies the replacement field name for all matches of the field. The  RenameByRegex  post processor setting specifies the field names and\ntext patterns that it should match, and replacement values for the matched\ntext. You can specify one or more renaming expressions in JSON objects\ncontaining the fields described in the following table: The following example setting instructs the post processor to perform the\nfollowing: When the connector applies the post processor to the  example key document \nand the  example value document ,\nit outputs the following: Key Document Value Document Key Name Description regexp Contains a regular expression that matches fields to perform the\nreplacement. pattern Contains a regular expression that matches on the text to replace. replace Contains the replacement text for all matches of the regular expression\nyou defined in the  pattern  field. Match any field names in the key document that start with \"date\". In the\nset of matching fields, replace all text that matches the pattern  _ \nwith the  -  character. Match any field names in the value document that are subdocuments of\n crepes . In the set of matching fields, replace all text that matches\nthe pattern  purchased  with  quantity . The target field names you set in your renamer post processors to may\nresult in duplicate field names in the same document. To avoid this, the\npost processor skips renaming when it would duplicate an existing field\nname at the same level of the document. If the built-in post processors do not cover your use case, you can create\na custom post processor class using the following steps: For example post processors, you can browse the\n source code for the built-in post processor classes . Create a Java class that extends the\n PostProcessor \nabstract class. Override the  process()  method in your class. You can update the\n SinkDocument , a BSON representation of the sink record key and value\nfields, and access the original Kafka  SinkRecord  in your method. Compile the class to a JAR file. Add the compiled JAR to the class path / plugin path for all your\nKafka workers. For more information about plugin paths, see the\nConfluent documentation on\n Manually Installing Community Connectors . Add your post processor full class name to the post processor chain\nconfiguration.",
            "code": [
                {
                    "lang": "properties",
                    "value": "post.processor.chain=com.mongodb.kafka.connect.sink.processor.KafkaMetaAdder,com.mongodb.kafka.connect.sink.processor.AllowListValueProjector"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.DocumentIdAdder"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.BlockListKeyProjector"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.BlockListValueProjector"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.AllowListKeyProjector"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.AllowListValueProjector``"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.KafkaMetaAdder"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.field.renaming.RenameByMapping"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.field.renaming.RenameByRegex"
                },
                {
                    "lang": "properties",
                    "value": "document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.UuidStrategy"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.id.strategy.KafkaMetaDataStrategy"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.id.strategy.FullKeyStrategy"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInKeyStrategy"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInValueStrategy"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.id.strategy.PartialKeyStrategy"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.id.strategy.UuidInKeyStrategy"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.id.strategy.UuidInValueStrategy"
                },
                {
                    "lang": "none",
                    "value": "com.mongodb.kafka.connect.sink.processor.id.strategy.UuidStrategy``"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"Sally Kimball\",\n  \"age\": 10,\n  \"address\": {\n    \"city\": \"Idaville\",\n    \"country\": \"USA\"\n  },\n  \"hobbies\": [\n    \"reading\",\n    \"solving crime\"\n  ]\n}"
                },
                {
                    "lang": "properties",
                    "value": "post.processor.chain=com.mongodb.kafka.connect.sink.processor.AllowListValueProjector\nvalue.projection.type=AllowList\nvalue.projection.list=name,address.city,hobbies"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"name\": \"Sally Kimball\",\n  \"address\": {\n    \"city\": \"Idaville\"\n  },\n  \"hobbies\": [\n    \"reading\",\n    \"solving crime\"\n  ]\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"username\": \"user5983\",\n  \"registration\": {\n    \"date\": \"2021-09-13\",\n    \"source\": \"mobile\"\n  },\n  \"authToken\": {\n    \"alg\": \"HS256\",\n    \"type\": \"JWT\",\n    \"payload\": \"zI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODk\"\n  }\n}"
                },
                {
                    "lang": "properties",
                    "value": "post.processor.chain=com.mongodb.kafka.connect.sink.processor.BlockListKeyProjector\nkey.projection.type=BlockList\nkey.projection.list=authToken,registration.source"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"username\": \"user5983\",\n  \"registration\": {\n    \"date\": \"2021-09-13\",\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"city\": \"Springfield\",\n  \"temperature\": {\n    \"high\": 28,\n    \"low\": 24,\n    \"units\": \"C\"\n  },\n  \"wind_speed_10m\": {\n    \"average\": 3,\n    \"units\": \"km/h\"\n  },\n  \"wind_speed_80m\": {\n    \"average\": 8,\n    \"units\": \"km/h\"\n  },\n  \"soil_conditions\": {\n    \"temperature\": {\n      \"high\": 22,\n      \"low\": 17,\n      \"units\": \"C\"\n    },\n    \"moisture\": {\n      \"average\": 340,\n      \"units\": \"mm\"\n    }\n  }\n}"
                },
                {
                    "lang": "properties",
                    "value": "post.processor.chain=com.mongodb.kafka.connect.sink.processor.AllowListValueProjector\nvalue.projection.type=AllowList\nvalue.projection.list=city,wind_speed*.average"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"city\": \"Springfield\",\n  \"wind_speed_10m\": {\n    \"average\": 3,\n  },\n  \"wind_speed_80m\": {\n    \"average\": 8,\n  }\n}"
                },
                {
                    "lang": "properties",
                    "value": "post.processor.chain=com.mongodb.kafka.connect.sink.processor.AllowListValueProjector\nvalue.projection.type=AllowList\nvalue.projection.list=**.low"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"temperature\": {\n    \"high\": 28,\n    \"low\": 24,\n    \"units\": \"C\"\n  },\n  \"soil_conditions\": {\n    \"temperature\": {\n      \"high\": 22,\n      \"low\": 17,\n      \"units\": \"C\"\n    }\n  }\n}"
                },
                {
                    "lang": "properties",
                    "value": "post.processor.chain=com.mongodb.kafka.connect.sink.processor.BlockListValueProjector\nvalue.projection.type=BlockList\nvalue.projection.list=*.*.temperature"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"city\": \"Springfield\",\n  \"temperature\": {\n    \"high\": 28,\n    \"low\": 24,\n    \"units\": \"C\"\n  },\n  \"wind_speed_10m\": {\n    \"average\": 3,\n    \"units\": \"km/h\"\n  },\n  \"wind_speed_80m\": {\n    \"average\": 8,\n    \"units\": \"km/h\"\n  },\n  \"soil_conditions\": {\n    \"moisture\": {\n      \"average\": 340,\n      \"units\": \"mm\"\n    }\n  }\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"location\": \"Provence\",\n  \"date_month\": \"October\",\n  \"date_day\": 17\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"flapjacks\": {\n    \"purchased\": 598,\n    \"size\": \"large\"\n  }\n}"
                },
                {
                    "lang": "properties",
                    "value": "field.renamer.mapping=[{\"oldName\":\"key.location\", \"newName\":\"country\"}]"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"country\": \"Provence\",\n  \"date_month\": \"October\",\n  \"date_day\": 17\n}"
                },
                {
                    "lang": "properties",
                    "value": "field.renamer.mapping=[{\"oldName\":\"value.flapjacks\", \"newName\":\"crepes\"}]"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"crepes\": {\n    \"purchased\": 598,\n    \"size\": \"large\"\n  }\n}"
                },
                {
                    "lang": "properties",
                    "value": "field.renamer.mapping=[{ \"oldName\":\"key.location\", \"newName\":\"city\" }, { \"oldName\":\"value.crepes\", \"newName\":\"flapjacks\" }]"
                },
                {
                    "lang": "properties",
                    "value": "field.renamer.regexp=[{\"regexp\":\"^key\\\\.date.*$\",\"pattern\":\"_\",\"replace\":\"-\"}, {\"regexp\":\"^value\\\\.crepes\\\\..*\",\"pattern\":\"purchased\",\"replace\":\"quantity\"}]"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"location\": \"Provence\",\n  \"date-month\": \"October\",\n  \"date-day\": 17\n}"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"crepes\": {\n    \"quantity\": 598,\n    \"size\": \"large\"\n  }\n}"
                }
            ],
            "preview": "On this page, you can learn how to configure post processors in your\nMongoDB Kafka sink connector. Post processors modify sink records that the connector reads from\na Kafka topic before the connector stores it in your MongoDB collection. A few\nexamples of data modifications post processors can make include:",
            "tags": null,
            "facets": null
        }
    ]
}