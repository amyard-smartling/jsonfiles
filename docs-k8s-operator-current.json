{
    "url": "http://mongodb.com/docs/kubernetes-operator/current",
    "includeInGlobalSearch": true,
    "documents": [
        {
            "slug": "manage-users",
            "title": "Manage Database Users",
            "headings": [],
            "paragraphs": "Manage database users using  LDAP (Lightweight Directory Access Protocol)  authentication on MongoDB\ndeployments. Manage database users using SCRAM authentication on MongoDB\ndeployments. Manage database users for deployments running with TLS and X.509\ninternal cluster authentication enabled.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "multi-cluster-deploy-replica-set",
            "title": "Deploy Replica Sets in a Multi-Kubernetes Cluster",
            "headings": [
                "Deploy a MongoDBMultiCluster Resource",
                "Deploy a MongoDBMultiCluster Resource",
                "Create the secret for the TLS certificate of your MongoDBMultiCluster resource.",
                "Create the ConfigMap to link your CA with your MongoDBMultiCluster resource.",
                "Configure kubectl to use the central cluster's namespace.",
                "Copy and paste the sample  resource.",
                "Change the settings to your preferred values.",
                "Add any additional accepted settings for a MongoDBMultiCluster resource deployment.",
                "Save this replica set config file with a .yaml extension.",
                "Start your replica set deployment.",
                "Track the status of your multi-Kubernetes-cluster deployment.",
                "Renew TLS Certificates for a MongoDBMultiCluster Resource",
                "Renew the secret for a MongoDBMultiCluster resource.",
                "Deploy a MongoDBMultiCluster Resource",
                "Configure kubectl to use the central cluster's namespace.",
                "Copy and paste the sample  resource.",
                "Change the settings to your preferred values.",
                "Add any additional accepted settings for a MongoDBMultiCluster resource deployment.",
                "Save this replica set config file with a .yaml extension.",
                "Start your replica set deployment.",
                "Track the status of your multi-Kubernetes-cluster deployment."
            ],
            "paragraphs": "Use this procedure to create a new replica set in a member  Kubernetes  cluster in a\n multi-Kubernetes-cluster deployment . This procedure allows you to set different settings for the replica set\nresource, such as overrides for statefulSet configuration. As an\nalternative to using this procedure, you can use the\n Multi-Kubernetes-Cluster Quick Start , which creates a\n multi-Kubernetes-cluster deployment  with default settings. Before you begin: Learn about  multi-Kubernetes-cluster deployments . Review the list of  multi-Kubernetes-cluster services and tools . Install the  Kubernetes Operator  in a  multi-Kubernetes-cluster deployment . See  Multi-Kubernetes-Cluster Quick Start . Complete the  prerequisites . Deploy  Ops Manager  resource on the central cluster. See  Deploy an  Ops Manager  Resource, deploy the Application Database, and Connect to  Ops Manager . You can use the following procedures in this\n TLS-Encrypted Connections  tab: These procedures establish  TLS (Transport Layer Security) -encrypted connections between\nMongoDB hosts in a replica set, and between client applications and\nMongoDB deployments. Before you begin, you must have valid certificates for  TLS (Transport Layer Security)  encryption. Deploy a  MongoDBMultiCluster  resource Renew TLS Certificates for a  MongoDBMultiCluster  resource Run the  kubectl  command to create a new secret that stores the\n MongoDBMultiCluster  resource  certificate: You must prefix your secrets with  <prefix>-<metadata.name> . If you call your deployment  my-deployment  and you set the\nprefix to  mdb , you must name the  TLS (Transport Layer Security)  secret for the\nclient  TLS (Transport Layer Security)  communications  mdb-my-deployment-cert . Also,\nyou must name the  TLS (Transport Layer Security)  secret for internal cluster authentication\n(if enabled)  mdb-my-deployment-clusterfile . Run the  kubectl  command to link your  CA (Certificate Authority)  to your  MongoDBMultiCluster  resource .\nSpecify the  CA (Certificate Authority)  certificate file that you must always name\n ca-pem  for the  MongoDBMultiCluster  resource : If you have not done so already, run the following commands to run\nall  kubectl  commands on the central cluster in the default\nnamespace. Copy the sample replica set  YAML (Yet Another Markup Language)  file and paste it into a new\ntext file. Change the file's settings to match your desired replica set configuration. Key Type Description Example metadata.name string Label for the  MongoDBMultiCluster  resource . See also  metadata.name  and  names \nin the  Kubernetes  documentation. Resource names must be 44 characters or less. multi-replica-set spec.version string Version of MongoDB that this  MongoDBMultiCluster  resource  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n MongoDB Versioning  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. 4.4.0-ent string Name of the  ConfigMap  with the  Ops Manager  connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value must exist on the  same  namespace as the resource\nyou want to create. The  Kubernetes Operator  tracks any changes to the ConfigMap and\nreconciles the state of the  MongoDB  resource . <my-project> string Name of the cluster in the  MongoDBMultiCluster  resource . cluster1.example.com integer The number of members in this cluster. 2 collection Optional. Provides the configuration for the  StatefulSet  override for each of\nthe cluster's StatefulSets in a  multi-Kubernetes-cluster deployment . If specified at an individual\ncluster level under  clusterSpecList , overrides the global configuration for\nthe StatefulSet for the entire  multi-Kubernetes-cluster deployment . See  Multi-Kubernetes-Cluster Resource Specification \nand  StatefulSet v1 apps Kubernetes documentation . See the example. collection Optional.  If specified, provides a per-cluster override for the default\nstorage size of the  volumeClaimtemplates , for the persistent volume that stores the data. See the example. spec.credentials string Name of the secret you\n created  as  Ops Manager   API (Application Programming Interface) \nauthentication credentials for the  Kubernetes Operator  to\ncommunicate with  Ops Manager . The  Ops Manager   Kubernetes   Secret  object\nholding the Credentials must exist on the  same  Namespace as\nthe resource you want to create. The  Kubernetes Operator  tracks any changes to the Secret and\nreconciles the state of the  MongoDB  resource . <mycredentials> spec.type string Type of  MongoDB  resource  to create. The only supported value for this\nfield is  ReplicaSet . See  Limitations . ReplicaSet You can also add any optional settings to the\n object  specification. See  Multi-Kubernetes-Cluster Resource Specification . In any directory, invoke the following  Kubernetes  command to create your\n replica set : To check the status of your  MongoDBMultiCluster  resource , use the following command on the central cluster: With the  -w  (watch) flag set, when the configuration changes, the output\nrefreshes immediately until the status phase achieves the  Running  state.\nTo learn more about resource deployment statuses, see  Troubleshoot the  Kubernetes Operator . Renew your  TLS (Transport Layer Security)  certificates periodically using the following procedure. Run this  kubectl  command to renew an existing  secret  that stores the certificates for the  MongoDBMultiCluster  resource : This procedure doesn't encrypt connections between MongoDB hosts in\na replica set, and between client applications and MongoDB deployments. If you have not done so already, run the following commands to run\nall  kubectl  commands on the central cluster in the default\nnamespace. Copy the sample replica set  YAML (Yet Another Markup Language)  file and paste it into a new\ntext file. Change the file's settings to match your desired replica set configuration. Key Type Description Example metadata.name string Label for the  MongoDBMultiCluster  resource . See also  metadata.name  and  names \nin the  Kubernetes  documentation. Resource names must be 44 characters or less. multi-replica-set spec.version string Version of MongoDB that this  MongoDBMultiCluster  resource  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n MongoDB Versioning  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. 4.4.0-ent string Name of the  ConfigMap  with the  Ops Manager  connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value must exist on the  same  namespace as the resource\nyou want to create. The  Kubernetes Operator  tracks any changes to the ConfigMap and\nreconciles the state of the  MongoDB  resource . <my-project> string Name of the cluster in the  MongoDBMultiCluster  resource . cluster1.example.com integer The number of members in this cluster. 2 collection Optional. Provides the configuration for the  StatefulSet  override for each of\nthe cluster's StatefulSets in a  multi-Kubernetes-cluster deployment . If specified at an individual\ncluster level under  clusterSpecList , overrides the global configuration for\nthe StatefulSet for the entire  multi-Kubernetes-cluster deployment . See  Multi-Kubernetes-Cluster Resource Specification \nand  StatefulSet v1 apps Kubernetes documentation . See the example. collection Optional.  If specified, provides a per-cluster override for the default\nstorage size of the  volumeClaimtemplates , for the persistent volume that stores the data. See the example. spec.credentials string Name of the secret you\n created  as  Ops Manager   API (Application Programming Interface) \nauthentication credentials for the  Kubernetes Operator  to\ncommunicate with  Ops Manager . The  Ops Manager   Kubernetes   Secret  object\nholding the Credentials must exist on the  same  Namespace as\nthe resource you want to create. The  Kubernetes Operator  tracks any changes to the Secret and\nreconciles the state of the  MongoDB  resource . <mycredentials> spec.type string Type of  MongoDB  resource  to create. The only supported value for this\nfield is  ReplicaSet . See  Limitations . ReplicaSet You can also add any optional settings to the\n object  specification. See  Multi-Kubernetes-Cluster Resource Specification . In any directory, invoke the following  Kubernetes  command to create your\n replica set : To check the status of your  MongoDBMultiCluster  resource , use the following command on the central cluster: With the  -w  (watch) flag set, when the configuration changes, the output\nrefreshes immediately until the status phase achieves the  Running  state.\nTo learn more about resource deployment statuses, see  Troubleshoot the  Kubernetes Operator .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\n  create secret tls <prefix>-<metadata.name>-cert \\\n  --cert=<resource-tls-cert> \\\n  --key=<resource-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\n  create configmap custom-ca -from-file=ca-pem=<your-custom-ca-file>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config use-context $MDB_CENTRAL_CLUSTER_FULL_NAME\nkubectl config set-context $(kubectl config current-context) \\\n  --namespace=mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# This example provides statefulSet overrides per cluster.\n\napiVersion: mongodb.com/v1\nkind: MongoDBMultiCluster\nmetadata:\n  name: multi-replica-set\nspec:\n  version: 4.4.0-ent\n  type: ReplicaSet\n  duplicateServiceObjects: false\n  credentials: my-credentials\n  opsManager:\n    configMapRef:\n      name: my-project\n  clusterSpecList:\n    - clusterName: cluster1.example.com\n      members: 2\n      statefulSet:\n        spec:\n          template:\n            spec:\n              containers:\n                # Example of custom sidecar containers. Remove it before using the file in production.\n                - name: sidecar1\n                  image: busybox\n                  command: [ \"sleep\" ]\n                  args: [ \"infinity\" ]\n          # Use the following settings to override the default storage size of the \"data\" Persistent Volume.\n          volumeClaimTemplates:\n            - metadata:\n                name: data\n              spec:\n                resources:\n                  requests:\n                    storage: 1Gi\n    - clusterName: cluster2.example.com\n      members: 1\n      statefulSet:\n        spec:\n          template:\n            spec:\n              containers:\n                # Example of custom sidecar containers. Remove it before using the file in production.\n                - name: sidecar2\n                  image: busybox\n                  command: [ \"sleep\" ]\n                  args: [ \"infinity\" ]\n          volumeClaimTemplates:\n            - metadata:\n                name: data\n              spec:\n                resources:\n                  requests:\n                    storage: 1Gi\n    - clusterName: cluster3.example.com\n      members: 1\n      statefulSet:\n        spec:\n          template:\n            spec:\n              containers:\n                # Example of custom sidecar containers. Remove it before using the file in production.\n                - name: sidecar3\n                  image: busybox\n                  command: [ \"sleep\" ]\n                  args: [ \"infinity\" ]\n          volumeClaimTemplates:\n            - metadata:\n                name: data\n              spec:\n                resources:\n                  requests:\n                    storage: 1Gi\n\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdbmc <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n--namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-cert \\\n--cert=<resource-tls-cert> \\\n--key=<resource-tls-key> \\\n--dry-run=client \\\n-o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config use-context $MDB_CENTRAL_CLUSTER_FULL_NAME\nkubectl config set-context $(kubectl config current-context) \\\n  --namespace=mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# This example provides statefulSet overrides per cluster.\n\napiVersion: mongodb.com/v1\nkind: MongoDBMultiCluster\nmetadata:\n  name: multi-replica-set\nspec:\n  version: 4.4.0-ent\n  type: ReplicaSet\n  duplicateServiceObjects: false\n  credentials: my-credentials\n  opsManager:\n    configMapRef:\n      name: my-project\n  clusterSpecList:\n    - clusterName: cluster1.example.com\n      members: 2\n      statefulSet:\n        spec:\n          template:\n            spec:\n              containers:\n                # Example of custom sidecar containers. Remove it before using the file in production.\n                - name: sidecar1\n                  image: busybox\n                  command: [ \"sleep\" ]\n                  args: [ \"infinity\" ]\n          # Use the following settings to override the default storage size of the \"data\" Persistent Volume.\n          volumeClaimTemplates:\n            - metadata:\n                name: data\n              spec:\n                resources:\n                  requests:\n                    storage: 1Gi\n    - clusterName: cluster2.example.com\n      members: 1\n      statefulSet:\n        spec:\n          template:\n            spec:\n              containers:\n                # Example of custom sidecar containers. Remove it before using the file in production.\n                - name: sidecar2\n                  image: busybox\n                  command: [ \"sleep\" ]\n                  args: [ \"infinity\" ]\n          volumeClaimTemplates:\n            - metadata:\n                name: data\n              spec:\n                resources:\n                  requests:\n                    storage: 1Gi\n    - clusterName: cluster3.example.com\n      members: 1\n      statefulSet:\n        spec:\n          template:\n            spec:\n              containers:\n                # Example of custom sidecar containers. Remove it before using the file in production.\n                - name: sidecar3\n                  image: busybox\n                  command: [ \"sleep\" ]\n                  args: [ \"infinity\" ]\n          volumeClaimTemplates:\n            - metadata:\n                name: data\n              spec:\n                resources:\n                  requests:\n                    storage: 1Gi\n\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdbmc <resource-name> -o yaml -w"
                }
            ],
            "preview": "Use this procedure to create a new replica set in a member Kubernetes cluster in a\nmulti-Kubernetes-cluster deployment.",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "faq",
            "title": "Frequently Asked Questions",
            "headings": [
                "What is an Operator?",
                "Why should I run MongoDB Server and MongoDB Enterprise Advanced in Kubernetes?",
                "Which Kubernetes platforms are supported for MongoDB Server deployments?",
                "How many deployments can MongoDB Enterprise Kubernetes Operator support?",
                "Should I run MongoDB Server in Kubernetes in the same cluster as the application using it?",
                "Can I deploy MongoDB Server across multiple Kubernetes clusters?",
                "What is the difference between using the Kubernetes Operator for managing multi-Kubernetes-cluster deployments and  managing a single Kubernetes cluster?",
                "Does MongoDB support running more than one Kubernetes Operator instance?"
            ],
            "paragraphs": "An operator is a standard mechanism that extends the control plane of\n Kubernetes  to managing custom  Kubernetes  resources. Because each operator is\nbuilt for its own Custom Resources (CRs), it can contain logic that\naddresses the type of service that the operator is built for. For the\n Kubernetes Operator , the operator includes the logic for the deployment of\nMongoDB Server and Ops Manager instances. Each CR used by the  Kubernetes Operator  represents an element of a MongoDB\nServer deployment in  Kubernetes , and has options for customizing that part\nof the deployment. Once you configure these objects in the  Kubernetes \ndeployment, the operator builds native  Kubernetes  objects, such as Stateful\nSets that are necessary to create Pods according to your specified\nrequirements for MongoDB Servers. The  Kubernetes Operator  also facilitates\nconfiguration of MongoDB Server features, such as database backups,\nthrough interaction with  MongoDB Cloud Manager  or  Ops Manager . When you deploy MongoDB Server or MongoDB Enterprise Advanced in  Kubernetes \nthrough the  MongoDB Enterprise Kubernetes Operator , your deployments can benefit from the\nresilience and simple orchestration that  Kubernetes  provides. The only supported way to deploy MongoDB Enterprise Advanced in  Kubernetes  is\nthrough the  MongoDB Enterprise Kubernetes Operator . The  MongoDB Enterprise Kubernetes Operator  simplifies your daily workflows\nand makes it easier for MongoDB technical support staff to assist you when needed. MongoDB Server supports any platform that builds upon native  Kubernetes  without\nchanging the default logic or behavior. In practice, this means that\nMongoDB Server supports any  Kubernetes  platform\n certified by the Cloud Native Computing Foundation .\nTo learn more, see  MongoDB Kubernetes Operator Compatibility . Kubernetes Operator  can support up to 50 deployments. However, changes made to\nlarge numbers of deployments at the same time result in long reconciliation times.\nTo avoid prolonged reconciliation times, limit a given  Kubernetes Operator  instance\nto 20 deployments. To learn more, see the  Deploy the Recommended Number of MongoDB Replica Sets . To help minimize latency, consider colocating your database and applications on\nthe same  Kubernetes  cluster if your deployment architecture allows this. Yes. To learn more, see  Deploy MongoDB Resources on Multiple Kubernetes Clusters .\nFor help, contact  MongoDB Support . To use the  Kubernetes Operator  for managing a  multi-Kubernetes-cluster deployment , you must set up a specific set of\n Kubernetes   Roles, ClusterRoles ,\n RoleBindings, ClusterRoleBindings ,\nand  ServiceAccounts . The  Kubernetes Operator  used for a  multi-Kubernetes-cluster deployment  can also reconcile a single  Kubernetes  cluster resource.\nTo learn more, see  Does MongoDB support running more than one  Kubernetes Operator  instance? . If possible, we recommend that you set up a single  Kubernetes Operator  instance to\nwatch one, many, or all namespaces within your  Kubernetes  cluster. By default,\nthe  Kubernetes Operator  watches all  custom resource  types that you choose\nto deploy, and you don't need to configure it to watch specific resource types. However, once you reach a  performance limit \nfor the number of deployments a single  Kubernetes Operator  instance can support,\nyou can set up an additional  Kubernetes Operator  instance. At this point,\nconsider how you want to divide up management of resources in the  Kubernetes  cluster.\nUse the following recommendations listed in the order of priority: Ensure that each  Kubernetes Operator  instance is watching different and non-overlapping\nnamespaces within the  Kubernetes  cluster. Alternatively, configure different instances of the  Kubernetes Operator  to watch\ndifferent resource types, either in different namespaces or overlapping namespaces. If you choose to use overlapping namespaces, ensure that each  Kubernetes Operator \ninstance watches different types of resources to avoid conflict that would\nresult in two instances of the  Kubernetes Operator  attempting to manage\nthe same resources. Before you configure another  Kubernetes Operator  instance, verify that none of its\nnamespaces are included in the subset of namespaces for the existing  Kubernetes Operator  instance.",
            "code": [],
            "preview": "An operator is a standard mechanism that extends the control plane of\nKubernetes to managing custom Kubernetes resources. Because each operator is\nbuilt for its own Custom Resources (CRs), it can contain logic that\naddresses the type of service that the operator is built for. For the\nKubernetes Operator, the operator includes the logic for the deployment of\nMongoDB Server and Ops Manager instances.",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "openshift-quick-start",
            "title": "OpenShift Quick Start",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Clone the MongoDB Enterprise Kubernetes Operator repository.",
                "Create a namespace for your Kubernetes deployment.",
                "Configure kubectl to default to your namespace.",
                "Create a secret that contains credentials authorized to pull images from the registry.connect.redhat.com repository.",
                "Install the MongoDB Enterprise Kubernetes Operator",
                "Create credentials and store them as a secret.",
                "Invoke the following command to create a ConfigMap.",
                "Find the organization's ID.",
                "Copy and paste the orgId into the URL.",
                "Deploy the replica set resource.",
                "Create a secret with your database user password",
                "Create a database user.",
                "Optional: View the newly created user in Cloud Manager or Ops Manager.",
                "Connect to the replica set."
            ],
            "paragraphs": "MongoDB Enterprise Kubernetes Operator  uses the  Kubernetes  API and tools to manage MongoDB\nclusters.  Kubernetes Operator  works together with MongoDB  Cloud Manager or Ops Manager . This\ntutorial demonstrates how to deploy and connect to your first replica\nset in  Cloud Manager or Ops Manager  from OpenShift with  Kubernetes Operator . This section is for single  Kubernetes  cluster deployments only. For\n multi-Kubernetes-cluster deployments , see  Multi-Kubernetes-Cluster Quick Start . This tutorial requires: A running  Cloud Manager or Ops Manager  cluster. By default, The  Kubernetes Operator  uses the  mongodb  namespace. To\nsimplify your installation, consider creating a namespace labeled\n mongodb  using the following  kubectl  command: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you created: If you use the  Kubernetes Operator  to deploy MongoDB\nresources to  multiple namespaces  or with\na  cluster-wide scope , create the secret\nonly in the namespace where you intend to deploy the  Kubernetes Operator . The\n Kubernetes Operator  synchronizes the secret across all watched namespaces. If you have not already, obtain a Red Hat subscription. Create a  Registry Service Account . Click on your Registry Service Account, then click the\n Docker Configuration  tab. Download the  <account-name>-auth.json  file and open it in a\ntext editor. Copy the  registry.redhat.io  object, and paste another instance\nof this object into the file. Remember to add a comma after the\nfirst object. Rename the second object\n registry.connect.redhat.com , then save the file: Create an  openshift-pull-secret.yaml  file and add the contents\nof the modified  <account-name>-auth.json  file as\n stringData  named  .dockerconfigjson  to the\n openshift-pull-secret.yaml  secret file. The value you provide in the  metadata.name  field contains\nthe secret name. Provide this value when asked for the\n <openshift-pull-secret> . Create a  secret  from the  openshift-pull-secret.yaml \nfile in the same namespace in which you will deploy the  Kubernetes Operator . Invoke the following  oc  command to install the  CustomResourceDefinitions  for\nMongoDB deployments: Add your  <openshift-pull-secret>  to the  ServiceAccount \ndefinitions in the  MongoDB Enterprise Kubernetes Operator   YAML (Yet Another Markup Language)  file. Invoke the following  oc  command to install  Kubernetes Operator : Run the following command: Provide your Public and Private Key values for the following\nparameters. To learn more, see  Create Credentials for the  Kubernetes Operator . Key Type Description Example metadata.name string Name of the  Kubernetes   object . Resource names must be 44 characters or less. Kubernetes  documentation on  names .\nThis name must follow  RFC1123  naming\nconventions, using only lowercase alphanumeric\ncharacters,  -  or  . , and must start and end with an\nalphanumeric character. my-project metadata.namespace string Kubernetes   namespace  where the  Kubernetes Operator  creates this\n MongoDB  resource  and other  objects . mongodb data.projectName string Label for your  Ops Manager \n Project . The  Kubernetes Operator  creates the  Ops Manager  project if it does\nnot exist. If you omit the  projectName , the  Kubernetes Operator \ncreates a project with the same name as your\n Kubernetes  resource. To use an existing project in a  Cloud Manager or Ops Manager \norganization, locate\nthe  projectName  by clicking the  All Clusters \nlink at the top left of the  Cloud Manager or Ops Manager  page, and\nsearching by name in the  Search \nbox, or scrolling to find the name in the list.\nEach card in this list represents the\ncombination of one  Cloud Manager or Ops Manager   Organization  and  Project . myProjectName data.orgId string Required . 24 character hex string that uniquely\nidentifies your\n Cloud Manager or Ops Manager   Organization . Specify an  existing Organization : If you provide an empty string as your  orgId ,  Kubernetes Operator \ncreates an organization with the same name as your project. You can use the  Kubernetes Operator  to deploy MongoDB resources with\n Cloud Manager  and with  Ops Manager  version 5.0.x or later. You can use the  Atlas Operator \nto deploy MongoDB resources to  Atlas . Click  Settings  in the left navigation bar. Select your organization, view the current  URL (Uniform Resource Locator) \nin your browser and copy the value displayed in\nthe  <orgId>  placeholder as follows: https://ops.example.com:8443/ \n v2#/org/<orgId>/projects You must have the  Organization Project Creator \nrole to create a new project within an existing\n Cloud Manager or Ops Manager  organization. 5b890e0feacf0b76ff3e7183 data.baseUrl string URL (Uniform Resource Locator)  to your  Ops Manager Application  including the  FQDN (fully qualified domain name)  and port\nnumber. If you deploy  Ops Manager  with the  Kubernetes Operator  and  Ops Manager  will\nmanage MongoDB database resources deployed  outside  of the  Kubernetes \ncluster it's deployed to, you must set  data.baseUrl  to the same\nvalue of the\n spec.configuration.mms.centralUrl \nsetting in the  Ops Manager  resource specification. Managing External MongoDB Deployments If you're using  Cloud Manager , set the  data.baseUrl  value\nto  https://cloud.mongodb.com . https://ops.example.com:8443 Copy and save the following  YAML (Yet Another Markup Language)  file: Run the following command: You can choose to use a cleartext password or a Base64-encoded\npassword. Plaintext passwords use  stringData.password  and\nBase64-encoded passwords use  data.password . For a cleartext password, create and save the following  YAML (Yet Another Markup Language)  file: For a Base64-encoded password, create and save the following YAML\nfile: Provide your values for the following parameters. To learn more,\nsee the  parameter descriptions . Copy and save the following  MongoDB User Resource Specification  file: Run the following command: You can view the newly-created user in  Cloud Manager or Ops Manager : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. Perform the following steps in the  Cloud Manager  application: Click  Deployment  in the left navigation. Click   for the deployment to which you want\nto connect. Click  Connect to this instance . Run the connection command in a terminal to connect to the\ndeployment.",
            "code": [
                {
                    "lang": "sh",
                    "value": "git clone https://github.com/mongodb/mongodb-enterprise-kubernetes.git"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=mongodb"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"auths\": {\n    \"registry.redhat.io\": {\n      \"auth\": \"<encoded-string>\"\n    },\n    \"registry.connect.redhat.com\": {\n      \"auth\": \"<encoded-string>\"\n    }\n  }\n}"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: openshift-pull-secret\nstringData:\n  .dockerconfigjson: |\n      {\n        \"auths\": {\n          \"registry.redhat.io\": {\n            \"auth\": \"<encoded-string>\"\n          },\n          \"registry.connect.redhat.com\": {\n            \"auth\": \"<encoded-string>\"\n          }\n        }\n      }\ntype: kubernetes.io/dockerconfigjson"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f openshift-pull-secret.yaml -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-appdb\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-database-pods\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f mongodb-enterprise-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb \\\n  create secret generic ops-manager-admin-key \\\n  --from-literal=\"publicKey=<publicKey>\" \\\n  --from-literal=\"privateKey=<privateKey>\""
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-project\n  namespace: mongodb\ndata:\n  projectName: myProjectName # this is an optional parameter; when omitted, the Operator creates a project with the resource name\n  orgId: 5b890e0feacf0b76ff3e7183 # this is a required parameter\n  baseUrl: https://ops.example.com:8443\n\nEOF"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: demo-mongodb-cluster-1\n  namespace: mongodb\nspec:\n  members: 3\n  version: 4.4.5-ent\n  type: ReplicaSet\n  security:\n    authentication:\n      enabled: true\n      modes: [\"SCRAM\"]\n  cloudManager:\n    configMapRef:\n      name: my-project\n  credentials: organization-secret\n  persistent: true\n  podSpec:\n    podTemplate:\n      spec:\n       containers:\n        - name: mongodb-enterprise-database\n          resources:\n            limits:\n              cpu: 2\n              memory: 1.5G\n            requests:\n              cpu: 1\n              memory: 1G\n            persistence:\n              single:\n                storage: 10Gi"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mms-user-1-password\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\nstringData:\n  password: <my-plain-text-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mms-user-1-password\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\ndata:\n  password: <base-64-encoded-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: mms-scram-user-1\nspec:\n  passwordSecretKeyRef:\n    name: mms-user-1-password\n    # Match to metadata.name of the User Secret\n    key: password\n  username: \"mms-scram-user-1\"\n  db: \"admin\" #\n  mongodbResourceRef:\n    name: \"demo-mongodb-cluster-1\"\n    # Match to MongoDB resource using authenticaiton\n  roles:\n  - db: \"admin\"\n    name: \"clusterAdmin\"\n  - db: \"admin\"\n    name: \"userAdminAnyDatabase\"\n  - db: \"admin\"\n    name: \"readWrite\"\n  - db: \"admin\"\n    name: \"userAdminAnyDatabase\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                }
            ],
            "preview": "MongoDB Enterprise Kubernetes Operator uses the Kubernetes API and tools to manage MongoDB\nclusters. Kubernetes Operator works together with MongoDB Cloud Manager or Ops Manager. This\ntutorial demonstrates how to deploy and connect to your first replica\nset in Cloud Manager or Ops Manager from OpenShift with Kubernetes Operator.",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "third-party-integrations",
            "title": "Third-Party Integrations",
            "headings": [],
            "paragraphs": "Kubernetes Operator  offers the following third-party integrations. Third-Party Service Configuration Details Grafana Kubernetes Operator  offers a  sample Grafana dashboard  that you can  import into Grafana . Prometheus You can use  Kubernetes Operator  to  deploy  a\n MongoDB resource  or\n application database  to use with\nPrometheus.",
            "code": [],
            "preview": "Kubernetes Operator offers the following third-party integrations.",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "deploy",
            "title": "Deploy a MongoDB Database Resource",
            "headings": [],
            "paragraphs": "Use  Kubernetes Operator  to deploy a new standalone MongoDB instance. Use  Kubernetes Operator  to deploy a replica set. Use  Kubernetes Operator  to deploy a sharded cluster. Use  Kubernetes Operator  to deploy a MongoDB resource to use with\nPrometheus monitoring enabled.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "multi-cluster-troubleshooting",
            "title": "Troubleshoot Deployments with Multiple Kubernetes Clusters",
            "headings": [
                "Recover from a Kubernetes Cluster Failure"
            ],
            "paragraphs": "To troubleshoot your  multi-Kubernetes-cluster deployments , use the procedures in\nthis section. This procedure uses the same cluster names as in the  Prerequisites .\nIf the cluster  MDB_CLUSTER_1  that holds MongoDB nodes goes down, and\nif you provision a new cluster named  MDB_CLUSTER_4  instead of\n MDB_CLUSTER_1  to hold the new MongoDB nodes, run the\n MongoDB kubectl plugin \nwith the updated list of member clusters, and then edit the  MongoDBMultiCluster  resource \nspec on the central cluster. To reconfigure the  multi-Kubernetes-cluster deployment  after a cluster failure, replace the\nfailed  Kubernetes  cluster with the newly provisioned cluster as follows: Also see  ConfigMap Name mongodb-enterprise-operator-member-list is Hard-Coded . Run the  MongoDB kubectl plugin \nwith the  recover  parameter and the new cluster  MDB_CLUSTER_4 \nspecified in the  -member-clusters  option. This enables the\n Kubernetes Operator  to communicate with the new cluster to schedule MongoDB\nnodes on it. In the following example,  -member-clusters  contains\n ${MDB_CLUSTER_4_FULL_NAME} . On the central cluster, locate and edit the  MongoDBMultiCluster  resource  spec to add\nthe new cluster name to the  clusterSpecList  and remove the failed\n Kubernetes  cluster from this list. The resulting list of cluster names should\nbe similar to the following example: Restart the  Kubernetes Operator  Pod. After the restart, the  Kubernetes Operator \nshould reconcile the MongoDB deployment on the newly created\n MDB_CLUSTER_4  cluster that you created as a replacement for\nthe  MDB_CLUSTER_1  failure. To learn more about resource\nreconciliation, see  Deployment Architecture and Diagrams .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl mongodb multicluster recover \\\n  --central-cluster=\"MDB_CENTRAL_CLUSTER_FULL_NAME\" \\\n  --member-clusters=\"${MDB_CLUSTER_2_FULL_NAME},${MDB_CLUSTER_3_FULL_NAME},${MDB_CLUSTER_4_FULL_NAME}\" \\\n  --member-cluster-namespace=\"mongodb\" \\\n  --central-cluster-namespace=\"mongodb\" \\\n  --operator-name=mongodb-enterprise-operator-multi-cluster \\\n  --source-cluster=\"${MDB_CLUSTER_2_FULL_NAME}\""
                },
                {
                    "lang": "sh",
                    "value": "clusterSpecList:\n  - clusterName: ${MDB_CLUSTER_4_FULL_NAME}\n    members: 3\n  - clusterName: ${MDB_CLUSTER_2_FULL_NAME}\n    members: 2\n  - clusterName: ${MDB_CLUSTER_3_FULL_NAME}\n    members: 3"
                }
            ],
            "preview": "To troubleshoot your multi-Kubernetes-cluster deployments, use the procedures in\nthis section.",
            "tags": "multicluster, multi-cluster",
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "kind-quick-start",
            "title": "Quick Start",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Add the MongoDB Helm Charts for Kubernetes repository to Helm.",
                "Install the MongoDB Enterprise Kubernetes Operator",
                "Configure kubectl to default to your namespace.",
                "Configure the Kubernetes Operator",
                "Copy and save the ConfigMap.",
                "Copy and save the Secret.",
                "Apply the ConfigMap and Secret.",
                "Deploy the replica set resource.",
                "Create a secret with your database user password",
                "Create a database user.",
                "Optional: View the newly created user in Cloud Manager or Ops Manager.",
                "Connect to the replica set."
            ],
            "paragraphs": "MongoDB Enterprise Kubernetes Operator  uses the  Kubernetes  API and tools to manage MongoDB\nclusters.  Kubernetes Operator  works together with MongoDB  Cloud Manager or Ops Manager . This\ntutorial demonstrates how to deploy and connect to your first replica\nset in  MongoDB Cloud Manager  with  Kubernetes Operator . You can use  Kind (Kubernetes in Docker)  to quickly set\nup a cluster. To learn more, see  Kind . This section is for single  Kubernetes  cluster deployments only. For\n multi-Kubernetes-cluster deployments , see  Multi-Kubernetes-Cluster Quick Start . This tutorial requires: A running  MongoDB Cloud Manager  cluster. A running  Kubernetes  cluster. Kubernetes  nodes running on  supported hardware architectures . To  install the Kubernetes Operator with the Helm Chart , see the\ninstructions in the repository. Example The following command installs the  MongoDB Enterprise Kubernetes Operator  in the  mongodb \nnamespace with the optional  --create-namespace  option. By\ndefault,  Kubernetes Operator  uses the  default  namespace. If you haven't already, run the following command to execute all\n kubectl  commands in the namespace you created: Go to the Kubernetes Setup Page in the Cloud Manager UI . Click  Create New API Keys  or\n Use Existing API Keys . Complete the form. To learn more, see\n Programmatic Access to Cloud Manager . Click  Generate Key and YAML . Copy and save the generated  config-map.yaml  file. Example: To learn more, see the  parameter descriptions . Copy and save the generated  secret.yaml  file. Example: For security purposes,  MongoDB Cloud Manager  displays this file only once. Run the following command: Copy and save the following  YAML (Yet Another Markup Language)  file: Run the following command: You can choose to use a cleartext password or a Base64-encoded\npassword. Plaintext passwords use  stringData.password  and\nBase64-encoded passwords use  data.password . For a cleartext password, create and save the following  YAML (Yet Another Markup Language)  file: For a Base64-encoded password, create and save the following YAML\nfile: Provide your values for the following parameters. To learn more,\nsee the  parameter descriptions . Copy and save the following  MongoDB User Resource Specification  file: Run the following command: You can view the newly-created user in  Cloud Manager or Ops Manager : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. Perform the following steps in the  Cloud Manager  application: Click  Deployment  in the left navigation. Click   for the deployment to which you want\nto connect. Click  Connect to this instance . Run the connection command in a terminal to connect to the\ndeployment.",
            "code": [
                {
                    "lang": "sh",
                    "value": "helm repo add mongodb https://mongodb.github.io/helm-charts"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator --namespace mongodb --create-namespace"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-project\n  namespace: mongodb\ndata:\n  baseUrl: https://cloud.mongodb.com\n\n  projectName: my-project # this is an optional parameter\n  orgId: 5ecd252f8c1a75033c74106c # this is a required parameter"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: organization-secret\n  namespace: mongodb\nstringData:\n  user: <public_key>\n  publicAPIKey: <private_key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f secret.yaml -f config-map.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: demo-mongodb-cluster-1\n  namespace: mongodb\nspec:\n  members: 3\n  version: 4.4.5-ent\n  type: ReplicaSet\n  security:\n    authentication:\n      enabled: true\n      modes: [\"SCRAM\"]\n  cloudManager:\n    configMapRef:\n      name: my-project\n  credentials: organization-secret\n  persistent: true\n  podSpec:\n    podTemplate:\n      spec:\n       containers:\n        - name: mongodb-enterprise-database\n          resources:\n            limits:\n              cpu: 2\n              memory: 1.5G\n            requests:\n              cpu: 1\n              memory: 1G\n            persistence:\n              single:\n                storage: 10Gi"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mms-user-1-password\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\nstringData:\n  password: <my-plain-text-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mms-user-1-password\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\ndata:\n  password: <base-64-encoded-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: mms-scram-user-1\nspec:\n  passwordSecretKeyRef:\n    name: mms-user-1-password\n    # Match to metadata.name of the User Secret\n    key: password\n  username: \"mms-scram-user-1\"\n  db: \"admin\" #\n  mongodbResourceRef:\n    name: \"demo-mongodb-cluster-1\"\n    # Match to MongoDB resource using authenticaiton\n  roles:\n  - db: \"admin\"\n    name: \"clusterAdmin\"\n  - db: \"admin\"\n    name: \"userAdminAnyDatabase\"\n  - db: \"admin\"\n    name: \"readWrite\"\n  - db: \"admin\"\n    name: \"userAdminAnyDatabase\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                }
            ],
            "preview": "MongoDB Enterprise Kubernetes Operator uses the Kubernetes API and tools to manage MongoDB\nclusters. Kubernetes Operator works together with MongoDB Cloud Manager or Ops Manager. This\ntutorial demonstrates how to deploy and connect to your first replica\nset in MongoDB Cloud Manager with Kubernetes Operator. You can use Kind (Kubernetes in Docker) to quickly set\nup a cluster. To learn more, see Kind.",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "multi-cluster-overview",
            "title": "Overview",
            "headings": [
                "Central and Member Clusters",
                "Migrating to Multi-Kubernetes-Cluster Deployments"
            ],
            "paragraphs": "Multi-Kubernetes-cluster deployments  enable different levels of resilience, depending on the\nneeds of your enterprise application: Multi-Kubernetes-cluster deployments  allow you to add MongoDB instances in global clusters\nthat span multiple geographic regions for increased availability and\nglobal distribution of data. Single Region, Multi AZ . One or more  Kubernetes  clusters where you deploy\neach cluster's nodes in different zones in the same region. Such deployments\nprotect MongoDB instances backing your enterprise applications against\nzone and  Kubernetes  cluster failures and offer increased availability,\ndisaster recovery, and data distribution within one cloud region. Multi Region . One or more  Kubernetes  clusters where you deploy each cluster\nin a different region, and within each region, deploy cluster nodes in\ndifferent availability zones. This gives your database resilience against\nthe loss of a  Kubernetes  cluster, a zone, or an entire cloud region. Identify one  Kubernetes  cluster that should act as a  central cluster  in your  multi-Kubernetes-cluster deployment . The following diagram shows the high-level architecture of a  multi-Kubernetes-cluster deployment \nacross regions and availability zones.\nTo learn more, see  Architecture, Capabilities, and Limitations . A  multi-Kubernetes-cluster deployment  that uses the  MongoDB Enterprise Kubernetes Operator  consists of one\n central cluster  and one or more  member clusters  in  Kubernetes : The  central cluster  has the following role: Hosts the  MongoDB Enterprise Kubernetes Operator Acts as the control plane for the  multi-Kubernetes-cluster deployment Hosts the  MongoDBMultiCluster  resource  spec for the MongoDB replica set Hosts  Ops Manager , if you deploy  Ops Manager  with the  Kubernetes Operator Can also host members of the MongoDB replica set Member clusters  host the MongoDB replica sets. This deployment uses a service mesh. This simplifies configuring  multi-Kubernetes-cluster deployments .\nA service mesh enables inter-cluster communication between the replica set\nmembers deployed in different  Kubernetes  clusters. MongoDB development has\ntested these deployments with  Istio , but any service mesh that provides  FQDN (fully qualified domain name) \nhostname resolution between Pods across clusters should work. To learn more,\nsee  Plan for External Connectivity . You can create a new  multi-Kubernetes-cluster deployment  and contact  MongoDB Support  to help you\nmigrate data from your existing  Kubernetes  deployment to a  multi-Kubernetes-cluster deployment .\nYou can't extend an existing single-Kubernetes cluster deployment to\nnew  Kubernetes  clusters.",
            "code": [],
            "preview": "Deploy MongoDB replica sets that span two or more Kubernetes clusters in a single region across more than one zone, or in more than one region and zone.",
            "tags": "multicluster, multi-Kubernetes cluster, kubernetes operator",
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "reference",
            "title": "Reference",
            "headings": [],
            "paragraphs": "Review the  MongoDBOpsManager   Kubernetes Operator  object specification. Review the  MongoDBUser   Kubernetes Operator  object specification. Review the  MongoDB   Kubernetes Operator  object specifications. Review logging configuration options for  MongoDB  and  MongoDBOpsManager  CRDs. Review the  MongoDBMultiCluster  resource  object specifications. Review the  Kubernetes Operator  installation settings. Review settings that only the  Kubernetes Operator  can set. Review the EOL dates for  Kubernetes Operator  versions. Review the available third-party integrations. Review the list of Open Source licenses that the  Kubernetes Operator  uses.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "multi-cluster-quick-start",
            "title": "Multi-Kubernetes-Cluster Quick Start",
            "headings": [
                "Prefer to Learn by Watching?",
                "Deploy a MongoDBMultiCluster Resource",
                "Create the secret for the TLS certificate of your MongoDBMultiCluster resource.",
                "Create the ConfigMap to link your CA with your MongoDBMultiCluster resource.",
                "Run the kubectl mongodb plugin.",
                "Optional: Set the Istio injection webhook in each member cluster.",
                "Configure kubectl to use the central cluster's namespace.",
                "Deploy the MongoDB Enterprise Kubernetes Operator in the central cluster.",
                "Create a secret to use with Ops Manager and create a ConfigMap.",
                "Deploy Ops Manager on the central cluster and connect to Ops Manager.",
                "Deploy the MongoDBMultiCluster resource.",
                "Verify that the MongoDBMultiCluster resources are running.",
                "Renew TLS Certificates for a MongoDBMultiCluster Resource",
                "Renew the secret for a MongoDBMultiCluster resource.",
                "Run the kubectl mongodb plugin.",
                "Optional: Set the Istio injection webhook in each member cluster.",
                "Configure kubectl to use the central cluster's namespace.",
                "Deploy the MongoDB Enterprise Kubernetes Operator in the central cluster.",
                "Create a secret to use with Ops Manager and create a ConfigMap.",
                "Deploy Ops Manager on the central cluster and connect to Ops Manager.",
                "Deploy the MongoDBMultiCluster resource.",
                "Verify that the MongoDBMultiCluster resources are running.",
                "Next Steps"
            ],
            "paragraphs": "Use the quick start to deploy a MongoDB replica set across three  Kubernetes \nmember clusters, using  GKE (Google Kubernetes Engine)  and  Istio  service mesh. Before you begin: Learn about  multi-Kubernetes-cluster deployments Review the list of  multi-Kubernetes-cluster services and tools Complete the  prerequisites The following procedures scope your  multi-Kubernetes-cluster deployment  to a single\n namespace  named  mongodb . You can configure your  multi-Kubernetes-cluster deployment  to\nwatch resources in  multiple namespaces \nor  all namespaces . Follow along with this video tutorial walk-through that demonstrates how to\ncreate a  multi-Kubernetes-cluster deployment . Duration: 12 Minutes Deploying a MongoDB Replica Set across Multiple Kubernetes Clusters Select the appropriate tab based on whether you want to encrypt replica\nset connections in your  multi-Kubernetes-cluster deployments  using  TLS (Transport Layer Security)  certificates. You can use the following procedures in this\n TLS-Encrypted Connections  tab: These procedures establish  TLS (Transport Layer Security) -encrypted connections between\nMongoDB hosts in a replica set, and between client applications and\nMongoDB deployments. Before you begin, you must have valid certificates for  TLS (Transport Layer Security)  encryption. Deploy a  MongoDBMultiCluster  resource Renew TLS Certificates for a  MongoDBMultiCluster  resource Run the  kubectl  command to create a new secret that stores the\n MongoDBMultiCluster  resource  certificate: You must prefix your secrets with  <prefix>-<metadata.name> . If you call your deployment  my-deployment  and you set the\nprefix to  mdb , you must name the  TLS (Transport Layer Security)  secret for the\nclient  TLS (Transport Layer Security)  communications  mdb-my-deployment-cert . Also,\nyou must name the  TLS (Transport Layer Security)  secret for internal cluster authentication\n(if enabled)  mdb-my-deployment-clusterfile . Run the  kubectl  command to link your  CA (Certificate Authority)  to your  MongoDBMultiCluster  resource .\nSpecify the  CA (Certificate Authority)  certificate file that you must always name\n ca-pem  for the  MongoDBMultiCluster  resource : By default, the  Kubernetes Operator  is scoped to the  mongodb  namespace. When you run the following command, the  kubectl mongodb  plugin : Run the  kubectl mongodb  plugin : Creates one central cluster, three member clusters, and a  namespace  labeled  mongodb  in each of the clusters. Creates a default ConfigMap with the hard-coded name  mongodb-enterprise-operator-member-list  that contains all the member clusters. You can't change the ConfigMap's name. Creates the  service accounts  and  Roles  required for running database workloads in the member clusters. If you're using Istio, run the following command on the central cluster, specifying the context\nfor each of the member clusters in the deployment. These commands add\nthe  istio-injection=enabled  label to the  mongodb  namespace on\neach member cluster. This label configures Istio's injection webhook\nwhich enables adding a sidecar to any Pods that you create in this\nnamespace. To learn more, see  Automatic sidecar injection \nin the Istio documentation. If you have not done so already, run the following commands to run\nall  kubectl  commands on the central cluster in the default\nnamespace. Deploy the  MongoDB Enterprise Kubernetes Operator  in the central cluster in the  mongodb  namespace with Helm or  kubectl . Add the  MongoDB Helm Charts for Kubernetes  repository to Helm. Use the  MongoDB Helm Charts for Kubernetes  to deploy the  Kubernetes Operator . Apply the  Kubernetes Operator  custom resources. Download the  Kubernetes Operator  YAML template. Optional: Customize the  Kubernetes Operator  YAML template. To learn about optional  Kubernetes Operator  installation settings, see  MongoDB Enterprise Kubernetes Operator   kubectl  and  oc  Installation Settings . Apply the  Kubernetes Operator  YAML file. Verify that the  Kubernetes Operator  is deployed. To verify that the  Kubernetes Operator  installed correctly, run the\nfollowing command and verify the output: By default, deployments exist in the  mongodb  namespace. If the\nfollowing error message appears, ensure you use the correct\nnamespace: To troubleshoot your  Kubernetes Operator , see  Review Logs from the  Kubernetes Operator \nand other  troubleshooting topics . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . Create a secret so that the  Kubernetes Operator \ncan create and update objects in your  Ops Manager  project.\nTo learn more, see  Create Credentials for the  Kubernetes Operator . Create a ConfigMap to link the  Kubernetes Operator \nto your  Ops Manager  project. To learn more, see  Create One Project using a ConfigMap . Review the  multi-Kubernetes-cluster deployment  exceptions and follow the linked procedures in  Deploy an  Ops Manager  Resource, deploy the Application Database, and Connect to  Ops Manager . Set  spec.credentials ,\n spec.opsManager.configMapRef.name , and\n security settings \nand deploy the  MongoDBMultiCluster  resource .\nIn the following code sample,  duplicateServiceObjects \nis set to  true  to enable\n DNS proxying \nin Istio. The  Kubernetes Operator  copies the ConfigMap with the  CA (Certificate Authority)  that you\ncreated in previous steps to each member cluster, generates a\nconcatenated  PEM (Privacy-Enhanced Mail)  secret, and distributes it to the member clusters. To enable the cross-cluster DNS resolution by the Istio\nservice mesh, this tutorial creates service objects with a\nsingle ClusterIP address per each  Kubernetes  Pod. For member clusters, run the following commands to verify that\nthe MongoDB Pods are in the running state: In the central cluster, run the following command to verify that\nthe  MongoDBMultiCluster  resource  is in the running state: Renew your  TLS (Transport Layer Security)  certificates periodically using the following procedure. Run this  kubectl  command to renew an existing  secret  that stores the certificates for the  MongoDBMultiCluster  resource : This procedure doesn't encrypt connections between MongoDB hosts in\na replica set, and between client applications and MongoDB deployments. By default, the  Kubernetes Operator  is scoped to the  mongodb  namespace. When you run the following command, the  kubectl mongodb  plugin : Run the  kubectl mongodb  plugin : Creates one central cluster, three member clusters, and a  namespace  labeled  mongodb  in each of the clusters. Creates a default ConfigMap with the hard-coded name  mongodb-enterprise-operator-member-list  that contains all the member clusters. You can't change the ConfigMap's name. Creates the  service accounts  and  Roles  required for running database workloads in the member clusters. If you're using Istio, run the following command on the central cluster, specifying the context\nfor each of the member clusters in the deployment. These commands add\nthe  istio-injection=enabled  label to the  mongodb  namespace on\neach member cluster. This label configures Istio's injection webhook\nwhich enables adding a sidecar to any Pods that you create in this\nnamespace. To learn more, see  Automatic sidecar injection \nin the Istio documentation. If you have not done so already, run the following commands to run\nall  kubectl  commands on the central cluster in the default\nnamespace. Deploy the  MongoDB Enterprise Kubernetes Operator  in the central cluster in the  mongodb  namespace with Helm or  kubectl . Add the  MongoDB Helm Charts for Kubernetes  repository to Helm. Use the  MongoDB Helm Charts for Kubernetes  to deploy the  Kubernetes Operator . Apply the  Kubernetes Operator  custom resources. Download the  Kubernetes Operator  YAML template. Optional: Customize the  Kubernetes Operator  YAML template. To learn about optional  Kubernetes Operator  installation settings, see  MongoDB Enterprise Kubernetes Operator   kubectl  and  oc  Installation Settings . Apply the  Kubernetes Operator  YAML file. Verify that the  Kubernetes Operator  is deployed. To verify that the  Kubernetes Operator  installed correctly, run the\nfollowing command and verify the output: By default, deployments exist in the  mongodb  namespace. If the\nfollowing error message appears, ensure you use the correct\nnamespace: To troubleshoot your  Kubernetes Operator , see  Review Logs from the  Kubernetes Operator \nand other  troubleshooting topics . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . Create a secret so that the  Kubernetes Operator \ncan create and update objects in your  Ops Manager  project.\nTo learn more, see  Create Credentials for the  Kubernetes Operator . Create a ConfigMap to link the  Kubernetes Operator \nto your  Ops Manager  project. To learn more, see  Create One Project using a ConfigMap . Review the  multi-Kubernetes-cluster deployment  exceptions and follow the linked procedures in  Deploy an  Ops Manager  Resource, deploy the Application Database, and Connect to  Ops Manager . Set  spec.credentials ,\n spec.opsManager.configMapRef.name , and\n security settings \nand deploy the  MongoDBMultiCluster  resource .\nIn the following code sample,  duplicateServiceObjects \nis set to  true  to enable\n DNS proxying \nin Istio. The  Kubernetes Operator  copies the ConfigMap with the  CA (Certificate Authority)  that you\ncreated in previous steps to each member cluster, generates a\nconcatenated  PEM (Privacy-Enhanced Mail)  secret, and distributes it to the member clusters. To enable the cross-cluster DNS resolution by the Istio\nservice mesh, this tutorial creates service objects with a\nsingle ClusterIP address per each  Kubernetes  Pod. For member clusters, run the following commands to verify that\nthe MongoDB Pods are in the running state: In the central cluster, run the following command to verify that\nthe  MongoDBMultiCluster  resource  is in the running state: After deploying your MongoDB replica set across three  Kubernetes \nmember clusters, you can add a database user so you can connect to your MongoDB database.\nSee  Manage Database Users .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\n  create secret tls <prefix>-<metadata.name>-cert \\\n  --cert=<resource-tls-cert> \\\n  --key=<resource-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\n  create configmap custom-ca -from-file=ca-pem=<your-custom-ca-file>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl mongodb multicluster setup \\\n  --central-cluster=\"${MDB_CENTRAL_CLUSTER_FULL_NAME}\" \\\n  --member-clusters=\"${MDB_CLUSTER_1_FULL_NAME},${MDB_CLUSTER_2_FULL_NAME},${MDB_CLUSTER_3_FULL_NAME}\" \\\n  --member-cluster-namespace=\"mongodb\" \\\n  --central-cluster-namespace=\"mongodb\" \\\n  --create-service-account-secrets \\\n  --install-database-roles=true"
                },
                {
                    "lang": "sh",
                    "value": "kubectl label \\\n  --context=$MDB_CLUSTER_1_FULL_NAME \\\n  namespace mongodb \\\n  istio-injection=enabled"
                },
                {
                    "lang": "sh",
                    "value": "kubectl label \\\n  --context=$MDB_CLUSTER_2_FULL_NAME \\\n  namespace mongodb \\\n  istio-injection=enabled"
                },
                {
                    "lang": "sh",
                    "value": "kubectl label \\\n  --context=$MDB_CLUSTER_3_FULL_NAME \\\n  namespace mongodb \\\n  istio-injection=enabled"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config use-context $MDB_CENTRAL_CLUSTER_FULL_NAME\nkubectl config set-context $(kubectl config current-context) \\\n  --namespace=mongodb"
                },
                {
                    "lang": "sh",
                    "value": "helm repo add mongodb https://mongodb.github.io/helm-charts"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade \\\n  --install \\\n    mongodb-enterprise-operator-multi-cluster \\\n    mongodb/enterprise-operator \\\n      --namespace mongodb \\\n      --set namespace=mongodb \\\n      --version <mongodb-kubernetes-operator-version> \\\n      --set operator.name=mongodb-enterprise-operator-multi-cluster \\\n      --set operator.createOperatorServiceAccount=false \\\n      --set \"multiCluster.clusters={$MDB_CLUSTER_1_FULL_NAME,$MDB_CLUSTER_2_FULL_NAME,$MDB_CLUSTER_3_FULL_NAME}\" \\\n      --set multiCluster.performFailover=false"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f https://raw.githubusercontent.com/mongodb/mongodb-enterprise-kubernetes/master/crds.yaml"
                },
                {
                    "lang": "sh",
                    "value": "curl https://raw.githubusercontent.com/mongodb/mongodb-enterprise-kubernetes/master/mongodb-enterprise-multi-cluster.yaml -o operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "Error from server (NotFound): deployments.apps \"mongodb-enterprise-operator\" not found"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe deployments mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "oc describe deployments mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f - <<EOF\napiVersion: mongodb.com/v1\nkind: MongoDBMultiCluster\nmetadata:\n name: multi-replica-set\nspec:\n version: 4.4.0-ent\n type: ReplicaSet\n persistent: false\n duplicateServiceObjects: true\n credentials: my-credentials\n opsManager:\n   configMapRef:\n     name: my-project\n security:\n   certsSecretPrefix: <prefix>\n   tls:\n     ca: custom-ca\n clusterSpecList:\n   - clusterName: ${MDB_CLUSTER_1_FULL_NAME}\n     members: 3\n   - clusterName: ${MDB_CLUSTER_2_FULL_NAME}\n     members: 2\n   - clusterName: ${MDB_CLUSTER_3_FULL_NAME}\n     members: 3\nEOF"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_1_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_2_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_3_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context=$MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace mongodb \\\n  get mdbmc multi-replica-set -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n--namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-cert \\\n--cert=<resource-tls-cert> \\\n--key=<resource-tls-key> \\\n--dry-run=client \\\n-o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl mongodb multicluster setup \\\n  --central-cluster=\"${MDB_CENTRAL_CLUSTER_FULL_NAME}\" \\\n  --member-clusters=\"${MDB_CLUSTER_1_FULL_NAME},${MDB_CLUSTER_2_FULL_NAME},${MDB_CLUSTER_3_FULL_NAME}\" \\\n  --member-cluster-namespace=\"mongodb\" \\\n  --central-cluster-namespace=\"mongodb\" \\\n  --create-service-account-secrets \\\n  --install-database-roles=true"
                },
                {
                    "lang": "sh",
                    "value": "kubectl label \\\n  --context=$MDB_CLUSTER_1_FULL_NAME \\\n  namespace mongodb \\\n  istio-injection=enabled"
                },
                {
                    "lang": "sh",
                    "value": "kubectl label \\\n  --context=$MDB_CLUSTER_2_FULL_NAME \\\n  namespace mongodb \\\n  istio-injection=enabled"
                },
                {
                    "lang": "sh",
                    "value": "kubectl label \\\n  --context=$MDB_CLUSTER_3_FULL_NAME \\\n  namespace mongodb \\\n  istio-injection=enabled"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config use-context $MDB_CENTRAL_CLUSTER_FULL_NAME\nkubectl config set-context $(kubectl config current-context) \\\n  --namespace=mongodb"
                },
                {
                    "lang": "sh",
                    "value": "helm repo add mongodb https://mongodb.github.io/helm-charts"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade \\\n  --install \\\n    mongodb-enterprise-operator-multi-cluster \\\n    mongodb/enterprise-operator \\\n      --namespace mongodb \\\n      --set namespace=mongodb \\\n      --version <mongodb-kubernetes-operator-version> \\\n      --set operator.name=mongodb-enterprise-operator-multi-cluster \\\n      --set operator.createOperatorServiceAccount=false \\\n      --set \"multiCluster.clusters={$MDB_CLUSTER_1_FULL_NAME,$MDB_CLUSTER_2_FULL_NAME,$MDB_CLUSTER_3_FULL_NAME}\" \\\n      --set multiCluster.performFailover=false"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f https://raw.githubusercontent.com/mongodb/mongodb-enterprise-kubernetes/master/crds.yaml"
                },
                {
                    "lang": "sh",
                    "value": "curl https://raw.githubusercontent.com/mongodb/mongodb-enterprise-kubernetes/master/mongodb-enterprise-multi-cluster.yaml -o operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "Error from server (NotFound): deployments.apps \"mongodb-enterprise-operator\" not found"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe deployments mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "oc describe deployments mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f - <<EOF\napiVersion: mongodb.com/v1\nkind: MongoDBMultiCluster\nmetadata:\n name: multi-replica-set\nspec:\n version: 4.4.0-ent\n type: ReplicaSet\n persistent: false\n duplicateServiceObjects: true\n credentials: my-credentials\n opsManager:\n   configMapRef:\n     name: my-project\n security:\n   certsSecretPrefix: <prefix>\n   tls:\n     ca: custom-ca\n clusterSpecList:\n   - clusterName: ${MDB_CLUSTER_1_FULL_NAME}\n     members: 3\n   - clusterName: ${MDB_CLUSTER_2_FULL_NAME}\n     members: 2\n   - clusterName: ${MDB_CLUSTER_3_FULL_NAME}\n     members: 3\nEOF"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_1_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_2_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_3_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context=$MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace mongodb \\\n  get mdbmc multi-replica-set -o yaml -w"
                }
            ],
            "preview": "Use the quick start to deploy a MongoDB replica set across three Kubernetes\nmember clusters, using GKE (Google Kubernetes Engine) and Istio service mesh.",
            "tags": "multicluster, multi-cluster",
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "multi-cluster-no-service-mesh-deploy-rs",
            "title": "Deploy Replica Sets in a Multi-Kubernetes Cluster without a Service Mesh",
            "headings": [
                "Before You Begin",
                "Overview",
                "Deploy a MongoDBMultiCluster Resource  without a Service Mesh",
                "Create the secret for the TLS certificate of your MongoDBMultiCluster resource.",
                "Create the ConfigMap to link your CA with your MongoDBMultiCluster resource.",
                "Configure kubectl to use the central cluster's namespace.",
                "Copy and paste the sample  resource.",
                "Define external connectivity settings.",
                "Define an external domain for each Kubernetes member cluster.",
                "Change the settings to your preferred values.",
                "Add any additional accepted settings for a MongoDBMultiCluster resource deployment.",
                "Save this replica set config file with a .yaml extension.",
                "Start your replica set deployment.",
                "Verify external connectivity for each member cluster.",
                "Track the status of your multi-Kubernetes-cluster deployment."
            ],
            "paragraphs": "Use this procedure to deploy a replica set in a  multi-Kubernetes-cluster deployment  without\nusing a service mesh for establishing external connectivity between member\n Kubernetes  clusters. As an alternative to using this procedure, you can use the\n Multi-Kubernetes-Cluster Quick Start , which uses a service mesh. Learn about  multi-Kubernetes-cluster deployments . Review the list of  multi-Kubernetes-cluster services and tools . Complete the  prerequisites , but don't set up a service mesh.\nInstead,  decide whether you need a service mesh .\nIf you choose to deploy without a service mesh,\n use external domains and configure DNS to enable external connectivity . As part of completing the prerequisites, generate valid certificates for\n TLS (Transport Layer Security)  encryption. To learn more, see  Prepare for TLS-Encrypted Connections . Deploy the  Ops Manager  resource on the central cluster, enable external\nconnectivity to the  Ops Manager  instance, and connect to  Ops Manager .\nTo learn more, see  Deploy an  Ops Manager  Resource, deploy the Application Database, and Connect to  Ops Manager . Install the  Kubernetes Operator  in a  multi-Kubernetes-cluster deployment . See  Multi-Kubernetes-Cluster Quick Start . In a  multi-Kubernetes-cluster deployment  without a service mesh, use the following\n MongoDBMultiCluster  resource  settings: Use the  spec.clusterSpecList.externalAccess.externalService \nsetting so that the  Kubernetes Operator  creates an external service, and as part of\nits default configuration, configures a load balancer with default settings.\nConfigure the load balancer to serve as the TCP proxy with a  TLS (Transport Layer Security)  passthrough\n(no TLS termination in the load balancer). Customize external services that the  Kubernetes Operator  creates for each Pod\nin the  Kubernetes  cluster. Use the global \"all-clusters\" configuration in the\n spec.externalAccess  settings and\n Kubernetes  cluster-specific overrides in the  spec.clusterSpecList.externalAccess.externalService  settings. Specify cloud provider-specific annotations for the load balancer in\nthe  spec.clusterSpecList.externalAccess.externalService.annotations \nfor each  Kubernetes  cluster. Specify an external domain in  spec.clusterSpecList.externalAccess.externalDomain .\nThis allows the  Kubernetes Operator  to register  mongod  processes by using\nthe domain suffix. This enables external connections to the  mongod  processes\nin a  multi-Kubernetes-cluster deployment . The following procedure establishes  TLS (Transport Layer Security) -encrypted connections between\nMongoDB hosts in a replica set, and between client applications and\nMongoDB deployments. Run the  kubectl  command to create a new secret that stores the\n MongoDBMultiCluster  resource  certificate: You must prefix your secrets with  <prefix>-<metadata.name> . If you call your deployment  my-deployment  and you set the\nprefix to  mdb , you must name the  TLS (Transport Layer Security)  secret for the\nclient  TLS (Transport Layer Security)  communications  mdb-my-deployment-cert . Also,\nyou must name the  TLS (Transport Layer Security)  secret for internal cluster authentication\n(if enabled)  mdb-my-deployment-clusterfile . Run the  kubectl  command to link your  CA (Certificate Authority)  to your  MongoDBMultiCluster  resource .\nSpecify the  CA (Certificate Authority)  certificate file that you must always name\n ca-pem  for the  MongoDBMultiCluster  resource : If you have not done so already, run the following commands to run\nall  kubectl  commands on the central cluster in the default\nnamespace. Copy the sample replica set  YAML (Yet Another Markup Language)  file and paste it into a new\ntext file. Change the file's settings to match your desired replica set configuration. Specify global values that affect all clusters in a  multi-Kubernetes-cluster deployment  using\nthe  spec.externalAccess  settings and\ncluster-specific overrides using the\n spec.clusterSpecList.externalAccess.externalService  settings. When you provide these settings in the  MongoDBMultiCluster  resource  specification,\nthe  Kubernetes Operator  creates external services for each Pod in all\n Kubernetes  clusters. You then use these services to establish external\nconnectivity to all  mongod  processes in your deployment. Define an external domain for each member cluster using the\n spec.clusterSpecList.externalAccess.externalDomain \nsetting. As a result, the  Kubernetes Operator  registers all  mongod  processes in the\n Kubernetes  member cluster under a hostname according to the following convention: For example, a  mongod  process may have the following hostname:\n my-replica-set-0-0.cluster-1.example.com . Key Type Description Example metadata.name string Label for the  MongoDBMultiCluster  resource . See also  metadata.name  and  names \nin the  Kubernetes  documentation. Resource names must be 44 characters or less. multi-replica-set spec.version string Version of MongoDB that this  MongoDBMultiCluster  resource  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n MongoDB Versioning  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. 4.4.0-ent string Name of the  ConfigMap  with the  Ops Manager  connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value must exist on the  same  namespace as the resource\nyou want to create. The  Kubernetes Operator  tracks any changes to the ConfigMap and\nreconciles the state of the  MongoDB  resource . <my-project> string Name of the cluster in the  MongoDBMultiCluster  resource . cluster1.example.com integer The number of members in this cluster. 2 collection Optional. Provides the configuration for the  StatefulSet  override for each of\nthe cluster's StatefulSets in a  multi-Kubernetes-cluster deployment . If specified at an individual\ncluster level under  clusterSpecList , overrides the global configuration for\nthe StatefulSet for the entire  multi-Kubernetes-cluster deployment . See  Multi-Kubernetes-Cluster Resource Specification \nand  StatefulSet v1 apps Kubernetes documentation . See the example. collection Optional.  If specified, provides a per-cluster override for the default\nstorage size of the  volumeClaimtemplates , for the persistent volume that stores the data. See the example. spec.credentials string Name of the secret you\n created  as  Ops Manager   API (Application Programming Interface) \nauthentication credentials for the  Kubernetes Operator  to\ncommunicate with  Ops Manager . The  Ops Manager   Kubernetes   Secret  object\nholding the Credentials must exist on the  same  Namespace as\nthe resource you want to create. The  Kubernetes Operator  tracks any changes to the Secret and\nreconciles the state of the  MongoDB  resource . <mycredentials> spec.type string Type of  MongoDB  resource  to create. The only supported value for this\nfield is  ReplicaSet . See  Limitations . ReplicaSet You can also add any optional settings to the\n object  specification. See  Multi-Kubernetes-Cluster Resource Specification . In any directory, invoke the following  Kubernetes  command to create your\n replica set : Check the status of external services in all member clusters: Kubernetes  should return one external service created for each Pod of the replica set in all member clusters. Verify that each external service is exposed externally and is reachable.\nRun the command similar to the following example: Connecting to  my-replica-set-0-0.cluster-0.example.com:27017  should\ndirect client traffic to an external service named  my-replica-set-0-0-svc-external ,\nwhich, in turn, directs traffic to the  mongod  process. Configure your  DNS (Domain Name System)  zone for the specified external domain to point to\nthe corresponding external services. This configuration depends on\nyour environment or the cloud provider you are using. To check the status of your  MongoDBMultiCluster  resource , use the following command on the central cluster: With the  -w  (watch) flag set, when the configuration changes, the output\nrefreshes immediately until the status phase achieves the  Running  state.\nTo learn more about resource deployment statuses, see  Troubleshoot the  Kubernetes Operator .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\n  create secret tls <prefix>-<metadata.name>-cert \\\n  --cert=<resource-tls-cert> \\\n  --key=<resource-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\n  create configmap custom-ca -from-file=ca-pem=<your-custom-ca-file>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config use-context $MDB_CENTRAL_CLUSTER_FULL_NAME\nkubectl config set-context $(kubectl config current-context) \\\n  --namespace=mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# Provides statefulSet override per cluster\n\napiVersion: mongodb.com/v1\nkind: MongoDBMultiCluster\nmetadata:\n  name: multi-replica-set\nspec:\n  version: 4.4.0-ent\n  type: ReplicaSet\n  credentials: my-credentials\n  opsManager:\n    configMapRef:\n      name: my-project\n  externalAccess:\n    externalService: \n      annotations:\n        # Global cloud-specific annotations added to external services in all clusters\n       spec:\n        # ServiceSpec attributes to override in external services in all clusters\n  clusterSpecList:\n    - clusterName: cluster1.example.com\n      members: 2\n      externalAccess:\n        # Domain suffix that mongod processes will use in cluster1\n        externalDomain: cluster1.example.com\n        externalService:\n          annotations:\n            # Cloud-specific annotations for external services\n          spec:\n            # ServiceSpec attributes to override if necessary\n    - clusterName: cluster2.example.com\n      members: 1\n      externalAccess:\n        # Domain suffix that mongod processes will use in cluster2\n        externalDomain: cluster2.example.com\n        externalService:\n          annotations:\n            # Cloud-specific annotations for external services\n          spec:\n            # ServiceSpec attributes to override if necessary\n    - clusterName: cluster3.example.com\n      members: 1\n      externalAccess:\n        # Domain suffix that mongod processes will use in cluster3\n        externalDomain: cluster3.example.com\n        externalService:\n          annotations:\n            # Cloud-specific annotations for external services\n          spec:\n            # ServiceSpec attributes to override if necessary\n\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "<pod-name>.<externalDomain>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get services"
                },
                {
                    "lang": "sh",
                    "value": "mongosh mongodb://my-replica-set-0-0.cluster-0.example.com:27017 \\\n-tls -tlsCAFile \"issuer-ca.pem\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdbmc <resource-name> -o yaml -w"
                }
            ],
            "preview": "Use this procedure to deploy a replica set in a multi-Kubernetes-cluster deployment without\nusing a service mesh for establishing external connectivity between member\nKubernetes clusters.",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "authentication",
            "title": "Enable Authentication",
            "headings": [
                "Example Deployment CRD",
                "Example User CRD"
            ],
            "paragraphs": "The  Kubernetes Operator  supports  X.509 , LDAP,\nand  SCRAM  user authentication. You must create an additional  CustomResourceDefinition  for your\nMongoDB users and the MongoDB Agent instances.\nThe  Kubernetes Operator  generates and distributes the certificate. See the full X.509 certificates configuration examples in the\n x509 Authentication  directory in\nthe  Authentication \nsamples directory. This directory also contains sample LDAP and SCRAM configurations. For LDAP configuration, see the\n spec.security.authentication.ldap.automationLdapGroupDN \nsetting. MongoDB User Resource Specification spec.security.authentication.ldap.automationLdapGroupDN Manage Database Users Using X.509 Authentication Manage Database Users Using SCRAM Authentication",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-tls-enabled-rs\nspec:\n  type: ReplicaSet\n  members: 3\n  version: \"4.0.4-ent\"\n  project: my-project\n  credentials: my-credentials\n  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: user-with-roles\nspec:\n  username: \"CN=mms-user-1,OU=cloud,O=MongoDB,L=New York,ST=New York,C=US\"\n  db: \"$external\"\n  project: my-project\n  roles:\n    - db: \"admin\"\n      name: \"clusterAdmin\""
                }
            ],
            "preview": "The Kubernetes Operator supports X.509, LDAP,\nand SCRAM user authentication.",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "verify-signatures",
            "title": "Verify MongoDB Signatures",
            "headings": [
                "Prerequisites",
                "Procedure"
            ],
            "paragraphs": "You can require that the MongoDB Agent verifies the signature file after it\ndownloads the MongoDB binary by enabling a setting in the  Ops Manager Resource Specification .\nOnce you enable signature verification, the MongoDB Agent requires signature files\nfor all MongoDB deployments that your  Ops Manager  instance manages.\nYou can enable signature verification for\n local or remote deployments . Your  Ops Manager  server must run over  HTTPS (Hypertext Transfer Protocol Secure)  so the MongoDB Agent downloads the\nsignature files. To learn more, see  Configure  Ops Manager  to Run over HTTPS . In the  Ops Manager Resource Specification , add\n spec.configuration.mms.featureFlag.automation.verifyDownloads  and set to  enabled .\nFor example: Once you enable signature verification, the MongoDB Agent requires signature\nfiles for all MongoDB binaries that it downloads. Ensure the MongoDB Agent can locate the MongoDB binary and its signature (.sig)\nfile from the same directory, the location of which depends on whether your\ndeployment is  local or remote . If your  Ops Manager  instance can access the Internet or a custom  HTTPS (Hypertext Transfer Protocol Secure) \nserver and you download the MongoDB binary from the official sources,\nthe MongoDB Agent automatically downloads the signature file along with\nthe MongoDB binary. If you don't download the MongoDB binary from the official sources,\nconfigure your  HTTPS (Hypertext Transfer Protocol Secure)  server to locate the MongoDB binary and its\nsignature file from the same link. If your  Ops Manager  instance can't access the Internet, the MongoDB binary\nand its signature file are stored in  /mongodb-ops-manager/mongodb-releases/ \nby default. Ensure the signature file is named the same as the MongoDB\nbinary and both are in the same directory. For example: Save and apply the  Ops Manager Resource Specification . After you've applied the  Ops Manager Resource Specification , the MongoDB Agent performs a\n rolling restart  on the cluster nodes, reconciling\nthe changes.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "spec:\n  configuration:\n    mms.featureFlag.automation.verifyDownloads=enabled"
                },
                {
                    "lang": "sh",
                    "value": "/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-rhel80-4.2.8.tgz.sig\n/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-rhel80-4.2.8.tgz"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <my-ops-manager-resource-specification>.yaml"
                }
            ],
            "preview": "You can require that the MongoDB Agent verifies the signature file after it\ndownloads the MongoDB binary by enabling a setting in the Ops Manager Resource Specification.\nOnce you enable signature verification, the MongoDB Agent requires signature files\nfor all MongoDB deployments that your Ops Manager instance manages.\nYou can enable signature verification for\nlocal or remote deployments.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "openshift-tutorials",
            "title": "Openshift Tutorials",
            "headings": [],
            "paragraphs": "This section contains the following: OpenShift Quick Start Deploy in Restricted Networks",
            "code": [],
            "preview": "This section contains the following:",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "mdb-resources",
            "title": "Deploy and Configure MongoDB Database Resources",
            "headings": [],
            "paragraphs": "You can use the  Kubernetes Operator  to deploy and manage MongoDB clusters\nfrom the  Kubernetes   API (Application Programming Interface) , without having to configure them in\n Ops Manager  or  Cloud Manager . Review the MongoDB database custom resources architecture. Configure the  Kubernetes Operator  to deploy MongoDB database resources. Deploy a standalone, replica set, or sharded cluster resource with\nor without  TLS (Transport Layer Security)  encryption. Modify the configuration of a MongoDB database resource. Configure authentication for client applications. Configure authentication for MongoDB database users. Configure continuous backups for a replica set or sharded cluster. Access database resources from inside or outside  Kubernetes . Delete a MongoDB database resource.",
            "code": [],
            "preview": "You can use the Kubernetes Operator to deploy and manage MongoDB clusters\nfrom the Kubernetes API (Application Programming Interface), without having to configure them in\nOps Manager or Cloud Manager.",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "upgrade",
            "title": "Upgrade the Kubernetes Operator from Prior Versions",
            "headings": [],
            "paragraphs": "Upgrade the  Kubernetes Operator  to its latest version. Migrate your  Kubernetes Operator  deployments from Ubuntu-based container\nimages to UBI-based container images. To upgrade the versions of your  Ops Manager  instance and  backing databases \nthat the  Kubernetes Operator  uses to manage your deployment, see  Upgrade Ops Manager and Backing Database Versions .\nTo upgrade the versions of your MongoDB resource, see  Upgrade MongoDB Version and FCV .",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "multi-cluster-services-tools",
            "title": "Services and Tools",
            "headings": [],
            "paragraphs": "To create a  multi-Kubernetes-cluster deployment , you can: Both methods of creating a  multi-Kubernetes-cluster deployment  rely on the following services,\ntools, and their documentation: Use the Quick Start , to deploy a\nMongoDB replica set across three  Kubernetes  member clusters, using  GKE (Google Kubernetes Engine) \nand a service mesh. Deploy a MongoDBMultiCluster Resource \non each of the member clusters. This allows you to set different settings\nfor the replica set resource, such as overrides for statefulSet configuration. Kubernetes  clusters. The procedures use  GKE (Google Kubernetes Engine)  to provision multiple  Kubernetes \nclusters. Each  Kubernetes  member cluster hosts a MongoDB replica set deployment\nand represents a data center that serves your application. MongoDB Enterprise Kubernetes Operator repository  with configuration files that\nthe  Kubernetes Operator  needs to deploy a  Kubernetes  cluster. MongoDB Helm Charts for Kubernetes  with\ncharts for  multi-Kubernetes-cluster deployments . Istio  service mesh. The quick start procedure uses  Istio  to facilitate\n DNS resolution \nfor MongoDB replica sets deployed in different  Kubernetes  clusters.\nYou can use another service mesh solution as long as you ensure that\ncross-cluster service FQDNs are resolvable. In addition, we offer the  install_istio_separate_network example script . This script is based on Istio documentation and provides an example installation\nthat uses the  multi-primary mode on different networks . We don't guarantee the script's maintenance with future Istio releases. If you choose to use the script, review the latest Istio documentation for\n installing a multicluster ,\nand, if necessary, adjust the script to match the documentation and your deployment.\nIf you use another service mesh solution, create your own script for\nconfiguring separate networks to facilitate DNS resolution. MongoDB kubectl plugin  that sets up\n multi-Kubernetes-cluster deployments  and allows you to run automatic and manual\n disaster recovery . To learn more,\nsee the  MongoDB Plugin Reference .",
            "code": [],
            "preview": "Review third-party and other tools, such as the MongoDB kubectl plugin and Helm Charts for Kubernetes that the MongoDB Enterprise Kubernetes Operator uses to deploy MongoDB multi-Kubernetes custom resources on nodes in multiple Kubernetes clusters.",
            "tags": "multicluster, multi-cluster, MongoDB, mongodbmulti, istio, mesh, gke, helm,charts, multi-Kubernetes",
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "multi-cluster-disaster-recovery",
            "title": "Disaster Recovery",
            "headings": [
                "Disaster Recovery Modes",
                "Manually Recover from a Failure Using the MongoDB Plugin",
                "Recover the multi-Kubernetes-cluster deployment using the MongoDB kubectl plugin.",
                "Rebalance the data nodes on the healthy Kubernetes clusters.",
                "Manually Recover from a Failure Using GitOps Workflows"
            ],
            "paragraphs": "The  Kubernetes Operator  can orchestrate the recovery of MongoDB replica set\nmembers to a healthy  Kubernetes  cluster when the  Kubernetes Operator  identifies\nthat the original  Kubernetes  cluster is down. The  Kubernetes Operator  can orchestrate either an automatic or manual remediation\nof the  MongoDBMultiCluster  resources  in a disaster recovery scenario, using one of the following modes: Auto Failover Mode  allows the  Kubernetes Operator  to shift the affected\nMongoDB replica set members from an unhealthy  Kubernetes  cluster to healthy\n Kubernetes  clusters. When the  Kubernetes Operator  performs this auto remediation,\nit evenly distributes replica set members across the healthy  Kubernetes  clusters. To enable this mode, use  --set multiCluster.performFailover=true  in\nthe MongoDB Helm Charts for  Kubernetes . In the  values.yaml  file in the\n MongoDB Helm Charts for Kubernetes  directory,\nthe environment's variable default value is  true . Alternatively, you can set the  multi-Kubernetes-cluster deployment  environment variable\n PERFORM_FAILOVER  to  true , as in the following abbreviated example: Manual(plugin-based) Failover Mode  allows you to use the\n MongoDB kubectl plugin  to\nreconfigure the  Kubernetes Operator  to use new healthy  Kubernetes  clusters.\nIn this mode, you distribute replica set members across the new healthy\nclusters by configuring the  MongoDBMultiCluster  resource  based on your configuration. To enable this mode, use  --set multiCluster.performFailover=true \nin the  MongoDB Helm Charts for Kubernetes ,\nor set the  multi-Kubernetes-cluster deployment  environment variable  PERFORM_FAILOVER \nto  false , as in the following abbreviated example: You can't rely on the  auto or manual failover modes \nwhen a  Kubernetes  cluster hosting one or more  Kubernetes Operator  instances goes\ndown, or the replica set member resides on the same failed  Kubernetes \ncluster as the  Kubernetes  that manages it. In such cases, to restore replica set members from lost  Kubernetes  clusters\nto the remaining healthy  Kubernetes  clusters, you must first restore the\n Kubernetes Operator  instance that manages your  multi-Kubernetes-cluster deployments , or\nredeploy the  Kubernetes Operator  to one of the remaining  Kubernetes  clusters,\nand rerun the  kubectl mongodb  plugin . To learn more, see  Manually Recover from a Failure Using the MongoDB Plugin . When a  Kubernetes  cluster hosting one or more  Kubernetes Operator  instances goes down,\nor the replica set member resides on the same failed  Kubernetes  cluster as the\n Kubernetes  that manages it, you can't rely on the\n auto or manual failover modes \nand must use the following procedure to manually recover from a failed\n Kubernetes  cluster. The following procedure uses the  MongoDB kubectl Plugin \nto: The following tutorial for manual disaster recovery assumes that you: The  Kubernetes Operator  periodically checks for connectivity to the clusters\nin the  multi-Kubernetes-cluster deployment  by pinging the  /healthz  endpoints of the\ncorresponding servers. To learn more about  /healthz , see  Kubernetes API health endpoints . In the case that  CLUSTER_3  in our example becomes unavailable, the\n Kubernetes Operator  detects the failed connections to the cluster and marks the\n MongoDBMultiCluster  resources  with the  failedClusters  annotation for subsequent reconciliations. The resources with data nodes deployed on this cluster fail reconciliation\nuntil you run the manual recovery steps as in the following procedure. To rebalance the MongoDB data nodes so that all the workloads run on\n CLUSTER_1  and  CLUSTER_2 : Configure new healthy  Kubernetes  clusters. Add these  Kubernetes  clusters as new member clusters to the  mongodb-enterprise-operator-member-list \nConfigMap for your  multi-Kubernetes-cluster deployment . Rebalance nodes hosting  MongoDBMultiCluster  resources  on the nodes in the healthy  Kubernetes  clusters. Deployed one central cluster and three member clusters, following the\n Multi-Kubernetes-Cluster Quick Start . In this case, the\n Kubernetes Operator  is installed with the automated failover disabled with\n --set multiCluster.performFailover=false . Deployed a  MongoDBMultiCluster  resource  as follows: This command: Reconfigures the  Kubernetes Operator  to manage workloads on the two healthy\n Kubernetes  clusters. (This list could also include new  Kubernetes  clusters). Marks  CLUSTER_1  as the source of configuration for the member node\nconfiguration for new  Kubernetes  clusters. Replicates Role and Service Account\nconfiguration to match the configuration in  CLUSTER_1 . Reconfigure the  MongoDBMultiCluster  resource  to rebalance the data nodes on the healthy\n Kubernetes  clusters by editing the resources affected by the change: For an example of use of the  MongoDB kubectl plugin \nin a GitOps workflow with  Argo CD , see\n multi-cluster plugin example for GitOps . GitOps recovery requires manual reconfiguration of\n Role Based Access Control \nusing  .yaml  resource files. To learn more, see\n Understand  Kubernetes  Roles and Role Bindings .",
            "code": [
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    ...\n    spec:\n      containers:\n      - name: mongodb-enterprise-operator\n        ...\n        env:\n        ...\n        - name: PERFORM_FAILOVER\n          value: \"true\"\n        ..."
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    ...\n    spec:\n      containers:\n      - name: mongodb-enterprise-operator\n        ...\n        env:\n        ...\n        - name: PERFORM_FAILOVER\n          value: \"false\"\n        ..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -n mongodb -f - <<EOF\napiVersion: mongodb.com/v1\nkind: MongoDBMultiCluster\nmetadata:\n name: multi-replica-set\nspec:\n version: 5.0.5-ent\n type: ReplicaSet\n persistent: false\n duplicateServiceObjects: true\n credentials: my-credentials\n opsManager:\n   configMapRef:\n     name: my-project\n security:\n   tls:\n     ca: custom-ca\n clusterSpecList:\n   - clusterName: ${MDB_CLUSTER_1_FULL_NAME}\n     members: 3\n   - clusterName: ${MDB_CLUSTER_2_FULL_NAME}\n     members: 2\n   - clusterName: ${MDB_CLUSTER_3_FULL_NAME}\n     members: 3\nEOF"
                },
                {
                    "lang": "sh",
                    "value": "kubectl mongodb multicluster recover \\\n  --central-cluster=\"MDB_CENTRAL_CLUSTER_FULL_NAME\" \\\n  --member-clusters=\"${MDB_CLUSTER_1_FULL_NAME},${MDB_CLUSTER_2_FULL_NAME}\" \\\n  --member-cluster-namespace=\"mongodb\" \\\n  --central-cluster-namespace=\"mongodb\" \\\n  --operator-name=mongodb-enterprise-operator-multi-cluster \\\n  --source-cluster=\"${MDB_CLUSTER_1_FULL_NAME}\""
                },
                {
                    "lang": "yaml",
                    "value": "kubectl apply -n mongodb -f - <<EOF\napiVersion: mongodb.com/v1\nkind: MongoDBMultiCluster\nmetadata:\n  name: multi-replica-set\nspec:\n  version: 5.0.5-ent\n  type: ReplicaSet\n  persistent: false\n  duplicateServiceObjects: true\n  credentials: my-credentials\n  opsManager:\n    configMapRef:\n      name: my-project\n security:\n   tls:\n     ca: custom-ca\n clusterSpecList:\n   - clusterName: ${MDB_CLUSTER_1_FULL_NAME}\n     members: 4\n   - clusterName: ${MDB_CLUSTER_2_FULL_NAME}\n     members: 3\n EOF"
                }
            ],
            "preview": "Set up automatic failover mode in multi-Kubernetes MongoDB deployments for some disaster recovery scenarios, and manually recover from the Kubernetes cluster failures using the kubectl mongodb plugin.",
            "tags": "Kubernetes, multicluster, multi-cluster, MongoDB, kubectl mongodb multicluster recover, multi-Kubernetes, cluster, MongoDBMultiCluster, argo-cd",
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "permissions",
            "title": "Verify Permissions",
            "headings": [
                "Default Permissions for Kubernetes Operator Objects"
            ],
            "paragraphs": "Objects in the  Kubernetes Operator  configuration use\ndefault permissions. These are the minimum\npermissions for the  Kubernetes Operator  to deploy and manage  Ops Manager \nand MongoDB resources in a  Kubernetes  cluster. Use the following chart to verify that the\nobjects in your  Kubernetes Operator  configuration have access to the\nrequired  Kubernetes   API verbs : Kubernetes Resources API Verbs Configmaps Require the following permissions: get ,  list ,  watch . The  Kubernetes Operator  reads the organization\nand project data from the specified  configmap . create ,  update . The  Kubernetes Operator  creates and updates  configmap \nobjects for configuring the  Application Database  instances. delete . The  Kubernetes Operator  needs the  delete   configmap  permission\nto support its  older versions .\nThis permission will be deleted when older versions reach their\nEnd of Life Date. Secrets Require the following permissions: get ,  list ,  watch . The  Kubernetes Operator  reads secret objects to\nretrieve sensitive data, such as  TLS  or\n X.509  access information. For example, it\nreads the credentials from a secret object to connect to the  Ops Manager . create ,  update . The  Kubernetes Operator  creates secret\nobjects holding  TLS  or\n X.509  access information. delete . The  Kubernetes Operator  deletes secret objects (containing passwords)\nrelated to the  Application Database . Services Require the following permissions: get ,  list ,  watch . The  Kubernetes Operator  reads and watches\nMongoDB services. For example, to communicate with the Ops Manager service,\nthe  Kubernetes Operator  needs  get ,  list  and  watch \npermissions to use the  Ops Manager  service's URL. create ,  update . To communicate with services, the  Kubernetes Operator \ncreates and updates service objects corresponding to  Ops Manager \nand MongoDB custom resources. StatefulSets Require the following permissions: get ,  list ,  watch . The  Kubernetes Operator  reacts to the changes in the\nStatefulSets it creates for the MongoDB custom resources. It also reads\nthe fields of  the StatefulSets it manages. create ,  update . The  Kubernetes Operator  creates and updates StatefulSets\ncorresponding to the mongoDB custom resources. delete . The  Kubernetes Operator  needs permissions to delete the StatefulSets\nwhen you delete the MongoDB custom resource. Pods Require the following permissions: get ,  list ,  watch . The  Kubernetes Operator  queries the\nApplication Database Pods to get information about its state. Namespaces Require the following permissions: list ,  watch . When you run the  Kubernetes Operator  in the cluster-wide mode,\nit needs  list  and  watch  permissions to all namespaces\nfor the MongoDB custom resources.",
            "code": [],
            "preview": "Objects in the Kubernetes Operator configuration use\ndefault permissions. These are the minimum\npermissions for the Kubernetes Operator to deploy and manage Ops Manager\nand MongoDB resources in a Kubernetes cluster.",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "",
            "title": "MongoDB Enterprise Kubernetes Operator",
            "headings": [],
            "paragraphs": "The  MongoDB Enterprise Kubernetes Operator  translates the human knowledge of creating a MongoDB\ninstance into a scalable, repeatable, and standardized method.\n Kubernetes  needs help creating and managing stateful applications like\ndatabases. It needs to configure the network, persist storage, and\ndedicate computing capacity without additional human effort on each\ncontainer. The  Kubernetes Operator  manages the typical lifecycle events for a MongoDB\ncluster: provisioning storage and computing power, configuring network\nconnections, setting up users, and changing these settings as needed.\nIt accomplishes this using the Kubernetes API and tools. You provide the  MongoDB Enterprise Kubernetes Operator  with the specifications for your MongoDB\ncluster. The  MongoDB Enterprise Kubernetes Operator  uses this information to specify to  Kubernetes  how to\nconfigure that cluster including provisioning storage, setting up the\nnetwork connections, and configuring other resources. The  MongoDB Enterprise Kubernetes Operator  works together with MongoDB  Cloud Manager or Ops Manager , which further\nconfigures to MongoDB clusters. When MongoDB is deployed and running in\n Kubernetes , you can manage MongoDB tasks using  Cloud Manager or Ops Manager . You can then deploy MongoDB databases as you deploy them now after the\ncluster is created. You can use the  Cloud Manager or Ops Manager  console to run MongoDB at\noptimal performance.",
            "code": [],
            "preview": "The MongoDB Enterprise Kubernetes Operator translates the human knowledge of creating a MongoDB\ninstance into a scalable, repeatable, and standardized method.\nKubernetes needs help creating and managing stateful applications like\ndatabases. It needs to configure the network, persist storage, and\ndedicate computing capacity without additional human effort on each\ncontainer.",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "encryption",
            "title": "Configure Encryption",
            "headings": [
                "Enable HTTPS",
                "Enable TLS"
            ],
            "paragraphs": "The  Kubernetes Operator  supports configuring  Ops Manager  to run over\n HTTPS . Enable  HTTPS (Hypertext Transfer Protocol Secure)  before deploying your  Ops Manager  resources to avoid a situation\nwhere the  Kubernetes Operator  reports your resources' status as  Failed . HTTPS Enabled After Deployment The  Kubernetes Operator  supports  TLS (Transport Layer Security)  encryption.\nUse  TLS (Transport Layer Security)  with your MongoDB deployment to encrypt your data over\nthe network. The configuration in the following example enables  TLS (Transport Layer Security)  for the replica\nset. When  TLS (Transport Layer Security)  is enabled, all traffic between members of the replica\nset and clients is encrypted using  TLS (Transport Layer Security)  certificates. To learn more about securing your MongoDB deployments using  TLS (Transport Layer Security) , see\n Deploy a Replica Set . The default  TLS (Transport Layer Security)  mode is  requireTLS . You can customize it using the\n spec.additionalMongodConfig.net.ssl.mode  configuration\nparameter, as shown in the following abbreviated example. See the full  TLS (Transport Layer Security)  configuration example in\n replica-set.yaml \nin the  TLS \nsamples directory. This directory also contains sample  TLS (Transport Layer Security)  configurations for\nsharded clusters and standalone deployments.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\nname: my-tls-enabled-rs\nspec:\n  type: ReplicaSet\n  members: 3\n  version: 4.4.0-ent\n\n opsManager:\n   configMapRef:\n     name: my-project\n credentials: my-credentials\n\n security:\n   tls:\n     enabled: true\n     ca: <custom-ca>\n\n ...\n additionalMongodConfig:\n   net:\n     ssl:\n      mode: \"preferSSL\""
                }
            ],
            "preview": "The Kubernetes Operator supports configuring Ops Manager to run over\nHTTPS.",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "connect",
            "title": "Access Database Resources",
            "headings": [],
            "paragraphs": "The following pages describe how to connect to a MongoDB\ndatabase resource that is deployed to  Kubernetes : Connect to a MongoDB database resource from inside\nof the  Kubernetes  cluster. Connect to a MongoDB database resource from outside\nof the  Kubernetes  cluster.",
            "code": [],
            "preview": "The following pages describe how to connect to a MongoDB\ndatabase resource that is deployed to Kubernetes:",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "encryption-at-rest",
            "title": "Configure KMIP (Key Management Interoperability) Encryption at Rest",
            "headings": [
                "Considerations",
                "Procedure",
                "Create the ConfigMap of the CA (Certificate Authority).",
                "Create the Secret for the Client Certificate and Private Key PEM.",
                "Configure the deployment to use the KMIP (Key Management Interoperability) server."
            ],
            "paragraphs": "You can configure  encryption at rest \nfor a MongoDB deployment managed by the  Kubernetes Operator \nby using a  KMIP (Key Management Interoperability)  server. Before configuring encryption at rest, consider the following: You must have a running  KMIP (Key Management Interoperability)  server. You can't transition your deployment that uses keyfile-based encryption\nat rest to  KMIP (Key Management Interoperability) -based encryption at rest. If you want to enable  KMIP (Key Management Interoperability)  encryption at rest for an already deployed MongoDB\nresource, contact  MongoDB Support . The following procedure describes how to configure\na sample  KMIP (Key Management Interoperability)  configuration for a MongoDB replica set.\nAdjust the file names and paths,  Kubernetes  namespace, resource names,\nand MongoDB version as necessary for your deployment. Run the following command to create a  ConfigMap \nto hold the  CA (Certificate Authority)  that signed the  KMIP (Key Management Interoperability)  server's certificate: Run the following command to create a  secret  to hold the\nconcatenated client certificate and private key for checking out\nthe master key from the  KMIP (Key Management Interoperability)  server: Configure the  additionalMongodConfig  settings\nin your  custom resource  specification to use the  KMIP (Key Management Interoperability) \nserver. For example: If you set the  spec.backup.encryption.kmip  setting\nin your resource, the  API (Application Programming Interface)  keys linked with\nthe value of  spec.credentials  must have the  Global Owner  role.",
            "code": [
                {
                    "lang": null,
                    "value": "kubectl -n mongodb create configmap mongodb-kmip-certificate-authority-pem --from-file=ca.pem"
                },
                {
                    "lang": null,
                    "value": "kubectl -n mongodb create secret generic mongodb-kmip-client-pem --from-file=cert.pem"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: kmip\n  namespace: mongodb\nspec:\n  type: ReplicaSet\n  members: 3\n  backup:\n    encryption:\n      kmip:\n        client:\n          clientCertificatePrefix: \"mdb\"\n  additionalMongodConfig:\n    security:\n      enableEncryption: true\n      kmip:\n        clientCertificateFile: /kmip/cert/cert.pem\n        serverCAFile: /kmip/ca/ca.pem\n        serverName: pykmip-server.pymongo\n        port: 5696\n  featureCompatibilityVersion: '6.0'\n  version: 6.0.14-ent\n  opsManager:\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  podSpec:\n    podTemplate:\n      spec:\n        containers:\n          - name: mongodb-enterprise-database\n            volumeMounts:\n              - name: mongodb-kmip-client-pem\n                mountPath: /kmip/cert\n              - name: mongodb-kmip-certificate-authority-pem\n                mountPath: /kmip/ca\n        volumes:\n          - name: mongodb-kmip-client-pem\n            secret:\n              secretName: mongodb-kmip-client-pem\n          - name: mongodb-kmip-certificate-authority-pem\n            configMap:\n              name: mongodb-kmip-certificate-authority-pem\n              items:\n                - key: ca.pem\n                  path: ca.pem"
                }
            ],
            "preview": "You can configure encryption at rest\nfor a MongoDB deployment managed by the Kubernetes Operator\nby using a KMIP (Key Management Interoperability) server.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "security",
            "title": "Security",
            "headings": [],
            "paragraphs": "The  MongoDB Enterprise Kubernetes Operator  provides various security features\nto secure your MongoDB deployments. Verify the permissions for your  Kubernetes Operator \nobjects. Verify the signature file before running the MongoDB binary. Control, audit, and debug your deployments by using policies\nfor the Gatekeeper Open Policy Agent (OPA). Configure  HTTPS (Hypertext Transfer Protocol Secure)  and  TLS (Transport Layer Security)  to encrypt your data over\nthe network. Configure encryption at rest by using a  KMIP (Key Management Interoperability)  server. Set up X.509, LDAP, or SCRAM user authentication. Use the secret storage tool to store sensitive information.",
            "code": [],
            "preview": "The MongoDB Enterprise Kubernetes Operator provides various security features\nto secure your MongoDB deployments.",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "installation",
            "title": "Install and Configure the Kubernetes Operator",
            "headings": [],
            "paragraphs": "Review  Kubernetes Operator  deployment architecture, scopes, considerations, and\nprerequisites. Install the  MongoDB Enterprise Kubernetes Operator . Upgrade from earlier versions of  Kubernetes Operator .",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "configure-k8s-operator-for-mdb-resources",
            "title": "Configure the Kubernetes Operator for MongoDB Database Resources",
            "headings": [],
            "paragraphs": "Create a  secret  so the  Kubernetes Operator  can create and update\n objects  in your  Cloud Manager or Ops Manager  Project. Create a  ConfigMap  to link the  Kubernetes Operator  to your  Cloud Manager or Ops Manager \nProject. Create an X.509 certificate to connect to an X.509-enabled\nMongoDB deployment.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "multi-cluster",
            "title": "Deploy MongoDB Resources on Multiple Kubernetes Clusters",
            "headings": [],
            "paragraphs": "You can deploy  MongoDB Enterprise Kubernetes Operator  to manage MongoDB deployments that span two\nor more  Kubernetes  clusters. The  Kubernetes Operator  supports deploying only\nreplica sets across two or more  Kubernetes  clusters. Deploying sharded\nclusters across two or more  Kubernetes  clusters is not supported. Learn about  multi-Kubernetes-cluster deployments . Learn about the  multi-Kubernetes-cluster deployment  architecture, capabilities, and\nlimitations, and view the deployment's diagram. Review the list of services and tools used in this quick start. Set up  GKE (Google Kubernetes Engine)  clusters, install tools, set the deployment's scope,\ninstall the  kubectl mongodb  plugin  and check connectivity across member clusters. Deploy a MongoDB replica set across three  Kubernetes  member clusters, using\n GKE (Google Kubernetes Engine)  and a service mesh. Deploy a  MongoDBMultiCluster  resource  as a replica set with a service mesh. Deploy a  MongoDBMultiCluster  resource  as a replica set without a service mesh. Modify the configuration of a  MongoDBMultiCluster  resource . Secure client connections in  multi-Kubernetes-cluster deployments . Connect to a  MongoDBMultiCluster  resource . Set a disaster recovery mode for your  multi-Kubernetes-cluster deployment . Troubleshoot your  multi-Kubernetes-cluster deployments . Review the  kubectl mongodb  plugin  options.",
            "code": [],
            "preview": "Deploy MongoDB replica sets that span two or more Kubernetes clusters in a single region across more than one zone, or in more than one region and zone.",
            "tags": "multicluster, multi-Kubernetes cluster, kubernetes operator",
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "multi-cluster-arch",
            "title": "Architecture, Capabilities, and Limitations",
            "headings": [
                "Multi-Kubernetes-Cluster Limitations",
                "Multi-Kubernetes-Cluster Deployment Capabilities",
                "Connect with DNS (Domain Name System) SRV (Service) Records",
                "Manage Security for Database Users",
                "Deploy an Ops Manager Resource, deploy the Application Database, and Connect to Ops Manager",
                "Set up Queryable Backups for Ops Manager Resources",
                "Deployment Architecture and Diagrams",
                "Diagram: Multi-Kubernetes Cluster Deployment with a Service Mesh",
                "Diagram: Multi-Kubernetes Cluster Deployment without a Service Mesh"
            ],
            "paragraphs": "The following limitations exist for  multi-Kubernetes-cluster deployments : Deploy only replica sets. Sharded cluster deployments aren't supported. Use  Ops Manager  versions later than 5.0.7. Deploy  Ops Manager  on a single cluster, and if deploying in  Kubernetes , deploy  Ops Manager \non a central cluster. To learn more, see  Using  Ops Manager   with Multi-Kubernetes-Cluster Deployments . The  MongoDB Enterprise Kubernetes Operator  doesn't support highly-available deployments of  Ops Manager \nacross multiple  Kubernetes  clusters. If your deployment requires multi-site\nresilience for  Ops Manager ,  deploy Ops Manager with High Availability \noutside of  Kubernetes  with both the Application Database replica set and\n Ops Manager  spanning all sites on which you have your  multi-Kubernetes-cluster deployment .\nIn this case, in a disaster recovery scenario, you can redeploy the\n multi-Kubernetes-cluster deployment  on another  Kubernetes  cluster on a remaining site and\nconnect the deployment to the  Ops Manager  instance running on that\nhealthy site outside of  Kubernetes . If you host  Ops Manager  in the same  Kubernetes  cluster as the  Kubernetes Operator  and\nthe cluster fails, you can restore the  multi-Kubernetes-cluster deployment  to a new  Kubernetes \ncluster. However, restoring  Ops Manager  into another cluster in this case\nis a lengthy manual process. To learn more, see  Recover the  Kubernetes Operator  and  Ops Manager  for Multi-Cluster AppDB Deployments . In addition to deploying the Application Database outside of  Kubernetes ,\nyou can deploy the Application Database on selected member  Kubernetes  clusters\nin your  multi-Kubernetes-cluster deployment . This mitigates some disaster recovery scenarios,\nsuch as regional failures, and increases the Application Database's\nresilience and availability in  Ops Manager . To learn more, see  Deploy an  Ops Manager  Resource, deploy the Application Database, and Connect to  Ops Manager . Use only  Kubernetes   secrets  for your secret storage tool. The  HashiCorp Vault \nsecret storage tool isn't supported. For deployments where the same  Kubernetes Operator  instance is not managing both the\n MongoDBOpsManager  and\n MongoDB  custom resources,\nyou must manually configure  KMIP (Key Management Interoperability)  backup encryption client settings in  Ops Manager .\nTo learn more, see  Manually Configure KMIP Backup Encryption . Don't add a  ServiceMonitor \nto your  MongoDBMultiCluster  resources . The  Kubernetes Operator  doesn't support integration with Prometheus. You can create a new  multi-Kubernetes-cluster deployment  and contact  MongoDB Support  to help you\nmigrate data from your existing  Kubernetes  deployment to a  multi-Kubernetes-cluster deployment .\nYou can't extend an existing single-Kubernetes cluster deployment to\nnew  Kubernetes  clusters. This section describes the  multi-Kubernetes-cluster deployment  capabilities that you can\nconfigure using the same procedures as the procedures for single clusters\ndeployed with the  Kubernetes Operator . Other  multi-Kubernetes-cluster deployment  capabilities have\ntheir own documentation in this guide. Connect with  DNS (Domain Name System)   SRV (Service)  Records Manage Security for Database Users Deploy an  Ops Manager  Resource, deploy the Application Database, and Connect to  Ops Manager Set up Queryable Backups for  Ops Manager  Resources To connect to the  multi-Kubernetes-cluster deployment  database as a user, you can use\nthe  connectionString.standardSrv :  DNS seed list connection string .\nThis string is included in the secret that the  Kubernetes Operator  creates for your  multi-Kubernetes-cluster deployment .\nUse the same procedure for connecting to the  multi-Kubernetes-cluster deployment  as for single\nclusters deployed with  Kubernetes Operator . See  Connect to a MongoDB Database Resource from Inside Kubernetes \nand select the tab  Using the Kubernetes Secret . Use these methods to manage security for database users: These procedures are the same as the procedures for single clusters deployed\nwith the  Kubernetes Operator , with the following exceptions: LDAP authentication SCRAM authentication TLS and X.509 for internal cluster authentication The procedures apply to replica sets only.  Multi-Kubernetes-cluster deployments   don't support creating sharded clusters . In the  mongodbResourceRef , specify the name of the  multi-Kubernetes-cluster deployment \nreplica set:  name: \"<my-multi-cluster-replica-set>\" . To deploy an  Ops Manager  instance in the central cluster and connect to it,\nuse the following procedures: These procedures are the same as the procedures for single clusters\ndeployed with the  Kubernetes Operator  with the following exceptions: Review the Ops Manager resource architecture Review the Ops Manager resource considerations and prerequisites Deploy an Ops Manager instance on the central cluster with TLS encryption Set the context and the namespace. If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Configure external connectivity for Ops Manager. To connect member clusters to the  Ops Manager  resource's deployment in the\ncentral cluster in a  multi-Kubernetes-cluster deployment , use one of the following methods: Set the  spec.externalConnectivity  to  true  and specify\nthe  Ops Manager  port in it. Use the  ops-manager-external.yaml \nexample script, modify it to your needs, and apply the configuration.\nFor example, run: Add the central cluster and all member clusters to the same service mesh.\nThe service mesh establishes communication from the the central and all\nmember clusters to the  Ops Manager  instance. To learn more, see the\n Multi-Kubernetes-Cluster Quick Start \nprocedures and see the step that references the  istio-injection=enabled \nlabel for Istio. Also, see  Automatic sidecar injection \nin the Istio documentation. Deploy Ops Manager and the Application Database on the central cluster. You can choose to deploy  Ops Manager  and the Application Database only on the central cluster,\nusing the same procedure as for single  Kubernetes  clusters. To learn more,\nsee  Deploy an Ops Manager instance on the central cluster with TLS encryption . Deploy Ops Manager on the central cluster and the Application Database on selected member clusters. You can choose to deploy  Ops Manager  on the central cluster and the Application\nDatabase on a subset of selected member clusters, to increase the\nApplication Database's resilience and availability in  Ops Manager . Configure\nthe following settings in the  Ops Manager  CRD: To learn more, see  Deploy Ops Manager ,\nreview the  multi-Kubernetes-cluster deployment  example and specify  MultiCluster  for\n topology . Use  topology  to specify the  MultiCluster  value. Specify the  clusterSpecList  and\ninclude in it the  clusterName \nof each selected  Kubernetes  member cluster on which you want to deploy the Application Database, and the\nnumber of  members \n(MongoDB nodes) in each  Kubernetes  member cluster. If you deploy the Application Database on selected member clusters in\nyour  multi-Kubernetes-cluster deployment , you must include the central cluster and\nmember clusters in the same service mesh configuration. This enables\nbi-directional communication from  Ops Manager  to the Application Database. If you deploy  Ops Manager  with the  Kubernetes Operator , the central cluster may\nalso host  Ops Manager . In this case, you can  configure queryable backups \nfor  Ops Manager  resources. You can create  multi-Kubernetes-cluster deployments  with or without relying on a service mesh.\nTo learn more, see  Plan for External Connectivity: Should You Use a Service Mesh? In both the following diagrams, the  MongoDB Enterprise Kubernetes Operator  performs these actions: Note that if the central cluster fails, you can't use the  Kubernetes Operator \nto change your deployment until you restore access to this cluster or\nuntil you redeploy the  Kubernetes Operator  to another available  Kubernetes  cluster.\nTo learn more, see  Disaster Recovery . Watches for the  MongoDBMultiCluster  resource  spec creation in the central cluster. Uses the mounted  kubeconfig  file to communicate with member clusters. Creates the necessary resources, such as ConfigMaps, Secrets, Services\nand StatefulSet  Kubernetes  objects in each member cluster corresponding to\nthe number of replica set members in the MongoDB cluster. Identifies the cluster for deploying each MongoDB replica set using\nthe corresponding  MongoDBMultiCluster  resource  spec, and deploys the MongoDB replica sets. Watches for the  CentralCluster  and  MemberCluster  events. Reconciles the resources it created to confirm that the  multi-Kubernetes-cluster deployment \nis in the desired state. A  multi-Kubernetes-cluster deployment  that uses the  MongoDB Enterprise Kubernetes Operator  consists of one\n central cluster  and one or more  member clusters  in  Kubernetes : The  central cluster  has the following role: Hosts the  MongoDB Enterprise Kubernetes Operator Acts as the control plane for the  multi-Kubernetes-cluster deployment Hosts the  MongoDBMultiCluster  resource  spec for the MongoDB replica set Hosts  Ops Manager , if you deploy  Ops Manager  with the  Kubernetes Operator Can also host members of the MongoDB replica set Member clusters  host the MongoDB replica sets. The following diagram shows the high-level architecture of a  multi-Kubernetes-cluster deployment \nacross regions and availability zones. This deployment uses a service mesh,\nsuch as Istio. The service mesh: You can host your application on any of the member clusters inside the\nservice mesh, such as: Manages the discovery of MongoDB nodes deployed in different  Kubernetes  member clusters. Handles communication between replica set members. On  Kubernetes  clusters outside of the ones that you deploy with the  Kubernetes Operator , or On the member clusters in a  multi-Kubernetes-cluster deployment . The following diagram shows the high-level architecture of a  multi-Kubernetes-cluster deployment \nacross regions and availability zones. This deployment doesn't rely on a\nservice mesh for connectivity between the  Kubernetes  clusters hosting Pods\nwith MongoDB instances. To handle external communication between MongoDB replica set members hosted\non Pods in distinct  Kubernetes  clusters,  use external domains and DNS zones . You can host your application on any of the member clusters.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply \\\n --context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" \\\n --namespace \"mongodb\" \\\n  -f https://raw.githubusercontent.com/mongodb/mongodb-enterprise-kubernetes/master/samples/ops-manager/ops-manager-external.yaml"
                }
            ],
            "preview": "The following limitations exist for multi-Kubernetes-cluster deployments:",
            "tags": "multicluster, multi-cluster",
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "plugin-reference",
            "title": "MongoDB Plugin Reference",
            "headings": [
                "setup Subcommand",
                "setup Subcommand Options",
                "recover Subcommand",
                "recover Subcommand Options"
            ],
            "paragraphs": "Before you begin,  install the kubectl mongodb plugin . The  kubectl mongodb  plugin  has the following subcommands: Use the  kubectl mongodb  plugin  to: Set up multi-Kubernetes-cluster Deployments Run automatic and manual disaster recovery setup Subcommand recover Subcommand The  kubectl mongodb multicluster setup  subcommand sets up the initial  multi-Kubernetes-cluster deployment .\nIt performs the following actions: Creates a default ConfigMap named  mongodb-enterprise-operator-member-list \nthat contains all the member clusters of the  multi-Kubernetes-cluster deployment . This name is\nhard-coded and you can't change it. See  Known Issues . Creates  ServiceAccounts ,\n Roles, ClusterRoles ,\n RoleBindings and ClusterRoleBindings \nin the central cluster and each member cluster. Applies the correct permissions for service accounts. Uses the preceding settings to create your  multi-Kubernetes-cluster deployment . The  setup  subcommand of the  kubectl mongodb  plugin  has the following options: For a full example of the  kubectl mongodb  plugin   setup  subcommand's usage,\nsee the  multi-Kubernetes-cluster quick start . Option Data Type Description central-cluster string Required. Central cluster that the  Kubernetes Operator \nwill be deployed in, such as:\n --central-cluster=\"MDB_CENTRAL_CLUSTER_FULL_NAME\" . central-cluster-namespace string Required. Namespace that the  Kubernetes Operator \nwill be deployed to, such as:\n --central-cluster-namespace=\"mongodb\" . cleanup boolean Optional. Flag that indicates whether to\ndelete all previously created resources\nexcept for namespaces.\nDefault value is  false . cluster-scoped boolean Optional. Flag that indicates whether to\ncreate ClusterRole and ClusterRoleBindings\nfor member clusters. Default value is  false . create-service-account-secrets boolean Optional. Flag that indicates whether to create secrets for the\n service accounts  in the  Kubernetes API server .\nDefault value is  false . install-database-roles boolean Optional. Flag that indicates whether to install\nthe ServiceAccounts and Roles required for running\nMongoDB workloads on the member clusters.\nDefault value is  false . member-clusters string Required. Comma-separated list that contains\nmember clusters, such as:\n -member-clusters=\"${MDB_CLUSTER_2_FULL_NAME},\n${MDB_CLUSTER_3_FULL_NAME},\n${MDB_CLUSTER_4_FULL_NAME}\" . member-cluster-namespace string Required. Namespace that the member cluster resources\nwill be deployed to, such as:\n -member-cluster-namespace=\"mongodb\" . service-account string Optional. Name of the service account for the\n Kubernetes Operator  to use to communicate with the\nmember clusters. Default value is\n mongodb-enterprise-operator-multi-cluster . The  kubectl mongodb multicluster  subcommand can automatically recover a\nfailed cluster topology in some cases. In other cases, you must\n manually recover from a failure . To\nlearn more, see  Disaster Recovery . The  recover  subcommand of the  kubectl mongodb  plugin  has the following options: For a full example of the  kubectl mongodb  plugin   recover  subcommand's usage,\nsee the  manual disaster recovery procedure . Option Data Type Description central-cluster string Required. Central cluster that the  Kubernetes Operator \nwill be deployed in, such as:\n --central-cluster=\"MDB_CENTRAL_CLUSTER_FULL_NAME\" . central-cluster-namespace string Required. Namespace that the  Kubernetes Operator \nwill be deployed to, such as:\n --central-cluster-namespace=\"mongodb\" . cleanup boolean Optional. Flag that indicates whether to\ndelete all previously created resources\nexcept for namespaces.\nDefault value is  false . cluster-scoped boolean Optional. Flag that indicates whether to\ncreate ClusterRole and ClusterRoleBindings\nfor member clusters. Default value is  false .",
            "code": [],
            "preview": "Use the kubectl mongodb plugin to set up a multi-Kubernetes-cluster MongoDB deployment and automatically recover it from cluster failures.",
            "tags": "multicluster, multi-kubernetes-cluster MongoDB, mongoDBmultiCluster resource, kubectl mongodb plugin, kubectl mongodb multicluster setup, kubectl mongodb multicluster recover, central cluster, member clusters",
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "om-resources",
            "title": "Deploy and Configure Ops Manager Resources",
            "headings": [],
            "paragraphs": "Review the  Ops Manager  resource architecture. Review the  Ops Manager  resource considerations and prerequisites. Use the  Kubernetes Operator  to deploy an  Ops Manager  instance and encrypt the\nconnection between the application database's replica set members. Use the  Kubernetes Operator  to configure  Ops Manager  to operate in  Remote  mode.\nIn Remote mode, the Backup Daemons and managed MongoDB resources download\ninstallation archives from HTTP endpoints on a web server or S3-compatible\nfile store deployed to your  Kubernetes  cluster instead of from the Internet. Use the  Kubernetes Operator  to configure  Ops Manager  to operate in  Local  mode.\nIn Local mode, the Backup Daemons and managed MongoDB resources download\ninstallation archives from a  Persistent Volume  that you create for the  Ops Manager \nStatefulSet instead of from the Internet. Upgrade the versions of your  Ops Manager  instance and  backing databases \nthat the  Kubernetes Operator  uses to manage your deployment. Configure queryable backups for  Ops Manager  deployments created with the  Kubernetes Operator . Configure backup snapshot storage for  Ops Manager  resources created with the  Kubernetes Operator . Configure  KMIP (Key Management Interoperability)  backup encryption. Configure automated certificate renewal for  Ops Manager  deployments with  cert-manager . Manually recover the  Kubernetes Operator  and  Ops Manager  for an  Ops Manager  resource with\nMulti-Cluster AppDB Deployments in the event that the  Kubernetes  cluster fails.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "opa-gatekeeper",
            "title": "Apply OPA Gatekeeper Policies",
            "headings": [
                "Control Your Deployments with Gatekeeper Policies",
                "List of Sample OPA Gatekeeper Policies"
            ],
            "paragraphs": "To control, audit, and debug your production deployments, you can use policies\nfor the  Gatekeeper \nOpen Policy Agent (OPA). Gatekeeper contains  CustomResourceDefinitions  for creating and extending\ndeployment constraints through the\n constraint templates . The  Kubernetes Operator  offers a  list of Gatekeeper policies \nthat you can customize and apply to your deployments. Each Gatekeeper policy consists of: You can use binary and configurable Gatekeeper policies: To use and apply Gatekeeper sample policies with the  Kubernetes Operator : <policy_name>.yaml  file constraints.yaml  file that is based on the  constraint template Binary policies allow or prevent specific configurations, such as\npreventing deployments that don't use TLS, or deploying only specific\nMongoDB or  Ops Manager  versions. Configurable policies allow you to specify configurations, such as the\ntotal number of replica sets that will be deployed for a specific\nMongoDB or  Ops Manager  custom resource. Install the OPA Gatekeeper  on your Kubernetes cluster. Review the list of available constraint templates and constraints: Navigate to the policy directory, select a policy from the list and\napply it and its constraints file: Review the Gatekeeper policies that are currently applied: The  Kubernetes Operator  offers the following sample policies in this\n OPA examples \nGitHub directory: Location Policy Description Debugging Blocks all MongoDB and  Ops Manager  resources. This allows you to use\nthe log output to craft your own policies. To learn more, see\n Gatekeeper Debugging . mongodb_allow_replicaset Allows deploying only replica sets for MongoDB resources and\nprevents deploying sharded clusters. mongodb_allowed_versions Allows deploying only specific MongoDB versions. ops_manager_allowed_versions Allows deploying only specific  Ops Manager  versions. mongodb_strict_tls Allows using strict TLS mode for MongoDB deployments. ops_manager_replica_members Allows deploying a specified number of  Ops Manager  replica set and\nApplication Database members. ops_manager_wizardless Allows installing  Ops Manager  in a non-interactive mode.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl get constrainttemplates\nkubectl get constraints"
                },
                {
                    "lang": "sh",
                    "value": "cd <policy_directory>\nkubectl apply -f <policy_name>.yaml\nkubectl apply -f constraints.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get constrainttemplates\nkubectl get contstraints"
                }
            ],
            "preview": "To control, audit, and debug your production deployments, you can use policies\nfor the Gatekeeper\nOpen Policy Agent (OPA). Gatekeeper contains CustomResourceDefinitions for creating and extending\ndeployment constraints through the\nconstraint templates.",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "multi-cluster-connect",
            "title": "Connect to MongoDB Multi-Kubernetes Cluster Resources",
            "headings": [],
            "paragraphs": "The following section describes how to connect to a  MongoDBMultiCluster  resource  that\nis deployed to  Kubernetes : Connect to Multi-Cluster Resource from Outside Kubernetes \nConnect to a  MongoDBMultiCluster  resource  from outside of the  Kubernetes  clusters. To connect to a  MongoDBMultiCluster  resource  from within the  Kubernetes  clusters,\nsee  Connect to a MongoDB Database Resource from Inside Kubernetes ,\nand select the tab  Using the Kubernetes Secret . This procedure\nis the same as for single clusters deployed with the  Kubernetes Operator .",
            "code": [],
            "preview": "The following section describes how to connect to a MongoDBMultiCluster resource that\nis deployed to Kubernetes:",
            "tags": "multicluster, multi-cluster",
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "multi-cluster-prerequisites",
            "title": "Prerequisites",
            "headings": [
                "Review Supported Hardware Architectures",
                "Clone the MongoDB Enterprise Kubernetes Operator Repository",
                "Set Environment Variables and GKE Zones",
                "Set up GKE Clusters",
                "Set up your Google Cloud account.",
                "Create a central cluster and member clusters.",
                "Obtain User Authentication Credentials for Central and Member Clusters",
                "Install Go and Helm",
                "Understand Kubernetes Roles and Role Bindings",
                "Set the Deployment's Scope",
                "Watch Resources in Multiple Namespaces",
                "Watch Resources in All Namespaces",
                "Plan for External Connectivity: Should You Use a Service Mesh?",
                "How Does the Kubernetes Operator Establish Connectivity?",
                "Optional: Install Istio",
                "Enable External Connectivity through External Domains and DNS Zones",
                "Check Connectivity Across Clusters",
                "Create a namespace in each cluster.",
                "Deploy the sample-service.yaml in both Kubernetes clusters.",
                "Deploy the sample application on CLUSTER_1.",
                "Ensure CLUSTER_1 is running.",
                "Deploy the sample application on CLUSTER_2.",
                "Ensure CLUSTER_2 is running.",
                "Verify CLUSTER_1 can connect to CLUSTER_2.",
                "Verify CLUSTER_2 can connect to CLUSTER_1.",
                "Review the Requirements for Deploying Ops Manager",
                "Prepare for TLS-Encrypted Connections",
                "Generate a TLS certificate for Kubernetes services.",
                "Generate one TLS certificate for your project's MongoDB Agents.",
                "Install mkcert.",
                "Set the context to the central cluster.",
                "Run the setup_tls script.",
                "Generate one TLS certificate for your project's MongoDB Agents.",
                "Generate a TLS certificate for SAN hostnames.",
                "Generate one TLS certificate for your project's MongoDB Agents.",
                "Choose GitOps or the kubectl MongoDB Plugin",
                "Install the kubectl MongoDB Plugin",
                "Download your desired Kubernetes Operator package version.",
                "Unpack the Kubernetes Operator package.",
                "Locate the kubectl mongodb plugin binary and copy it to its desired destination.",
                "Configure Resources for GitOps",
                "Create and apply RBAC resources to each cluster.",
                "Create and apply the ConfigMap file.",
                "Configure the kubeconfig secret for the Kubernetes Operator."
            ],
            "paragraphs": "Before you create a  multi-Kubernetes-cluster deployment  using either the quick start or a\ndeployment procedure, complete the following tasks: See  supported hardware architectures . Clone the  MongoDB Enterprise Kubernetes Operator repository : Set the environment variables with cluster names and the\n available GKE zones \nwhere you deploy the clusters, as in this example: Set up  GKE (Google Kubernetes Engine)  clusters: If you have not done so already, create a Google Cloud project, enable billing on the project, enable the Artifact Registry and GKE APIs, and launch Cloud Shell by following the relevant procedures in the  Google Kubernetes Engine Quickstart  in the Google Cloud documentation. Create  one central cluster and one or more member clusters , specifying the GKE zones, the number of nodes,\nand the instance types, as in these examples: Obtain user authentication credentials for the central and member  Kubernetes \nclusters and save the credentials. You will later use these credentials\nfor running  kubectl  commands on these clusters. Run the following commands: Install the following tools: Install  Go  v1.17 or later. Install  Helm . To use a  multi-Kubernetes-cluster deployment , you must have a specific set of\n Kubernetes   Roles, ClusterRoles ,\n RoleBindings, ClusterRoleBindings ,\nand  ServiceAccounts , which you can configure in any of the following ways: Follow the  Multi-Kubernetes-Cluster Quick Start , which tells you how to use the\n MongoDB Plugin  to automatically create the required objects\nand apply them to the appropriate clusters within your  multi-Kubernetes-cluster deployment . Use  Helm  to configure the required  Kubernetes  Roles and service accounts for each member cluster: Manually create  Kubernetes  object  .yaml  files and add the required  Kubernetes  Roles and\nservice accounts to your  multi-Kubernetes-cluster deployment  with the  kubectl apply  command. This may be\nnecessary for certain highly automated workflows. MongoDB provides sample configuration files. For custom resources scoped to a subset of namespaces: For custom resources scoped to a cluster-wide namespace: Each file defines multiple resources. To support your deployment, you\nmust replace the placeholder values in the following fields: After modifying the definitions, apply them by running the following\ncommand for each file: Roles, Role Bindings, and Service Accounts for your Central Cluster Roles, Role Bindings, and Service Accounts for your Member Clusters ClusterRoles, ClusterRoleBindings, and ServiceAccounts for your Central Cluster ClusterRoles, ClusterRoleBindings, and ServiceAccounts for your Member Cluster subjects.namespace  in each  RoleBinding  or  ClusterRoleBinding  resource metadata.namespace  in each  ServiceAccount  resource By default, the multi-cluster  Kubernetes Operator  is scoped to the  namespace \nin which you install it. The  Kubernetes Operator  reconciles the  MongoDBMultiCluster  resource \ndeployed in the same namespace as the  Kubernetes Operator . When you run the  MongoDB kubectl plugin \nas part of the  multi-cluster quick start , and don't modify the  kubectl mongodb  plugin \nsettings, the plugin: Once the  Kubernetes Operator  creates the  multi-Kubernetes-cluster deployment , the  Kubernetes Operator \nstarts watching  MongoDB  resources  in the  mongodb   namespace . To configure the  Kubernetes Operator  with the correct permissions to deploy\nin a subset or all namespaces, run the following command and specify the\nnamespaces that you would like the  Kubernetes Operator  to watch. When you install the  multi-Kubernetes-cluster deployment  to multiple or all  namespaces , you\ncan configure the  Kubernetes Operator  to: Creates a default ConfigMap named  mongodb-enterprise-operator-member-list \nthat contains all the member clusters of the  multi-Kubernetes-cluster deployment . This name is\nhard-coded and you can't change it. See  Known Issues . Creates  ServiceAccounts ,\n Roles, ClusterRoles ,\n RoleBindings and ClusterRoleBindings \nin the central cluster and each member cluster. Applies the correct permissions for service accounts. Uses the preceding settings to create your  multi-Kubernetes-cluster deployment . Watch Resources in Multiple Namespaces Watch Resources in All Namespaces Install and set up a single  Kubernetes Operator  instance and configure it\nto watch one, many, or all custom resources in different, non-overlapping\nsubsets of namespaces. See also  Does MongoDB support running more than one  Kubernetes Operator  instance? If you set the scope for the  multi-Kubernetes-cluster deployment  to many  namespaces , you can\nconfigure the  Kubernetes Operator  to watch  MongoDB  resources  in these namespaces\nin the  multi-Kubernetes-cluster deployment . Set the  spec.template.spec.containers.name.env.name:WATCH_NAMESPACE \nin the  mongodb-enterprise.yaml \nfile from the  MongoDB Enterprise Kubernetes Operator  GitHub Repository to the comma-separated list\nof namespaces that you would like the  Kubernetes Operator  to watch: Run the following command and replace the values in the last line\nwith the namespaces that you would like the  Kubernetes Operator  to\nwatch. If you set the scope for the  multi-Kubernetes-cluster deployment  to all  namespaces  instead\nof the default  mongodb  namespace, you can configure the  Kubernetes Operator \nto watch  MongoDB  resources  in all namespaces in the  multi-Kubernetes-cluster deployment . Set the  spec.template.spec.containers.name.env.name:WATCH_NAMESPACE \nin  mongodb-enterprise.yaml \nto  \"*\" . You must include the double quotation marks ( \" )\naround the asterisk ( * ) in the YAML file. Run the following command: A service mesh enables inter-cluster communication between the replica set\nmembers deployed in different  Kubernetes  clusters. Using a service mesh greatly\nsimplifies creating  multi-Kubernetes-cluster deployments  and is the recommended way of deploying\nMongoDB across multiple  Kubernetes  clusters. However, if your IT organization doesn't\nuse a service mesh, you can deploy a replica set in a  multi-Kubernetes-cluster deployment  without it. Depending on your environment, do the following: If you can use a service mesh,  install Istio . If you can't use a service mesh: Review the section  How Does the  Kubernetes Operator  Establish Connectivity? Enable external connectivity through external domains and DNS zones . Deploy a multi-Kubernetes cluster without a service mesh . Regardless of the deployment type, a MongoDB deployment in  Kubernetes  must\nestablish the following connections: When the  Kubernetes Operator  deploys the MongoDB resources, it treats these\nconnectivity requirements in the following ways, depending on the type of deployment: From the  Ops Manager  MongoDB Agent in the Pod to its  mongod  process,\nto enable MongoDB deployment's lifecycle management and monitoring. From the  Ops Manager  MongoDB  Agent in the Pod to the  Ops Manager  instance,\nto enable automation. Between all  mongod  processes, to allow replication. In a single  Kubernetes  cluster deployment, the  Kubernetes Operator  configures hostnames\nin the replica set as  FQDN (fully qualified domain name) s of a  Headless Service . This is a single\nservice that resolves the  DNS (Domain Name System)  of a direct IP address of each Pod hosting\na MongoDB instance by the Pod's  FQDN (fully qualified domain name) , as follows:\n <pod-name>.<replica-set-name>-svc.<namespace>.svc.cluster.local . In a  multi-Kubernetes-cluster deployment  that uses a service mesh, the  Kubernetes Operator  creates\na separate StatefulSet for each MongoDB replica set member in the  Kubernetes \ncluster. A service mesh allows communication between  mongod  processes\nacross distinct  Kubernetes  clusters. Using a service mesh allows the  multi-Kubernetes-cluster deployment  to: Achieve global  DNS (Domain Name System)  hostname resolution across  Kubernetes  clusters and\nestablish connectivity between them. For each MongoDB deployment Pod\nin each  Kubernetes  cluster, the  Kubernetes Operator  creates a ClusterIP service\nthrough the  spec.duplicateServiceObjects: true  configuration in\nthe  MongoDBMultiCluster  resource . Each process has a hostname defined to this\nservice's  FQDN (fully qualified domain name) :  <pod-name>-svc.<namespace>.svc.cluster.local .\nThese hostnames resolve from DNS to a service's ClusterIP in each member cluster. Establish communication between Pods in different  Kubernetes  clusters.\nAs a result, replica set members hosted on different clusters form a\nsingle replica set across these clusters. In a  multi-Kubernetes-cluster deployment  without a service mesh, the  Kubernetes Operator  uses the\nfollowing  MongoDBMultiCluster  resource  settings to expose all its  mongod  processes\nexternally. This enables DNS resolution of hostnames between distinct\n Kubernetes  clusters, and establishes connectivity between Pods routed through\nthe networks that connect these clusters. spec.clusterSpecList.externalAccess.externalService spec.externalAccess spec.clusterSpecList.externalAccess.externalService.annotations spec.clusterSpecList.externalAccess.externalDomain Install  Istio  in a  multi-primary mode on different networks , using the Istio documentation.\nIstio is a service mesh that simplifies  DNS (Domain Name System)  resolution and helps establish\ninter-cluster communication between the member  Kubernetes  clusters in a  multi-Kubernetes-cluster deployment .\nIf you choose to use a service mesh, you need to install it. If you can't\nutilize a service mesh, skip this section and  use external domains\nand configure DNS to enable external connectivity . In addition, we offer the  install_istio_separate_network example script . This script is based on Istio documentation and provides an example installation\nthat uses the  multi-primary mode on different networks . We don't guarantee the script's maintenance with future Istio releases. If you choose to use the script, review the latest Istio documentation for\n installing a multicluster ,\nand, if necessary, adjust the script to match the documentation and your deployment.\nIf you use another service mesh solution, create your own script for\nconfiguring separate networks to facilitate DNS resolution. If you don't use a service mesh, do the following to enable external\nconnectivity to and between  mongod  processes and the\n Ops Manager  MongoDB Agent: After you complete these prerequisites, you can\n deploy a multi-Kubernetes cluster without a service mesh . When you create a  multi-Kubernetes-cluster deployment , use\nthe  spec.clusterSpecList.externalAccess.externalDomain \nsetting to specify an external domain and instruct the  Kubernetes Operator  to\nconfigure hostnames for  mongod  processes in the following pattern: You can specify external domains  only  for new deployments. You\ncan't change external domains after you configure a  multi-Kubernetes-cluster deployment . After you configure an external domain in this way, the  Ops Manager \nMongoDB Agents and  mongod  processes use this domain to connect\nto each other. Customize external services that the  Kubernetes Operator  creates for each Pod\nin the  Kubernetes  cluster. Use the global configuration in the  spec.externalAccess \nsettings and  Kubernetes  cluster-specific overrides in the  spec.clusterSpecList.externalAccess.externalService  settings. Configure Pod hostnames in a  DNS (Domain Name System)  zone to ensure that each  Kubernetes  Pod\nhosting a  mongod  process allows establishing an external connection\nto the other  mongod  processes in a  multi-Kubernetes-cluster deployment . A Pod is considered\n\"exposed externally\" when you can connect to a  mongod  process by\nusing the  <pod-name>.<externalDomain>  hostname on ports 27017\n(this is the default database port) and 27018 (this is the database port + 1).\nYou may also need to configure firewall rules to allow TCP traffic on\nports 27017 and 27018. Follow the steps in this procedure to verify that service  FQDN (fully qualified domain name) s are\nreachable across  Kubernetes  clusters. In this example, you deploy a sample application defined in\n sample-service.yaml \nacross two  Kubernetes  clusters. Create a namespace in each of the  Kubernetes  clusters to deploy the  sample-service.yaml . In certain service mesh solutions, you might need to annotate\nor label the namespace. Check that the  CLUSTER_1  hosting Pod is in the  Running  state. Check that the  CLUSTER_2  hosting Pod is in the  Running  state. Deploy the Pod in  CLUSTER_1  and check that you can reach the sample application in  CLUSTER_2 . You should see output similar to this example: Deploy the Pod in  CLUSTER_2  and check that you can reach the sample application in  CLUSTER_1 . You should see output similar to this example: As part of the Quick Start, you deploy an  Ops Manager  resource on the central\ncluster. To learn more, see  Deploy an  Ops Manager  Resource, deploy the Application Database, and Connect to  Ops Manager . If you plan to secure your  multi-Kubernetes-cluster deployment  using  TLS (Transport Layer Security)  encryption,\ncomplete the following tasks to enable internal cluster authentication and generate  TLS (Transport Layer Security) \ncertificates for member clusters and the MongoDB Agent: You must possess the  CA (Certificate Authority)  certificate and the key that you used to\nsign your  TLS (Transport Layer Security)  certificates. Use one of the following options: Generate a wildcard  TLS (Transport Layer Security)  certificate that covers hostnames\nof the services that the  Kubernetes Operator  creates for each Pod\nin the deployment. If you generate wildcard certificates, you can continue using\nthe same certificates when you scale up or rebalance nodes in\nthe  Kubernetes  member clusters, for example for  disaster recovery . For example, add the hostname similar to the following format\nto the  SAN (Subject Alternative Name) : For each  Kubernetes  service that the  Kubernetes Operator  generates corresponding\nto each Pod in each member cluster, add  SAN (Subject Alternative Name) s to the certificate.\nIn your  TLS (Transport Layer Security)  certificate, the  SAN (Subject Alternative Name)  for each  Kubernetes  service must\nuse the following format: where  n  ranges from  0  to  clusterSpecList[member_cluster_index].members - 1 . For the MongoDB Agent  TLS (Transport Layer Security)  certificate: The Common Name in the  TLS (Transport Layer Security)  certificate must not be empty. The combined Organization and Organizational Unit in each  TLS (Transport Layer Security) \ncertificate must differ from the Organization and Organizational\nUnit in the  TLS (Transport Layer Security)  certificate for your replica set members. To speed up creating  TLS (Transport Layer Security)  certificates for member  Kubernetes  clusters,\nwe offer the  setup_tls script . We don't guarantee the script's maintenance. If you choose to use the script,\ntest it and adjust it to your needs. The script does the following: To use the script: Creates the  cert-manager  namespace in the connected cluster and installs  cert-manager  using  Helm  in the  cert-manager  namespace. Installs a local  CA (Certificate Authority)  using  mkcert . Downloads  TLS (Transport Layer Security)  certificates from  downloads.mongodb.com  and concatenates them with the  CA (Certificate Authority)  file name and  ca-chain . Creates a ConfigMap that includes the  ca-chain  files. Creates an  Issuer  resource, which cert-manager uses to generate certificates. Creates a  Certificate  resource, which cert-manager uses to create a key object for the certificates. Install  mkcert  on the machine you plan to run this script. The output includes: A secret containing the  CA (Certificate Authority)  named  ca-key-pair . A secret containing the server certificates on the central n   clustercert-${resource}-cert . A ConfigMap containing the  CA (Certificate Authority)  certificates named  issuer-ca . For the MongoDB Agent  TLS (Transport Layer Security)  certificate: The Common Name in the  TLS (Transport Layer Security)  certificate must not be empty. The combined Organization and Organizational Unit in each  TLS (Transport Layer Security) \ncertificate must differ from the Organization and Organizational\nUnit in the  TLS (Transport Layer Security)  certificate for your replica set members. Use one of the following options: Generate a wildcard  TLS (Transport Layer Security)  certificate that contains all\n externalDomains \nthat you created in the  SAN (Subject Alternative Name) . For example, add the hostnames\nsimilar to the following format to the  SAN (Subject Alternative Name) : If you generate wildcard certificates, you can continue using\nthem when you scale up or rebalance nodes in the  Kubernetes  member\nclusters, for example for  disaster recovery . Generate a  TLS (Transport Layer Security)  certificate for each MongoDB replica set member\nhostname in the  SAN (Subject Alternative Name) . For example, add the hostnames similar\nto the following to the  SAN (Subject Alternative Name) : If you generate an individual  TLS (Transport Layer Security)  certificate that contains\nall the specific hostnames, you must create a new certificate\neach time you scale up or rebalance nodes in the  Kubernetes  member\nclusters, for example for  disaster recovery . For the MongoDB Agent  TLS (Transport Layer Security)  certificate: The Common Name in the  TLS (Transport Layer Security)  certificate must not be empty. The combined Organization and Organizational Unit in each  TLS (Transport Layer Security) \ncertificate must differ from the Organization and Organizational\nUnit in the  TLS (Transport Layer Security)  certificate for your replica set members. The  Kubernetes Operator  uses  kubernetes.io/tls  secrets\nto store  TLS (Transport Layer Security)  certificates and private keys for  Ops Manager  and MongoDB\nresources. Starting in  Kubernetes Operator  version 1.17.0, the\n Kubernetes Operator  doesn't support concatenated  PEM (Privacy-Enhanced Mail)  files stored as\n Opaque secrets . You can choose to create and maintain the resource files needed for the  MongoDBMultiCluster  resources  deployment in a GitOps environment. If you use a GitOps workflow, you can't use the  kubectl mongodb plugin , which automatically configures  role-based access control (RBAC)  and creates the  kubeconfig  file that allows the central cluster to communicate with its member clusters. Instead, you must manually configure or build your own automation for configuring the RBAC and  kubeconfig  files based on the procedure and examples in  Configure Resources for GitOps . The following prerequisite sections describe how to  install the kubectl MongoDB plugin  if you don't use GitOps or  configure resources for GitOps  if you do. To install the  kubectl mongodb  plugin : Use the  kubectl mongodb  plugin  to: Set up multi-Kubernetes-cluster Deployments Run automatic and manual disaster recovery If you use GitOps, you can't use the  kubectl mongodb  plugin . Instead, follow the procedure in  Configure Resources for GitOps . Download your desired  Kubernetes Operator  package version from the\n Release Page of the MongoDB Enterprise Kubernetes Operator Repository . The package's name uses this pattern:\n kubectl-mongodb-multicluster_{{ .Version }}_{{ .Os }}_{{ .Arch }}.tar.gz . Use one of the following packages: kubectl-mongodb-multicluster_{{ .Version }}_darwin_amd64.tar.gz kubectl-mongodb-multicluster_{{ .Version }}_darwin_arm64.tar.gz kubectl-mongodb-multicluster_{{ .Version }}_linux_amd64.tar.gz kubectl-mongodb-multicluster_{{ .Version }}_linux_arm64.tar.gz Unpack the package, as in the following example: Find the  kubectl-mongodb  binary in the unpacked directory and move it\nto its desired destination, inside the PATH for the  Kubernetes Operator  user,\nas shown in the following example: Now you can run the  kubectl mongodb  plugin  using the following commands: To learn more about the supported flags,\nsee the  MongoDB kubectl plugin Reference . If you use a GitOps workflow, you won't be able to use the  kubectl mongodb plugin  to automatically configure  role-based access control (RBAC)  or the  kubeconfig  file that allows the central cluster to communicate with its member clusters. Instead, you must manually configure and apply the following resource files or build your own automation based on the information below. To configure RBAC and the  kubeconfig  for GitOps: To learn how the  kubectl mongodb  plugin  automates the following steps,\n view the code  in GitHub. Use  these RBAC resource examples  to create your own. To learn more about these\nRBAC resources, see  Understand  Kubernetes  Roles and Role Bindings . To apply them to your central and member clusters with GitOps, you can use a tool like  Argo CD . The  Kubernetes Operator  keeps track of its member clusters using a  ConfigMap  file. Copy, modify, and apply the following example ConfigMap: The  Kubernetes Operator , which runs in the central cluster, communicates with the Pods in\nthe member clusters through the Kubernetes API. For this to work, the  Kubernetes Operator \nneeds a  kubeconfig \nfile that contains the service account tokens of the member clusters. Create this\n kubeconfig  file by following these steps: Obtain a list of  service accounts  configured in the  Kubernetes Operator 's namespace.  For example, if you chose to use the default  mongodb  namespace, then you can obtain the service accounts using the following command: Get the secret for each service account that belongs to a member cluster. In each service account secret, copy the  CA (Certificate Authority)  certificate and token. For example, copy  <ca_certificate>  and  <token>  from the secret, as shown in the following example: Copy the following  kubeconfig  example for the central cluster and replace\nthe placeholders with the  <ca_certificate>  and  <token>  you copied from the service account secrets. Save the  kubeconfig  file. Create a secret in the central cluster that you mount in the  Kubernetes Operator  as illustrated in  the reference Helm chart . For example:",
            "code": [
                {
                    "lang": "sh",
                    "value": "git clone https://github.com/mongodb/mongodb-enterprise-kubernetes.git"
                },
                {
                    "lang": "sh",
                    "value": "export MDB_GKE_PROJECT={GKE project name}\n\nexport MDB_CENTRAL_CLUSTER=\"mdb-central\"\nexport MDB_CENTRAL_CLUSTER_ZONE=\"us-west1-a\"\n\nexport MDB_CLUSTER_1=\"mdb-1\"\nexport MDB_CLUSTER_1_ZONE=\"us-west1-b\"\n\nexport MDB_CLUSTER_2=\"mdb-2\"\nexport MDB_CLUSTER_2_ZONE=\"us-east1-b\"\n\nexport MDB_CLUSTER_3=\"mdb-3\"\nexport MDB_CLUSTER_3_ZONE=\"us-central1-a\"\n\nexport MDB_CENTRAL_CLUSTER_FULL_NAME=\"gke_${MDB_GKE_PROJECT}_${MDB_CENTRAL_CLUSTER_ZONE}_${MDB_CENTRAL_CLUSTER}\"\n\nexport MDB_CLUSTER_1_FULL_NAME=\"gke_${MDB_GKE_PROJECT}_${MDB_CLUSTER_1_ZONE}_${MDB_CLUSTER_1}\"\nexport MDB_CLUSTER_2_FULL_NAME=\"gke_${MDB_GKE_PROJECT}_${MDB_CLUSTER_2_ZONE}_${MDB_CLUSTER_2}\"\nexport MDB_CLUSTER_3_FULL_NAME=\"gke_${MDB_GKE_PROJECT}_${MDB_CLUSTER_3_ZONE}_${MDB_CLUSTER_3}\""
                },
                {
                    "lang": "sh",
                    "value": "gcloud container clusters create $MDB_CENTRAL_CLUSTER \\\n  --zone=$MDB_CENTRAL_CLUSTER_ZONE \\\n  --num-nodes=5 \\\n  --machine-type \"e2-standard-2\""
                },
                {
                    "lang": "sh",
                    "value": "gcloud container clusters create $MDB_CLUSTER_1 \\\n  --zone=$MDB_CLUSTER_1_ZONE \\\n  --num-nodes=5 \\\n  --machine-type \"e2-standard-2\""
                },
                {
                    "lang": "sh",
                    "value": "gcloud container clusters create $MDB_CLUSTER_2 \\\n  --zone=$MDB_CLUSTER_2_ZONE \\\n  --num-nodes=5 \\\n  --machine-type \"e2-standard-2\""
                },
                {
                    "lang": "sh",
                    "value": "gcloud container clusters create $MDB_CLUSTER_3 \\\n  --zone=$MDB_CLUSTER_3_ZONE \\\n  --num-nodes=5 \\\n  --machine-type \"e2-standard-2\""
                },
                {
                    "lang": "sh",
                    "value": "gcloud container clusters get-credentials $MDB_CENTRAL_CLUSTER \\\n  --zone=$MDB_CENTRAL_CLUSTER_ZONE\n\ngcloud container clusters get-credentials $MDB_CLUSTER_1 \\\n  --zone=$MDB_CLUSTER_1_ZONE\n\ngcloud container clusters get-credentials $MDB_CLUSTER_2 \\\n  --zone=$MDB_CLUSTER_2_ZONE\n\ngcloud container clusters get-credentials $MDB_CLUSTER_3 \\\n  --zone=$MDB_CLUSTER_3_ZONE"
                },
                {
                    "lang": "sh",
                    "value": "helm template --show-only \\\n  templates/database-roles.yaml \\\n  mongodb/enterprise-operator \\\n  --set namespace=mongodb | \\\nkubectl apply -f - \\\n  --context=$MDB_CLUSTER_1_FULL_NAME \\\n  --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "helm template --show-only \\\n  templates/database-roles.yaml \\\n  mongodb/enterprise-operator \\\n  --set namespace=mongodb | \\\nkubectl apply -f - \\\n  --context=$MDB_CLUSTER_2_FULL_NAME \\\n  --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "helm template --show-only \\\n  templates/database-roles.yaml \\\n  mongodb/enterprise-operator \\\n  --set namespace=mongodb | \\\nkubectl apply -f - \\\n  --context=$MDB_CLUSTER_3_FULL_NAME \\\n  --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <fileName>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl mongodb multicluster setup \\\n  --central-cluster=\"${MDB_CENTRAL_CLUSTER_FULL_NAME}\" \\\n  --member-clusters=\"${MDB_CLUSTER_1_FULL_NAME},${MDB_CLUSTER_2_FULL_NAME},${MDB_CLUSTER_3_FULL_NAME}\" \\\n  --member-cluster-namespace=\"mongodb2\" \\\n  --central-cluster-namespace=\"mongodb2\" \\\n  --cluster-scoped=\"true\""
                },
                {
                    "lang": "sh",
                    "value": "WATCH_NAMESPACE: \"$namespace1,$namespace2,$namespace3\""
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade \\\n  --install \\\n  mongodb-enterprise-operator-multi-cluster \\\n  mongodb/enterprise-operator \\\n  --namespace mongodb \\\n  --set namespace=mongodb \\\n  --version <mongodb-kubernetes-operator-version>\\\n  --set operator.name=mongodb-enterprise-operator-multi-cluster \\\n  --set operator.createOperatorServiceAccount=false \\\n  --set \"multiCluster.clusters={$MDB_CLUSTER_1_FULL_NAME,$MDB_CLUSTER_2_FULL_NAME,$MDB_CLUSTER_3_FULL_NAME}\" \\\n  --set operator.watchNamespace=\"$namespace1,$namespace2,$namespace3\""
                },
                {
                    "lang": "sh",
                    "value": "WATCH_NAMESPACE: \"*\""
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade \\\n  --install \\\n  mongodb-enterprise-operator-multi-cluster \\\n  mongodb/enterprise-operator \\\n  --namespace mongodb \\\n  --set namespace=mongodb \\\n  --version <mongodb-kubernetes-operator-version>\\\n  --set operator.name=mongodb-enterprise-operator-multi-cluster \\\n  --set operator.createOperatorServiceAccount=false \\\n  --set \"multiCluster.clusters={$MDB_CLUSTER_1_FULL_NAME,$MDB_CLUSTER_2_FULL_NAME,$MDB_CLUSTER_3_FULL_NAME}\" \\\n  --set operator.watchNamespace=\"*\""
                },
                {
                    "lang": "sh",
                    "value": "<pod-name>.<externalDomain>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create --context=\"${CTX_CLUSTER_1}\" namespace sample\nkubectl create --context=\"${CTX_CLUSTER_2}\" namespace sample"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply --context=\"${CTX_CLUSTER_1}\" \\\n   -f sample-service.yaml \\\n   -l service=helloworld1 \\\n   -n sample\n\nkubectl apply --context=\"${CTX_CLUSTER_2}\" \\\n   -f sample-service.yaml \\\n   -l service=helloworld2 \\\n   -n sample"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply --context=\"${CTX_CLUSTER_1}\" \\\n   -f sample-service.yaml \\\n   -l version=v1 \\\n   -n sample"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pod --context=\"${CTX_CLUSTER_1}\" \\\n   -n sample \\\n   -l app=helloworld"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply --context=\"${CTX_CLUSTER_2}\" \\\n   -f sample-service.yaml \\\n   -l version=v2 \\\n   -n sample"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pod --context=\"${CTX_CLUSTER_2}\" \\\n   -n sample \\\n   -l app=helloworld"
                },
                {
                    "lang": "sh",
                    "value": "kubectl run  --context=\"${CTX_CLUSTER_1}\" \\\n   -n sample \\\n   curl --image=radial/busyboxplus:curl \\\n   -i --tty \\\n   curl -sS helloworld2.sample:5000/hello"
                },
                {
                    "lang": "sh",
                    "value": "Hello version: v2, instance: helloworld-v2-758dd55874-6x4t8"
                },
                {
                    "lang": "sh",
                    "value": "kubectl run --context=\"${CTX_CLUSTER_2}\" \\\n   -n sample \\\n   curl --image=radial/busyboxplus:curl \\\n   -i --tty \\\n   curl -sS helloworld1.sample:5000/hello"
                },
                {
                    "lang": "sh",
                    "value": "Hello version: v1, instance: helloworld-v1-758dd55874-6x4t8"
                },
                {
                    "lang": "sh",
                    "value": "*.<namespace>.svc.cluster.local"
                },
                {
                    "lang": "none",
                    "value": "<metadata.name>-<member_cluster_index>-<n>-svc.<namespace>.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n--namespace=<metadata.namespace> \\"
                },
                {
                    "lang": "sh",
                    "value": "curl https://raw.githubusercontent.com/mon  mongodb-enterprise-kubernetes/master/tools/multicluster/setup_tl  -o  setup_tls.sh"
                },
                {
                    "lang": "sh",
                    "value": "*.cluster-0.example.com, *.cluster-1.example.com"
                },
                {
                    "lang": "sh",
                    "value": "my-replica-set-0-0.cluster-0.example.com,\nmy-replica-set-0-1.cluster-0.example.com,\nmy-replica-set-1-0.cluster-1.example.com,\nmy-replica-set-1-1.cluster-1.example.com"
                },
                {
                    "lang": "sh",
                    "value": "tar -zxvf kubectl-mongodb_<version>_darwin_amd64.tar.gz"
                },
                {
                    "lang": "sh",
                    "value": "mv kubectl-mongodb /usr/local/bin/kubectl-mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl mongodb multicluster setup\nkubectl mongodb multicluster recover"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: ConfigMap\ndata:\n  cluster1: \"\"\n  cluster2: \"\"\nmetadata:\n  namespace: <namespace>\n  name: mongodb-enterprise-operator-member-list\n  labels:\n    multi-cluster: \"true\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get serviceaccounts -n mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get secret <service-account-name> -n mongodb -o yaml"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-service-account\n  namespace: mongodb\ndata:\n  ca.crt: <ca_certificate>\n  token: <token>"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: <cluster-1-ca.crt>\n    server: https://:\n  name: kind-e2e-cluster-1\n- cluster:\n    certificate-authority-data: <cluster-2-ca.crt>\n    server: https://:\n  name: kind-e2e-cluster-2\ncontexts:\n- context:\n    cluster: kind-e2e-cluster-1\n    namespace: mongodb\n    user: kind-e2e-cluster-1\n  name: kind-e2e-cluster-1\n- context:\n    cluster: kind-e2e-cluster-2\n    namespace: mongodb\n    user: kind-e2e-cluster-2\n  name: kind-e2e-cluster-2\nkind: Config\nusers:\n- name: kind-e2e-cluster-1\n  user:\n    token: <cluster-1-token>\n- name: kind-e2e-cluster-2\n  user:\n    token: <cluster-2-token>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context=\"${CTX_CENTRAL_CLUSTER}\" -n <operator-namespace> create secret --from-file=kubeconfig=<path-to-kubeconfig-file> <kubeconfig-secret-name>"
                }
            ],
            "preview": "Review and complete the prerequisite tasks before you deploy multi-Kubernetes-cluster MongoDB resources.",
            "tags": "multicluster, multi-Kubernetes-cluster MongoDB, mongoDBmultiCluster resource, kubectl mongodb plugin, central cluster, member clusters",
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "third-party-licenses",
            "title": "Third-Party Licenses",
            "headings": [
                "Apache License 2.0",
                "BSD (Berkeley Software Distribution) 2-Clause",
                "BSD (Berkeley Software Distribution) 3-Clause",
                "ISC (Internet Systems Consortium) License",
                "MIT (Massachusetts Institute of Technology) License",
                "MPL (Mozilla Public License) 2.0"
            ],
            "paragraphs": "MongoDB  Kubernetes Operator  uses third-party libraries or other resources\nthat may be distributed under licenses different than the MongoDB\nsoftware. Kubernetes Operator  depends upon the following third-party packages. These\npackages are licensed as shown in the following list. Should MongoDB\nhave accidentally failed to list a required license, please\n contact the MongoDB Legal Department . License:  TL;DR  |  Full Text Package Version github.com/go-jose/go-jose/v3 v3.0.1 github.com/go-logr/logr v1.2.4 github.com/go-openapi/jsonpointer v0.19.5 github.com/go-openapi/jsonreference v0.20.0 github.com/go-openapi/swag v0.19.14 github.com/golang/groupcache/lru v0.0.0-20210331224755-41bb18bfe9da github.com/google/gnostic v0.5.7-v3refs github.com/google/gofuzz v1.1.0 github.com/matttproud/golang_protobuf_extensions/pbutil v1.0.4 github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd github.com/modern-go/reflect2 v1.0.2 github.com/prometheus/client_golang/prometheus v1.17.0 github.com/prometheus/client_model/go v0.4.1-0.20230718164431-9a2bf3000d16 github.com/prometheus/common v0.44.0 github.com/prometheus/procfs v0.11.1 github.com/xdg/stringprep v1.0.3 gomodules.xyz/jsonpatch/v2 v2.2.0 gopkg.in/yaml.v2 v2.4.0 k8s.io/api v0.26.10 k8s.io/apiextensions-apiserver/pkg/apis/apiextensions v0.26.10 k8s.io/apimachinery/pkg v0.26.10 k8s.io/client-go v0.26.10 k8s.io/component-base/config v0.26.10 k8s.io/klog/v2 v2.80.1 k8s.io/kube-openapi/pkg v0.0.0-20221012153701-172d655c2280 k8s.io/kube-openapi/pkg/validation/spec v0.0.0-20221012153701-172d655c2280 k8s.io/utils v0.0.0-20221128185143-99ec85e7a448 sigs.k8s.io/controller-runtime v0.14.7 sigs.k8s.io/json v0.0.0-20220713155537-f223a00ba0e2 sigs.k8s.io/structured-merge-diff/v4 v4.2.3 License:  TL;DR  |  Full Text Package Version github.com/pkg/errors v0.9.1 github.com/vmihailenco/msgpack/v5 v5.3.5 github.com/vmihailenco/tagparser/v2 v2.0.0 License:  TL;DR  |  Full Text Package Version github.com/evanphx/json-patch v5.6.0 github.com/evanphx/json-patch/v5 v5.6.0 github.com/fsnotify/fsnotify v1.6.0 github.com/go-jose/go-jose/v3/json v3.0.1 github.com/gogo/protobuf v1.3.2 github.com/golang/protobuf v1.5.3 github.com/google/go-cmp/cmp v0.5.9 github.com/google/uuid v1.3.1 github.com/imdario/mergo v0.3.15 github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 github.com/pmezard/go-difflib/difflib v1.0.0 github.com/prometheus/common/internal/bitbucket.org/ww/goautoneg v0.44.0 github.com/spf13/pflag v1.0.5 google.golang.org/protobuf v1.31.0 gopkg.in/inf.v0 v0.9.1 k8s.io/apimachinery/third_party/forked/golang v0.26.10 k8s.io/kube-openapi/pkg/internal/third_party/go-json-experiment/json v0.0.0-20221012153701-172d655c2280 k8s.io/utils/internal/third_party/forked/golang/net v0.0.0-20221128185143-99ec85e7a448 License:  TL;DR  |  Full Text Package Version github.com/davecgh/go-spew/spew v1.1.1 License:  TL;DR  |  Full Text Package Version github.com/beorn7/perks/quantile v1.0.1 github.com/blang/semver v3.5.1 github.com/cenkalti/backoff/v3 v3.0.0 github.com/cespare/xxhash/v2 v2.2.0 github.com/emicklei/go-restful/v3 v3.9.0 github.com/ghodss/yaml v1.0.0 github.com/josharian/intern v1.0.0 github.com/json-iterator/go v1.1.12 github.com/mailru/easyjson v0.7.6 github.com/mitchellh/mapstructure v1.5.0 github.com/ryanuber/go-glob v1.0.0 github.com/spf13/cast v1.5.1 github.com/stretchr/objx v0.5.1 github.com/stretchr/testify/assert v1.8.4 go.uber.org/multierr v1.10.0 go.uber.org/zap v1.26.0 gopkg.in/yaml.v3 v3.0.1 sigs.k8s.io/yaml v1.3.0 License:  TL;DR  |  Full Text Package Version github.com/hashicorp/errwrap v1.1.0 github.com/hashicorp/go-cleanhttp v0.5.2 github.com/hashicorp/go-multierror v1.1.1 github.com/hashicorp/go-retryablehttp v0.7.4 github.com/hashicorp/go-rootcerts v1.0.2 github.com/hashicorp/go-secure-stdlib/parseutil v0.1.6 github.com/hashicorp/go-secure-stdlib/strutil v0.1.2 github.com/hashicorp/go-sockaddr v1.0.2 github.com/hashicorp/hcl v1.0.0 github.com/hashicorp/vault/api v1.10.0 github.com/r3labs/diff/v3 v3.0.1",
            "code": [],
            "preview": "MongoDB Kubernetes Operator uses third-party libraries or other resources\nthat may be distributed under licenses different than the MongoDB\nsoftware.",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "release-notes",
            "title": "Release Notes for MongoDB Enterprise Kubernetes Operator",
            "headings": [
                "MongoDB Enterprise Kubernetes Operator 1.24 Series",
                "MongoDB Enterprise Kubernetes Operator 1.24.0",
                "MongoDBOpsManager Resource",
                "New Features",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.23 Series",
                "MongoDB Enterprise Kubernetes Operator 1.23.0",
                "Warnings and Breaking Changes",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.22 Series",
                "MongoDB Enterprise Kubernetes Operator 1.22.0",
                "Breaking Changes",
                "MongoDBOpsManager Resource",
                "Improvements",
                "MongoDB Resource",
                "Improvements",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.21 Series",
                "MongoDB Enterprise Kubernetes Operator 1.21.0",
                "Breaking Changes",
                "Bug Fixes",
                "Improvements",
                "MongoDBOpsManager Resource",
                "Breaking Changes and Deprecations",
                "Bug Fixes",
                "Improvements",
                "MongoDB Enterprise Kubernetes Operator 1.20 Series",
                "MongoDB Enterprise Kubernetes Operator 1.20.1",
                "Breaking Changes",
                "Bug Fixes",
                "MongoDBOpsManager Resource",
                "New Images",
                "MongoDB Enterprise Kubernetes Operator 1.20.0",
                "MongoDB Enterprise Kubernetes Operator 1.19 Series",
                "MongoDB Enterprise Kubernetes Operator 1.19.1",
                "Breaking Changes",
                "Improvements",
                "MongoDBMultiCluster Resource",
                "MongoDB Resource",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.19.0",
                "Breaking Changes",
                "MongoDB Enterprise Kubernetes Operator 1.18 Series",
                "MongoDB Enterprise Kubernetes Operator 1.18.0",
                "Improvements",
                "MongoDB Resource",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.17 Series",
                "MongoDB Enterprise Kubernetes Operator 1.17.2",
                "MongoDB Enterprise Kubernetes Operator 1.17.1",
                "Breaking Changes",
                "Improvements",
                "MongoDB Enterprise Kubernetes Operator 1.17.0",
                "Improvements",
                "Breaking Changes and Deprecations",
                "MongoDB Enterprise Kubernetes Operator 1.16 Series",
                "MongoDB Enterprise Kubernetes Operator 1.16.4",
                "MongoDB Resource",
                "MongoDB Enterprise Kubernetes Operator 1.16.3",
                "MongoDB Resource",
                "MongoDB Enterprise Kubernetes Operator 1.16.2",
                "MongoDB Resource",
                "MongoDBOpsManager Resource",
                "MongoDB Multi-Cluster Resource",
                "MongoDB Enterprise Kubernetes Operator 1.16.1",
                "MongoDB Resource",
                "MongoDB Enterprise Kubernetes Operator 1.16.0",
                "MongoDB Resource",
                "MongoDBOpsManager Resource",
                "MongoDBUser Resource",
                "MongoDB Enterprise Kubernetes Operator 1.15 Series",
                "MongoDB Enterprise Kubernetes Operator 1.15.2",
                "MongoDBOpsManager Resource",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.15.1",
                "MongoDB Resource",
                "Changes",
                "MongoDBOpsManager Resource",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.15.0",
                "MongoDB Resource",
                "Changes",
                "MongoDBOpsManager Resource",
                "Changes",
                "New Images",
                "MongoDB Enterprise Kubernetes Operator 1.14 Series",
                "MongoDB Enterprise Kubernetes Operator 1.14.0",
                "Kubernetes Operator",
                "Changes",
                "MongoDB Resource",
                "Changes",
                "Bug Fixes",
                "MongoDBOpsManager Resource",
                "Changes",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.13 Series",
                "MongoDB Enterprise Kubernetes Operator 1.13.0",
                "Kubernetes Operator",
                "Changes",
                "MongoDB Resource",
                "Changes",
                "Bug Fixes",
                "MongoDBOpsManager Resource",
                "MongoDBUser Resource",
                "Miscellaneous",
                "MongoDB Enterprise Kubernetes Operator 1.12 Series",
                "MongoDB Enterprise Kubernetes Operator 1.12.0",
                "MongoDB Resource",
                "Bug Fixes",
                "MongoDBOpsManager Resource",
                "Changes to Images and Supported Versions",
                "MongoDB Enterprise Kubernetes Operator 1.11 Series",
                "MongoDB Enterprise Kubernetes Operator 1.11.0",
                "Kubernetes Operator",
                "Bug Fixes",
                "New Images",
                "MongoDBOpsManager Resource",
                "MongoDB Enterprise Kubernetes Operator 1.10 Series",
                "MongoDB Enterprise Kubernetes Operator 1.10.0",
                "Kubernetes Operator",
                "Changes",
                "Bug Fixes",
                "MongoDBOpsManager Resource",
                "Known Issues",
                "MongoDB Enterprise Kubernetes Operator 1.9 Series",
                "MongoDB Enterprise Kubernetes Operator 1.9.2",
                "Kubernetes Operator",
                "Bug Fixes",
                "MongoDBOpsManager Resource",
                "Known Issues",
                "New Images",
                "MongoDB Enterprise Kubernetes Operator 1.9.1",
                "Kubernetes Operator",
                "Bug Fixes",
                "MongoDB Resource",
                "Bug Fixes",
                "MongoDBOpsManager Resource",
                "Known Issues",
                "Changes",
                "New Images",
                "MongoDB Enterprise Kubernetes Operator 1.9.0",
                "Kubernetes Operator",
                "Bug Fixes",
                "MongoDB Resource",
                "Changes",
                "MongoDBOpsManager Resource",
                "Known Issues",
                "Changes",
                "New Images",
                "MongoDB Enterprise Kubernetes Operator 1.8 Series",
                "MongoDB Enterprise Kubernetes Operator 1.8.2",
                "Known Issues",
                "Bug Fix",
                "MongoDB Enterprise Kubernetes Operator 1.8.1",
                "Known Issues",
                "Bug Fixes",
                "Improvements",
                "New Images",
                "New Ops Manager Images",
                "MongoDB Enterprise Kubernetes Operator 1.8.0",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes",
                "Bug Fixes",
                "Known Issues",
                "MongoDB Enterprise Kubernetes Operator 1.7 Series",
                "MongoDB Enterprise Kubernetes Operator 1.7.1",
                "MongoDB Resource Changes",
                "Bug Fixes",
                "Known Issues",
                "MongoDB Enterprise Kubernetes Operator 1.7.0",
                "Docker Image Changes",
                "MongoDB Resource Changes",
                "Bug Fixes",
                "Known Issues",
                "MongoDB Enterprise Kubernetes Operator 1.6 Series",
                "MongoDB Enterprise Kubernetes Operator 1.6.1",
                "Ops Manager Resource Changes",
                "Docker Image Changes",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.6.0",
                "MongoDB Resource Changes",
                "Kubernetes Operator Changes",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.5 Series",
                "MongoDB Enterprise Kubernetes Operator 1.5.5",
                "MongoDB Resource Changes",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.5.4",
                "MongoDB Resource Changes",
                "Kubernetes Operator Changes",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.5.3",
                "Kubernetes Operator Changes",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.5.2",
                "Ops Manager Resource Changes",
                "Kubernetes Operator Changes",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.5.1",
                "Bug Fixes",
                "Known Issues",
                "MongoDB Enterprise Kubernetes Operator 1.5.0",
                "Kubernetes Operator Changes",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes",
                "MongoDB Enterprise Kubernetes Operator 1.4 Series",
                "MongoDB Enterprise Kubernetes Operator 1.4.5",
                "CVE Description",
                "Common Weakness Enumeration",
                "Affected Versions",
                "Fixed Versions",
                "MongoDB Enterprise Kubernetes Operator 1.4.4",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes (Beta Release)",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.4.3",
                "Kubernetes Operator Changes",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes (Beta Release)",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.4.2",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes (Beta Release)",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.4.1",
                "MongoDB Enterprise Kubernetes Operator 1.4.0",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes (Beta Release)",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.3 Series",
                "MongoDB Enterprise Kubernetes Operator 1.3.1",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes (Alpha Release)",
                "MongoDB Enterprise Kubernetes Operator 1.3.0",
                "Specification Schema Changes",
                "Ops Manager Resource Changes (Alpha Release)",
                "Bug Fixes",
                "MongoDB Enterprise Kubernetes Operator 1.2 Series",
                "MongoDB Enterprise Kubernetes Operator 1.2.5",
                "CVE Description",
                "Common Weakness Enumeration",
                "Affected Versions",
                "Fixed Versions",
                "MongoDB Enterprise Kubernetes Operator 1.2.4",
                "MongoDB Enterprise Kubernetes Operator 1.2.3",
                "MongoDB Enterprise Kubernetes Operator 1.2.2",
                "MongoDB Enterprise Kubernetes Operator 1.2.1",
                "MongoDB Enterprise Kubernetes Operator 1.2.0",
                "GA Release",
                "Alpha Release",
                "MongoDB Enterprise Kubernetes Operator 1.1 Series",
                "MongoDB Enterprise Kubernetes Operator 1.1",
                "MongoDB Enterprise Kubernetes Operator 1.0 Series",
                "MongoDB Enterprise Kubernetes Operator 1.0",
                "MongoDB Enterprise Kubernetes Operator Beta Series",
                "MongoDB Enterprise Kubernetes Operator 0.12",
                "MongoDB Enterprise Kubernetes Operator 0.11",
                "MongoDB Enterprise Kubernetes Operator 0.10",
                "MongoDB Enterprise Kubernetes Operator 0.9",
                "MongoDB Enterprise Kubernetes Operator 0.8",
                "MongoDB Enterprise Kubernetes Operator 0.7",
                "MongoDB Enterprise Kubernetes Operator 0.6",
                "MongoDB Enterprise Kubernetes Operator 0.5",
                "MongoDB Enterprise Kubernetes Operator 0.4",
                "MongoDB Enterprise Kubernetes Operator 0.3",
                "MongoDB Enterprise Kubernetes Operator 0.2",
                "MongoDB Enterprise Kubernetes Operator 0.1"
            ],
            "paragraphs": "Released 2023-12-21 Adds support for the upcoming  Ops Manager  7.0.x series. Fixes an issue that prevented terminating a backup correctly. Released 2023-11-13 Aligns the component image version numbers with the  Kubernetes Operator  release tag so\nit's clear which images go with which version of the  Kubernetes Operator .\nThis affects the following images: To learn more, see  MongoDB Enterprise Kubernetes Operator   kubectl  and  oc  Installation Settings \nand  MongoDB Enterprise Kubernetes Operator  Helm Installation Settings . quay.io/mongodb/mongodb-enterprise-database-ubi quay.io/mongodb/mongodb-enterprise-init-database-ubi quay.io/mongodb/mongodb-enterprise-init-appdb-ubi quay.io/mongodb/mongodb-enterprise-init-ops-manager-ubi Replaces  spec.exposedExternally  (deprecated in  Kubernetes Operator  1.19) with\n spec.externalAccess . Fixes an issue with scaling a replica set in a  multi-Kubernetes-cluster deployment  when a member\ncluster has lost connectivity. The fix addresses both the manual and automated\nrecovery procedures. Fixes an issue where changing the names of the Automation Agent and MongoDB\naudit logs prevented them from being sent to the  Kubernetes  Pod logs. There are no\nrestrictions on the file names of MongoDB audit logs as of  Kubernetes Operator  1.22. Allows the following new log types from the  mongodb-enterprise-database \ncontainer to stream directly to  Kubernetes  logs: agent-launcher-script monitoring-agent backup-agent Fixes an issue that prevented storing the  MongoDBUser  resource in the\nnamespace set in  spec.mongodbResourceRef.namespace . Released 2023-09-21 The  Kubernetes Operator  no longer uses the  Reconciling  state for all custom resources.\nIn most cases this state has been replaced with  Pending  and a corresponding\nmessage. If you use monitoring tools with the custom MongoDB resources deployed\nwith the  Kubernetes Operator , you might need to adjust your dashboards and alerting rules\nto use the  Pending  state name. Adds support for configuring  logRotate \non the MongoDB Agent for the Application Database by adding the following new fields\nto the  MongoDBOpsManager  resource: spec.applicationDatabase.agent.logRotate spec.applicationDatabase.agent.logRotate.numTotal spec.applicationDatabase.agent.logRotate.numUncompressed spec.applicationDatabase.agent.logRotate.percentOfDiskspace spec.applicationDatabase.agent.logRotate.sizeThresholdMB spec.applicationDatabase.agent.logRotate.timeThresholdHrs You can now configure the  systemLog \nto send logs to a custom location other than the default  /var/log/mongodb-mms-automation  directory using the following new fields\nin the  MongoDBOpsManager  resource: spec.applicationDatabase.agent.systemLog spec.applicationDatabase.agent.systemLog.path spec.applicationDatabase.agent.systemLog.logAppend spec.applicationDatabase.agent.systemLog.destination Improves handling of Application Database clusters in  multi-Kubernetes-cluster deployments . In the last release, to scale down processes, the  Kubernetes Operator  required\na connection to the  Kubernetes  cluster. This could block the reconciliation\nprocess due to a full-cluster outage. In this release, the  Kubernetes Operator  successfully manages the remaining\nhealthy clusters as long as they have a majority of votes to elect a primary.\nThe  Kubernetes Operator  doesn't remove associated processes from the automation\nconfiguration and replica set configuration. The  Kubernetes Operator  deletes\nthese processes only if you delete the corresponding cluster from\n spec.applicationDatabase.clusterSpecList \nor change the number of the cluster members to zero. When the  Kubernetes Operator \ndeletes these processes, it scales down the replica set by removing\nprocesses tied to that cluster one at a time. Adds an automatic recovery mechanism for  MongoDB \nresources when a custom resource remains in a  Pending \nor  Failed  state for a longer period of time. In addition,\nintroduces the following environment variables: To learn more, see  Recover Resource Due to Broken Automation Configuration . MDB_AUTOMATIC_RECOVERY_ENABLE MDB_AUTOMATIC_RECOVERY_BACKOFF_TIME_S Allows you to route the audit logs for the  MongoDB  resource to the  Kubernetes \nPod logs. Ensure that you write the  MongoDB  resource's audit logs\nto the  /var/log/mongodb-mms-automation/mongodb-audit.log  file.\nThe Pod hosting the resource monitors this file and appends its content\nto its  Kubernetes  logs. To send audit logs to the  Kubernetes  Pod logs, use the following example\nconfiguration in the  MongoDB  resource: The  Kubernetes Operator  tags audit log entries with the  mongodb-audit  key\nin the Pod logs. To extract audit log entries, use a command similar to the following example: Fixes an issue where you couldn't set the  spec.backup.autoTerminateOnDeletion \nsetting to  true  for sharded clusters. This setting controls whether the\n Kubernetes Operator  stops and terminates the backup when you delete a MongoDB\nresource. If omitted, the default value is  false . Released 2023-08-25 Renames the environment variable  CURRENT_NAMESPACE  to  NAMESPACE . This variable\ntracks the namespace of the  Kubernetes Operator . If you've set this variable by editing the\n MongoDB  resources , update  CURRENT_NAMESPACE  to  NAMESPACE  while\n upgrading the Kubernetes Operator . Fixes an issue where  StatefulSet  override labels failed to override the  StatefulSet . Supports configuring backups of the Application Database and MongoDB for the  MongoDBMultiCluster  resource . Adds documentation for configuring a  MongoDBMultiCluster  resources  deployment in a GitOps environment.\nTo learn more, see  Configure Resources for GitOps . Adds  MetadataWrapper , a label and annotations wrapper, to the  MongoDB  resource ,\n MongoDBMultiCluster  resource  and  MongoDBOpsManager  resources. The wrapper supports\noverriding  metadata.Labels  and  metadata.Annotations . The  appdb-ca  is not automatically added to the JVM trust store in  Ops Manager . The\n appdb-ca  is the  CA (Certificate Authority)  saved in the ConfigMap specified in\n spec.applicationDatabase.security.tls.ca . This impacts you if: If you need to use the same custom certificate for  appdb-ca  and the  S3 (Simple Storage Service)  snapshot\nstore, specify the CA with  spec.backup.s3Stores.customCertificateSecretRefs . You use the same custom certificate for the  appdb-ca  and your  S3 (Simple Storage Service)  snapshot store. You use a version of  Kubernetes Operator  earlier than 1.17.0 or you've mounted your own\ntrust store to  Ops Manager . Deprecates the  spec.backup.s3Stores.customCertificate  and\n spec.backup.s3OpLogStores.customCertificate  settings. Use\n spec.backup.s3OpLogStores.customCertificateSecretRefs  and\n spec.backup.s3Stores.customCertificateSecretRefs  instead. Fixes an issue that prevented setting an arbitrary port number for\n spec.externalConnectivity.port  when using the  LoadBalancer  service type\nto expose  Ops Manager  externally. Fixes an issue that caused  Ops Manager  to reject certificates by enabling the  Kubernetes Operator \nto import the  appdb-ca , which is a bundle of CAs, into the  Ops Manager  JVM trust store. Supports configuring the  MongoDBOpsManager  resource with a highly available\nApplication Database across multiple  Kubernetes  clusters by adding the following new fields\nto the  MongoDBOpsManager  resource: The default value for the new optional  spec.applicationDatabase.topology \nfield is  singleCluster , and it is used if you omit the value. To upgrade to  Kubernetes Operator \n1.21, you don't need to update your  MongoDBOpsManager  resources. This makes the addition\nof the  spec.applicationDatabase.topology  setting backward-compatible with\nsingle  Kubernetes  cluster deployments of the Application Database. To learn more, see  Deploy an  Ops Manager  Resource  and\nthe  Ops Manager Resource Specification . spec.applicationDatabase.topology spec.applicationDatabase.clusterSpecList Allows you to add a list of custom certificates for backups in the  S3 (Simple Storage Service)  snapshot store\nusing the  spec.backup.s3Stores.customCertificateSecretRefs  and\n spec.backup.s3OpLogStores.customCertificateSecretRefs  fields in the  MongoDBOpsManager  resource. Released 2023-06-07 This release fixes an issue that prevented upgrading the  Kubernetes Operator  to 1.20.0 in OpenShift. Removes  appdb.connectionSpec.Project , which was deprecated more than two years ago. Fixes an issue where the  MongoDBMultiCluster  resource was not watching  Ops Manager 's connection ConfigMap and secret. Fixes support for rotating the  clusterfile  secret, which is used for internal X.509 authentication in the  MongoDB  and  MongoDBMultiCluster  resources. Adds support for votes, priority, and tags by introducing the\n spec.applicationDatabase.memberConfig.votes ,\n spec.applicationDatabase.memberConfig.priority ,\nand  spec.applicationDatabase.memberConfig.tags  settings. Changes the container registry for the Application Database image from\n quay.io/mongodb/mongodb-enterprise-appdb-database-ubi  to  quay.io/mongodb/mongodb-enterprise-server . This results in the following changes when you upgrade to\nthis release: The Helm chart setting for the Application Database image,  values.mongodb.name ,\ndefaults to  mongodb-enterprise-server . The  Kubernetes Operator  updates your Application Database replica set Pods to use the new\nimages referenced in the  values.mongodb.name  Helm setting. The new images are\nfunctionally equivalent to the previous ones assuming that the MongoDB version is the same. The  Kubernetes Operator  automatically updates the tag suffix for all Application Database\nimages that reference the new container registry from  -ent  to  -ubi8  or the\nsuffix set in  MDB_IMAGE_TYPE  or  mongodb.imageType . For example,\nthe  Kubernetes Operator  changes  quay.io/mongodb/mongodb-enterprise-server:4.4.5-ent \nto  quay.io/mongodb/mongodb-enterprise-server:4.4.5-ubi8 . You don't need to update\nthe  applicationDatabase.version  setting in the  MongoDBOpsManager  resource. You can stop the  Kubernetes Operator  from automatically updating the tag suffix by setting\n MDB_APPDB_ASSUME_OLD_FORMAT  or  mongodb.appdbAssumeOldFormat  to  true . For example, you might want\nto stop the automatic suffix change if you're mirroring this image from your own repository. Adds support for specifying versions without a suffix in\n spec.applicationDatabase.version . For example, you can specify a version, such as\n 6.0.5 , without adding the  -ubi8  suffix. The  Kubernetes Operator  automatically converts\nthis to  6.0.5-${MDB_IMAGE_TYPE} . The default for the  MDB_IMAGE_TYPE  environment\nvariable is  -ubi8 . Changes all images to reference UBI variants by default. The new images use the suffix  -ubi . quay.io/mongodb/mongodb-enterprise-database-ubi quay.io/mongodb/mongodb-enterprise-init-database-ubi quay.io/mongodb/mongodb-enterprise-ops-manager-ubi quay.io/mongodb/mongodb-enterprise-init-ops-manager-ubi quay.io/mongodb/mongodb-enterprise-init-appdb-ubi quay.io/mongodb/mongodb-agent-ubi quay.io/mongodb/mongodb-enterprise-appdb-database-ubi Changes the default Application Database image repository to use the official MongoDB Enterprise repository by setting  values.mongodb.name  to  quay.io/mongodb/mongodb-enterprise-server  by default. Introduces the  values.mongodb.imageType  environment variable to override the new default  -ubi8  Application Database image tag suffix used by the  MongoDBOpsManager  resource. Released 2023-06-07 This release (1.20.0) is tied to a broken release on the Openshift Marketplace.\nDon't upgrade to 1.20.0, and instead upgrade to the 1.20.1 release. Released 2023-03-30 This release fixes an issue that prevented upgrading the  Kubernetes Operator  to 1.19.0.\nUpgrade to this release. Makes the  data.orgId  field required for the ConfigMap of the  MongoDB  resources .\nIf you provide an empty  orgId ,  as in:  orgId = \"\" ,  Ops Manager \ncreates an organization with the project name. Before upgrading the\n Kubernetes Operator  to 1.19.1, set the  orgId:\"\"  in the  Ops Manager  ConfigMap\nand reapply it. Introduces  multi-Kubernetes-cluster deployments . To learn more, see  Deploy MongoDB Resources on Multiple Kubernetes Clusters . Makes the following changes to the  multi-Kubernetes-cluster deployment  support compared with\nthe Beta version of the  multi-Kubernetes-cluster deployment  support in  Kubernetes Operator  1.18.x: Renames the  MongoDBMulti  resource in Beta versions to the  MongoDBMultiCluster  resource . Renames the shortcut name of the  MongoDBMultiCluster  resource  to  mdbmc . Use this\nshortcut name in all commands on the  MongoDBMultiCluster  resource . For example, to\ncheck the status of your  MongoDBMultiCluster  resource , run: Renames the \"multi-cluster CLI\" tool to the \" kubectl mongodb  plugin \".\nTo learn more, see the  MongoDB Plugin Reference . Removes the unnecessary intermediate object  clusterSpecs  from the\n clusterSpecList  in the  MongoDBMultiCluster  resource  specification. For a valid\nexample of a  MongoDBMultiCluster  resource  configuration file, see the\n Multi-Kubernetes-Cluster Resource Specification . Adds support for  Kubernetes  1.26 and OpenShift 4.12. To learn more, see\n MongoDB Enterprise Kubernetes Operator  Compatibility . Allows you to configure  podSpec  per shard in a MongoDB sharded cluster\nby specifying an array of  podSpecs  under the  spec.shardSpecificPodSpec \nsetting for each shard. Makes the  data.orgId  field required for the ConfigMap of the  MongoDB  resources .\nIf you provide an empty  orgId ,  as in:  orgId = \" \" ,  Ops Manager \ncreates an organization with the project name. Adds documentation for the  Multi-Kubernetes-Cluster Resource Specification . Adds the  Frequently Asked Questions  for the  Kubernetes Operator  to the documentation. Adds documentation for  configuring file system backup stores \nin the  Kubernetes Operator  MongoDB deployments. Adds the  spec.clusterSpecList.externalAccess.externalService ,\n spec.clusterSpecList.externalAccess.externalService.annotations ,\n spec.clusterSpecList.externalAccess.externalService ,\nand  spec.clusterSpecList.externalAccess.externalDomain \nsettings to configure external connectivity settings for  MongoDBMultiCluster  resources . Use these settings to\n connect to a Multi-Cluster Resource from outside Kubernetes . Adds the  spec.clusterSpecList.memberConfig.votes \nand  spec.clusterSpecList.memberConfig.priority \nsettings for configuring replica set member votes and member priority for  MongoDBMultiCluster  resources . Adds the  spec.clusterSpecList.memberConfig.tags \nsetting for adding tags to replica set members in  MongoDBMultiCluster  resources . Adds the  spec.security.authentication.ldap.timeoutMS \nsetting that specifies how many milliseconds an authentication request should\nwait before timing out. Adds the  spec.memberConfig.votes  and  spec.memberConfig.priority \nsettings for configuring replica set member votes and member priority. Adds the  spec.memberConfig.tags  setting for adding tags\nto replica set members. Adds the  spec.podSpec.podTemplate.affinity.podAffinity  setting to determine whether\nmultiple  MongoDB  resource   Pods  must be co-located with other\n Pods  in sharded MongoDB cluster deployments. To learn more about\nthe use cases, see  Affinity and Anti-Affinity  in the  Kubernetes  documentation. Adds the  spec.externalAccess  setting for configuring external\nconnectivity for MongoDB resources. Use this setting to\n connect to a MongoDB Resource from outside Kubernetes . Deprecates the  spec.exposedExternally  setting. This setting\nwill be removed in the  Kubernetes Operator  1.23.0 release. To\n connect to a MongoDB Resource from outside Kubernetes , use the  spec.externalAccess  setting instead. Fixes the handling of  WATCH_NAMESPACE='*'  environment variable for\n multi-Kubernetes-cluster deployments . In the following cases, API clients for member clusters\nare configured incorrectly resulting in deployment errors: This leads to the following errors: To avoid this issue, set the  WATCH_NAMESPACE  environment variable to\nspecific namespaces instead of  '*' , and verify that the  kubeconfig \nsettings for member clusters don't specify a namespace. To set the namespace\nfor  multi-Kubernetes-cluster deployments , see  Set the Deployment's Scope  and the  MongoDB Plugin Reference . The  WATCH_NAMESPACE='*'  environment variable is specified for\nthe  multi-Kubernetes-cluster deployment . A specific namespace is set in  kubeconfig  for member clusters. The  kubectl mongodb  plugin  isn't used for configuring  multi-Kubernetes-cluster deployments . Fixes an issue when  CertificatesSecretsPrefix  is set but no related\n spec.security.tls  settings, such as  tls.additionalCertificateDomains \nor  tls.ca  are  provided. Fixes an issue that allows you to explicitly specify the value  none \nfor the  spec.security.authentication.ldap.transportSecurity \nwhen  TLS (Transport Layer Security)  isn't used. Previously, the  Kubernetes Operator  treated this\nsetting as  none  when you omitted the value and didn't specify the\n tls  value, but the  Kubernetes Operator  didn't allow you to specify the\nvalue  none  explicitly. Released 2023-03-28 This release removes Ubuntu-based images. Ubuntu-based images were deprecated\nin favor of UBI-based images in the  Kubernetes Operator  in 1.17.0.\n Migrate the Kubernetes Operator from Ubuntu-based Images to UBI-based images .\nAll existing Ubuntu-based images will continue to be supported until their version's\n End of Life (EOL) dates . This release is tied to a broken release on the Openshift Marketplace.\nDon't upgrade to this release (1.19.0), and instead upgrade to the\n Kubernetes Operator  1.19.1 release. Released 2022-12-29 Adds support for  SCRAM-SHA-1  for user and MongoDB Agent\nauthentication. To enable either authentication, use  MONGODB-CR \nand  SCRAM-SHA-1  in the\n spec.security.authentication.modes  and\n spec.security.authentication.agents.mode  settings. Adds support for the following features for OpsManager Backup configuration: KMIP Backup Configuration support through the\n spec.backup.encryption.kmip  parameter in\n OpsManager backup.encryption.kmip \nand  MongoDB backup.encryption.kmip  settings.\nTo learn more, see  Configure KMIP Backup Encryption for Ops Manager . Backup assignment labels settings in\n spec.backup.[*].assignmentLabels  elements of the OpsManager and\nMongoDB resources for backups. Use assignment labels to identify that\nspecific backup stores are associated with particular projects.\nTo learn more, see: backup.assignment.Labels for Ops Manager resources backup.assignment.Labels for MongoDB resources Configure MongoDB Database Backups Deploy an  Ops Manager  Resource Backup snapshot schedule configuration through the\n spec.backup.snapshotSchedule  setting in the OpsManager\nresource. To learn more, see  Configure MongoDB Database Backups . Adds support for disaster recovery in  multi-Kubernetes-cluster deployments , which are in\nthe beta release. To learn more, see  Disaster Recovery  and\n MongoDB kubectl Plugin Reference . Adds documentation for\n spec.additionalMongodConfig.net.tls.disabledProtocols . Fixes the issue where you configure a liveness probe and it reports a\npositive result when you terminate a MongoDB Agent's process. This could\ncause Pods hosting MongoDB resources to run without the MongoDB  Agent.\nIn addition to this fix, consider configuring\n readiness probe overrides . Fixes the startup script in database Pod that might report errors when\nthe Pod restarts. Released 2022-10-18 Fixes the OpenShift installation issue mentioned in the\n v1.17.1 release notes . The  Kubernetes Operator \nLifecycle Manager upgrade graph automatically skips the v1.17.1\nrelease and performs an upgrade from v1.17.0 directly to this release. Improves the reliability of upgrades by adding startup probes for\nMongoDB and OpsManager custom resources with some defaults. Use\n spec.podSpec.podTemplate  to override probe configurations. Released 2022-10-10 As a result of this issue, installing this release could result in\n ImagePullBackOff  errors in Pods hosting AppDB, the  database for  Ops Manager .\nErrors will look similar to the following: To continue using the  Kubernetes Operator  v1.17.1, use the following workaround\nand update the  Kubernetes Operator  Subscription with the following  spec.config.env : Remove this workaround as soon as you install the new  Kubernetes Operator  v1.17.2. This release has the following additional breaking change: This release has invalid  quay.io \ndigests referenced in the certified bundle's CSV. This affects only\nOpenShift deployments when you install or upgrade  Kubernetes Operator  from\nthe certified bundle (OperatorHub) in  quay.io . If you use  Kubernetes Operator  with OpenShift, we recommend that you do NOT\nupgrade to this release (v1.17.1), and instead upgrade to the\n Kubernetes Operator  v1.17.2, which is due the week commencing 17th October 2022. Removes the  operator.deployment_name  parameter from  Kubernetes Operator  Helm charts.\nIn previous releases, you might have used this parameter to customize the name of the\n Kubernetes Operator  container. Starting with this release, the value of the  operator.name  Helm chart parameter\ndetermines the name of the  Kubernetes Operator  container. This is a breaking change only if you set  operator.deployment_name  to a different\nvalue than  operator.name  and if you configured tooling to rely on the value of\n operator.deployment_name . Uses  Quay  as an image registry for\n Kubernetes Operator  on OpenShift. When you upgrade your  Kubernetes Operator \ndeployment, it automatically pulls new images from Quay. You don't need to\ntake any action. Released 2022-09-19 Introduces support for  Ops Manager  6.0. Introduces the  spec.backup.s3OpLogStores.s3RegionOverride \nand  spec.backup.s3Stores.s3RegionOverride  parameters\nfor specifying the regions where the custom  S3 (Simple Storage Service) -compatible buckets\nthat you use for the  oplog store  or a\n snapshot store  should reside. Improves security by introducing: The  readOnlyRootFilesystem  setting for all deployed containers.\nThis change also introduces additional\n volumes and volume mounts . The  allowPrivilegeEscalation  setting. This setting is by default\nset to  false  for all deployed containers. This release: Removes support for  Ops Manager  4.4 due to its\n End of Life .\nIf you're using  Ops Manager  4.4,  upgrade \nto a newer  Ops Manager  version before you upgrade to  Kubernetes Operator  1.17. Deprecates Ubuntu-based images. Starting with  Kubernetes Operator  1.19.0,\nUbuntu-based images will no longer be made available. All existing\nUbuntu-based images will continue to be supported until their version's\n End of Life (EOL) dates . We strongly\nrecommend that you  Migrate  MongoDB Enterprise Kubernetes Operator  from Ubuntu-based Images to UBI-based Images  as soon as possible. Removes support for  TLS (Transport Layer Security)  certificates in concatenated  PEM (Privacy-Enhanced Mail)  format.\nThese certificates were deprecated in  Kubernetes Operator  1.13.0.\nIf you want to use these certificates, the last version to which you\ncan upgrade is  Kubernetes Operator  1.16.4. Starting with the  Kubernetes Operator  1.17.0 release, you must manually\nmigrate old-style  TLS (Transport Layer Security)  secrets from opaque to\n kubernetes.io/tls \ntype secrets by creating new secrets that contain the relevant\ncertificates and signing keys. To learn how to create these secrets,\nsee the following resources: Deploy a Replica Set Secure Internal Authentication with X.509 Released 2022-08-03 Init-Ops-Manager and Operator binaries now use Go 1.18.4, which\naddresses security issues. Released 2022-07-15 Fixed a bug where  securityContext  defined at the Pod level is not\nrespected as the  Kubernetes Operator  overrides it with a\n securityContext  at the container level. To learn more, see the\ndescription of the  spec.persistent  setting. Adds  timeoutMS , and  userCacheInvalidationInterval  fields to the\n spec.security.authentication.ldap  object. Fixes behavior where the  additionalMongodConfig.net.tls.mode  setting\nwas ignored for  mongos ,  configSrv , and  shard  objects when\nconfiguring  ShardedCluster  resources. Released 2022-06-28 This release removes WiredTiger cache computation, which was required\nfor MongoDB versions earlier than 4.0.9. Before you upgrade\nto this release, you must upgrade your database deployment to use\nMongoDB version 4.0.9 or later. To learn how to upgrade your deployment, see\n Upgrade MongoDB Version and FCV . Removes the  spec.podSpec.podAntiAffinityTopologyKey ,\n spec.podSpec.podAffinity , and  spec.podSpec.nodeAffinity \nsettings. Instead, use  spec.podSpec.podTemplate  to configure these\nparameters. Removes the  spec.applicationDatabase.podSpec.podAntiAffinityTopologyKey ,\n spec.applicationDatabase.podSpec.podAffinity , and\n spec.applicationDatabase.podSpec.nodeAffinity  settings. Instead, use  spec.applicationDatabase.podSpec.podTemplate  to configure these\nparameters. Added support for  LDAP client authentication  and for\n managing database users with LDAP \nto  multi-Kubernetes-cluster deployments . This feature is a beta release. Use  multi-Kubernetes-cluster deployment \ndeployments only in development environments. Released 2022-05-24 Deprecates the  spec.service  parameter. Use\n spec.statefulSet.spec.serviceName  to provide a custom service name. Released 2022-04-29 Removes the  spec.security.tls.secretRef.name  parameter. Kubernetes Operator  version  v1.10.0  deprecated this parameter. To specify the secret name containing the certificate for the\ndatabase, use  spec.security.certsSecretPrefix . Create the secret containing the certificates accordingly. Removes the  spec.podSpec.cpu  and\n spec.podSpec.memory  parameters. To override the CPU/Memory resources for the database pod, set the\n statefulset  parameter under\n spec.podSpec.podTemplate.spec.containers . Propagates custom labels specified under  metadata.labels \nto the database  StatefulSet  and the  Persistent Volume Claim  objects. Allows adding Prometheus scraping endpoints to the MongoDB resources\nusing the  spec.prometheus  configuration attribute. Find a sample Prometheus configuration in the\n GitHub \nrepository. Removes the\n spec.applicationDatabase.security.tls.secretRef.name \nparameter. Kubernetes Operator  version  v1.10.0  deprecated this parameter. To specify the secret name containing the certificate for AppDB,\nuse the\n spec.applicationDatabase.security.certsSecretPrefix \nparameter. Create the secret containing the certificates accordingly. Removes  spec.applicationDatabase.podSpec.cpu  and\n spec.applicationDatabase.podSpec.memory . To override the CPU/Memory resources for the appDB pod, use the\n statefulset  parameter under\n spec.applicationDatabase.podSpec.podTemplate.spec.containers . Propagates custom labels specified under\n metadata.labels  to the  Ops Manager , AppDB and BackupDaemon\n StatefulSets  and the  Persistent Volume Claim  objects. Allows adding Prometheus scraping endpoints to the\n ApplicationDatabase  resources using the\n spec.applicationDatabase.prometheus  configuration\nattribute. Adds the optional parameter  spec.connectionStringSecretName . This\nparameter provides a deterministic secret name for the user-specific\nconnection string secret that  Kubernetes Operator  generates. Released 2022-03-24 To enable custom  TLS (Transport Layer Security)  certificates for  S3 (Simple Storage Service)  oplog stores,\nyou must configure the following settings: Specify  spec.security.tls.ca . Specify  spec.security.certsSecretPrefix . Set  spec.backup.s3OpLogStores.customCertificate \nto  true . To enable custom  TLS (Transport Layer Security)  certificates for  S3 (Simple Storage Service)  snapshot stores,\nyou must configure the following settings: Specify  spec.security.tls.ca . Specify  spec.security.certsSecretPrefix . Set  spec.backup.s3Stores.customCertificate \nto  true . Fixes an issue where the  Kubernetes Operator  mounted the incorrect  CA (Certificate Authority)  to the Application Database Pod. Released 2022-03-04 Init-database, Init-Ops-Manager, and Operator binaries now use\nGo 1.17.7 to prevent  CVE-2022-23773 . Fixes an issue that prevented the Operator upgrade when managing a\nTLS-enabled ApplicationDB whose TLS certificate is stored in a\n Secret  of type Opaque. Released 2022-02-11 Kubernetes Operator  version  1.15.1  fixes an issue\nthat prevented the  Kubernetes Operator  upgrade when managing a\nTLS-enabled Application Database whose TLS certificate is stored in an\n Opaque \nsecret. We recommend that you upgrade to  Kubernetes Operator  version 1.16.0\nor later. We strongly advise against upgrading to  Kubernetes Operator  version 1.14.0\nor 1.15.0. The  spec.security.tls.enabled  and\n spec.security.tls.secretRef.prefix  fields are now\n deprecated  and will be removed in a future release. To enable  TLS (Transport Layer Security)  for MongoDB database resources, provide a value for\nthe  spec.security.certsSecretPrefix  field. Adds the  spec.backup.queryableBackupSecretRef  field.\nThis field's value references a  secret  that stores\ncertificates for  Queryable Backups . Adds two fields to enable support for configuring custom  TLS (Transport Layer Security) \ncertificates for the  S3 (Simple Storage Service)  Oplog and Snapshot Stores for backup:\n spec.security.tls.ca  and\n spec.security.tls.secretRef . Adds the ability to back up Application Databases. To back up an\napplication database, you must first disable its processes using the\n spec.applicationDatabase.automationConfig.processes[n].disabled \nfield. The  spec.security.tls.enabled ,\n spec.security.tls.secretRef.prefix ,\n spec.applicationDatabase.security.tls.enabled  and\n spec.applicationDatabase.security.tls.prefix  fields are\nnow  deprecated  and will be removed in a future release. To enable  TLS (Transport Layer Security)  for  Ops Manager  resources, provide a value for\nthe  spec.security.certsSecretPrefix  field. To enable  TLS (Transport Layer Security)  for Application Database resources, provide a value for\nthe  spec.applicationDatabase.security.certsSecretPrefix \nfield. Find all new images at: https://quay.io/repository/mongodb  (ubuntu-based) https://connect.redhat.com/  (rhel-based) Released 2021-12-16 Kubernetes Operator  version  1.15.1  fixes an issue\nthat prevented the  Kubernetes Operator  upgrade when managing a\nTLS-enabled Application Database whose TLS certificate is stored in an\n Opaque \nsecret. We recommend that you upgrade to  Kubernetes Operator  version 1.16.0\nor later. We strongly advise against upgrading to  Kubernetes Operator  version 1.14.0\nor 1.15.0. The  Kubernetes Operator  now supports  HashiCorp Vault  as a  secret storage tool . To\nstore secrets in  Vault  instead of  Kubernetes   secrets , see\n Configure Secret Storage . This release adds the  spec.backup.autoTerminateOnDeletion  setting,\nwhich indicates if the  Kubernetes Operator \nshould stop and terminate the backup when you delete the  MongoDB  resource. Fixes an issue that caused a  ShardedCluster  resource to fail when disabling authentication. This release adds the ability to configure  S3 (Simple Storage Service)  oplog stores using the\n spec.backup.s3OpLogStores.name \nsetting and other related settings. Fixes an issue that prevented the  Kubernetes Operator  from triggering a\nresource reconciliation when rotating the Application Database  TLS (Transport Layer Security)  certificate. Fixes an issue where the  Kubernetes Operator  didn't mount the custom  CA (Certificate Authority)  specified in the  MongoDBOpsManager  resource\ninto the Backup Daemon Pod. This issue prevented backups from working\nwhen you configured  Ops Manager  to run in  hybrid mode  and used a custom  CA (Certificate Authority) . Released 2021-10-21 The  Kubernetes Operator  no longer generates  TLS (Transport Layer Security)  certificates for\n MongoDB  and  MongoDBOpsManager  resources. The  Kubernetes Operator  now integrates with the Gatekeeper Open Policy Agent (OPA).\nThis allows you to\n control your deployments with policies set in the OPA Gatekeeper . The  Kubernetes Operator  can now watch a list of namespaces. To learn more,\nsee  Operator Uses a Subset of Namespaces . When deploying resources to  more than one namespace , create  imagePullSecrets  only in the\nnamespace where you installed the  Kubernetes Operator . The  Kubernetes Operator \nsynchronizes this secret across all watched namespaces. The  spec.credentials  secret  now accepts  fields named\n publicKey  and  privateKey . Use these fields instead of the\n user  and  publicApiKey  fields supported in previous releases. This release deprecates  generic type secrets  for\n TLS (Transport Layer Security)  certificates. The  Kubernetes Operator  now supports  TLS (Transport Layer Security)  secrets of the\n kubernetes.io/tls  type. The  Kubernetes Operator  reads these secrets and automatically generates\nnew  .pem  files that contain the concatenated  tls.crt  and\n tls.key  fields when you update these secrets. This removes the need to manually concatenate these vales to create\n .pem  files and enables you to natively reference secrets\nthat  Kubernetes -native tools, such as  cert-manager , generate. For  TLS (Transport Layer Security) -enabled resources, the operator now watches the ConfigMap\nthat contains the  CA (Certificate Authority)  and the secrets that contain  TLS (Transport Layer Security) \ncertificates. Changes to these ConfigMaps and secrets now trigger a\nreconciliation of the related resource. This release removes the  spec.project  setting from the\n MongoDB Database Resource Specification . If your  MongoDB  resource specifications use the\n spec.project  setting, update your specifications to instead\nuse  spec.opsManager.configMapRef.name  or\n spec.cloudManager.configMapRef.name  before you upgrade the\n Kubernetes Operator  to 1.13.0 or later. This release adds several new fields that determine the names that you\nmust give the secrets that contain your  TLS (Transport Layer Security)  and X.509 certificates\nfor MongoDB resources. To learn more, see  spec.security.certsSecretPrefix  and the\n Secure Client Connections  tutorials. Fixes an issue where Sharded Cluster backups could not be correctly\nconfigured using  MongoDB  resource specifications. Fixes an issue where Backup Daemon fails to start after you update an\n Ops Manager  deployment by updating  spec.version . The  Kubernetes Operator  now reports the status of file system snapshot\nstores that you configure in the   spec.backup.fileSystemStores \nsetting in the  MongoDBOpsManager  resource specification. You must manually configure the file system snapshot stores. This release adds a new field,  spec.backup.externalServiceEnabled ,\nto the  MongoDBOpsManager  resource specification. By default, the  Kubernetes Operator  creates a  LoadBalancer  service when\nyou  enable queryable backups . Set  spec.backup.externalServiceEnabled  to  false  before you\nenable queryable backups to prevent the  Kubernetes Operator  from creating a\nLoadBalancer service. The  Kubernetes Operator  now automatically upgrades personal API keys to\nprogrammatic API keys when you upgrade an  Ops Manager  deployment to\nversion 5.0.0 or later. You no longer must change the keys manually to\nupgrade your deployment. This release adds the  spec.security.certsSecretPrefix  field to\ndetermine the name that you must give the secret that contains your\n TLS (Transport Layer Security)  certificate for  MongoDBOpsManager  resources. To learn more, see  spec.security.certsSecretPrefix  and\nthe  HTTPS  tab in the  Deploy an  Ops Manager  Resource  tutorial. This release removes the spec.project setting from the  MongoDBUser \nCustomResourceDefinition. If your  MongoDBUser  resource specifications use the\nspec.project setting, update your specifications to instead\nuse  spec.MongoDBResourceRef.name  before you upgrade the\n Kubernetes Operator  to 1.13.0 or later. Ops Manager  4.4.7, 4.4.9, 4.4.10, 4.4.11, 4.4.12 and 4.4.13 base images\nhave been updated to Ubuntu 20.04. Ops Manager  versions 4.4.16 and 5.0.1 are now supported. Released 2021-07-15 If you set  spec.externalConnectivity  to  false  after it\nwas set to  true , the  Kubernetes Operator  deletes the corresponding service. Fixes a bug where you could specify  net.ssl.mode  and not  net.tls.mode \nin  spec.additionalMongodConfig . If you set  spec.externalConnectivity  to  false  after it\nwas set to  true , the  Kubernetes Operator  deletes the corresponding service. You can specify the number of backup daemon Pods with  spec.backup.members .\nIf not set, the value defaults to  1 . The  Kubernetes Operator  now supports the following  Ops Manager  versions: 4.4.13, 4.4.14, 4.4.15, 4.2.25 and 5.0.0. Before upgrading  Ops Manager  to version 5.0.0, check that the\n Kubernetes Operator  uses a  programmatic API key . Ubuntu based  Kubernetes Operator  images are now based on Ubuntu 20.04 instead\nof Ubuntu 16.04. Ubuntu based MongoDB images starting from 2.0.1 are based on Ubuntu 18.04\ninstead of Ubuntu 16.04. MongoDB 4.0. does not support Ubuntu 18.04. If you want to use\nMongoDB 4.0. with the  Kubernetes Operator , use previously released\nimages. Ubuntu based  Ops Manager  images after 4.4.13 are based on Ubuntu 20.04\ninstead of Ubuntu 16.04. Newly released UBI images for the  Kubernetes Operator ,  Ops Manager  and\nMongoDB are based on  ubi-minimal  instead of  ubi . Released 2021-06-03 Removes the topic \"Migrate to One Resource per Project (Required for\nVersion 1.3.0)\" from the current documentation because  v.1.3.0 is EOL . This topic has been\n archived . Fixes an issue with the  Liveness Probe \nthat could cause the database Pods to be restarted in the middle of\na restore operation from Backup. mongodb-agent 10.29.0.6830-1  located in the following registries: UBI images:  quay.io/mongodb/mongodb-agent-ubi:10.29.0.6830-1 Ubuntu images:  quay.io/mongodb/mongodb-agent:10.29.0.6830-1 mongodb-enterprise-appdb-database  located in the following registries: UBI images:  quay.io/mongodb/mongodb-enterprise-appdb-database-ubi Ubuntu images:  quay.io/mongodb/mongodb-enterprise-appdb-database mongodb-enterprise-init-appdb 1.0.7  located in the following registries: UBI images:  quay.io/mongodb/mongodb-enterprise-init-appdb-ubi:1.0.7 Ubuntu images:  quay.io/mongodb/mongodb-enterprise-init-appdb:1.0.7 mongodb-enterprise-init-database 1.0.3  located in the following registries: UBI images:  quay.io/mongodb/mongodb-enterprise-init-database-ubi:1.0.3 Ubuntu images:  quay.io/mongodb/mongodb-enterprise-init-database:1.0.3 Beginning with this release, you can use any version of MongoDB\nfor the Application Database. You must specify this version\nexplicitly when you deploy the  MongoDBOpsManager  resource. To upgrade the  Kubernetes Operator , you must specify the Application\nDatabase's version. Check that the  spec.applicationDatabase.version \nhas a value in your configuration files for the  MongoDBOpsManager \ncustom resource deployment. Each Application Database Pod consists of the following containers\n(instead of one container with a bundled MongoDB version, as in previous\nreleases): mongodb mongodb-agent mongodb-agent-monitoring The  spec.applicationDatabase.persistent  setting is removed. The\n Kubernetes Operator  always uses persistent volumes for the Application\nDatabase deployed by your  MongoDBOpsManager  custom resources. Released 2020-03-25 Updates the  CustomResourceDefinitions  from the  v1beta1  version to the  v1 \nversion. Clusters on  Kubernetes  1.16 and higher should remain unimpacted. The  CustomResourceDefinitions  cannot install in clusters on  Kubernetes  versions lower than 1.16. Fixes an issue that prevented multiple  Ops Manager  resources from\nhaving the same name in different namespaces. Fixes an issue that caused new MongoDB resources created with\n spec.backup.mode=disabled  to fail. Fixes an issue with saving changes on the  S3 Store  page. Fixes an issue that changed the replica set status to  Fail ,\nincreased the replica set members, and disabled  TLS (Transport Layer Security) . When you use remote or hybrid mode, and set\n automation.versions.download.baseUrl , you must set the\n automation.versions.download.baseUrl.allowOnlyAvailableBuilds \nproperty to  false .  Ops Manager  4.4.11 fixes this issue. Released 2020-02-05 Fixes errors in the CSV (This only effects the Red Hat market) You can't use MongoDB 4.4 as an application database for an  Ops Manager \nresource. You can find all images in the following registries: mongodb-enterprise-operator:1.9.2 Ubuntu-based images :  https://quay.io/repository/mongodb RHEL-based images :  /mongodb-enterprise-operator Released 2020-01-15 Fixes an issue where you could not specify the\n service-account-name  in the  StatefulSet   podSpec \noverride. Removes the unnecessary  delete service  permission from Operator\nrole. Fixes an issue where removing the\n privileges  array in\n spec.security.roles  caused the resource to\nenter a bad state. This release introduces: A new Application Database image,\n mongodb-enterprise-appdb:10.2.15.5958-1_4.2.11-ent . The image\nincludes MongoDB  4.2.11-ent  instead of\n 4.2.2-ent . You must push the new image to any private\nrepositories that your  Kubernetes Operator  installation uses, otherwise\nthe  MongoDBOpsManager  resource won't start. A new required environment variable,\n APPDB_AGENT_VERSION . If you don't set  APPDB_AGENT_VERSION ,\nthe  MongoDBOpsManager  resource can't fetch the MongoDB Agent\nversion for the Application Database. You can't use MongoDB 4.4 as an application database for an  Ops Manager \nresource. The  Ops Manager  user now has  backup ,  restore  and  hostManager  roles, allowing for backups\nand restores on the Application Database. If you omit  spec.applicationDatabase.version , the\n Kubernetes Operator  uses  4.2.11-ent  as the default MongoDB version. You can find all images in the following registries: mongodb-enterprise-operator:1.9.1 mongodb-enterprise-appdb:10.2.15.5958-1_4.2.11-ent mongodb-enterprise-init-appdb:1.0.2 mongodb-enterprise-init-database:1.0.6 Ubuntu-based images :  https://quay.io/repository/mongodb RHEL-based images :  https://catalog.redhat.com/software/containers/mongodb/enterprise-operator/5b8052d069aea356ff258479 Released 2020-12-08 Fixes an issue where the  Kubernetes Operator  didn't close connections to\n Ops Manager , causing too many open file descriptors. You can now configure continuous backup for a MongoDB database\nresource in its  CustomResourceDefinition . To enable continuous backup in the MongoDB  CustomResourceDefinition , you must\n enable backup  in an  Ops Manager \ninstance that you deployed using the  Kubernetes Operator . You can't use MongoDB 4.4 as an application database for an  Ops Manager \nresource. When you upgrade the  Kubernetes Operator  to this version, the\n Kubernetes Operator  deletes and re-creates the Backup Daemon statefulset. This is a safe operation. The new  Kubernetes  service that enables Queryable Backups requires a change\nto the  matchLabels  Backup Daemon  StatefulSet  attribute. The  Kubernetes Operator  changes the way it collects the status of\nMongoDB Agents in Application Database  Pods . You can find all images in the following registries: mongodb-enterprise-operator:1.9.0 Ubuntu-based images :  https://quay.io/repository/mongodb RHEL-based images :  https://catalog.redhat.com/software/containers/mongodb/enterprise-operator/5b8052d069aea356ff258479 Released 2020-11-16 You can't use MongoDB 4.4 as an application database for an  Ops Manager \nresource. Fixes an issue where the  Ops Manager  resource would reach a  Failing \nstate when both  spec.externalConnectivity  and\n spec.backup.enabled  were enabled. Released 2020-11-13 You can't use MongoDB 4.4 as an application database for an  Ops Manager \nresource. When both  spec.externalConnectivity  and\n spec.backup.enabled  are enabled in  Ops Manager  at the same\ntime, the  Ops Manager  resource fails to reconcile. Fixes a bug where\n spec.security.authentication.ignoreUnknownUsers  could not\nbe modified after creating a MongoDB resource. Fixes failed queryable backups. The  Kubernetes Operator  now creates a\n Kubernetes  Service that  Ops Manager  uses to access backups. Fixes an issue that made it impossible to move from non- TLS (Transport Layer Security)  to a\n TLS (Transport Layer Security) -enabled Application Database. Init containers do not run as root. Ops Manager  Backup daemon runs in unprivileged mode. To manage Database Pod resources, use the\n spec.podSpec.podTemplate  MongoDB Custom Resource attribute.\nFor an example resource definition of each supported type, see the\n samples/mongodb/podspec \ndirectory. The following attributes are deprecated: spec.podSpec.cpu spec.podSpec.cpuRequests spec.podSpec.memory spec.podSpec.memoryRequests Init-database 1.0.1 Ubi Ubuntu Init-ops-manager 1.0.3 Ubi Ubuntu Init-appdb 1.0.5 Ubi Ubuntu For a list of the packages installed and any security vulnerabilities\ndetected in the build process, see the Quay repository for the\n MongoDB Enterprise Operator \nand the  MongoDB Enterprise Database . Version 4.4.5 Ubi Ubuntu Version 4.2.21 Ubi Ubuntu Version 4.2.20 Ubi Ubuntu Released 2020-09-30 The MongoDB Enterprise Database image now requires an init container.\nIf you are using a private repository, you must set the  INIT_DATABASE_IMAGE_REPOSITORY \nenvironment variable in the Operator deployment, and the new\ninit container must exist inside this repository. Introduces new configuration fields: spec.security.authentication.requireClientTLSAuthentication \nfor using the MongoDB Agent client certificate authentication in\nconjunction with any other authentication mechanism. spec.security.authentication.agents.clientCertificateSecretRef \nfor configuring the client TLS certificate used by the MongoDB\nAgent when enabling ClientTLSAuthentication. Changes the default permissions of volumes created from secrets from  0644 \nto  0640 . Allows the Application Database to be configured with SCRAM-SHA-256\nauthentication when using  Ops Manager  4.4 or newer version. Changes the validation of the  Ops Manager   spec.version  field\nto allow for tags that do not match the  semver \nrequirements. The  spec.version  field must start with the\n Major.Minor.Patch  string that represents the  Ops Manager  version. To\nlearn more about this field, see  Ops Manager Resource Specification . Fixes an issue that caused the Operator to choose an incorrect project\nname when creating MongoDB users. Fixes an issue that caused the MongoDB  Ops Manager  CRD to have the CA\npath in the incorrect location. Fixes a bug where the MongoDB Agent could not correctly recognize the\nparameters that passed through  spec.agent.startupOptions . Fixes an issue that could cause potential deadlock when certain\nconfiguration options are modified in parallel. You can't use MongoDB 4.4 as an application database for an  Ops Manager \nresource. When you enable queryable backup, you must manually create two\nadditional services for: Exposing the queryable backup port (default: 25999) for the  Ops Manager \npod. The Backup Daemon pod, to ensure that it is resolvable from the  Ops Manager  pod. If you deploy  Ops Manager  in\n local mode  and upgrade from\nv4.4.1, you must upgrade the MongoDB tools located in the\n automation.versions.directory , which defaults to\n /mongodb-ops-manager/mongodb-releases/ . Configure an  Ops Manager  Resource to use Local Mode . Released 2020-09-02 Supports setting the Distinguished Name (DN) of the LDAP group to\nwhich the MongoDB Agent user belongs with the\n spec.security.authentication.ldap.automationLdapGroupDN \nsetting. Requires you to provide\n spec.security.authentication.agents.mode  if you specify\nmore than one mode in  spec.security.authentication.modes . Supports setting MongoDB Agent startup parameters for MongoDB Database\nresources with the following settings: spec.applicationDatabase.agent.startupOptions spec.agent.startupOptions spec.configSrv.agent.startupOptions spec.mongos.agent.startupOptions spec.shard.agent.startupOptions Ops Manager  resources: Fixes a bug where you could not enable  SCRAM-SHA \nauthentication for application database resources using certain\nMongoDB versions with  Ops Manager  4.4. Fixes a bug where application database monitoring was not correctly\nconfigured in  Ops Manager  when you enabled  TLS (Transport Layer Security)  for the application\ndatabase. Fixes a bug to move the  Ops Manager   CA (Certificate Authority)  configuration from\n spec.applicationDatabase.security.tls.ca  to\n spec.security.tls.ca . MongoDB resources: Fixes a bug that prevented you from increasing or decreasing the number\nof members in a replica set or a sharded cluster by more than one\nmember at a time for MongoDB 4.4 deployments. Fixes an issue where the  Kubernetes Operator  could not enable agent\nauthentication if you enabled  LDAP  authentication for a MongoDB\nresource. Fixes an issue where you could not create  SCRAM  users and enable\n SCRAM  authentication in any order for a MongoDB resource. Fixes an issue where the  Kubernetes Operator  did not remove the backup\nautomation configuration before starting the agent on a MongoDB\nresource  Pod . If you enable  TLS (Transport Layer Security)  on the application database, you must not provide the\n spec.applicationDatabase.version  field in an  Ops Manager \nresource definition. You can't use MongoDB 4.4 as an application database for an  Ops Manager \nresource. When you upgrade to the  Kubernetes Operator  1.7.1, you might have to delete\nthe  mongodb-enterprise-operator  deployment due to deployment\nconfiguration changes. This is a safe operation. Deleting the\n mongodb-enterprise-operator  Pod does not affect the MongoDB\n custom resources . If you use  TLS (Transport Layer Security)  certificates signed by a custom  CA (Certificate Authority) , you must: Omit the  spec.version.applicationDatabase  setting from\nyour  Ops Manager  resource definition, and Deploy  Ops Manager  in  local mode . You must manually copy\ninstallation archives for all MongoDB versions you want to use to\na  Persistent Volume  for the  Ops Manager  StatefulSet. Released 2020-08-14 Kubernetes Operator  1.7.x is the final minor version release series that\nsupports OpenShift 3.11. Do not upgrade to any future major or minor\nversion releases if you want to continue to deploy the  Kubernetes Operator \nusing OpenShift 3.11. The planned end of life for the  Kubernetes Operator  1.7.x release series\nis July 2021. All  Kubernetes Operator  Red Hat Docker images are now based on UBI 8. In\nthe previous release,  Kubernetes Operator  Red Hat Docker images were based\non UBI 7. Supports LDAP as an authorization mechanism for MongoDB database\nresources you deploy with the  Kubernetes Operator . For more information,\nsee the sample LDAP configurations on  GitHub Fixes a bug that prevented scaling down a replica set from three\nmembers to one member. Ops Manager  cannot monitor Application Databases secured using  TLS (Transport Layer Security) . For MongoDB 4.4 deployments, you can increase or decrease the number\nof members in a replica set or a sharded cluster by only one member\nat a time. Released 2020-07-30 Ops Manager  image for version 4.4.0 is available. The Red Hat  database  and  operator  Docker images are now based\non the latest UBI 7 release. Two high criticality issues have been\nresolved. The following Docker images have been released: Image Type Ubuntu 16.04 Red Hat UBI 7 Kubernetes Operator quay.io/mongodb/mongodb-enterprise-operator:1.6.1 quay.io/mongodb/mongodb-enterprise-operator-ubi:1.6.1 MongoDB Database quay.io/mongodb/mongodb-enterprise-database:1.6.1 quay.io/mongodb/mongodb-enterprise-database-ubi:1.6.1 Ops Manager quay.io/mongodb/mongodb-enterprise-ops-manager:4.4.0 quay.io/mongodb/mongodb-enterprise-ops-manager-ubi:4.4.0 Fixes a bug where the  Kubernetes Operator  did not store a configuration of\nyour deployed resources in a  secret . Fixes a bug where the  Kubernetes Operator  did not allow passwords of any\nlength or complexity for Application Database, oplog store, and\nblockstore database resources defined in  Ops Manager  resources. Fixes a bug where the authentication configuration was not removed\nfrom  Ops Manager  or  Cloud Manager  projects when you remove a MongoDB\ndatabase resource. Released 2020-07-16 Supports LDAP as an authentication mechanism for MongoDB database\nresources you deploy with the  Kubernetes Operator . For more information,\nsee the sample LDAP configurations on  GitHub . LDAP authorization is not yet supported. Preserves backup history by retaining  Ops Manager  cluster records when\nyou enable backup. Fixes a bug that prevented the  Kubernetes Operator  from raising errors when\na  projectName  contained spaces. Fixes a bug that prevented  Ops Manager  to monitor for all MongoDB\ndatabase resources that you deploy with the  Kubernetes Operator . Released 2020-07-02 Provides additional options for more granular configuration of\n mongod  /  mongos  processes. You can find an example of how to apply\nthese options in the  /samples/mongodb/mongodb-options  file of the\n MongoDB Enterprise Kubernetes Operator repository . Fixes a bug introduced in 1.5.4 where  MongoDB Enterprise Kubernetes Operator  would not tag\nprojects correctly when working on  Ops Manager  versions older than 4.2.2.\nIn this version,  MongoDB Enterprise Kubernetes Operator  tags the projects correctly. Released 2020-06-22 Allows modification of authentication settings using the  Cloud Manager or Ops Manager  UI if\nthe  spec.security.authentication  setting is not provided\nin the MongoDB resource object definition. Supports Helm  installation  with\n helm install  in addition to  helm template | kubectl apply .\n helm install  is now the recommended way to install with Helm. Supports configuring the MongoDB Agent authentication mechanism\nindependently from the cluster authentication mechanism. Supports configuring monitoring for the Application Database to send\nmetrics to  Ops Manager . To learn more about the monitoring function of\nthe MongoDB Agent, see\n MongoDB Agent . Fixes a bug that affected transitioning authentication mechanisms\nfrom X.509 to SCRAM. Fixes a bug that prevented the MongoDB Agent from reaching a goal\nstate if SCRAM configuration was changed in the  Ops Manager  UI. Released 2020-05-29 Passes  Ops Manager  and MongoDB deployment configuration properties as\n Secret environment variables . Correctly configures shutdown timeouts for  Ops Manager  and the Backup\nDaemon. Fixes an issue where  Kubernetes Operator -watched Secrets and ConfigMaps\ntriggered unnecessary reconciliations. Fixes an issue where the status of custom resources failed to update\nin OpenShift 3.11. Released 2020-05-08 Runs  Ops Manager  and Backup Daemon pods under a dedicated service\naccount. Can configure the  Kubernetes Operator  to watch a subset of provided\n CustomResourceDefinitions . You can find more information in the documentation. Can generate  CustomResourceDefinitions  without using subresources. Some versions of\nOpenshift 3.11 require this capability. To avoid using subresources,\nuse  --set subresourceEnabled=false  when installing the\n Kubernetes Operator  with helm. Fixes setting the  spec.statefulSet  and\n spec.backup.statefulSet  fields on the\n MongoDBOpsManager  Resource. Fixes an issue that requires a restart of the  Kubernetes Operator  during\nsetup of webhook. Fixes an issue that could make an  Ops Manager  resource to reach an\nunrecoverable state if the provided admin password has insufficient\nstrength. Released 2020-04-30 Deprecates the generation of  TLS (Transport Layer Security)  certificates by the  Kubernetes Operator .\nIf you use  Kubernetes Operator -generated certificates, warning messages now\nappear in the  Kubernetes Operator  logs. To configure secure deployments, see\n Secure Client Connections . Fixes an issue where, when no authentication is configured by the\n Kubernetes Operator , the  Kubernetes Operator  disables authentication in  Cloud Manager or Ops Manager .\nThe  Kubernetes Operator  no longer disables authentication unless you\nexplicitly set  spec.security.authentication.enabled  to\n false . When you configure the\n spec.statefulSet.spec  and\n spec.backup.statefulSet.spec  settings of the\n MongoDBOpsManager  resource, you can only\nconfigure the  spec.statefulSet.spec.template  and\n spec.backup.statefulSet.spec.template  fields. Any other\n spec.statefulSet.spec  or\n spec.backup.statefulSet.spec  field has no effect. Released 2020-04-24 Adds the ability to start the  Kubernetes Operator  with some but not all\nMongoDB  CustomResourceDefinitions  installed. Administrators can specify the container\nargument  watch-resource  to limit the  Kubernetes Operator  to deploy either\nMonogDB instances or  Ops Manager , or both. Adds the following new  Kubernetes Operator  configuration properties: When using a private docker registry, these properties must point\nto the relevant registries after you copy the images from the MongoDB distribution channels. INIT_OPS_MANAGER_IMAGE_REPOSITORY INIT_APPDB_IMAGE_REPOSITORY APPDB_IMAGE_REPOSITORY Increases support for custom  TLS (Transport Layer Security)  certificates with the\n spec.security.tls.secretRef  and  spec.security.tls.ca \nconfiguration settings. Deprecates  TLS (Transport Layer Security)  certificate generation by the  Kubernetes Operator .\nMigrating to custom  TLS (Transport Layer Security)  certificates is recommended. See the  sample YAML files \nfor new feature usage examples. Releases the  MongoDBOpsManager  resource as\nGenerally Available (GA). MongoDB now supports using the  Kubernetes Operator \nto deploy  Ops Manager  resources to  Kubernetes  in production environments. Supports Backup Blockstore Snapshot Stores. Defaults to the Application Database as a metadata database for Backup\n S3 (Simple Storage Service)  Snapshot Stores. Supports  spec.jvmParameters  and  spec.backup.jvmParameters  to add or\noverride JVM parameters in  Ops Manager  and Backup Daemon processes. Automatically configures  Ops Manager  and Backup Daemon JVM memory\nparameters based on Pod memory availability. Supports  TLS (Transport Layer Security)  for  Ops Manager  and the Application Database. Adds more detailed information to the  status  field. Supports  Ops Manager  Local Mode for  MongoDBOpsManager  resources with\nmultiple replicas by enabling users to specify\n PersistentVolumeClaimTemplates  in  spec.statefulSet.spec . Implements a new image versioning scheme. Removes the  spec.podSpec  configuration setting. Use\n spec.statefulSet.spec  instead. Removes the  spec.backup.podSpec  configuration setting. Use\n spec.backup.statefulSet.spec  instead. Fixes CVE-2020-7922:  Kubernetes  Operator generates potentially insecure certificates. X.509 certificates generated by the  MongoDB Enterprise Kubernetes Operator  may allow an attacker with\naccess to the  Kubernetes  cluster improper access to MongoDB instances.\nCustomers who do not use X.509 authentication, and those who do not use\nthe  Kubernetes Operator  to generate their X.509 certificates are unaffected. CWE-295: Improper Certificate Validation\nCVSS score: 6.4\nCVSS:3.1/AV:A/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:N 1.0, 1.1 1.2.0 - 1.2.4 1.3.0 - 1.3.1 1.4.0 - 1.4.4 1.2.5 1.4.5 and above Supports changes in the  Cloud Manager API . Properly terminates resources with a termination hook. Implements stricter validations. MongoDB resources: Fixes an issue when working with  Ops Manager  with custom  HTTPS (Hypertext Transfer Protocol Secure) \ncertificates. Released 2020-02-24 Adds a  webhook  to validate\na  Kubernetes Operator  configuration. Adds support for sidecars for  MongoDB  resource  pods using the\n spec.podSpec.podTemplate  setting. Allows users to change the  PodSecurityContext  to allow privileged\nsidecar containers. Adds the  spec.podSpec  configuration settings for\n Ops Manager , the Backup Daemon, and the Application Database. See\n Ops Manager Resource Specification . Ops Manager  image for version 4.2.8 is available. See the  sample YAML files  for new\nfeature usage examples. MongoDB resources: Fixes potential race conditions when deleting  MongoDB  resources . Ops Manager  resources: Supports the  spec.clusterDomain  setting for  Ops Manager \nand Application Database resources. No longer starts monitoring and backup processes for the Application\nDatabase. Released 2020-01-24 Runs MongoDB database  Kubernetes  Pods under a dedicated  Kubernetes  service\naccount:  mongodb-enterprise-database-pods . Adds the  spec.podSpec.podTemplate  setting, which allows\nyou to apply templates to  Kubernetes  Pods that the  Kubernetes Operator \ngenerates for each database  StatefulSet . Renames the  spec.clusterName  setting to\n spec.clusterDomain . Adds  offline mode support  for the Application\nDatabase. Bundles MongoDB Enterprise version 4.2.2 with the\nApplication Database image. Internet access is not required to\ninstall the application database if\n spec.applicationDatabase.version  is set to\n \"4.2.2-ent\"  or omitted. Renames the  spec.clusterName  setting to\n spec.clusterDomain . Ops Manager  images for versions 4.2.6 and 4.2.7 are available. See the  sample YAML files  for new\nfeature usage examples. MongoDB resources: Fixes the order of sharded cluster component creation. Allows  TLS (Transport Layer Security)  to be enabled on Amazon EKS. Ops Manager  resources: Enables the  Kubernetes Operator  to use the  spec.clusterDomain  setting. Released 2019-12-13 Includes  CVE fixes  and\n RHSA security fixes . Fixes an issue that prevented backup from starting on MongoDB 4.0. Released 2019-12-09 Adds split horizon DNS support for MongoDB replica sets, which allows\nclients to connect to a replica set from outside of the  Kubernetes \ncluster. Supports requests for  Kubernetes Operator -generated certificates for\nadditional certificate domains, which makes them valid for the\nspecified subdomains. For more information on how to enable new features, see the sample YAML\nfiles in the  samples directory . Promotes the  MongoDBOpsManager   resource  to Beta.  Ops Manager  version\n4.2.4 is available. Supports Backup and restore in  Kubernetes Operator -deployed  Ops Manager \ninstances. This is a semi-automated process that deploys everything\nyou need to enable backups in  Ops Manager . You can enable Backup by\nsetting the  spec.backup.enabled  setting in the  Ops Manager \ncustom resource. You can configure the Head Database, Oplog Store, and\nS3 Snapshot Store by using the  MongoDBOpsManager   resource\nspecification . Supports access to  Ops Manager  from outside the  Kubernetes \ncluster through the  spec.externalConnectivity  setting. Enables SCRAM-SHA-1 authentication  on  Ops Manager 's\nApplication Database by default. Adds support for OpenShift (Red Hat UBI Images). Improves overall stability of X.509 user management. Released 2019-11-08 This release introduces significant changes that may not be\ncompatible with previous deployments or resource configurations.\n Migrate to one resource per project  before\nupgrading the  Kubernetes Operator . Requires one MongoDB resource per  Ops Manager  project. If you\nhave more than one MongoDB resource in a project, all resources will\nchange to a  Pending  status and the  Kubernetes Operator  won\u2019t perform\nany changes on them. The existing MongoDB databases will still be\naccessible. You must  migrate to one resource per project . Supports  SCRAM-SHA  authentication mode. See  the MongoDB\nEnterprise Kubernetes Operator GitHub repository \nfor examples. Requires that the project ( ConfigMap ) and\ncredentials ( secret )\nreferenced from a MongoDB resource be in the same namespace. Adds OpenShift installation files ( YAML (Yet Another Markup Language)  file and Helm chart\nconfiguration). Supports highly available  Ops Manager resources  by introducing the  spec.replicas \nsetting. Runs  Pods  as a non-root user. Released 2019-10-25 This release introduces significant changes that may not be\ncompatible with previous deployments or resource configurations.\n Migrate to one resource per project \nbefore installing or upgrading the  Kubernetes Operator . Moves to a  one resource per project configuration .\nThis follows the warnings introduced in a  previous version of the operator .\nThe operator now requires each cluster to be contained within a new project. Authentication settings are now contained within the\n security section  of the MongoDB resource\nspecification rather than the project ConfigMap. Replaces the  project  field with the\n spec.opsManager.configMapRef.name  or\n spec.cloudManager.configMapRef.name  fields. User resources  now refer to MongoDB\nresources rather than project ConfigMaps. No longer requires  data.projectName  in the project ConfigMap. The\nname of the project defaults to the name of the MongoDB resource in\n Kubernetes . This release introduces significant changes to the  Ops Manager  resource's\narchitecture. The  Ops Manager  application database is now managed by\nthe  Kubernetes Operator , not by  Ops Manager . Stops unnecessary recreation of NodePorts. Fixes logging so it's always in JSON format. Sets  USER  in the  Kubernetes Operator  Docker image. Fixes CVE-2020-7922:  Kubernetes  Operator generates potentially insecure\ncertificates. X.509 certificates generated by the  MongoDB Enterprise Kubernetes Operator  may allow an attacker with\naccess to the  Kubernetes  cluster improper access to MongoDB instances.\nCustomers who do not use X.509 authentication, and those who do not use\nthe  Kubernetes Operator  to generate their X.509 certificates are unaffected. CWE-295: Improper Certificate Validation\nCVSS score: 6.4\nCVSS:3.1/AV:A/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:N 1.0, 1.1 1.2.0 - 1.2.4 1.3.0 - 1.3.1 1.4.0 - 1.4.4 1.2.5 1.4.5 and above Released 2019-10-02 Increases stability of Sharded Cluster deployments. Improves internal testing infrastructure. Released 2019-09-13 Update:  The  MongoDB Enterprise Kubernetes Operator  will remove support for multiple\nclusters per project in a future release. If a project contains more\nthan one cluster, a warning will be added to the status of the\nMongoDB Resources. Additionally, any new cluster being added to a\nnon-empty project will result in a  Failed  state, and won't\nbe processed. Fix:  The overall stability of the operator has been improved. The\noperator is now more conservative in resource updates both on\n Kubernetes  and  Cloud Manager or Ops Manager . Released 2019-08-30 Security Fix:  Clusters configured by  Kubernetes Operator  versions\n1.0 through 1.2.1 used an insufficiently strong keyfile for internal\ncluster authentication between  mongod  processes. This only affects\nclusters which are using X.509 for user authentication, but are not\nusing X.509 for internal cluster authentication. Users are advised to\nupgrade to version 1.2.2, which will replace all managed keyfiles. Security Fix:  Clusters configured by  Kubernetes Operator  versions 1.0\nthrough 1.2.1 used an insufficiently strong password to authenticate\nthe MongoDB Agent. This only affects clusters which have been manually\nconfigured to enable  SCRAM-SHA-1 , which is not a supported\nconfiguration. Users are advised to upgrade to version 1.2.2, which\nwill reset these passwords. Released 2019-08-23 Fix:  The  Kubernetes Operator  no longer recreates  CSRs (Certificate Signing Requests)  when X.509\nauthentication is enabled and the approved  CSRs (Certificate Signing Requests)  have been deleted. Fix:  If the  OPERATOR_ENV  environment variable is set to\nsomething unrecognized by the  Kubernetes Operator , it will no longer result\nin a  CrashLoopBackOff  of the pod. A default value of  prod  is\nused. The  Kubernetes Operator  now supports more than 100 agents in a given\nproject. Released 2019-08-13 Adds a\n readinessprobe \nto the MongoDB Pods to improve the reliability of rolling upgrades. This feature is an alpha release. It is not ready for production use. Can use the  Kubernetes Operator  to manage  Ops Manager  4.2. To\n deploy an |onprem| instance ,\nyou use a new  resource :  MongoDBOpsManager . Released 2019-07-19 Fix:  Adds sample yaml files, in particular, the attribute related\nto\n featureCompatibilityVersion . Fix:   TLS (Transport Layer Security)  can be disabled in a deployment. Improvement:  Adds\n script \nin the\n support  directory that can gather\ninformation of your MongoDB resources in Kubernetes. Improvement:  In a  TLS (Transport Layer Security)  environment, the  Kubernetes Operator  can use a\ncustom  CA (Certificate Authority) . All the certificates must be passed as  secret \nobjects. Released 2019-06-18 Supports Kubernetes v1.11 or later. Provisions any kind of MongoDB deployment in the Kubernetes Cluster\nof your Organization: Standalone Replica Set Sharded Cluster Configures  TLS (Transport Layer Security)  on the MongoDB deployments and encrypt all traffic.\nHosts and clients can verify each other\u2019s identities. Manages MongoDB users. Supports X.509 authentication to your MongoDB databases. To learn how to install and configure the Operator, see  Install and Configure the  Kubernetes Operator . If you have any questions regarding this release, use the\n #enterprise-kubernetes \nSlack channel. Released 2019-06-07 Rolling upgrades of MongoDB resources ensure that\n rs.stepDown()  is called for the primary\nmember. Requires MongoDB patch version 4.0.8 and later or MongoDB\npatch version 4.1.10 and later. During a MongoDB major version upgrade, the\n featureCompatibilityVersion  field can be set. Fixed a bug where replica sets with more than seven members could\nnot be created. X.509 Authentication can be enabled at the\n Project level . Requires  Cloud Manager ,\n Ops Manager  patch version 4.0.11 and later, or  Ops Manager  patch version\n4.1.7 and later. Internal cluster authentication based on X.509 can be enabled at the\n deployment  level. MongoDB users with X.509 authentication can be created, using the\nnew  MongoDBUser  custom resource. Released 2019-04-29 NodePort  service creation can be disabled. TLS (Transport Layer Security)  can be enabled for internal authentication between MongoDB in\nreplica sets and sharded clusters. The  TLS (Transport Layer Security)  certificates are created\nautomatically by the  Kubernetes Operator . Refer to the sample\n .yaml  files in the\n GitHub repository \nfor examples. Wide or asterisk roles have been replaced with strict listing of\nverbs in  roles.yaml . Printing  mdb  objects with  kubectl  will provide more\ninformation about the MongoDB object: type, state, and MongoDB server\nversion. Released 2019-04-02 The  Kubernetes Operator  and database images are now based on ubuntu:16.04. The  Kubernetes Operator  now uses a single  CustomResourceDefinition  named  MongoDB \ninstead of the  MongoDbReplicaSet ,  MongoDbShardedCluster , and\n MongoDbStandalone  CRDs. Follow the  upgrade procedure  to\ntransfer existing  MongoDbReplicaSet ,  MongoDbShardedCluster ,\nand  MongoDbStandalone  resources to the new format. For a list of the packages installed and any security vulnerabilities\ndetected in our build process, see: MongoDB Enterprise Operator MongoDB Enterprise Database Released 2019-03-19 The Operator and Database images are now based on\n debian:stretch-slim  which is the latest and up-to-date Docker\nimage for Debian 9. Released 2019-02-26 Perform  Ops Manager  clean-up on deletion of MongoDB resource without the\nuse of finalisers. Bug fix:  Race conditions when communicating with  Ops Manager . Bug fix:   ImagePullSecrets  being incorrectly initialized in\nOpenShift. Bug fix:  Unintended fetching of closed projects. Bug fix:  Creation of duplicate organizations. Bug fix:  Reconciliation could fail for the MongoDB resource if\nsome other resources in  Ops Manager  were in error state. Released 2019-02-01 Improved detailed status field for MongoDB resources. The  Kubernetes Operator  watches changes to configuration parameters in a\nproject configMap and the credentials secret then performs a rolling\nupgrade for relevant Kubernetes resources. Added  JSON (Javascript Object Notation)  structured logging for MongoDB Agent Pods. Support  DNS (Domain Name System)   SRV (Service)  records for MongoDB access. Bug fix: Avoiding unnecessary reconciliation. Bug fix: Improved Ops Manager/Cloud Manager state management for\ndeleted resources. Released 2018-12-17 Refactored code to use the  controller-runtime  library to fix issues\nwhere Operator could leave resources in inconsistent state. This also\nintroduced a proper reconciliation process. Added new  status  field for all MongoDB Kubernetes resources. Can configure Operator to watch any single namespace or all\nnamespaces in a cluster (requires cluster role). Improved database logging by adding a new configuration property\n logLevel . This property is set to  INFO  by default.\nMongoDB Agent and MongoDB logs are merged in to a single log\nstream. Added new configuration Operator timeout. It defines waiting time\nfor database pods start while updating  MongoDB  resources . Fix:  Fixed failure detection for  mongos . Released 2018-11-14 Image for database no longer includes the binary for the Automation\nAgent. The container downloads the Automation Agent binary from\n Ops Manager  when it starts. Fix:  Communication with  Ops Manager  failed if the project with the same\nname existed in different organization. Released 2018-10-04 If a backup was enabled in  Ops Manager  for a Replica Set or Sharded\nCluster that the  Kubernetes Operator  created, then the  Kubernetes Operator \ndisables the backup before removing a resource. Improved persistence support: The data, journal and log directories are mounted to three\nmountpoints in one or three volumes depending upon the\n podSpec.persistence  setting. Prior to this release, only the data directory was mounted to\npersistent storage. Setting Mount Directories to podSpec.persistence.single One volume podSpec.persistence.multiple Three volumes A new parameter,  labelSelector , allows you to specify the\nselector for volumes that  Kubernetes Operator  should consider mounting. If  StorageClass  is not specified in the  persistence \nconfiguration, then the default  StorageClass  for the cluster is\nused. In most of public cloud providers, this results in dynamic\nvolume provisioning. Released 2018-08-07 The Operator no longer creates the CustomResourceDefinition objects.\nThe user needs to create them manually. Download and apply\n this new yaml file \n( crd.yaml ) to create/configure these objects. ClusterRoles are no longer required. How the Operator watches\nresources has changed. Until the last release, the Operator would\nwatch for any resource on any  namespace . With 0.3, the Operator\nwatches for resources in the same namespace in which it was created.\nTo support multiple namespaces, multiple Operators can be installed.\nThis allows isolation of MongoDB deployments. Permissions changes were made to how PersistentVolumes are mounted. Added configuration to Operator to not create\n SecurityContexts \nfor  Pods . This solves an issue with OpenShift which does not\nallow this setting when  SecurityContextContraints  are used. If you are using Helm, set  managedSecurityContext  to  true .\nThis tells the Operator to not create  SecurityContext  for\n Pods , satisfying the OpenShift requirement. The combination of  projectName  and  orgId  replaces\n projectId  alone to configure the connection to  Ops Manager .\nThe project is created if it doesn't exist. Released 2018-08-03 Calculates WiredTiger memory cache. Released 2018-06-27 Initial Release Can deploy standalone instances, replica sets, sharded clusters\nusing  Kubernetes  configuration files.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "spec:\n  additionalMongodConfig:\n    auditLog:\n      destination: file\n      format: JSON\n      path: /var/log/mongodb-mms-automation/mongodb-audit.log"
                },
                {
                    "lang": "yaml",
                    "value": "kubectl logs -c mongodb-enterprise-database replica-set-0 | \\\njq -r 'select(.logType == \"mongodb-audit\") | .contents'"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdbmc <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "The secret object 'mdb-multi-rs-cert' does not contain all the valid\ncertificates needed: secrets \"mdb-multi-rs-cert-pem\" already exists"
                },
                {
                    "lang": "sh",
                    "value": "Failed to pull image \"quay.io/mongodb/mongodb-agent-ubi@sha256:a4cadf209ab87eb7d121ccd8b1503fa5d88be8866b5c3cb7897d14c36869abf6\": rpc error: code = Unknown desc = reading manifest sha256:a4cadf209ab87eb7d121ccd8b1503fa5d88be8866b5c3cb7897d14c36869abf6 in quay.io/mongodb/mongodb-agent-ubi: manifest unknown: manifest unknown"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n config:\n   env:\n     - name: AGENT_IMAGE\n       value: >-\n       quay.io/mongodb/mongodb-agent-ubi@sha256:ffa842168cc0865bba022b414d49e66ae314bf2fd87288814903d5a430162620\n     - name: RELATED_IMAGE_AGENT_IMAGE_11_0_5_6963_1\n       value: >-\n       quay.io/mongodb/mongodb-agent-ubi@sha256:e7176c627ef5669be56e007a57a81ef5673e9161033a6966c6e13022d241ec9e\n     - name: RELATED_IMAGE_AGENT_IMAGE_11_12_0_7388_1\n       value: >-\n       quay.io/mongodb/mongodb-agent-ubi@sha256:ffa842168cc0865bba022b414d49e66ae314bf2fd87288814903d5a430162620\n     - name: RELATED_IMAGE_AGENT_IMAGE_12_0_4_7554_1\n       value: >-\n       quay.io/mongodb/mongodb-agent-ubi@sha256:3e07e8164421a6736b86619d9d72f721d4212acb5f178ec20ffec045a7a8f855"
                }
            ],
            "preview": "Released 2023-12-21",
            "tags": null,
            "facets": {
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "reference/k8s-op-exclusive-settings",
            "title": "MongoDB Kubernetes Operator Exclusive Settings",
            "headings": [
                "Kubernetes Operator Overrides Some Ops Manager Settings"
            ],
            "paragraphs": "At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . Some settings that you configure using  Kubernetes Operator  cannot be set or\noverridden in the  Ops Manager Application . Settings that the  Kubernetes Operator  does\nnot manage are accepted. The following list of settings are exclusive to  Kubernetes . This list may\nchange at a later date. These settings can be found on the\n automation configuration \npage: In addition to the list of Automation settings, the  Kubernetes Operator  uses attributes\noutside of the deployment from the Monitoring and Backup Agent configurations. auth.autoAuthMechanisms auth.authoritativeSet auth.autoPwd auth.autoUser auth.deploymentAuthMechanisms auth.disabled auth.key auth.keyfile auth.keyfileWindows auth.usersWanted auth.usersWanted[n].mechanisms auth.usersWanted[n].roles auth.usersWanted[n].roles[m].role auth.usersWanted[n].roles[m].db auth.usersWanted[n].user auth.usersWanted[n].authenticationRestrictions processes.args2_6.net.port processes.args2_6.net.tls.certificateKeyFile processes.args2_6.net.tls.clusterFile processes.args2_6.net.tls.PEMKeyFile processes.args2_6.replication.replSetName processes.args2_6.sharding.clusterRole processes.args2_6.security.clusterAuthMode processes.args2_6.storage.dbPath processes.args2_6.systemLog.destination processes.args2_6.systemLog.path processes.authSchemaVersion processes.cluster  (mongos processes) processes.featureCompatibilityVersion processes.hostname processes.name processes.version replicaSets._id replicaSets.members._id replicaSets.members.host replicaSets.members replicaSets.version sharding.clusterRole  (config server) sharding.configServerReplica sharding.name sharding.shards._id sharding.shards.rs ssl.CAFilePath ssl.autoPEMKeyFilePath ssl.clientCertificateMode backupAgentTemplate.username backupAgentTemplate.sslPEMKeyFile monitoringAgentTemplate.username monitoringAgentTemplate.sslPEMKeyFile Kubernetes Operator  creates a replica set of 3 members. You changed  storage.wiredTiger.engineConfig.cacheSizeGB \nto  40 . This setting is not in the  Kubernetes Operator  exclusive settings\nlist. You then use the  Kubernetes Operator  to scale the replica set to\n5 members. The  storage.wiredTiger.engineConfig.cacheSizeGB  on the\nnew members should still be  40 .",
            "code": [],
            "preview": "Some settings that you configure using Kubernetes Operator cannot be set or\noverridden in the Ops Manager Application. Settings that the Kubernetes Operator does\nnot manage are accepted.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "reference/helm-operator-settings",
            "title": "MongoDB Enterprise Kubernetes Operator Helm Installation Settings",
            "headings": [
                "appDb.name",
                "appDb.version",
                "database.name",
                "database.version",
                "initAppDb.name",
                "initAppDb.version",
                "initDatabase.name",
                "initDatabase.version",
                "initOpsManager.name",
                "initOpsManager.version",
                "managedSecurityContext",
                "mongodb.appdbAssumeOldFormat",
                "mongodb.imageType",
                "multiCluster.clusterClientTimeout",
                "namespace",
                "needsCAInfrastructure",
                "operator.deployment_name",
                "operator.env",
                "operator.name",
                "operator.version",
                "operator.watchNamespace",
                "operator.watchedResources",
                "opsManager.name",
                "registry.appDb",
                "registry.imagePullSecrets",
                "registry.initAppDb",
                "registry.initOpsManager",
                "registry.operator",
                "registry.opsManager",
                "subresourceEnabled"
            ],
            "paragraphs": "To provide optional settings, pass them to Helm using the  --set  argument.\nUse the following files that list value settings for your deployment type: To learn about optional  Kubernetes Operator  installation settings,\nsee  Operator Helm Installation Settings . Run the command as in the following example and the options that you\nspecified will be passed to your configuration: Vanilla  Kubernetes :  values.yaml OpenShift:  values-openshift.yaml Name of the Application Database image. The default value is  mongodb-enterprise-appdb . Version of the image that contains the MongoDB Agent that the Application\nDatabase uses. The default value is 10.2.15.5958-1_4.2.11-ent. Name of the MongoDB Enterprise Database image. The default value is  mongodb-enterprise-database . Version of the MongoDB Enterprise Database image that the  Kubernetes Operator \ndeploys. Name of the  initContainer  image that contains the Application\nDatabase start-up scripts and the readiness probe. The default value is  mongodb-enterprise-init-appdb . Version of the  initContainer  image that contains the Application\nDatabase start-up scripts and the readiness probe. The default value is  1.24 . Name of the  initContainer  image that contains the MongoDB Agent\nstart-up scripts and the readiness probe. The default value is  mongodb-enterprise-init-database . Version of the  initContainer  image that contains the MongoDB Agent\nstart-up scripts and the readiness probe. The default value is  1.24 . Version of the  initContainer  image that contains the  Ops Manager \nstart-up scripts and the readiness probe. The default value is  mongodb-enterprise-init-ops-manager . Version of the  initContainer  image that contains the  Ops Manager \nstart-up scripts and the readiness probe. The default value is  1.24 . Flag that determines whether or not the  Kubernetes Operator  inherits the\n securityContext  settings that your  Kubernetes  cluster manages. This value must be  true  if you want to run the  Kubernetes Operator \nin OpenShift or in a restrictive environment. The default value is  false . The default value is  true . The default value is  false , which automatically updates the image suffix. In  Kubernetes Operator  1.20, the  container registry  changed for the  application database  image and the images use a new tag suffix. When you  upgrade the Kubernetes Operator , the  Kubernetes Operator  automatically updates the earlier suffix,  -ent , for all images that reference the new container registry to  -ubi8  or the suffix set in  MDB_IMAGE_TYPE  or  mongodb.imageType . For example, the  Kubernetes Operator  changes  quay.io/mongodb/mongodb-enterprise-server:4.4.5-ent  to  quay.io/mongodb/mongodb-enterprise-server:4.4.5-ubi8 . To stop the  Kubernetes Operator  from automatically updating the suffix, set  MDB_APPDB_ASSUME_OLD_FORMAT  or  mongodb.appdbAssumeOldFormat  to  true . For example, you might want to stop the automatic suffix change if you're mirroring this image from your own repository. The suffix of the  application database  image. The default is  ubi8 . We recommend using only  ubi  images for consistency, but if you need to, you can change this setting to  ubuntu2204 . In  Kubernetes Operator  1.20, the  container registry  changed for the  application database  image and the images use a new tag suffix. When you  upgrade the Kubernetes Operator , the  Kubernetes Operator  automatically updates the earlier suffix,  -ent , for all images that reference the new container registry to  -ubi8  or the suffix set in  MDB_IMAGE_TYPE  or  mongodb.imageType . For example, the  Kubernetes Operator  changes  quay.io/mongodb/mongodb-enterprise-server:4.4.5-ent  to  quay.io/mongodb/mongodb-enterprise-server:4.4.5-ubi8 . To stop the  Kubernetes Operator  from automatically updating the suffix, set  MDB_APPDB_ASSUME_OLD_FORMAT  or  mongodb.appdbAssumeOldFormat  to  true . For example, you might want to stop the automatic suffix change if you're mirroring this image from your own repository. Time, in seconds, the  Kubernetes Operator  attempts to connect to a cluster's  Kubernetes API server  endpoint. This timeout is set for all  Kubernetes  clusters in  multi-Kubernetes-cluster deployments . If\nthe  Kubernetes Operator  doesn't get a response from the  Kubernetes  API server within the specified time, it logs the cluster's status as \"unhealthy\". To learn more, see  Troubleshooting Kubernetes Clusters . The default value is  10 . namespaces  in which you want to deploy the  Kubernetes Operator . To use a namespace other than the default, specify the namespace in\nwhich you want to deploy the  Kubernetes Operator . The default value is  mongodb . Flag that determines whether  Kubernetes  creates a  ClusterRole  that allows the\n Kubernetes Operator  to sign  TLS (Transport Layer Security)  certificates using the\n certificates.k8s.io \nAPI. The default value is  true . Name of the  Kubernetes Operator  container. The default value is  mongodb-enterprise-operator . Label for the  Kubernetes Operator s deployment environment. This value\naffects the default timeouts and the logging level and format: The default value is  prod . If the value is Log Level is set to Log Format is set to dev debug text prod info json Name that  Kubernetes  assigns to  Kubernetes Operator  objects, such as Deployments,\nServiceAccounts, Roles, and Pods. This value also corresponds to the name of the container registry where\nthe  Kubernetes Operator  is located. The default value is  mongodb-enterprise-operator . Version of the  Kubernetes Operator  that you want to deploy. The default value is  1.24 . Namespaces that the  Kubernetes Operator  watches for  MongoDB  resource \nchanges. If this  namespace  differs from the default, ensure that\nthe  Kubernetes Operator  ServiceAccount  can access \nthis namespace. The default value is  <metadata.namespace> . To watch  all namespaces , specify  *  and assign the  ClusterRole  to the\n mongodb-enterprise-operator  ServiceAccount that you use to run the\n Kubernetes Operator . To watch a  subset of all namespaces , specify them in a\ncomma-separated list, escape each comma with a backslash,\nand surround the list in quotes, such as\n \"operator.watchNamespace=ns1\\,ns2\" . Watching a subset of namespaces is useful in deployments where a single\n Kubernetes Operator  instance watches a different cluster resource type.\nFor example, you can configure the  Kubernetes Operator  to watch  MongoDB  resources \nin one subset of namespaces, and to watch  MongoDBMultiCluster  resources  in another\nsubset of namespaces. To avoid race conditions during resource reconciliation,\nfor each custom resource type that you want the  Kubernetes Operator  to watch,\nensure that you set scope to a distinct subset of namespaces. To deploy  Ops Manager  and  MongoDB  resources  to one or more  namespaces  other\nthan the one where you deploy the  Kubernetes Operator ,\nsee  Set Scope for  MongoDB Enterprise Kubernetes Operator  Deployment  for values you must use and\nadditional steps you might have to perform. Custom resources that the  Kubernetes Operator  watches. The  Kubernetes Operator  installs the  CustomResourceDefinitions  for and watches only the resources you specify. The  Kubernetes Operator  accepts the following values: Value Description mongodb Install the CustomResourceDefinitions for database resources\nand watch those resources. mongodbusers Install the CustomResourceDefinitions for MongoDB user resources\nand watch those resources. opsmanagers Install the CustomResourceDefinitions for  Ops Manager  resources\nand watch those resources. Name of the  Ops Manager  image. The default value is  mongodb-enterprise-ops-manager . URL (Uniform Resource Locator)  of the repository from which the  Kubernetes Operator  downloads\nthe Application Database image. The default value is  quay.io/mongodb . The default value is  registry.connect.redhat.com/mongodb . secret  that contains the credentials required to pull\nimages from the repository. OpenShift requires this setting. Define it in the\n imagePullSecrets  setting in this file or pass it when you install\nthe  Kubernetes Operator  using Helm.\nIf you use the  Kubernetes Operator  to deploy MongoDB resources to\n multiple namespaces  or with a\n cluster-wide scope , create the secret\nonly in the namespace where you installed the  Kubernetes Operator .\nThe  Kubernetes Operator  synchronizes the secret across all watched\nnamespaces. URL (Uniform Resource Locator)  of the repository from which the  initContainer  image that\ncontains the Application Database start-up scripts and the readiness\nprobe is downloaded. The default value is  quay.io/mongodb . The default value is  registry.connect.redhat.com/mongodb . URL (Uniform Resource Locator)  of the repository from which the  initContainer  image that\ncontains the  Ops Manager  start-up scripts and the readiness probe is\ndownloaded. The default value is  quay.io/mongodb . The default value is  registry.connect.redhat.com/mongodb . Repository from which the  Kubernetes Operator  image is pulled.\nSpecify this value if you want to pull the  Kubernetes Operator  image\nfrom a private repository. The default value is  quay.io/mongodb . The default value is  registry.connect.redhat.com/mongodb . URL (Uniform Resource Locator)  of the repository from which the image for an  Ops\nManager resource  is downloaded. The default value is  quay.io/mongodb . The default value is  registry.connect.redhat.com/mongodb . Flag that indicates whether subresources can be defined in the\n Kubernetes Operator   CustomResourceDefinition . The default value is  true .",
            "code": [
                {
                    "lang": "sh",
                    "value": "helm upgrade enterprise-operator mongodb/enterprise-operator \\\n  --set registry.pullPolicy='IfNotPresent'"
                },
                {
                    "lang": "yaml",
                    "value": "appDb:\n  name: mongodb-enterprise-appdb\n  version: 10.2.15.5958-1_4.2.11-ent"
                },
                {
                    "lang": "yaml",
                    "value": "appDb:\n  name: mongodb-enterprise-appdb\n  version: 10.2.15.5958-1_4.2.11-ent"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-database\n  version: 1.24"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-database\n  version: 1.24"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-appdb\n  version: 1.24"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-appdb\n  version: 1.24"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-database\n  version: 1.24"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-database\n  version: 1.24"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-ops-manager\n  version: 1.24"
                },
                {
                    "lang": "yaml",
                    "value": "database:\n  name: mongodb-enterprise-init-ops-manager\n  version: 1.24"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: true"
                },
                {
                    "lang": "yaml",
                    "value": "mongodb:\n  appdbAssumeOldFormat: false"
                },
                {
                    "lang": "yaml",
                    "value": "mongodb:\n  imageType: ubi8"
                },
                {
                    "lang": "yaml",
                    "value": "multiCluster:\n  clusterClientTimeout: 10"
                },
                {
                    "lang": "yaml",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "needsCAInfrastructure: true"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  deployment_name: mongodb-enterprise-operator"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  # Execution environment for the operator, dev or prod.\n  # Use dev for more verbose logging\n  env: prod"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  name: mongodb-enterprise-operator"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  version: 1.24"
                },
                {
                    "lang": "yaml",
                    "value": "# Watch one namespace\nhelm install enterprise-operator mongodb/enterprise-operator \\\n  --set operator.watchNamespace='namespace-to-watch' <...>"
                },
                {
                    "lang": "yaml",
                    "value": "# Watch both namespace-a and namespace-b\nhelm install enterprise-operator mongodb/enterprise-operator \\\n  --set operator.watchNamespace=\"namespace-a\\,namespace-b\""
                },
                {
                    "lang": "yaml",
                    "value": "# Operator with name `mongodb-enterprise-operator-qa-envs` will\n# watch ns-dev, ns-qa and ns-uat namespaces\n\nhelm install mongodb-enterprise-operator-qa-envs mongodb/enterprise-operator \\\n  --set operator.watchNamespace=\"ns-dev\\,ns-qa\\,ns-uat\""
                },
                {
                    "lang": "yaml",
                    "value": "# Operator with name `mongodb-enterprise-operator-staging` will\n# watch ns-staging and ns-pre-prod\nhelm install mongodb-operator helm-chart --set operator.watchNamespace=\"ns-staging\\,ns-pre-prod\" mongodb-enterprise-operator-staging"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchedResources:\n    - mongodbusers\n    - mongodb\n    - opsmanagers"
                },
                {
                    "lang": "yaml",
                    "value": "opsManager:\n  name: mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n# Specify the secret in the ``imagePullSecrets`` setting. If you\n# use the MongoDB Kubernetes Operator to deploy MongoDB resources\n# into multiple namespaces, create the secret only in the namespace\n# where you installed the Operator. The Operator synchronizes\n# the secret across all watched namespaces.\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "subresourceEnabled: true"
                }
            ],
            "preview": "To provide optional settings, pass them to Helm using the --set argument.\nUse the following files that list value settings for your deployment type:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "reference/operator-settings",
            "title": "MongoDB Enterprise Kubernetes Operator Installation Settings",
            "headings": [],
            "paragraphs": "When you install the  Kubernetes Operator , you can provide optional settings\nthat affect your deployment.  How you provide these settings depends on\nthe environment to which you deploy the  Kubernetes Operator . Review the settings that you can use when you install the\n Kubernetes Operator  using  kubectl  or  oc . Review the settings that you can use when you install the\n Kubernetes Operator  using Helm.",
            "code": [],
            "preview": "When you install the Kubernetes Operator, you can provide optional settings\nthat affect your deployment.  How you provide these settings depends on\nthe environment to which you deploy the Kubernetes Operator.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "reference/kubectl-operator-settings",
            "title": "MongoDB Enterprise Kubernetes Operator kubectl and oc Installation Settings",
            "headings": [
                "APPDB_AGENT_VERSION",
                "APPDB_IMAGE_REPOSITORY",
                "CLUSTER_CLIENT_TIMEOUT",
                "DATABASE_VERSION",
                "IMAGE_PULL_POLICY",
                "INIT_APPDB_IMAGE_REPOSITORY",
                "INIT_APPDB_VERSION",
                "INIT_DATABASE_IMAGE_REPOSITORY",
                "INIT_DATABASE_VERSION",
                "INIT_OPS_MANAGER_IMAGE_REPOSITORY",
                "INIT_OPS_MANAGER_VERSION",
                "MANAGED_SECURITY_CONTEXT",
                "MDB_APPDB_ASSUME_OLD_FORMAT",
                "MDB_AUTOMATIC_RECOVERY_ENABLE",
                "MDB_AUTOMATIC_RECOVERY_BACKOFF_TIME_S",
                "MDB_IMAGE_TYPE",
                "MONGODB_ENTERPRISE_DATABASE_IMAGE",
                "OPERATOR_ENV",
                "OPS_MANAGER_IMAGE_PULL_POLICY",
                "OPS_MANAGER_IMAGE_REPOSITORY",
                "READINESS_PROBE_LOGGER_BACKUPS",
                "READINESS_PROBE_LOGGER_MAX_SIZE",
                "READINESS_PROBE_LOGGER_MAX_AGE",
                "WATCH_NAMESPACE"
            ],
            "paragraphs": "To provide optional settings, edit the  YAML (Yet Another Markup Language)  file that corresponds to\nyour deployment type in the directory where you cloned the\n Kubernetes Operator  repository: If the setting that you want to add doesn't exist in the  YAML (Yet Another Markup Language)  file,\nadd it as a new array of key-value pair mappings in the\n spec.template.spec.containers.name.env.  collection: To fill in values for container images, see  Container Images . Vanilla  Kubernetes  using  kubectl :  mongodb-enterprise.yaml OpenShift using  oc :  mongodb-enterprise-openshift.yaml Set the value of the  spec.template.spec.containers.name.env.name \nkey to the setting's name. Set the value of the  spec.template.spec.containers.name.env.value \nkey to the setting's value. Version of the image that contains the MongoDB Agent that the Application\nDatabase uses. The default value is 10.2.15.5958-1_4.2.11-ent. URL (Uniform Resource Locator)  of the repository from which the  Kubernetes Operator  downloads\nthe Application Database image. Time, in seconds, the  Kubernetes Operator  attempts to connect to a cluster's  Kubernetes API server  endpoint. This timeout is set for all  Kubernetes  clusters in  multi-Kubernetes-cluster deployments . If\nthe  Kubernetes Operator  doesn't get a response from the  Kubernetes  API server within the specified time, it logs the cluster's status as \"unhealthy\". To learn more, see  Troubleshooting Kubernetes Clusters . The default value is  10 . Version of the MongoDB Enterprise Database image that the  Kubernetes Operator \ndeploys. Pull policy  for the MongoDB\nEnterprise database image the  Kubernetes Operator  deploys. The  Kubernetes Operator  accepts the following values:   Always ,\n IfNotPresent ,  Never . The default value is  Always . URL (Uniform Resource Locator)  of the repository from which the  initContainer  image that\ncontains the Application Database start-up scripts and the readiness\nprobe is downloaded. Version of the  initContainer  image that contains the Application\nDatabase start-up scripts and the readiness probe. The default value is  1.24 . URL (Uniform Resource Locator)  of the repository from which the  initContainer  image that\ncontains the MongoDB Agent start-up scripts and the readiness probe is\ndownloaded. Version of the  initContainer  image that contains the MongoDB Agent\nstart-up scripts and the readiness probe. The default value is  1.24 . URL (Uniform Resource Locator)  of the repository from which the  initContainer  image that\ncontains the  Ops Manager  start-up scripts and the readiness probe is\ndownloaded. Version of the  initContainer  image that contains the  Ops Manager \nstart-up scripts and the readiness probe. The default value is  1.24 . Flag that determines whether or not the  Kubernetes Operator  inherits the\n securityContext  settings that your  Kubernetes  cluster manages. This value must be  true  if you want to run the  Kubernetes Operator \nin OpenShift or in a restrictive environment. The default value is  false . The default value is  true . The default value is  false , which automatically updates the image suffix. In  Kubernetes Operator  1.20, the  container registry  changed for the  application database  image and the images use a new tag suffix. When you  upgrade the Kubernetes Operator , the  Kubernetes Operator  automatically updates the earlier suffix,  -ent , for all images that reference the new container registry to  -ubi8  or the suffix set in  MDB_IMAGE_TYPE  or  mongodb.imageType . For example, the  Kubernetes Operator  changes  quay.io/mongodb/mongodb-enterprise-server:4.4.5-ent  to  quay.io/mongodb/mongodb-enterprise-server:4.4.5-ubi8 . To stop the  Kubernetes Operator  from automatically updating the suffix, set  MDB_APPDB_ASSUME_OLD_FORMAT  or  mongodb.appdbAssumeOldFormat  to  true . For example, you might want to stop the automatic suffix change if you're mirroring this image from your own repository. Flag that determines whether or not the  Kubernetes Operator \nenables automatic recovery for  MongoDB  resources\nper Pod. The default value is  true . Number of seconds that a custom resource can remain in a\n Pending  or  Failed  state before the  Kubernetes Operator \nautomatically recovers your  MongoDB  resources. The default value\nis  1200  seconds (20 minutes). To disable automatic recovery, set the\n MDB_AUTOMATIC_RECOVERY_ENABLE \nenvironment variable to  false . The suffix of the  application database  image. The default is  -ubi8 . In  Kubernetes Operator  1.20, the  container registry  changed for the  application database  image and the images use a new tag suffix. When you  upgrade the Kubernetes Operator , the  Kubernetes Operator  automatically updates the earlier suffix,  -ent , for all images that reference the new container registry to  -ubi8  or the suffix set in  MDB_IMAGE_TYPE  or  mongodb.imageType . For example, the  Kubernetes Operator  changes  quay.io/mongodb/mongodb-enterprise-server:4.4.5-ent  to  quay.io/mongodb/mongodb-enterprise-server:4.4.5-ubi8 . To stop the  Kubernetes Operator  from automatically updating the suffix, set  MDB_APPDB_ASSUME_OLD_FORMAT  or  mongodb.appdbAssumeOldFormat  to  true . For example, you might want to stop the automatic suffix change if you're mirroring this image from your own repository. URL (Uniform Resource Locator)  of the MongoDB Enterprise Database image that the  Kubernetes Operator \ndeploys. Label for the  Kubernetes Operator s deployment environment. This value\naffects the default timeouts and the logging level and format: The default value is  prod . If the value is Log Level is set to Log Format is set to dev debug text prod info json Pull policy  for the\n Ops Manager  images the  Kubernetes Operator  deploys. The  Kubernetes Operator  accepts the following values:  Always ,\n IfNotPresent , and  Never . The default value is  Always . URL (Uniform Resource Locator)  of the repository from which the image for an  Ops\nManager resource  is downloaded. The number of rotated  readiness.log  backup files maintained by the\nReadiness Probe. The default value is  5 . The maximum size of the  readiness.log  file in MB. The default value is  100 . The maximum number of days to retain rotated  readiness.log  files,\nstarting with the date in the file timestamp. If set to  0 , the\n Kubernetes Operator  doesn't remove  readiness.log  files on the basis of\nage, instead relying on  READINESS_PROBE_LOGGER_BACKUPS  to determine\nretention. The default value is  0 . Namespaces that the  Kubernetes Operator  watches for  MongoDB  resource \nchanges. If this  namespace  differs from the default, ensure that\nthe  Kubernetes Operator  ServiceAccount  can access \nthis namespace. The default value is  <metadata.namespace> . To watch  all namespaces , specify  *  and assign the  ClusterRole  to the\n mongodb-enterprise-operator  ServiceAccount that you use to run the\n Kubernetes Operator . To watch a  subset of all namespaces , specify them in a\ncomma-separated list, escape each comma with a backslash,\nand surround the list in quotes, such as\n \"operator.watchNamespace=ns1\\,ns2\" . Watching a subset of namespaces is useful in deployments where a single\n Kubernetes Operator  instance watches a different cluster resource type.\nFor example, you can configure the  Kubernetes Operator  to watch  MongoDB  resources \nin one subset of namespaces, and to watch  MongoDBMultiCluster  resources  in another\nsubset of namespaces. To avoid race conditions during resource reconciliation,\nfor each custom resource type that you want the  Kubernetes Operator  to watch,\nensure that you set scope to a distinct subset of namespaces. To deploy  Ops Manager  and  MongoDB  resources  to one or more  namespaces  other\nthan the one where you deploy the  Kubernetes Operator ,\nsee  Set Scope for  MongoDB Enterprise Kubernetes Operator  Deployment  for values you must use and\nadditional steps you might have to perform.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_APPDB_IMAGE_REPOSITORY\n              value: quay.io/mongodb/mongodb-enterprise-init-appdb-ubi"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: APPDB_AGENT_VERSION\n              value: 10.2.15.5958-1_4.2.11-ent"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n     spec:\n      serviceAccountName: mongodb-enterprise-operator\n        containers:\n          - name: mongodb-enterprise-operator\n            image: <operatorVersionUrl>\n            imagePullPolicy: <policyChoice>\n            env:\n             - name: APPDB_IMAGE_REPOSITORY\n               value: quay.io/mongodb/mongodb-enterprise-appdb-ubi"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: CLUSTER_CLIENT_TIMEOUT\n              value: 10"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: DATABASE_VERSION\n              value: 1.24"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: IMAGE_PULL_POLICY\n              value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n     spec:\n       serviceAccountName: mongodb-enterprise-operator\n         containers:\n           - name: mongodb-enterprise-operator\n             image: <operatorVersionUrl>\n             imagePullPolicy: <policyChoice>\n             env:\n               - name: INIT_APPDB_IMAGE_REPOSITORY\n                 value: quay.io/mongodb/mongodb-enterprise-init-appdb-ubi"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_APPDB_VERSION\n              value: 1.24"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_DATABASE_IMAGE_REPOSITORY\n              value: quay.io/mongodb/mongodb-enterprise-init-database-ubi"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_DATABASE_VERSION\n              value: 1.24"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n           - name: INIT_OPS_MANAGER_IMAGE_REPOSITORY\n             value: quay.io/mongodb/mongodb-enterprise-init-ops-manager-ubi"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n        serviceAccountName: mongodb-enterprise-operator\n        containers:\n          - name: mongodb-enterprise-operator\n            image: <operatorVersionUrl>\n            imagePullPolicy: <policyChoice>\n            env:\n              - name: INIT_OPS_MANAGER_VERSION\n                value: 1.24"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: MANAGED_SECURITY_CONTEXT\n              value: false"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: MANAGED_SECURITY_CONTEXT\n              value: true"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: MDB_APPDB_ASSUME_OLD_FORMAT\n              value: false"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n     spec:\n        serviceAccountName: mongodb-enterprise-operator\n        containers:\n          - name: mongodb-enterprise-operator\n            image: <operatorVersionUrl>\n            imagePullPolicy: <policyChoice>\n            env:\n             - name: MDB_AUTOMATIC_RECOVERY_ENABLE\n               value: false"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n     spec:\n        serviceAccountName: mongodb-enterprise-operator\n        containers:\n          - name: mongodb-enterprise-operator\n            image: <operatorVersionUrl>\n            imagePullPolicy: <policyChoice>\n            env:\n             - name: MDB_AUTOMATIC_RECOVERY_BACKOFF_TIME_S\n               value: 3600"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: MDB_IMAGE_TYPE\n              value: ubi8"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n     spec:\n        serviceAccountName: mongodb-enterprise-operator\n        containers:\n          - name: mongodb-enterprise-operator\n            image: <operatorVersionUrl>\n            imagePullPolicy: <policyChoice>\n            env:\n             - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n               value: quay.io/mongodb/mongodb-enterprise-database-ubi"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: OPERATOR_ENV\n              value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: OPS_MANAGER_IMAGE_PULL_POLICY\n              value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n           - name: OPS_MANAGER_IMAGE_REPOSITORY\n             value: quay.io/mongodb/mongodb-enterprise-ops-manager-ubi"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n           - name: READINESS_PROBE_LOGGER_BACKUPS\n             value: 1"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n           - name: READINESS_PROBE_LOGGER_MAX_SIZE\n             value: 125"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n           - name: READINESS_PROBE_LOGGER_MAX_AGE\n             value: 3"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: WATCH_NAMESPACE\n              value: <testNamespace>"
                }
            ],
            "preview": "To provide optional settings, edit the YAML (Yet Another Markup Language) file that corresponds to\nyour deployment type in the directory where you cloned the\nKubernetes Operator repository:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "reference/troubleshooting",
            "title": "Troubleshoot the Kubernetes Operator",
            "headings": [
                "Get Status of a Deployed Resource",
                "Review the Logs",
                "Logging Process",
                "Review Logs from the Kubernetes Operator",
                "Find a Specific Pod",
                "Review Logs from a Specific Pod",
                "Review a Specific Log",
                "Audit Logs",
                "Check Messages from the Validation Webhook",
                "View All MongoDB resource Specifications",
                "Restore StatefulSet that Failed to Deploy",
                "Replace a ConfigMap to Reflect Changes",
                "Remove Kubernetes Components",
                "Remove a MongoDB resource",
                "Remove the Kubernetes Operator",
                "Remove the CustomResourceDefinitions",
                "Remove the namespace",
                "Create a New Persistent Volume Claim after Deleting a Pod",
                "Disable Ops Manager Feature Controls",
                "Debug a Failing Container",
                "Verify Correctness of Domain Names in TLS Certificates",
                "Verify the MongoDB Version when Running in Local Mode",
                "Upgrade Fails Using kubectl or oc",
                "Upgrade Fails Using Helm Charts",
                "Two Operator Instances After an Upgrade",
                "Recover Resource Due to Broken Automation Configuration"
            ],
            "paragraphs": "This section is for single  Kubernetes  cluster deployments only. For\n multi-Kubernetes-cluster deployments , see  Troubleshoot Deployments with Multiple Kubernetes Clusters . To find the status of a resource deployed with the  Kubernetes Operator ,\ninvoke one of the following commands: The following key-value pairs describe the resource deployment statuses: For  Ops Manager  resource deployments: The  status.applicationDatabase.phase  field displays the\nApplication Database resource deployment status. The  status.backup.phase  displays the backup daemon resource\ndeployment status. The  status.opsManager.phase  field displays the  Ops Manager  resource\ndeployment status. The  Cloud Manager or Ops Manager  controller watches the database resources\ndefined in the following settings: spec.backup.opLogStores spec.backup.s3Stores spec.backup.blockStores For MongoDB resource deployments: The  status.phase  field displays the MongoDB resource deployment\nstatus. Key Value message Message explaining why the resource is in a  Pending  or\n Failed  state. phase Status Meaning Pending The  Kubernetes Operator  is unable to reconcile the resource\ndeployment state. This happens when a reconciliation\ntimes out or if the  Kubernetes Operator  requires you to take\naction for the resource to enter a running state. If a resource is pending because a reconciliation timed\nout, the  Kubernetes Operator  attempts to reconcile the\nresource state in 10 seconds. Pending The  Kubernetes Operator  is reconciling the resource state. Resources enter this state after you create or update\nthem or if the  Kubernetes Operator  is attempting to reconcile\na resource previously in a  Failed  state. The  Kubernetes Operator  attempts to reconcile the resource\nstate in 10 seconds. Running The resource is running properly. Failed The resource is not running properly. The  message \nfield provides additional details. The  Kubernetes Operator  attempts to reconcile the resource\nstate in 10 seconds. lastTransition Timestamp in  ISO 8601  date and time format in  UTC (Coordinated Universal Time)  when the last reconciliation happened. link Deployment  URL (Uniform Resource Locator)  in  Ops Manager . backup.statusName If you enabled continuous backups with  spec.backup.mode \nin  Kubernetes  for your MongoDB resource, this field indicates\nthe status of the backup, such as  backup.statusName:\"STARTED\" .\nPossible values are  STARTED ,  STOPPED , and  TERMINATED . Resource specific fields For descriptions of these fields, see\n MongoDB Database Resource Specification . To see the status of a replica set named  my-replica-set  in\nthe  developer  namespace, run: If  my-replica-set  is running, you should see: If  my-replica-set  is not running, you should see: Keep and review adequate logs to help debug issues and monitor cluster\nactivity. Use the recommended  logging architecture \nto retain  Pod  logs even after a Pod is deleted. The  Kubernetes Operator  writes to the Pod logs by using\na wrapper that converts logs from the MongoDB Agent and  mongod \ncomponents on the database deployment Pod into a\nstructured logging entry in the following  JSON (Javascript Object Notation)  format: The  Kubernetes Operator  supports the following log types: When you read logs from a database container,\nthe  Kubernetes Operator  returns the structured  JSON (Javascript Object Notation)  entry\nthat contains logs from different sources. automation-agent-verbose automation-agent-stderr mongodb mongodb-audit agent-launcher-script automation-agent monitoring-agent backup-agent To review the  Kubernetes Operator  logs, invoke this command: You could check the  Ops Manager Logs  as\nwell to see if any issues were reported to  Ops Manager . To find which pods are available, invoke this command first: Kubernetes  documentation on  kubectl get . If you want to narrow your review to a specific  Pod , you can\ninvoke this command: If your  replica set  is labeled  myrs , run: This returns the  Automation Agent Log  for this\nreplica set. You can narrow your review to a specific log type. For example,\nthe following command returns audit logs from the  Kubernetes  logs\nof the specified Pod by specifying the  mongodb-audit  log type: The command returns an entry similar to the following output: To include audit logs in the  Kubernetes  Pod's logs,\nadd the following  additionalMongodConfig.auditLog  configuration\nto your resource definition. You can update the provided file name as needed. The  Kubernetes Operator  uses a validation  Webhook  to prevent users\nfrom applying invalid resource definitions. The webhook rejects invalid\nrequests. The  ClusterRole  and  ClusterRoleBinding  for the webhook are included in the default\nconfiguration files that you apply during the installation. To create\nthe role and binding, you must have  cluster-admin privileges . If you create an invalid resource definition, the webhook returns\na message similar to the following that describes the error to the shell: When the  Kubernetes Operator  reconciles each resource, it also validates that\nresource. The  Kubernetes Operator  doesn't require the validation webhook to\ncreate or update resources. If you omit the validation webhook, or if you remove the webhook's role\nand binding from the default configuration, or have insufficient\nprivileges to run the configuration, the  Kubernetes Operator  issues warnings,\nas these are not critical errors. If the  Kubernetes Operator  encounters a\ncritical error, it marks the resource as  Failed . GKE (Google Kubernetes Engine)  has a known issue with the webhook when deploying to private\nclusters. To learn more, see  Update Google Firewall Rules to Fix WebHook Issues . To view all  MongoDB  resource  specifications in the provided\n namespace : To read details about the  dublin  standalone resource, run\nthis command: This returns the following response: A StatefulSet  Pod  may hang with a status of  Pending  if it\nencounters an error during deployment. Pending   Pods  do not automatically terminate, even if you\nmake  and apply  configuration changes to resolve the error. To return the StatefulSet to a healthy state, apply the configuration\nchanges to the MongoDB resource in the  Pending  state, then delete\nthose pods. A host system has a number of running  Pods : my-replica-set-2  is stuck in the  Pending  stage. To gather\nmore data on the error, run: The output indicates an error in memory allocation. Updating the memory allocations in the MongoDB resource is\ninsufficient, as the pod does not terminate automatically after\napplying configuration updates. To remedy this issue, update the configuration, apply the\nconfiguration, then delete the hung pod: Once this hung pod is deleted, the other pods restart with your new\nconfiguration as part of rolling upgrade of the Statefulset. To learn more about this issue, see\n Kubernetes Issue 67250 . If you cannot modify or redeploy an already-deployed resource\n ConfigMap  file using the  kubectl apply  command, run: This deletes and re-creates the  ConfigMap  resource file. This command is useful in cases where you want to make an immediate\nrecursive change, or you need to update resource files that cannot\nbe updated once initialized. To remove any component, you need the following permissions: Cluster Roles mongodb-enterprise-operator-mongodb-webhook mongodb-enterprise-operator-mongodb-certs Cluster Role Bindings mongodb-enterprise-operator-mongodb-webhook-binding mongodb-enterprise-operator-mongodb-certs To remove any instance that  Kubernetes  deployed, you must use  Kubernetes . You can use only the  Kubernetes Operator  to remove  Kubernetes -deployed\ninstances. If you use  Ops Manager  to remove the instance,  Ops Manager  throws an\nerror. Deleting a MongoDB resource doesn't remove it from the  Ops Manager  UI.\nYou must remove the resource from  Ops Manager  manually. To learn more, see\n Remove a Process from Monitoring . Deleting a MongoDB resource for which you enabled backup doesn't\ndelete the resource's snapshots. You must  delete snapshots\nin Ops Manager . To remove a single MongoDB instance you created using  Kubernetes : To remove all MongoDB instances you created using  Kubernetes : To remove the  Kubernetes Operator : Remove all Kubernetes resources : Remove the  Kubernetes Operator : To remove the  CustomResourceDefinitions : Remove all Kubernetes resources : Remove the  CustomResourceDefinitions : To remove the  namespace : Remove all Kubernetes resources : Remove the  namespace : If you accidentally delete the MongoDB replica set Pod and its  Persistent Volume Claim ,\nthe  Kubernetes Operator  fails to reschedule the MongoDB Pod and issues\nthe following error message: To recover from this error, you must  manually create a new PVC \nwith the PVC object's name that corresponds to this replica set Pod,\nsuch as  data-<replicaset-pod-name> . When you manage an  Ops Manager  project through the  Kubernetes Operator , the\n Kubernetes Operator  places the  EXTERNALLY_MANAGED_LOCK \n feature control policy \non the project. This policy disables certain features in the  Ops Manager \napplication that might compromise your  Kubernetes Operator  configuration. If\nyou need to use these blocked features, you can remove the policy\nthrough the  feature controls API ,\nmake changes in the  Ops Manager  application, and then restore the original\npolicy through the  API . The following procedure enables you to use features in the  Ops Manager \napplication that are otherwise blocked by the  Kubernetes Operator . Retrieve the feature control policies \nfor your  Ops Manager  project. Save the response that the API returns. After you make changes in\nthe  Ops Manager  application, you must add these policies back to\nthe project. Your response should be similar to: Note the highlighted fields and values in the following sample\nresponse. You must send these same fields and values in later\nsteps when you remove and add feature control policies. The  externalManagementSystem.version  field corresponds to the\n Kubernetes Operator  version. You must send the exact same field value\nin your requests later in this task. Update \nthe  policies  array with an empty list: The previously blocked features are now available in the\n Ops Manager  application. The values you provide for the  externalManagementSystem \nobject, like the  externalManagementSystem.version  field, must\nmatch values that you received in the response in Step 1. Make your changes in the  Ops Manager  application. Update \nthe  policies  array with the original feature control policies: The features are now blocked again, preventing you from making\nfurther changes through the  Ops Manager  application. However, the\n Kubernetes Operator  retains any changes you made in the  Ops Manager \napplication while features were available. The values you provide for the  externalManagementSystem \nobject, like the  externalManagementSystem.version  field, must\nmatch values that you received in the response in Step 1. A container might fail with an error that results in  Kubernetes  restarting\nthat container in a loop. You may need to interact with that container to inspect files or run\ncommands. This requires you to prevent the container from restarting. In your preferred text editor, open the MongoDB resource you need to\nrepair. To this resource, add a  podSpec  collection that resembles the\nfollowing. The  sleep  command in the\n spec.podSpec.podTemplate.spec  instructs the container to\nwait for the number of seconds you specify. In this example, the\ncontainer will wait for 1 hour. Apply this change to the resource. Invoke the shell inside the container. A MongoDB replica set or sharded cluster may fail to reach\nthe  READY  state if the  TLS (Transport Layer Security)  certificate is invalid. When you  configure TLS  for MongoDB replica sets or sharded clusters, verify\nthat you specify a valid certificate. If you don't specify the correct Domain Name for each  TLS (Transport Layer Security)  certificate,\nthe  Kubernetes Operator   logs  may contain an error\nmessage similar to the following, where  foo.svc.local  is the\nincorrectly-specified Domain Name for the cluster member's Pod: To check whether you have correctly configured  TLS (Transport Layer Security)  certificates: To learn more about  TLS (Transport Layer Security)  certificate requirements, see the\nprerequisites on the  TLS-Encrypted Connections  tab in\n Deploy a Replica Set  or in  Deploy a Sharded Cluster . Each certificate should include a valid Domain Name. For each replica set or sharded cluster member, the Common Name, also\nknown as the Domain Name, for that member's certificate must match\nthe  FQDN (fully qualified domain name)  of the pod this cluster member is deployed on. The  FQDN (fully qualified domain name)  name in each certificate has the following syntax:\n pod-name.service-name.namespace.svc.cluster.local . This name is\ndifferent for each Pod hosting a member of the replica set or a\nsharded cluster. For example, for a member of a replica set deployed on a Pod with\nthe name  rs-mongos-0-0 , in the  Kubernetes Operator  service\nnamed  mongo-0  that is created in the default  mongodb \nnamespace, the  FQDN (fully qualified domain name)  is: Run: Check for  TLS (Transport Layer Security) -related messages in the  Kubernetes Operator  log files. MongoDB  CustomResource  may fail to reach a  Running  state\nif  Ops Manager  is running in  Local Mode  and you specify either a version of MongoDB\nthat doesn't exist, or a valid version of MongoDB for which\n Ops Manager  deployed in local mode did not download a corresponding MongoDB archive. If you specify a MongoDB version that doesn't exist, or a valid MongoDB\nversion for which  Ops Manager  could not download a MongoDB archive, then\neven though the Pods can reach the  READY  state,\nthe  Kubernetes Operator   logs  contain an\nerror message similar to the following: This may mean that the MongoDB Agent could not successfully download a\ncorresponding MongoDB binary to the  /var/lib/mongodb-mms-automation \ndirectory. In cases when the MongoDB Agent can download the MongoDB\nbinary for the specified MongoDB version successfully, this directory\ncontains a MongoDB binary folder, such as  mongodb-linux-x86_64-4.4.0 . To check whether a MongoDB binary folder is present: Specify the Pod's name to this command: Check whether a MongoDB binary folder is present in the\n /var/lib/mongodb-mms-automation  directory. If you cannot locate a MongoDB binary folder,\n copy the MongoDB archive \ninto the  Ops Manager  Persistent Volume for each deployed  Ops Manager  replica set. To resolve this error: You might receive the following error when you upgrade the\n Kubernetes Operator : Remove the old  Kubernetes Operator  deployment. Removing the  Kubernetes Operator  deployment doesn\u2019t affect the lifecycle\nof your MongoDB resources. Repeat the  kubectl apply  command to upgrade to the new\nversion of the  Kubernetes Operator . To resolve this error: You might receive the following error when you upgrade the\n Kubernetes Operator : Remove the old  Kubernetes Operator  deployment. Removing the  Kubernetes Operator  deployment doesn\u2019t affect the lifecycle\nof your MongoDB resources. Repeat the  helm  command to upgrade to the new version of the\n Kubernetes Operator . After you upgrade from  Kubernetes Operator  version 1.10 or earlier to a\nversion 1.11 or later, your  Kubernetes  cluster might have two instances of\nthe  Kubernetes Operator  deployed. Use the  get pods  command to view your  Kubernetes Operator  pods: If the response contains both an  enterprise-operator  and a\n mongodb-enterprise-operator  pod, your cluster has two  Kubernetes Operator \ninstances: You can safely remove the  enterprise-operator  deployment. Run the\nfollowing command to remove it: If you deployed the  Kubernetes Operator  to OpenShift, replace the\n kubectl  commands in this section with  oc  commands. If a custom resource remains in a  Pending  or  Failed  state\nfor a longer period of time,  Kubernetes Operator  automatically recovers\nyour  MongoDB  resources by pushing the  automation configuration  to  Ops Manager . This prevents a deadlock\nwhen the MongoDB Agent can't push an updated automation configuration change\nbecause the StatefulSet is stuck in a  Pending  state due to a previous\npush of an invalid automation configuration. To configure automatic recovery, define the following environmental variables\nin your  mongodb-enterprise.yaml \nfile: To learn how to define environment variables, see\n Define Environment Variables for a Container . MDB_AUTOMATIC_RECOVERY_ENABLE \nto enable or disable automatic recovery for  MongoDB  resources per Pod. MDB_AUTOMATIC_RECOVERY_BACKOFF_TIME_S \nto set the number of seconds that a custom resource can remain in a  Pending  or\n Failed  state before the  Kubernetes Operator  automatically recovers your\n MongoDB  resources.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl get <resource-name> -n <metadata.namespace> -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <metadata.namespace> -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb my-replica-set -n developer -o yaml"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n    lastTransition: \"2019-01-30T10:51:40Z\"\n    link: http://ec2-3-84-128-187.compute-1.amazonaws.com:9080/v2/5c503a8a1b90141cbdc60a77\n    members: 1\n    phase: Running\n    version: 4.2.2-ent"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  lastTransition: 2019-02-01T13:00:24Z\n  link: http://ec2-34-204-36-217.compute-1.amazonaws.com:9080/v2/5c51c040d6853d1f50a51678\n  members: 1\n  message: 'Failed to create/update replica set in Ops Manager: Status: 400 (Bad Request),\n    Detail: Something went wrong validating your Automation Config. Sorry!'\n  phase: Failed\n  version: 4.2.2-ent"
                },
                {
                    "lang": "json",
                    "value": "{ \"logType\": \"<log-type>\", \"contents\": \"<log line from a log file>\" }"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs -f deployment/mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs <podName> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs myrs-0 -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs -c mongodb-enterprise-database replica-set-0 | jq -r 'select(.logType == \"mongodb-audit\") | .contents'"
                },
                {
                    "lang": "json",
                    "value": "{{{ \"atype\":\"startup\",\"ts\":{\"$date\":\"2023-08-30T20:43:54.649+00:00\"},\"uuid\":{\"$binary\":\"oDcPEY69R1yiUtpMupaXOQ==\",\"$type\":\"04\"},\"local\":{\"isSystemUser\":true},\"remote\":{\"isSystemUser\":true},\"users\":[],\"roles\":[],\"param\":{\"options\":{\"auditLog\":{\"destination\":\"file\",\"format\":\"JSON\",\"path\":\"/var/log/mongodb-mms-automation/mongodb-audit.log\"},\"config\":\"/data/automation-mongod.conf\",\"net\":{\"bindIp\":\"0.0.0.0\",\"port\":27017,\"tls\":{\"mode\":\"disabled\"}},\"processManagement\":{\"fork\":true},\"replication\":{\"replSetName\":\"replica-set\"},\"storage\":{\"dbPath\":\"/data\",\"engine\":\"wiredTiger\"},\"systemLog\":{\"destination\":\"file\",\"path\":\"/var/log/mongodb-mms-automation/mongodb.log\"}}},\"result\":0}\n{\"atype\":\"startup\",\"ts\":{\"$date\":\"2023-08-30T20:44:05.466+00:00\"},\"uuid\":{\"$binary\":\"OUbUWC1DQM6k/Ih4hKZq4g==\",\"$type\":\"04\"},\"local\":{\"isSystemUser\":true},\"remote\":{\"isSystemUser\":true},\"users\":[],\"roles\":[],\"param\":{\"options\":{\"auditLog\":{\"destination\":\"file\",\"format\":\"JSON\",\"path\":\"/var/log/mongodb-mms-automation/mongodb-audit.log\"},\"config\":\"/data/automation-mongod.conf\",\"net\":{\"bindIp\":\"0.0.0.0\",\"port\":27017,\"tls\":{\"mode\":\"disabled\"}},\"processManagement\":{\"fork\":true},\"replication\":{\"replSetName\":\"replica-set\"},\"storage\":{\"dbPath\":\"/data\",\"engine\":\"wiredTiger\"},\"systemLog\":{\"destination\":\"file\",\"path\":\"/var/log/mongodb-mms-automation/mongodb.log\"}}},\"result\":0}}}"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n   additionalMongodConfig:\n      auditLog:\n         destination: file\n            format: JSON\n            path: /var/log/mongodb-mms-automation/mongodb-audit.log"
                },
                {
                    "lang": "sh",
                    "value": "error when creating \"my-ops-manager.yaml\":\nadmission webhook \"ompolicy.mongodb.com\" denied the request:\nshardPodSpec field is not configurable for application databases as\nit is for sharded clusters and appdb replica sets"
                },
                {
                    "lang": "shell",
                    "value": "kubectl get mdb -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl get mdb dublin -n <metadata.namespace> -o yaml"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"mongodb.com/v1\",\"kind\":\"MongoDB\",\"metadata\":{\"annotations\":{},\"name\":\"dublin\",\"namespace\":\"mongodb\"},\"spec\":{\"credentials\":\"credentials\",\"persistent\":false,\"podSpec\":{\"memory\":\"1Gi\"},\"project\":\"my-om-config\",\"type\":\"Standalone\",\"version\":\"4.0.0-ent\"}}\n  clusterDomain: \"\"\n  creationTimestamp: 2018-09-12T17:15:32Z\n  generation: 1\n  name: dublin\n  namespace: mongodb\n  resourceVersion: \"337269\"\n  selfLink: /apis/mongodb.com/v1/namespaces/mongodb/mongodbstandalones/dublin\n  uid: 7442095b-b6af-11e8-87df-0800271b001d\nspec:\n  credentials: my-credentials\n  type: Standalone\n  persistent: false\n  project: my-om-config\n  version: 4.2.2-ent"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods\n\nmy-replica-set-0     1/1 Running 2 2h\nmy-replica-set-1     1/1 Running 2 2h\nmy-replica-set-2     0/1 Pending 0 2h"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe pod my-replica-set-2\n\n<describe output omitted>\n\nWarning FailedScheduling 15s (x3691 over 3h) default-scheduler\n0/3 nodes are available: 1 node(s) had taints that the pod\ndidn't tolerate, 2 Insufficient memory."
                },
                {
                    "lang": "sh",
                    "value": "vi <my-replica-set>.yaml\n\nkubectl apply -f <my-replica-set>.yaml\n\nkubectl delete pod my-replica-set-2"
                },
                {
                    "lang": "shell",
                    "value": "kubectl replace -f <my-config-map>.yaml"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb <name> -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete deployment mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete crd mongodb.mongodb.com\nkubectl delete crd mongodbusers.mongodb.com\nkubectl delete crd opsmanagers.mongodb.com"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete namespace <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "scheduler error: pvc not found to schedule the pod"
                },
                {
                    "lang": "sh",
                    "value": "curl --user \"{USERNAME}:{APIKEY}\" --digest \\\n     --header \"Accept: application/json\" \\\n     --header \"Content-Type: application/json\" \\\n     --include \\\n     --request GET \"https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true\""
                },
                {
                    "lang": "json",
                    "value": "{\n \"created\": \"2020-02-25T04:09:42Z\",\n \"externalManagementSystem\": {\n   \"name\": \"mongodb-enterprise-operator\",\n   \"systemId\": null,\n   \"version\": \"1.4.2\"\n },\n \"policies\": [\n   {\n     \"disabledParams\": [],\n     \"policy\": \"EXTERNALLY_MANAGED_LOCK\"\n   },\n   {\n     \"disabledParams\": [],\n     \"policy\": \"DISABLE_AUTHENTICATION_MECHANISMS\"\n   }\n ],\n \"updated\": \"2020-02-25T04:10:12Z\"\n}"
                },
                {
                    "lang": "sh",
                    "value": "curl --user \"{USERNAME}:{APIKEY}\" --digest \\\n     --header \"Accept: application/json\" \\\n     --header \"Content-Type: application/json\" \\\n     --include \\\n     --request PUT \"https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true\" \\\n     --data\n       '{\n         \"externalManagementSystem\": {\n           \"name\": \"mongodb-enterprise-operator\",\n           \"systemId\": null,\n           \"version\": \"1.4.2\"\n         },\n         \"policies\": []\n       }'"
                },
                {
                    "lang": "sh",
                    "value": "curl --user \"{USERNAME}:{APIKEY}\" --digest \\\n     --header \"Accept: application/json\" \\\n     --header \"Content-Type: application/json\" \\\n     --include \\\n     --request PUT \"https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true\" \\\n     --data\n       '{\n         \"externalManagementSystem\": {\n           \"name\": \"mongodb-enterprise-operator\",\n           \"systemId\": null,\n           \"version\": \"1.4.2\"\n         },\n         \"policies\": [\n           {\n             \"disabledParams\": [],\n             \"policy\": \"EXTERNALLY_MANAGED_LOCK\"\n           },\n           {\n             \"disabledParams\": [],\n             \"policy\": \"DISABLE_AUTHENTICATION_MECHANISMS\"\n           }\n         ]\n       }'"
                },
                {
                    "lang": "yaml",
                    "value": "podSpec:\n  podTemplate:\n    spec:\n      containers:\n      - name: mongodb-enterprise-database\n        command: ['sh', '-c', 'echo \"Hello!\" && sleep 3600' ]"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl exec -it <pod-name> bash"
                },
                {
                    "lang": "sh",
                    "value": "TLS attempt failed : x509: certificate is valid for foo.svc.local,\nnot mongo-0-0.mongo-0.mongodb.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "rs-mongos-0-0.mongo-0.mongodb.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs -f <pod_name>"
                },
                {
                    "lang": "sh",
                    "value": "Failed to create/update (Ops Manager reconciliation phase):\nStatus: 400 (Bad Request), Detail:\nInvalid config: MongoDB <version> is not available."
                },
                {
                    "lang": "sh",
                    "value": "kubectl exec --stdin --tty $<pod_name> /bin/sh"
                },
                {
                    "lang": "sh",
                    "value": "Forbidden: updates to statefulset spec for fields other than\n'replicas', 'template', and 'updateStrategy' are forbidden"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete deployment/mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Error: UPGRADE FAILED: cannot patch \"mongodb-enterprise-operator\"\nwith kind Deployment: Deployment.apps \"mongodb-enterprise-operator\"\nis invalid: ... field is immutable"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete deployment/mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                           READY   STATUS    RESTARTS   AGE\nenterprise-operator-767884c9b4-ltkln           1/1     Running   0          122m\nmongodb-enterprise-operator-6d69686679-9fzs7   1/1     Running   0          68m"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete deployment/enterprise-operator"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n     spec:\n        serviceAccountName: mongodb-enterprise-operator\n        containers:\n          - name: mongodb-enterprise-operator\n            image: <operatorVersionUrl>\n            imagePullPolicy: <policyChoice>\n            env:\n             - name: MDB_AUTOMATIC_RECOVERY_ENABLE\n               value: true\n             - name: MDB_AUTOMATIC_RECOVERY_BACKOFF_TIME_S\n               value: 1200"
                }
            ],
            "preview": "To find the status of a resource deployed with the Kubernetes Operator,\ninvoke one of the following commands:",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "reference/k8s-operator-mongodbuser-specification",
            "title": "MongoDB User Resource Specification",
            "headings": [
                "Example",
                "Required MongoDBUser Resource Settings",
                "Optional MongoDBUser Resource Settings"
            ],
            "paragraphs": "The  MongoDBUser  custom resource lets you configure the authentication and  roles  required for a user to access a MongoDB database. The following example shows a resource specification for a   MongoDBUser  custom resource: This section describes settings that you must use for all  MongoDBUser  resources. Type : string Version of the MongoDB  Kubernetes  resource schema. Type : string Kind of MongoDB  Kubernetes  resource to create. Set this to  MongoDBUser . Type : string Human-readable name so you can identify this particular  MongoDBUser  resource. Resource names must be 44 characters or less. Type : string Name of the MongoDB database that these users will access. The default is  admin . Type : string Human-readable label that identifies the user needed to authenticate to the MongoDB database or collection. The  MongoDBUser  custom resource can use the following settings: Type : string Name of the connection string secret that the  Kubernetes Operator  creates. When you create a new MongoDB database user,  Kubernetes Operator  automatically\ncreates a new  Kubernetes   secret . The  Kubernetes   secret \ncontains the following information about the new database user: username : Username for the database user password : Password for the database user connectionString.standard :  Standard connection string \nthat can connect you to the database as this database user. connectionString.standardSrv :  DNS seed list connection string  that can\nconnect you to the database as this database user. Type : string Name of the  MongoDB resource  to which this user is associated. For example,  my-resource . Type : string The  namespace  that contains the  secret  for this user.\nIf unspecified, the  Kubernetes Operator  keeps connection secrets in the same  namespace  as the  MongoDBUser  custom resource. Type : string metadata.name  value of the secret that stores the user's password.\nFor example,  my-secret-name . Type : string Name of the field in the  secret  that\ncontains the password for this MongoDB database user. To learn more, see the  Kubernetes documentation . Type : string MongoDB database on which the  role  can act. For example,  admin . Type : string Name of the  role  to grant the database user.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: ldap-user-1\nspec:\n  username: \"uid=mdb0,dc=example,dc=org\"\n  db: \"$external\" \n  mongodbResourceRef:\n    name: ldap-replica-set\n  roles:\n  - db: \"admin\"\n    name: \"clusterAdmin\"\n  - db: \"admin\"\n    name: \"readWriteAnyDatabase\"\n  - db: \"admin\"\n    name: \"dbAdminAnyDatabase\"\n\n..."
                }
            ],
            "preview": "The MongoDBUser custom resource lets you configure the authentication and roles required for a user to access a MongoDB database.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "reference/support-lifecycle",
            "title": "Support Lifecycle",
            "headings": [
                "Considerations"
            ],
            "paragraphs": "Starting with  Kubernetes Operator  1.20, the support lifecycle of the  MongoDB Enterprise Kubernetes Operator \nis one year from the release date of a major version. Prior to version 1.20,\nthe support lifecycle was 9 months x 31 days each month (279 days total)\nfrom the release date of a major version. Versioning of the  Kubernetes Operator  doesn't follow the semantic versioning\nconvention. A  major version of the  Kubernetes Operator  is X.X, for example,\n1.22 or 1.23. The End of Life dates in this section might be subject to change. Kubernetes Operator  Version End of Life Date 1.24.x 2024-12-21 1.23.x 2024-11-13 1.22.x 2024-09-21 1.21.x 2024-08-25 1.20.x 2024-06-07 1.19.x 2024-01-03 1.18.x 2023-10-04 1.17.x 2023-06-25 1.16.x 2023-02-02 1.15.x 2022-11-16 1.14.x 2022-09-21 1.13.x 2022-07-26 1.12.x 2022-04-20 1.11.x 2022-03-09 1.10.x 2021-12-30 1.9.x 2021-09-13 1.8.x 2021-07-06 1.7.x 2021-07-01 1.6.x 2021-04-13 1.5.x 2021-01-28 1.4.x 2020-09-13 1.3.x 2020-07-30 1.2.x 2020-05-21 1.1 2020-04-23 1.0 2020-03-15",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "reference/k8s-operator-crd-logging-specification",
            "title": "MongoDB CRD Log Rotation Settings",
            "headings": [
                "MongoDB Log Rotation",
                "Automation Agent Logs",
                "Example Manifest",
                "Application Database Log Rotation",
                "MongoDB Logs",
                "Automation Agent Logs",
                "Audit Logs",
                "Example Manifest"
            ],
            "paragraphs": "You can configure most log rotation settings for  CustomResourceDefinitions \nin the  MongoDB  and\n Ops Manager   CRD (Custom Resource Definition)  manifests. The following table outlines logging configuration fields\nthat you can define in the MongoDB  CRD (Custom Resource Definition)  manifests,\nand where they can be defined or accessed elsewhere if they\ncan't be defined directly in the  CRD (Custom Resource Definition)  manifests. Component Log type Configuration location MongoDB Automation agent logs spec.agent.startupOptions MongoDB Monitoring agent logs Ops Manager API or UI MongoDB Backup logs Ops Manager API or UI MongoDB MongoDB logs Ops Manager UI MongoDB Audit logs Ops Manager UI MongoDB Readiness Probe Container Environment Variables Application Database MongoDB Logs spec.applicationDatabase.agent.logRotate Application Database Automation Agent Logs spec.applicationDatabase.agent.startupOptions Application Database Monitoring agent logs Forwarded to  stdout  only (kubectl logs).\nManaged and stored in the  Kubernetes  control plane. Application Database Audit logs spec.applicationDatabase.agent.logRotate.includeAuditLogsWithMongoDBLogs You can configure a log's rotation in the MongoDB  CRD (Custom Resource Definition) , as shown\nin the following example manifest. See the preceding table for more\ninformation about configuring the rotation of other MongoDB-specific logs. You can configure the MongoDB automation agent logs in the MongoDB  CRD (Custom Resource Definition)  manifest\nunder  spec.agent.startupOptions . You can configure the following log's rotation in the  MongoDBOpsManager   CRD (Custom Resource Definition) ,\nas shown in the following example manifest. See the preceding table for more\ninformation about configuring the rotation of other Application Database-specific logs. You can configure the Application Database MongoDB log rotation in the\n MongoDBOpsManager  manifest under  spec.applicationDatabase.agent.logRotate .\nMongoDB compresses log files beyond the two most recent logs by default. /var/log/mongodb-mms-automation/mongodb.log You can configure the Application Database Automation Agent log rotation in the\n MongoDBOpsManager  manifest under\n spec.applicationDatabase.agent.startupOptions . /var/log/mongodb-mms-automation/automation-agent-stderr.log /var/log/mongodb-mms-automation/automation-agent-verbose.log /var/log/mongodb-mms-automation/automation-agent.log You can configure the Application Database Audit log rotation in the  MongoDBOpsManager \nmanifest under\n spec.applicationDatabase.agent.logRotate.includeAuditLogsWithMongoDBLogs . /var/log/mongodb-mms-automation/mongodb-audit.log",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: log-rotate\n  namespace: mongodb\nspec:\n  podSpec:\n    podTemplate:\n      spec:\n        containers:\n        - name: mongodb-enterprise-database\n          env:\n          - name: READINESS_PROBE_LOGGER_MAX_SIZE\n            value: \"50\"\n  version: 4.4.0-ent\n  opsManager:\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  type: Standalone\n  persistent: false\n  # https://docs.opsmanager.mongodb.com/current/reference/mongodb-agent-settings/\n  agent:\n    startupOptions:\n      maxLogFiles: \"4\"\n      maxLogFileDurationHrs: \"1\"\n      maxUncompressedLogFiles: \"2\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: ops-manager\n  namespace: mongodb\nspec:\n  replicas: 1\n  version: 6.0.19\n  adminCredentials: ops-manager-admin-secret\n  applicationDatabase:\n    version: \"6.0.11-ent\"\n    members: 3\n    agent:\n      #AppDB (mongod) log rotation\n      logRotate:\n        numTotal: 4\n        numUncompressed: 2\n        timeThresholdHrs: 1\n        sizeThresholdMB: \"10\"\n      #Automation Agent log rotation\n      startupOptions:\n        maxLogFiles: \"4\"\n        maxLogFileDurationHrs: \"1\"\n        maxUncompressedLogFiles: \"2\""
                }
            ],
            "preview": "You can configure most log rotation settings for CustomResourceDefinitions\nin the MongoDB and\nOps Manager CRD (Custom Resource Definition) manifests.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "reference/known-issues",
            "title": "Known Issues in the MongoDB Enterprise Kubernetes Operator",
            "headings": [
                "Underprovisioned EBS Volume Causes Long IOPS Wait Times",
                "ConfigMap Name mongodb-enterprise-operator-member-list is Hard-Coded",
                "mongos Instances Fail to Reach Ready State After Disabling Authentication",
                "Update Google Firewall Rules to Fix WebHook Issues",
                "Configure Persistent Storage Correctly",
                "Remove Resources before Removing Kubernetes",
                "Create Separate Namespaces for Kubernetes Operator and MongoDB Resources",
                "HTTPS Enabled After Deployment",
                "Unable to Update the MongoDB Agent on Application Database Pods",
                "Unable to Pull Enterprise Kubernetes Operator Images from IBM Cloud Paks",
                "Machine Memory vs. Container Memory"
            ],
            "paragraphs": "If you used  kops  to provision a  Kubernetes  cluster in  AWS (Amazon Web Services)  and\nare experiencing poor performance and high  IOPS (Input/Output Operations per Second)  wait times, your Elastic Block Store\n(EBS) volume may be underprovisioned. To improve performance, increase the storage-to-IOPS ratio for your EBS volume. For\nexample, if your database is 500 GB, increase IOPS to 1500, a 3:1 ratio per GB. To learn\nmore about increasing IOPS, see the\n AWS documentation . When you run the  kubectl mongodb  plugin , such as during the\n multi-Kubernetes-cluster quick start procedure ,\nthe plugin creates a default ConfigMap named  mongodb-enterprise-operator-member-list .\nThis ConfigMap contains all the members of the  multi-Kubernetes-cluster deployment . You can't change\nthe ConfigMap's name. To learn more about plugin's flags and actions,\nsee  MongoDB Plugin Reference . If you disable authentication by setting\n spec.security.auth.enabled  to   false , the  mongos  Pods\nnever reach a  ready  state. As a workaround, delete each  mongos  Pod in your deployment. Run the following command to list all of your Pods: For each Pod with a name that contains  mongos , delete it with the\nfollowing command: When you delete a Pod, Kubernetes recreates it. Each Pod that Kubernetes\nrecreates receives the updated configuration and can reach a  READY \nstate. To confirm that all of your  mongos  Pods are  READY , run the\nfollowing command: A response like the following indicates that all of your  mongos  Pods\nare  READY : This issue applies only to  sharded clusters \nthat meet the following criteria: Deployed using the  Kubernetes Operator  1.13.0 Use X.509 authentication Use  kubernetes.io/tls  secrets for  TLS (Transport Layer Security) \ncertificates for the MongoDB Agent When you deploy  Kubernetes Operator  to  GKE (Google Kubernetes Engine)  private clusters, the\n MongoDB  resources  or MongoDBOpsManager resource creation could time out.\nThe following message might appear in the logs:  Error setting state\nto reconciling: Timeout: request did not complete within requested\ntimeout 30s . Google configures its firewalls to restrict access to your  Kubernetes \n Pods . To use the webhook service,\n add a new firewall rule \nto grant  GKE (Google Kubernetes Engine)  control plane access to your webhook service. The  Kubernetes Operator  webhook service runs on port 443. If there are no\n persistent volumes \navailable when you create a resource, the resulting  Pod  stays in\ntransient state and the Operator fails  (after 20 retries) with the\nfollowing error: To prevent this error, either: For testing only, you may also set  persistent : false . This\n must not be used in production , as data is not preserved between\nrestarts. Provide  Persistent Volumes  or Set  persistent : false  for the resource Sometimes  Ops Manager  can diverge from  Kubernetes . This mostly occurs when\n Kubernetes  resources are removed manually.  Ops Manager  can keep displaying an\nAutomation Agent which has been shut down. If you want to remove deployments of MongoDB on  Kubernetes , use the\nresource specification to delete resources first so no dead Automation\nAgents remain. The best strategy is to create  Kubernetes Operator  and its resources in\ndifferent namespaces so that the following operations would work\ncorrectly: or If the  Kubernetes Operator  and resources sit in the same  mongodb \n namespace , then operator would also be removed in the same operation.\nThis would mean that it could not clean the configurations, which\nwould have to be done in the  Ops Manager Application . We recommend that you enable  HTTPS (Hypertext Transfer Protocol Secure)   before  deploying your  Ops Manager  resources.\nHowever, if you enable  HTTPS (Hypertext Transfer Protocol Secure)  after deployment,\nyour managed resources can no longer communicate with  Ops Manager  and\nthe  Kubernetes Operator  reports your resources' status as  Failed . To resolve this issue, you must delete your  Pods  by\nrunning the following command for each Pod: After deletion,  Kubernetes  automatically restarts the deleted Pods.\nDuring this period, the resource is unreachable and incurs\ndowntime. Configure  Ops Manager  to Run over HTTPS Troubleshoot the  Kubernetes Operator You can't use  Ops Manager  to upgrade the MongoDB Agents that run on the\nApplication Database Pods. The MongoDB Agent version that runs on these\nPods is embedded in the Application Database Docker image. You can use the  Kubernetes Operator  to upgrade the MongoDB Agent version on\nApplication Database Pods as MongoDB publishes new images. APPDB_AGENT_VERSION appDb.version If you pull the  Kubernetes Operator  images from a container registry hosted in\n IBM Cloud Paks , the IBM Cloud Paks\nchanges the names of the images by adding a digest SHA to the official image\nnames. This action results in error messages from the  Kubernetes Operator  similar\nto the following: As a workaround, update the Ops Manager Application Database resource definition\nin  spec.applicationDatabase.podSpec. podTemplate  to\nspecify the new names for the  Kubernetes Operator  images that contain the digest SHAs,\nsimilar to the following example. The Automation Agent in  Cloud Manager  and  Ops Manager  reports\nhost memory (RAM) usage instead of the  Kubernetes  container memory usage.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl get pods"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete pod <podname>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                           READY   STATUS    RESTARTS   AGE\nmongodb-enterprise-operator-6495bdd947-ttwqf   1/1     Running   0          50m\nmy-sharded-cluster-0-0                         1/1     Running   0          12m\nmy-sharded-cluster-1-0                         1/1     Running   0          12m\nmy-sharded-cluster-config-0                    1/1     Running   0          12m\nmy-sharded-cluster-config-1                    1/1     Running   0          12m\nmy-sharded-cluster-mongos-0                    1/1     Running   0          11m\nmy-sharded-cluster-mongos-1                    1/1     Running   0          11m\nom-0                                           1/1     Running   0          42m\nom-db-0                                        2/2     Running   0          44m\nom-db-1                                        2/2     Running   0          43m\nom-db-2                                        2/2     Running   0          43m"
                },
                {
                    "lang": "sh",
                    "value": "Failed to update Ops Manager automation config: Some agents failed to register"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete pods --all"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete pod <replicaset-pod-name>"
                },
                {
                    "lang": "sh",
                    "value": "Failed to apply default image tag \"cp.icr.io/cp/cpd/ibm-cpd-mongodb-agent@\nsha256:10.14.24.6505-1\": couldn't parse image reference \"cp.icr.io/cp/cpd/\nibm-cpd-mongodb-agent@sha256:10.14.24.6505-1\": invalid reference format"
                },
                {
                    "lang": "sh",
                    "value": "applicationDatabase:\n  # The version specified must match the one in the image provided in the `mongod` field\n  version: 4.4.11-ubi8\n  members: 3\n  podSpec:\n    podTemplate:\n      spec:\n        containers:\n          - name: mongodb-agent\n            image: 'cp.icr.io/cp/cpd/ibm-cpd-mongodb-agent@sha256:689df23cc35a435f5147d9cd8a697474f8451ad67a1e8a8c803d95f12fea0b59'"
                }
            ],
            "preview": "If you used kops to provision a Kubernetes cluster in AWS (Amazon Web Services) and\nare experiencing poor performance and high IOPS (Input/Output Operations per Second) wait times, your Elastic Block Store\n(EBS) volume may be underprovisioned.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/plan-k8s-op-prerequisites",
            "title": "Prerequisites",
            "headings": [
                "Procedure",
                "Have a Kubernetes solution available to use, and verify hardware architecture support.",
                "Clone the MongoDB Enterprise Kubernetes Operator repository.",
                "Create a namespace for your Kubernetes deployment.",
                "Optional: Have a running Ops Manager.",
                "Required for OpenShift Installs: Create a secret that contains credentials authorized to pull images from the registry.connect.redhat.com repository."
            ],
            "paragraphs": "To install the MongoDB  Kubernetes Operator , you must: If you need a  Kubernetes  solution, see the  Kubernetes \n documentation on picking the right solution .\nReview  supported hardware architectures . You can use  Helm  to install the\n Kubernetes Operator . To learn how to install Helm, see its\n documentation on GitHub . By default, The  Kubernetes Operator  uses the  mongodb  namespace. To\nsimplify your installation, consider creating a namespace labeled\n mongodb  using the following  kubectl  command: If you do not want to use the  mongodb  namespace, you can label\nyour namespace anything you like: Set Scope for  MongoDB Enterprise Kubernetes Operator  Deployment If you don't deploy an  Ops Manager  resource with the\n Kubernetes Operator , you must have an  Ops Manager  running outside of your\n Kubernetes  cluster. If you will deploy an  Ops Manager  resource in  Kubernetes  with\nthe  Kubernetes Operator , skip this prerequisite. Your  Ops Manager  installation must run an active  NTP (Network Time Protocol)  service. If\nthe  Ops Manager  host's clock falls out of sync, that host can't\ncommunicate with the  Kubernetes Operator . To learn how to check your  NTP (Network Time Protocol)  service for your  Ops Manager \nhost, see the documentation for  RHEL . If you use the  Kubernetes Operator  to deploy MongoDB\nresources to  multiple namespaces  or with\na  cluster-wide scope , create the secret\nonly in the namespace where you intend to deploy the  Kubernetes Operator . The\n Kubernetes Operator  synchronizes the secret across all watched namespaces. If you have not already, obtain a Red Hat subscription. Create a  Registry Service Account . Click on your Registry Service Account, then click the\n Docker Configuration  tab. Download the  <account-name>-auth.json  file and open it in a\ntext editor. Copy the  registry.redhat.io  object, and paste another instance\nof this object into the file. Remember to add a comma after the\nfirst object. Rename the second object\n registry.connect.redhat.com , then save the file: Create an  openshift-pull-secret.yaml  file and add the contents\nof the modified  <account-name>-auth.json  file as\n stringData  named  .dockerconfigjson  to the\n openshift-pull-secret.yaml  secret file. The value you provide in the  metadata.name  field contains\nthe secret name. Provide this value when asked for the\n <openshift-pull-secret> . Create a  secret  from the  openshift-pull-secret.yaml \nfile in the same namespace in which you will deploy the  Kubernetes Operator .",
            "code": [
                {
                    "lang": "sh",
                    "value": "git clone https://github.com/mongodb/mongodb-enterprise-kubernetes.git"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace <namespaceName>"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"auths\": {\n    \"registry.redhat.io\": {\n      \"auth\": \"<encoded-string>\"\n    },\n    \"registry.connect.redhat.com\": {\n      \"auth\": \"<encoded-string>\"\n    }\n  }\n}"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: openshift-pull-secret\nstringData:\n  .dockerconfigjson: |\n      {\n        \"auths\": {\n          \"registry.redhat.io\": {\n            \"auth\": \"<encoded-string>\"\n          },\n          \"registry.connect.redhat.com\": {\n            \"auth\": \"<encoded-string>\"\n          }\n        }\n      }\ntype: kubernetes.io/dockerconfigjson"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f openshift-pull-secret.yaml -n <metadata.namespace>"
                }
            ],
            "preview": "To install the MongoDB Kubernetes Operator, you must:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/recover-om-appdb-deployments",
            "title": "Recover the Kubernetes Operator and Ops Manager for Multi-Cluster AppDB Deployments",
            "headings": [
                "Prerequisites",
                "Considerations",
                "Application Database Architecture",
                "Procedure",
                "Configure the Kubernetes Operator in a new cluster.",
                "Retrieve the backed-up resources from the failed Ops Manager resource.",
                "Re-apply the Ops Manager resource to the new cluster.",
                "Re-apply the MongoDB resources to the new cluster."
            ],
            "paragraphs": "If you host an  Ops Manager  resource in the same  Kubernetes  cluster as\nthe  Kubernetes Operator  and have the Application Database (AppDB)\ndeployed on selected member clusters in your  multi-Kubernetes-cluster deployment ,\nyou can manually recover the  Kubernetes Operator  and  Ops Manager \nin the event that the cluster fails. To learn more about deploying  Ops Manager  on a central\ncluster and the Application Database across member clusters,\nsee  Using  Ops Manager   with Multi-Kubernetes-Cluster Deployments . Before you can recover the  Kubernetes Operator  and  Ops Manager , ensure\nthat you meet the following requirements: Configure backups for your  Ops Manager  and\nApplication Database resources, including any\n ConfigMaps  and  secrets  created by the  Kubernetes Operator ,\nto indicate the previous running state of  Ops Manager .\nTo learn more, see  Backup . The Application Database must have at least three healthy\nnodes remaining after failure of the  Kubernetes Operator 's cluster. The healthy clusters in your  multi-Kubernetes-cluster deployment  must contain\na sufficient number of members to elect a primary node.\nTo learn more, see  Application Database Architecture . Because the  Kubernetes Operator  doesn't support forcing a replica set\nreconfiguration, the healthy  Kubernetes  clusters\nmust contain a sufficient number of Application Database members to elect a primary node\nfor this manual recovery process. A majority of the Application Database\nmembers must be available to elect a primary. To learn more, see\n Replica Set Deployment Architectures . If possible, use an odd number of member  Kubernetes  clusters. Proper distribution of your\nApplication Database members can help to maximize the likelihood that\nthe remaining replica set members can form a majority during an outage.\nTo learn more, see  Replica Sets Distributed Across Two or More Data Centers . Consider the following examples: For a five-member Application Database, some possible distributions of members include: Two clusters: three members to Cluster 1 and two members to Cluster 2. If Cluster 2 fails, there are enough members on Cluster 1 to elect a primary node. If Cluster 1 fails, there are not enough members on Cluster 2 to elect a primary node. Three clusters: two members to Cluster 1, two members to Cluster 2, and one member to Cluster 3. If any single cluster fails, there are enough members on the remaining clusters to elect a primary node. If two clusters fail, there are not enough members on any remaining cluster to elect a primary node. For a seven-member Application Database, consider the following distribution of members: Although Cluster 2 meets the three member minimum for the Application Database,\na majority of the Application Database's seven members must be available\nto elect a primary node. Two clusters: four members to Cluster 1 and three members to Cluster 2. If Cluster 2 fails, there are enough members on Cluster 1 to elect a primary node. If Cluster 1 fails, there are not enough members on Cluster 2 to elect a primary node. To recover the  Kubernetes Operator  and  Ops Manager ,\nrestore the  Ops Manager  resource on a new  Kubernetes  cluster: Follow the instructions to  install the Kubernetes Operator  in a new  Kubernetes  cluster. If you plan to re-use a member cluster, ensure that the\nappropriate service account and role exist. These values can overlap\nand have different permissions between the central cluster and member\ncluster. To see the appropriate role required for the\n Kubernetes Operator , refer to the  sample in the public repository . Copy the  object  specification for the failed  Ops Manager  resource and\nretrieve the following resources, replacing the placeholder text with\nyour specific  Ops Manager  resource name and namespace. Then, paste the specification that you copied into a new file and\nconfigure the new resource by using the preceding values. To\nlearn more, see  Deploy an  Ops Manager  Resource . Resource Type Values Secrets <om-name>-db-om-password <om-name>-db-agent-password <om-name>-db-keyfile <om-name>-db-om-user-scram-credentials <om-namespace>-<om-name>-admin-key <om-name>-admin-secret <om-name>-gen-key TLS (Transport Layer Security)  certificate secrets (optional) ConfigMaps <om-name>-db-cluster-mapping <om-name>-db-member-spec Custom CA for  TLS (Transport Layer Security)  certificates (optional) OpsManager <om-name> Use the following command to apply the updated resource: To check the status of your  Ops Manager  resource, use the following command: Once the central cluster reaches a  Running  state, you can\nre-scale the Application Database to your desired\ndistribution of member clusters. To host your  MongoDB  resource  or  MongoDBMultiCluster  resource  on the new\n Kubernetes Operator  instance, apply the following resources to the\nnew cluster: The  ConfigMap  used to create the initial project. The  secrets  used in the previous  Kubernetes Operator \ninstance. The  MongoDB  or  MongoDBMulticluster   custom resource  at its last\navailable state on the source cluster, including any  Annotations  added by the  Kubernetes Operator \nduring its lifecycle. If you deployed a  MongoDB  resource  and not a  MongoDBMultiCluster  resource \nand wish to migrate the failed  Kubernetes  cluster's data\nto the new cluster, you must complete the following additional steps: If you deployed a  MongoDBMultiCluster  resource , you must re-scale the resource that you\napplied on the new healthy clusters if the failed cluster contained any\nApplication Database nodes. Create a new  MongoDB  resource  on the new cluster. Migrate the data to the new resource by\n Backing Up and Restoring \nthe data in  Ops Manager .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply \\\n  --context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" \\\n  --namespace \"mongodb\"\n   -f https://raw.githubusercontent.com/mongodb/mongodb-enterprise-kubernetes/master/samples/ops-manager/ops-manager-external.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                }
            ],
            "preview": "If you host an Ops Manager resource in the same Kubernetes cluster as\nthe Kubernetes Operator and have the Application Database (AppDB)\ndeployed on selected member clusters in your multi-Kubernetes-cluster deployment,\nyou can manually recover the Kubernetes Operator and Ops Manager\nin the event that the cluster fails.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "reference/k8s-operator-specification",
            "title": "MongoDB Database Resource Specification",
            "headings": [
                "Common Resource Settings",
                "Required",
                "Conditional",
                "Optional",
                "Deployment-Specific Resource Settings",
                "Standalone Settings",
                "Replica Set Settings",
                "Sharded Cluster Settings",
                "Prometheus Settings",
                "Security Settings",
                "Examples",
                "StatefulSet Settings"
            ],
            "paragraphs": "Each  MongoDB  resource  uses an object specification in  YAML (Yet Another Markup Language)  to define the\ncharacteristics and settings of the MongoDB object: standalone,\n replica set , and  sharded cluster . At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . The  MongoDB Enterprise Kubernetes Operator \ncreates  Kubernetes   StatefulSets  from specification files that you write. The  Kubernetes Operator  creates MongoDB-specific resources in  Kubernetes  as\n custom resources . To manage these custom resources, use the following process: Create or update a  MongoDB  resource  specification. Direct  MongoDB Enterprise Kubernetes Operator  to apply it to your  Kubernetes  environment.\nAs a result,  Kubernetes Operator  performs these actions: Creates the defined  StatefulSets , services and other  Kubernetes  resources. Updates the  Ops Manager  deployment configuration to reflect changes. Deployment Type StatefulSets Size of StatefulSet Standalone 1 1  Pod Replica Set 1 1  Pod  per member Sharded Cluster <numberOfShards> + 2 1  Pod  per  mongos , shard, or config server member Every resource type must use the following settings: Type : string Version of the  MongoDB  resource  schema. Type : string Kind of  MongoDB  resource  to create. Set this to  MongoDB Type : string Name of the  MongoDB  resource  that you create. Resource names must be 44 characters or less. Type : string Required.  Name of the  Kubernetes   secret  you  created  as  Ops Manager   API (Application Programming Interface)  authentication credentials for\nthe  Kubernetes Operator  to communicate with  Cloud Manager or Ops Manager . The  Ops Manager   Kubernetes   Secret  object\nholding the Credentials must exist on the  same  Namespace as\nthe resource you want to create. The  Kubernetes Operator  tracks any changes to the Secret and\nreconciles the state of the  MongoDB  resource . Type : boolean Default : True Grant your containers permission to write to your  Persistent Volume .\nThe  Kubernetes Operator  sets  fsGroup = 2000 ,  runAsUser = 2000 ,\nand  runAsNonRoot = true  in  securityContext .  Kubernetes Operator \nsets  fsgroup  equal to  runAsUser  to make the volume writable\nfor a user that runs the main process in the container. To learn\nmore, see  Configure a\nSecurity Context for a Pod or Container  and the related\n discussion  in\nthe  Kubernetes  documentation. If redeploying the resource doesn't fix\nissues with your Persistent Volume, contact  MongoDB Support . If you do not use  Persistent Volumes , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. Type : string Type of  MongoDB  resource  to create. Accepted values are: Standalone ReplicaSet ShardedCluster Type : string Version of MongoDB that you installed on this  MongoDB  resource . Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. If you update this value to a later version of MongoDB for your\ndatabase resources, the Feature Compatibility Version (FCV) changes\nautomatically to this version unless you set  spec.featureCompatibilityVersion \nto the original version. Consider setting  spec.featureCompatibilityVersion \nto the original version to give yourself the option to downgrade if necessary. Every resource must use  one  of the following settings: Type : string Name of the  ConfigMap  with the  Cloud Manager or Ops Manager  connection\nconfiguration. The  spec.cloudManager.configMapRef.name \nsetting is an alias for this setting and can be used in its place. This value must exist on the  same  namespace as the resource\nyou want to create. The  Kubernetes Operator  tracks any changes to the ConfigMap and\nreconciles the state of the  MongoDB  resource . Type : string Alias for  spec.opsManager.configMapRef.name . Every resource type may use the following settings: Type : string Limits changes to data that occur with an upgrade to a\nnew major version. This allows you to downgrade to the previous major\nversion. To learn more about feature compatibility, see\n setFeatureCompatibilityVersion  in the MongoDB Manual. Type : string Default : cluster.local Domain name of the  Kubernetes  cluster where you deploy the  Kubernetes Operator . When\n Kubernetes  creates a  StatefulSet , the  Kubernetes  assigns each  Pod  a\n FQDN (fully qualified domain name) . To update  Cloud Manager or Ops Manager , the  Kubernetes Operator  calculates the  FQDN (fully qualified domain name)  for\neach  Pod  using a provided cluster name.  Kubernetes  doesn't provide\nan  API (Application Programming Interface)  to query these hostnames. You must set  spec.clusterDomain  if your  Kubernetes  cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n Kubernetes Operator  might not function as expected. Type : string Default : cluster.local Domain name of the  Kubernetes  cluster where you deploy the  Kubernetes Operator .\nWhen  Kubernetes  creates a  StatefulSet , the  Kubernetes  assigns each  Pod \na  FQDN (fully qualified domain name) . To update  Cloud Manager or Ops Manager , the  Kubernetes Operator  calculates the  FQDN (fully qualified domain name)  for\neach  Pod  using a provided cluster name.  Kubernetes  doesn't provide\nan  API (Application Programming Interface)  to query these hostnames. Use  spec.clusterDomain  instead. You must set  spec.clusterDomain  if your  Kubernetes  cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n Kubernetes Operator  might not function as expected. Type : string Kubernetes   namespace  where you create this  MongoDB  resource  and other\n objects . Type : string Default : <resource_name>+\"-svc\" and <resource_name>+\"-svc-external\" Name of the  Kubernetes  service to be created or used for a\n StatefulSet . If the service with this name already exists, the\n MongoDB Enterprise Kubernetes Operator  does not delete or recreate it. This setting lets\nyou create your own custom services and lets the  Kubernetes Operator \nreuse them. Use  spec.statefulSet.spec.serviceName  instead. Type : string Default : INFO Configures the level of Automation Agent logging inside the\n Pod . Accepted values include: DEBUG INFO WARN ERROR FATAL Type : boolean Default :  false Determines whether you can modify database users that were not\nconfigured through the  Kubernetes Operator , or the  Cloud Manager or Ops Manager  user interface. To manage database users directly through the  mongod  or  mongos , set\nthis setting to  true . Other settings you can and must use in a  MongoDB  resource  specification\ndepend upon which MongoDB deployment item you want to create: Standalone Settings Replica Set Settings Sharded Cluster Settings All of the  Standalone Settings  also apply to replica set\nresources. Type : collection Additional  configuration options  with\nwhich you want to start MongoDB processes. The  Kubernetes Operator  supports all configuration options that the MongoDB\nversion you deploy through the MongoDB Agent supports, except that the\n Kubernetes Operator  overrides values that you provide for any of the\nfollowing options: To learn more about the configuration options that the  Kubernetes Operator \nowns, see  MongoDB  Kubernetes Operator  Exclusive Settings . To learn which configuration options you can use, see\n Advanced Options for MongoDB Deployments  in the  Ops Manager \ndocumentation. net.port net.tls.certificateKeyFile net.tls.clusterFile net.tls.PEMKeyFile replication.replSetName security.clusterAuthMode sharding.clusterRole storage.dbPath systemLog.destination systemLog.path Type : collection MongoDB Agent configuration settings for MongoDB database resource. Type : collection MongoDB Agent settings with which you want to start MongoDB database resource. You must provide MongoDB Agent settings as key-value pairs. The values\nmust be strings. For a list of supported MongoDB Agent settings, see: MongoDB Agent Settings \nfor  Cloud Manager  projects. MongoDB Agent Settings  for the  Ops Manager  version you\ndeployed with the  Kubernetes Operator . Type : collection Specification to expose your cluster for external connections.\nTo learn how to connect to your MongoDB resource from outside of the  Kubernetes  cluster, see\n Connect to a MongoDB Database Resource from Outside Kubernetes . If you add  spec.externalAccess , the  Kubernetes Operator  creates an external service\nfor each Pod in a replica set. External services provide an external entry point\nfor each MongoDB database Pod in a cluster. Each external service has\n selectors \nthat match the external service to a specific Pod. If you add this setting without any values, the  Kubernetes Operator  creates\nan external service with the following default values: Field Value Description Name <pod-name>-svc-external Name of the external service. You can't change this value. Type LoadBalancer Creates an external  LoadBalancer  service. Port <Port Number> A port for  mongod . publishNotReadyAddress true Specifies that  DNS records \nare created even if the Pod isn't ready.\nDo not set to  false  for any database Pod. If you set  spec.externalAccess.externalDomain ,\nthe external service adds another port ( Port Number + 1 ) for backups. Type : collection Specification for overriding the default values in  spec.externalAccess . When you set the  spec.externalAccess  setting, the  Kubernetes Operator \nautomatically creates an external load balancer service with  default values .\nYou can override certain values or add new values depending on your needs.\nFor example, if you intend to create  NodePort services \nand don't need a load balancer, you must configure overrides in your\n Kubernetes  specification: For more information about the  Kubernetes  specification, see  ServiceSpec \nin the  Kubernetes  documentation. Type : collection Key-value pairs that let you add cloud provider-specific\nconfiguration settings to all clusters in your deployment. To learn more about annotations, see the\n Kubernetes documentation \nand the documentation for your  Kubernetes  cloud provider. Type : collection Configuration for the  ServiceSpec .\nTo learn more, see  spec.externalAccess.externalService . Type : collection Has  Kubernetes Operator  create one  Persistent Volume Claim  and mount all\nthree directories for data, journal, and logs to the same  Persistent Volume . You must set the values in this collection if\n spec.persistent   : true . You may set this collection or the  persistence.multiple \ncollections but not both. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum size of  Persistent Volume  that should be mounted. This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is 16Gi. If  standalone deployment  in requires 60 gigabytes of\nstorage space, set this value to  60Gi . storageClass string Type of storage specified in a  Persistent Volume Claim . You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : collection Has  Kubernetes Operator  create a  Persistent Volume Claim  and mount a\ndirectory for data to its own  Persistent Volume . You must set the values in this collection if\n spec.persistent   : true . You may set this collection or the  persistence.single \ncollection but not both. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum storage capacity that must be available on a  Kubernetes \n node  to host  standalone deployment  on  Kubernetes . This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is 16Gi. If this  MongoDB  resource  requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage needed for  standalone deployment . You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : collection Has  Kubernetes Operator  create a  Persistent Volume Claim  and mount a\ndirectory for journal to its own  Persistent Volume . You must set the values in this collection if\n spec.persistent   : true . You may set this collection or the  persistence.single \ncollection but not both. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum storage capacity that must be available on a  Kubernetes \n node  to host  standalone deployment  on  Kubernetes . This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is 1Gi. If this  MongoDB  resource  requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage needed for  standalone deployment . You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : collection Has  Kubernetes Operator  create a  Persistent Volume Claim  and mount a\ndirectory for logs to its own  Persistent Volume . You must set the values in this collection if\n spec.persistent   : true . You may set this collection or the  persistence.single \ncollection but not both. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum storage capacity that must be available on a  Kubernetes \n node  to host  standalone deployment  on  Kubernetes . This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is 3Gi. If this  MongoDB  resource  requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage needed for  standalone deployment . You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : Struct Kubernetes   rule  to place  Pods  for  replica set  on a\nspecific range of  nodes . For optimized read-write performance, use node affinity rules that\nrestrict  Pods  to run on particular  nodes , or to prefer\nto run on particular  nodes . Type : Struct Kubernetes   rule  to determine whether multiple  MongoDB  resource   Pods \nmust be co-located with other  Pods . To learn more about the use cases,\nsee  Affinity and Anti-Affinity  in the  Kubernetes  documentation. Type : Struct Default : kubernetes.io/hostname Sets a  rule  to spread  Pods  hosting  MongoDB  resource \nto different locations. A location can be a single node, rack, or\nregion. By default,  Kubernetes Operator  tries to spread pods across\ndifferent nodes. Type : Struct Default : kubernetes.io/hostname This key defines which\n label \nis used to determine which topology\n domain \na node belongs to. Type : collection Template \nfor the  Kubernetes  Pods that the  MongoDB Enterprise Kubernetes Operator  creates for MongoDB database resources. Template values take precedence over values specified in  spec.podSpec . The  Kubernetes Operator  doesn't validate the fields you provide\nin  spec.podSpec.podTemplate . Type : collection Metadata for the  Kubernetes  Pods that the  MongoDB Enterprise Kubernetes Operator  creates for\nMongoDB database resources. To review which fields you can add to  spec.podSpec.podTemplate.metadata , see\nthe  Kubernetes documentation . Type : collection Specifications of the  Kubernetes  Pods that the  MongoDB Enterprise Kubernetes Operator  creates for\nMongoDB database resources. To review which fields you can add to  spec.podSpec.podTemplate.spec , see the\n Kubernetes PodSpec v1 core API . Use this setting to specify the CPU and RAM allocations for each pod. For\nexamples, see  the samples on GitHub . When you add containers to  spec.podSpec.podTemplate.spec.containers , the\n Kubernetes Operator  adds them to the  Kubernetes  pod. These containers are\nappended to MongoDB database resources containers in the pod. The following settings apply to replica set resource types: The following settings apply only to replica set resource types: All of the  Standalone Settings  also apply to replica set\nresources. Type : collection The collection container for  spec.backup.mode ,\nwhich enables continuous backups for MongoDB resources in  Kubernetes Operator . Type : array A comma-separated list of labels to assign backup daemons, oplog stores,\nblockstores,  S3 (Simple Storage Service)  snapshot stores, and file system stores to specific\nprojects or groups. Use assignment labels to identify that specific\nbackup stores are associated with particular projects. If you set assignment labels using the  Kubernetes Operator , the values that\nyou set in the  Kubernetes  configuration file for assignment labels override\nthe values defined in the  Ops Manager  UI. Assignment labels that you don't set\nusing the  Kubernetes Operator  continue to use the values set in the  Ops Manager  UI. If you set this parameter, the API key linked with the value of\n spec.credentials  must have a  Global Owner  role. Type : string Enables continuous backups for a MongoDB resource.\nPossible values are  enabled ,  disabled , and\n terminated . After you enable continuous backups for your MongoDB resource with\n spec.backup.mode , you can  check the backup status . The  spec.backup.mode \nsetting relies on  Backup  that is\nenabled in the  Ops Manager  and requires that\n spec.backup.enabled \nvalue in the  Ops Manager   resource specification  is set to  true . Type : object Object that contains the backup encryption configuration settings. Type : object Object that contains the  KMIP (Key Management Interoperability)  backup encryption configuration\nsettings. To learn more, see  Configure KMIP Backup Encryption for Ops Manager . Type : object Object that contains the  KMIP (Key Management Interoperability)  backup encryption client\nconfiguration settings. Type : string Type : collection Collection container for snapshot schedule settings for\ncontinuous backups for MongoDB resources in  Kubernetes Operator . Type : number Number of hours between snapshots. You can set a value of  6 ,\n 8 ,  12 , or  24 . Type : number Number of days to keep recent snapshots. You can set a value\nbetween  2  and  5 , inclusive. Type : number Number of days to keep daily snapshots. You can set a value\nbetween  1  and  365 , inclusive. Setting the value to  0 \ndisables this rule. Type : number Number of weeks to keep weekly snapshots. You can set a value\nbetween  1  and  52 , inclusive. Setting the value to  0 \ndisables this rule. Type : number Number of months to keep monthly snapshots. You can set a value\nbetween  1  and  36 , inclusive. Setting the value to  0 \ndisables this rule. Type : number Number of hours in the past for which you can create a point-in-time\nsnapshot. Type : number UTC (Coordinated Universal Time)  hour of the day to schedule snapshots using a 24 hour clock.\nYou can set a value between  0  and  23 , inclusive. Type : number UTC (Coordinated Universal Time)  minute of the hour to schedule snapshots. You can\nset a value between  0  and  59 , inclusive. Type : string Day of the week when  Ops Manager  takes a full snapshot. This setting\nensures a recent complete backup.  Ops Manager  sets the default value to\n SUNDAY . Type : string Default : cluster.local Domain name of the  Kubernetes  cluster where you deploy the  Kubernetes Operator .\nWhen  Kubernetes  creates a  StatefulSet , the  Kubernetes  assigns each  Pod \na  FQDN (fully qualified domain name) . To update  Cloud Manager or Ops Manager , the  Kubernetes Operator  calculates the  FQDN (fully qualified domain name)  for\neach  Pod  using a provided cluster name.  Kubernetes  doesn't provide\nan  API (Application Programming Interface)  to query these hostnames. Use  spec.clusterDomain  instead. You must set  spec.clusterDomain  if your  Kubernetes  cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n Kubernetes Operator  might not function as expected. Type : collection Allows you to provide different  DNS (Domain Name System)  settings for client\napplications and the MongoDB Agents. The  Kubernetes Operator  uses split\nhorizon  DNS (Domain Name System)  for replica set members. This feature allows\ncommunication both within the  Kubernetes  cluster and from outside  Kubernetes . You may add multiple external mappings per host. Make sure that each value in this array is unique. Make sure that the number of entries in this array matches the\nvalue given in  spec.members . Provide a value for the\n spec.security.certsSecretPrefix  setting to\nenable  TLS (Transport Layer Security) . This method to use split horizons requires the\nServer Name Indication extension of the  TLS (Transport Layer Security)  protocol. Configure the routing for external hostnames . In this example, the replica set members communicate amongst\nthemselves on the  example-localhost  horizon. Clients\ncommunicate with the replica set using the  example-website \nhorizon. Type : string An external domain used to externally expose your replica set deployment. By default, each replica set member uses the  Kubernetes  Pod's  FQDN (fully qualified domain name) \n( *.svc.cluster.local ) as the default hostname. However, if you add an\nexternal domain to this setting, the replica set uses a hostname that is a\nsubdomain of the specified domain instead. This hostname uses the following\nformat: For example: After you deploy the replica set with this setting, the\n Kubernetes Operator  uses the hostname with the external domain to override\nthe  processes[n].hostname  field in the  Ops Manager   automation configuration . Then, the MongoDB Agent uses this hostname to\nconnect to  mongod . To specify other hostnames for connecting to the replica set, you can use the\n spec.connectivity.replicaSetHorizons  setting. However, the following\nconnections still use the hostname with the external domain: <replica-set-name>-<pod-idx>.<externalDomain> replica-set-1.example.com The MongoDB Agent to connect to  mongod . mongod  to connect to other  mongod  instances. Specifying this field changes how  Ops Manager  registers  mongod  processes.\nYou can specify this field only for new replica set deployments starting in  Kubernetes Operator \nversion 1.19. You can't change the value of this field or any  processes[n].hostname  fields\nin the  Ops Manager   automation configuration  for a running\nreplica set deployment. Type : integer Required . Number of Members of the Replica Set. Type : collection Specification for each MongoDB replica set member deployed from\nthe  MongoDB  resource . The order of the elements in the array must reflect the order of members\nin the replica set. For example, the first element of the array affects\nthe Pod at index  0 , the second element affects index  1 , and so on. Consider the following example specification for a\nthree-member replica set: Type : string Number that indicates the relative likelihood of a MongoDB replica set member to become the  primary . For example, a member with a  memberConfig.priority  of  1.5  is more likely than a member with a  memberConfig.priority  of  0.5  to become the primary. A member with a  memberConfig.priority  of  0  is ineligible to become the primary. To learn more, see  Member Priority . To increase the relative likelihood that a replica set member becomes the primary, specify a higher  priority  value. To decrease the relative likelihood that a replica set member becomes the primary, specify a lower  priority  value. Type : map Map of  replica set tags  for directing\nread and write operations to specific members of your MongoDB replica set. Type : number Determines whether a MongoDB replica set member can vote in an  election . Set to  1  to allow the member to vote. Set to  0  to exclude the member from an election. Type : boolean Flag that controls whether the  Kubernetes Operator  stops and terminates the\nbackup when you delete a MongoDB resource. If omitted, the default value is  false .\nSetting this flag to  true  is useful when you want to delete the\nMongoDB custom resource while the  spec.backup.mode  setting\nis set to  enabled . The following settings apply only to sharded cluster resource types: All of the  Replica Set Settings  also apply to sharded cluster\nresources unless otherwise specified. Type : number Number of minutes between successive cluster checkpoints. This setting\napplies only to sharded clusters that run MongoDB with FCV of 4.0 or\nearlier. This number determines the granularity of point-in-time\nrestores for sharded clusters. You can set a value of  15 ,  30 ,\nor  60 . Type : integer Required . Number of members in the  config server . Type : collection Additional  configuration options  with\nwhich you want to start each  config server  member. The  Kubernetes Operator  supports all configuration options that the MongoDB\nversion you deploy through the MongoDB Agent supports, except that the\n Kubernetes Operator  overrides values that you provide for any of the\nfollowing options: To learn more about the configuration options that the  Kubernetes Operator \nowns, see  MongoDB  Kubernetes Operator  Exclusive Settings . To learn which configuration options you can use, see\n Advanced Options for MongoDB Deployments  in the  Ops Manager \ndocumentation. net.tls.certificateKeyFile net.tls.clusterFile net.tls.PEMKeyFile replication.replSetName security.clusterAuthMode sharding.clusterRole storage.dbPath systemLog.destination systemLog.path Type : collection MongoDB Agent configuration settings for each  config server  member. Type : collection MongoDB Agent settings with which you want to start each  config server  member. You must provide MongoDB Agent settings as key-value pairs. The values\nmust be strings. For a list of supported MongoDB Agent settings, see: MongoDB Agent Settings \nfor  Cloud Manager  projects. MongoDB Agent Settings  for the  Ops Manager  version you\ndeployed with the  Kubernetes Operator . Type : collection Has  Kubernetes Operator  create one  Persistent Volume Claim  and mount all\nthree directories for data, journal, and logs to the same  Persistent Volume . You must set the values in this collection if\n spec.persistent   : true . You may set this collection or the  persistence.multiple \ncollections but not both. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum size of  Persistent Volume  that should be mounted. This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is 5Gi. If each  config server  member in requires 60 gigabytes of\nstorage space, set this value to  60Gi . storageClass string Type of storage specified in a  Persistent Volume Claim . You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : collection Has  Kubernetes Operator  create a  Persistent Volume Claim  and mount a\ndirectory for data to its own  Persistent Volume . You must set the values in this collection if\n spec.persistent   : true . You may set this collection or the  persistence.single \ncollection but not both. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum storage capacity that must be available on a  Kubernetes \n node  to host each  config server  member on  Kubernetes . This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is 16Gi. If this  MongoDB  resource  requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage needed for each  config server  member. You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : collection Has  Kubernetes Operator  create a  Persistent Volume Claim  and mount a\ndirectory for journal to its own  Persistent Volume . You must set the values in this collection if\n spec.persistent   : true . You may set this collection or the  persistence.single \ncollection but not both. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum storage capacity that must be available on a  Kubernetes \n node  to host each  config server  member on  Kubernetes . This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is 1Gi. If this  MongoDB  resource  requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage needed for each  config server  member. You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : collection Has  Kubernetes Operator  create a  Persistent Volume Claim  and mount a\ndirectory for logs to its own  Persistent Volume . You must set the values in this collection if\n spec.persistent   : true . You may set this collection or the  persistence.single \ncollection but not both. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum storage capacity that must be available on a  Kubernetes \n node  to host each  config server  member on  Kubernetes . This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is 3Gi. If this  MongoDB  resource  requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage needed for each  config server  member. You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : collection Template \nfor the  Kubernetes  Pods that the  MongoDB Enterprise Kubernetes Operator  creates for each  config server  member. Template values take precedence over values specified in  spec.configSrvPodSpec . The  Kubernetes Operator  doesn't validate the fields you provide\nin  spec.configSrvPodSpec.podTemplate . Type : collection Kubernetes   rule  to determine whether multiple  MongoDB  resource   Pods \nmust be co-located with other  Pods . To learn more about the use cases,\nsee  Affinity and Anti-Affinity  in the  Kubernetes  documentation. Type : collection Kubernetes   rule  to place  Pods  for  replica set  on a\nspecific range of  nodes . For optimized read-write performance, use node affinity rules that\nrestrict  Pods  to run on particular  nodes , or to prefer\nto run on particular  nodes . Type : string Default : kubernetes.io/hostname Sets a  rule  to spread  Pods  hosting  MongoDB  resource \nto different locations. A location can be a single node, rack, or\nregion. By default,  Kubernetes Operator  tries to spread pods across\ndifferent nodes. Type : string Default : kubernetes.io/hostname This key defines which\n label \nis used to determine which topology\n domain \na node belongs to. Type : collection Metadata for the  Kubernetes  Pods that the  MongoDB Enterprise Kubernetes Operator  creates for\neach  config server  member. To review which fields you can add to  spec.configSrvPodSpec.podTemplate.metadata , see\nthe  Kubernetes documentation . Type : collection Specifications of the  Kubernetes  Pods that the  MongoDB Enterprise Kubernetes Operator  creates for\neach  config server  member. To review which fields you can add to  spec.configSrvPodSpec.podTemplate.spec , see the\n Kubernetes PodSpec v1 core API . Use this setting to specify the CPU and RAM allocations for each pod. For\nexamples, see  the samples on GitHub . When you add containers to  spec.configSrvPodSpec.podTemplate.spec.containers , the\n Kubernetes Operator  adds them to the  Kubernetes  pod. These containers are\nappended to each  config server  member containers in the pod. Type : integer Required . Number of members per  shard . Type : integer Required . Number of  mongos  instances in the  sharded cluster . Type : collection Additional  configuration options  with\nwhich you want to start each  mongos  instance. The  Kubernetes Operator  supports all configuration options that the MongoDB\nversion you deploy through the MongoDB Agent supports, except that the\n Kubernetes Operator  overrides values that you provide for any of the\nfollowing options: To learn more about the configuration options that the  Kubernetes Operator \nowns, see  MongoDB  Kubernetes Operator  Exclusive Settings . To learn which configuration options you can use, see\n Advanced Options for MongoDB Deployments  in the  Ops Manager \ndocumentation. net.tls.certificateKeyFile net.tls.clusterFile net.tls.PEMKeyFile replication.replSetName security.clusterAuthMode sharding.clusterRole storage.dbPath systemLog.destination systemLog.path Type : collection MongoDB Agent configuration settings for each  mongos  instance. Type : collection MongoDB Agent settings with which you want to start each  mongos  instance. You must provide MongoDB Agent settings as key-value pairs. The values\nmust be strings. For a list of supported MongoDB Agent settings, see: MongoDB Agent Settings \nfor  Cloud Manager  projects. MongoDB Agent Settings  for the  Ops Manager  version you\ndeployed with the  Kubernetes Operator . Type : collection Template \nfor the  Kubernetes  Pods that the  MongoDB Enterprise Kubernetes Operator  creates for each  mongos  instance. Template values take precedence over values specified in  spec.mongosPodSpec . The  Kubernetes Operator  doesn't validate the fields you provide\nin  spec.mongosPodSpec.podTemplate . Type : collection Optional.   Kubernetes   rule  to determine if multiple\n MongoDB  resource   Pods  must be co-located with other  Pods . Type : collection Kubernetes   rule  to place  Pods  for  replica set  on a\nspecific range of  nodes . For optimized read-write performance, use node affinity rules that\nrestrict  Pods  to run on particular  nodes , or to prefer\nto run on particular  nodes . Type : string Default : kubernetes.io/hostname Sets a  rule  to spread  Pods  hosting  MongoDB  resource \nto different locations. A location can be a single node, rack, or\nregion. By default,  Kubernetes Operator  tries to spread pods across\ndifferent nodes. Type : string Default : kubernetes.io/hostname This key defines which\n label \nis used to determine which topology\n domain \na node belongs to. Type : collection Metadata for the  Kubernetes  Pods that the  MongoDB Enterprise Kubernetes Operator  creates for\neach  mongos  instance. To review which fields you can add to  spec.mongosPodSpec.podTemplate.metadata , see\nthe  Kubernetes documentation . Type : collection Specifications of the  Kubernetes  Pods that the  MongoDB Enterprise Kubernetes Operator  creates for\neach  mongos  instance. To review which fields you can add to  spec.mongosPodSpec.podTemplate.spec , see the\n Kubernetes PodSpec v1 core API . Use this setting to specify the CPU and RAM allocations for each pod. For\nexamples, see  the samples on GitHub . When you add containers to  spec.mongosPodSpec.podTemplate.spec.containers , the\n Kubernetes Operator  adds them to the  Kubernetes  pod. These containers are\nappended to each  mongos  instance containers in the pod. Type : integer Required . Number of shards in the  sharded cluster . Type : collection Additional  configuration options  with\nwhich you want to start each  sharded cluster  shard member. The  Kubernetes Operator  supports all configuration options that the MongoDB\nversion you deploy through the MongoDB Agent supports, except that the\n Kubernetes Operator  overrides values that you provide for any of the\nfollowing options: To learn more about the configuration options that the  Kubernetes Operator \nowns, see  MongoDB  Kubernetes Operator  Exclusive Settings . To learn which configuration options you can use, see\n Advanced Options for MongoDB Deployments  in the  Ops Manager \ndocumentation. net.tls.certificateKeyFile net.tls.clusterFile net.tls.PEMKeyFile replication.replSetName security.clusterAuthMode sharding.clusterRole storage.dbPath systemLog.destination systemLog.path Type : collection MongoDB Agent configuration settings for each  sharded cluster  shard member. Type : collection MongoDB Agent settings with which you want to start each  sharded cluster  shard member. You must provide MongoDB Agent settings as key-value pairs. The values\nmust be strings. For a list of supported MongoDB Agent settings, see: MongoDB Agent Settings \nfor  Cloud Manager  projects. MongoDB Agent Settings  for the  Ops Manager  version you\ndeployed with the  Kubernetes Operator . Type : collection Has  Kubernetes Operator  create one  Persistent Volume Claim  and mount all\nthree directories for data, journal, and logs to the same  Persistent Volume . You must set the values in this collection if\n spec.persistent   : true . You may set this collection or the  persistence.multiple \ncollections but not both. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum size of  Persistent Volume  that should be mounted. This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is 16Gi. If each  sharded cluster  shard member in requires 60 gigabytes of\nstorage space, set this value to  60Gi . storageClass string Type of storage specified in a  Persistent Volume Claim . You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : collection Has  Kubernetes Operator  create a  Persistent Volume Claim  and mount a\ndirectory for data to its own  Persistent Volume . You must set the values in this collection if\n spec.persistent   : true . You may set this collection or the  persistence.single \ncollection but not both. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum storage capacity that must be available on a  Kubernetes \n node  to host each  sharded cluster  shard member on  Kubernetes . This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is 16Gi. If this  MongoDB  resource  requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage needed for each  sharded cluster  shard member. You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : collection Has  Kubernetes Operator  create a  Persistent Volume Claim  and mount a\ndirectory for journal to its own  Persistent Volume . You must set the values in this collection if\n spec.persistent   : true . You may set this collection or the  persistence.single \ncollection but not both. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum storage capacity that must be available on a  Kubernetes \n node  to host each  sharded cluster  shard member on  Kubernetes . This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is 1Gi. If this  MongoDB  resource  requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage needed for each  sharded cluster  shard member. You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : collection Has  Kubernetes Operator  create a  Persistent Volume Claim  and mount a\ndirectory for logs to its own  Persistent Volume . You must set the values in this collection if\n spec.persistent   : true . You may set this collection or the  persistence.single \ncollection but not both. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum storage capacity that must be available on a  Kubernetes \n node  to host each  sharded cluster  shard member on  Kubernetes . This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is 3Gi. If this  MongoDB  resource  requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage needed for each  sharded cluster  shard member. You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : collection Template \nfor the  Kubernetes  Pods that the  MongoDB Enterprise Kubernetes Operator  creates for each  sharded cluster  shard member. Template values take precedence over values specified in  spec.shardPodSpec . The  Kubernetes Operator  doesn't validate the fields you provide\nin  spec.shardPodSpec.podTemplate . Type : string Kubernetes   rule  to determine whether multiple  MongoDB  resource   Pods \nmust be co-located with other  Pods . To learn more about the use cases,\nsee  Affinity and Anti-Affinity  in the  Kubernetes  documentation. Type : string Kubernetes   rule  to place  Pods  for  replica set  on a\nspecific range of  nodes . For optimized read-write performance, use node affinity rules that\nrestrict  Pods  to run on particular  nodes , or to prefer\nto run on particular  nodes . Type : string Default : kubernetes.io/hostname Sets a  rule  to spread  Pods  hosting  MongoDB  resource \nto different locations. A location can be a single node, rack, or\nregion. By default,  Kubernetes Operator  tries to spread pods across\ndifferent nodes. Type : string Default : kubernetes.io/hostname This key defines which\n label \nis used to determine which topology\n domain \na node belongs to. Type : collection Metadata for the  Kubernetes  Pods that the  MongoDB Enterprise Kubernetes Operator  creates for\neach  sharded cluster  shard member. To review which fields you can add to  spec.shardPodSpec.podTemplate.metadata , see\nthe  Kubernetes documentation . Type : collection Specifications of the  Kubernetes  Pods that the  MongoDB Enterprise Kubernetes Operator  creates for\neach  sharded cluster  shard member. To review which fields you can add to  spec.shardPodSpec.podTemplate.spec , see the\n Kubernetes PodSpec v1 core API . Use this setting to specify the CPU and RAM allocations for each pod. For\nexamples, see  the samples on GitHub . When you add containers to  spec.shardPodSpec.podTemplate.spec.containers , the\n Kubernetes Operator  adds them to the  Kubernetes  pod. These containers are\nappended to each  sharded cluster  shard member containers in the pod. Type : array List that contains  StatefulSet  overrides per shard. Type : collection Has  Kubernetes Operator  create one  Persistent Volume Claim  and mount all\nthree directories for data, journal, and logs to the same  Persistent Volume . You must set the values in this collection if\n spec.persistent   : true . You may set this collection or the  persistence.multiple \ncollections but not both. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum size of  Persistent Volume  that should be mounted. This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is 16Gi. If the specific shard in requires 60 gigabytes of\nstorage space, set this value to  60Gi . storageClass string Type of storage specified in a  Persistent Volume Claim . You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : collection Has  Kubernetes Operator  create a  Persistent Volume Claim  and mount a\ndirectory for data to its own  Persistent Volume . You must set the values in this collection if\n spec.persistent   : true . You may set this collection or the  persistence.single \ncollection but not both. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum storage capacity that must be available on a  Kubernetes \n node  to host the specific shard on  Kubernetes . This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is 16Gi. If this  MongoDB  resource  requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage needed for the specific shard. You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : collection Has  Kubernetes Operator  create a  Persistent Volume Claim  and mount a\ndirectory for journal to its own  Persistent Volume . You must set the values in this collection if\n spec.persistent   : true . You may set this collection or the  persistence.single \ncollection but not both. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum storage capacity that must be available on a  Kubernetes \n node  to host the specific shard on  Kubernetes . This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is 1Gi. If this  MongoDB  resource  requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage needed for the specific shard. You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : collection Has  Kubernetes Operator  create a  Persistent Volume Claim  and mount a\ndirectory for logs to its own  Persistent Volume . You must set the values in this collection if\n spec.persistent   : true . You may set this collection or the  persistence.single \ncollection but not both. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum storage capacity that must be available on a  Kubernetes \n node  to host the specific shard on  Kubernetes . This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is 3Gi. If this  MongoDB  resource  requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage needed for the specific shard. You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : collection Template \nfor the  Kubernetes  Pods that the  MongoDB Enterprise Kubernetes Operator  creates for the specific shard. Template values take precedence over values specified in  spec.shardSpecificPodSpec . The  Kubernetes Operator  doesn't validate the fields you provide\nin  spec.shardSpecificPodSpec.podTemplate . Type : string Kubernetes   rule  to determine whether multiple  MongoDB  resource   Pods \nmust be co-located with other  Pods . To learn more about the use cases,\nsee  Affinity and Anti-Affinity  in the  Kubernetes  documentation. Type : string Default : kubernetes.io/hostname Sets a  rule  to spread  Pods  hosting  MongoDB  resource \nto different locations. A location can be a single node, rack, or\nregion. By default,  Kubernetes Operator  tries to spread pods across\ndifferent nodes. Type : string Default : kubernetes.io/hostname This key defines which\n label \nis used to determine which topology\n domain \na node belongs to. Type : collection Metadata for the  Kubernetes  Pods that the  MongoDB Enterprise Kubernetes Operator  creates for\nthe specific shard. To review which fields you can add to  spec.shardSpecificPodSpec.podTemplate.metadata , see\nthe  Kubernetes documentation . Type : collection Specifications of the  Kubernetes  Pods that the  MongoDB Enterprise Kubernetes Operator  creates for\nthe specific shard. To review which fields you can add to  spec.shardSpecificPodSpec.podTemplate.spec , see the\n Kubernetes PodSpec v1 core API . Use this setting to specify the CPU and RAM allocations for each pod. For\nexamples, see  the samples on GitHub . When you add containers to  spec.shardSpecificPodSpec.podTemplate.spec.containers , the\n Kubernetes Operator  adds them to the  Kubernetes  pod. These containers are\nappended to the specific shard containers in the pod. You can use Prometheus with your standalone resource, replica sets, or\nsharded clusters. To learn more, see  Deploy a Resource to Use with Prometheus . To view\nan example, see  MongoDB Resource with Prometheus . The following settings apply when you use Prometheus\nwith your MongoDB resource: Type : array Optional List that contains the parameters for exposing metrics to Prometheus. Type : string Optional Default :  \"/metrics\" Human-readable string that indicates the path to the metrics\nendpoint. If you don't specify this setting, the default applies. Type : object Conditional Object that contains the details of the  secret  for\nbasic HTTP authentication. If you want to use Prometheus with your\nMongoDB resource, you must specify this setting. Type : string Optional Default :  \"password\" Human-readable string that indentifies the key in the  secret \nthat stores the password for basic HTTP authentication. If you don't\nspecify this setting, the default applies. Type : string Conditional Human-readable label that identifies the  secret  that contains\nthe password for basic HTTP authentication. If you want to use\nPrometheus with your MongoDB resource, you must specify this setting. Type : integer Optional Default:  9216 Number that identifies the port that the metrics endpoint will\nbind to. If you don't specify this setting, the default applies. Type : object Optional Object that contains the details of the  secret  for  TLS (Transport Layer Security) \nauthentication. Type : string Optional Default :  \"password\" Human-readable string that indentifies the key in the  secret \nthat stores the password for  TLS (Transport Layer Security)  authentication. If you don't\nspecify this setting, the default applies. Type : string Conditional Human-readable label that identifies the  secret  that contains\nthe password for  TLS (Transport Layer Security)  authentication. If you want to use\nPrometheus with your MongoDB resource and you want to use  TLS (Transport Layer Security) \nauthentication, you must specify this setting. Type : string Conditional Human-readable label that identifies the user for basic HTTP\nauthentication. If you want to use Prometheus with your MongoDB\nresource, you must specify this setting. The following security settings apply only to replica set and sharded\ncluster resource types: Type : boolean Default :  false Encrypts communications using TLS certificates between: By default,  net.ssl.mode  is set to  requireSSL . To change the\n TLS (Transport Layer Security)  mode used for client and database connections, see\n spec.additionalMongodConfig.net.ssl.mode . spec.security.tls.enabled  is deprecated starting in  Kubernetes Operator  version 1.19.\nTo enable  TLS (Transport Layer Security) , provide a value for\nthe  spec.security.certsSecretPrefix  setting. MongoDB hosts in a replica set or sharded cluster configuration Clients ( mongo  shell, drivers,  MongoDB Compass , and others)\nand the MongoDB deployment Type : string Provide the name of the  ConfigMap  that stores the  CA (Certificate Authority)  for the  MongoDB  resource . If you use a custom  CA (Certificate Authority)  to sign your  TLS (Transport Layer Security)  certificates for the  MongoDB  resource ,\nyou must specify this parameter. The  Kubernetes Operator  requires that you name the\n MongoDB  resource  certificate  ca-pem  in the ConfigMap. Type : string Text to prefix to the  Kubernetes   secrets  that you\ncreated that contain your replica set's or sharded cluster's  TLS (Transport Layer Security) \nkeys and certificates. To learn more about naming the secrets that contain your  TLS (Transport Layer Security) \ncertificates, see the topic in  Deploy a Replica Set  that applies to your\ndeployment. You must prefix your secrets with  <prefix>-<metadata.name> . If you call your deployment  my-deployment  and you set the\nprefix to  mdb , you must name the  TLS (Transport Layer Security)  secret for the\nclient  TLS (Transport Layer Security)  communications  mdb-my-deployment-cert . Also,\nyou must name the  TLS (Transport Layer Security)  secret for internal cluster authentication\n(if enabled)  mdb-my-deployment-clusterfile . Type : boolean List of every domain that should be added to  TLS (Transport Layer Security)  certificates to\neach pod in this deployment. When you set this parameter, every  CSR (Certificate Signing Request) \nthat the  Kubernetes Operator  transforms into a  TLS (Transport Layer Security)  certificate includes\na  SAN (Subject Alternative Name)  in the form  <pod name>.<additional cert domain> . Replica set resources don't need this parameter. Use\n spec.connectivity.replicaSetHorizons  instead. If you add this parameter to a  TLS (Transport Layer Security) -enabled resource,  Kubernetes \ndisplays an error when the resource reaches the  Pending  state.\nThis error displays:  Please manually remove the |csr| in order\nto proceed.  To remedy this issue: Remove any existing  CSR (Certificate Signing Request) s so that  Kubernetes  can generate new\n CSR (Certificate Signing Request) s. To learn how to delete a resource, see the\n deleting resources  in the\n Kubernetes  documentation. Approve the  CSR (Certificate Signing Request) s after  Kubernetes  generates them. Type : string Default :  requireSSL Specifies which  sslMode  is used for network connections.\nThe following are valid options: Value Description allowSSL Connections between servers do not use  TLS (Transport Layer Security) . For incoming\nconnections, the server accepts both  TLS (Transport Layer Security)  and\nnon-TLS. preferSSL Connections between servers use  TLS (Transport Layer Security) . For incoming\nconnections, the server accepts both  TLS (Transport Layer Security)  and\nnon-TLS. requireSSL The server uses and accepts only  TLS (Transport Layer Security)  encrypted connections. Type : string New in MongoDB version 4.2. Prevents a MongoDB server running with  TLS (Transport Layer Security)  from accepting incoming connections\nthat use a specific protocol or protocols. To specify multiple protocols, enter\na comma separated list of protocols. For example,  TLS1_0,TLS1_1 . This setting recognizes the following protocols:  TLS1_0 ,  TLS1_1 ,  TLS1_2 ,\nand starting in MongoDB 4.0.4 (and 3.6.9),  TLS1_3 . If you specify an unrecognized protocol, the\nserver won't start. On macOS, you can't disable  TLS1_1  and enable both  TLS1_0  and  TLS1_2 .\nYou must disable at least  TLS1_0  or  TLS1_2  also. For example,  TLS1_0,TLS1_1 \ndisables  TLS1_2  on macOS. The list of protocols that you disable replaces the default list of disabled protocols. Starting in MongoDB version 4.0, MongoDB disables the use of  TLS (Transport Layer Security)  1.0 if  TLS (Transport Layer Security)  1.1+\nis available on the system. To enable the disabled  TLS (Transport Layer Security)  1.0, specify  none  as the value for\n spec.additionalMongodConfig.net.tls.disabledProtocols .\nTo learn more about this setting, see  Disable TLS 1.0 . Members of replica sets and sharded clusters must speak at least one protocol in common. Type : collection Authentication specifications for your MongoDB deployment. Type : boolean Default :  false Specifies whether authentication is enabled on the  Cloud Manager or Ops Manager \nproject. If set to  true , you must set an authentication mechanism in\n spec.security.authentication.modes . The  Kubernetes Operator  manages authentication for this MongoDB\nresource if you include this setting, even if it's set to\n false . You can't configure authentication for this\nresource using the  Cloud Manager or Ops Manager  UI  or APIs while this setting\nexists in the resource specification. Omit this setting if you want to manage authentication using the\n Cloud Manager or Ops Manager  UI or APIs. Type : array Specifies the authentication mechanism that your MongoDB deployment\nuses. Valid values are  SCRAM ,  SCRAM-SHA-1 ,  MONGODB-CR ,\n X509 , and  LDAP . We recommend  SCRAM-SHA-256  ( SCRAM )\nover  SCRAM-SHA-1 . If you specify  SCRAM-SHA-1 , you must also\nspecify  MONGODB-CR . If you provide more than one value for\n spec.security.authentication.modes , you must also specify a\nvalue for  spec.security.authentication.agents.mode . To enable  X.509 internal cluster authentication  for the  Cloud Manager or Ops Manager  project, set this\nvalue to  [\"X509\"]  and specify the following settings: spec.security.authentication.internalCluster   : \"X509\" provide a value for the\n spec.security.certsSecretPrefix  setting.` Type : string Specifies whether  X.509 internal cluster authentication  is enabled. To enable X.509 internal cluster authentication, set to  \"X509\" .\nRequires that the following settings be specified: The  Kubernetes Operator  accepts the following values: spec.security.authentication.modes   : [\"X509\"] spec.security.certsSecretPrefix [\"X509\"] : X.509 internal cluster authentication is enabled. \"\"  or omitted: internal cluster authentication is not enabled. After you enable internal cluster authentication, you can't disable\nit. Type : boolean Default :  false Specifies whether the MongoDB host requires clients to connect using a\n TLS (Transport Layer Security)  certificate. Defaults to  true  if you enable  TLS (Transport Layer Security) \nauthentication. To enable  TLS (Transport Layer Security)  authentication, provide a value for the  spec.security.certsSecretPrefix  setting. Type : collection Required for LDAP authentication. Configures  LDAP (Lightweight Directory Access Protocol)  authentication for the  Cloud Manager or Ops Manager  project. To enable\n LDAP (Lightweight Directory Access Protocol)  authentication, set\n spec.security.authentication.modes  to  [\"LDAP\"] . Type : array of strings Required for LDAP authentication. List of hostnames and ports of the  LDAP (Lightweight Directory Access Protocol)  servers. Specify\nhostnames with their respective ports in the following format: Type : integer Specifies how many milliseconds an authentication request should\nwait before timing out. Type : string Required for LDAP authentication. Specifies whether the  LDAP (Lightweight Directory Access Protocol)  server accepts  TLS (Transport Layer Security) . If the  LDAP (Lightweight Directory Access Protocol)  server accepts  TLS (Transport Layer Security) , set the value to  tls . If\nthe  LDAP (Lightweight Directory Access Protocol)  server doesn't accept  TLS (Transport Layer Security) , leave this value blank or set\nthe value to  none . If you specify a string other than  none  or  tls ,\n Kubernetes Operator  still sets the setting to  tls . Type : collection Required for LDAP authentication with TLS. ConfigMap  that contains a  CA (Certificate Authority)  which validates the  LDAP (Lightweight Directory Access Protocol) \nserver's  TLS (Transport Layer Security)  certificate. Type : string Required for LDAP authentication with TLS. Name of the  ConfigMap  that contains a  CA (Certificate Authority)  which validates\nthe  LDAP (Lightweight Directory Access Protocol)  server's  TLS (Transport Layer Security)  certificate. Type : string Required for LDAP authentication with TLS. Field name that stores the  CA (Certificate Authority)  which validates the  LDAP (Lightweight Directory Access Protocol) \nserver's  TLS (Transport Layer Security)  certificate. Type : string Required for LDAP authentication. LDAP (Lightweight Directory Access Protocol)  Distinguished Name to which MongoDB binds when connecting to\nthe  LDAP (Lightweight Directory Access Protocol)  server. Type : collection Required for LDAP authentication. Specifies the  secret  that contains the password with which\nMongoDB binds when connecting to the  LDAP (Lightweight Directory Access Protocol)  server. Type : string Required for LDAP authentication. Name of the  secret  that contains the password with which MongoDB\nbinds when connecting to the  LDAP (Lightweight Directory Access Protocol)  server. The  secret  must contain only one  password  field which stores\nthe password. Type : string Required for LDAP authorization. An  RFC4515  and  RFC4516  LDAP-formatted query URL\ntemplate executed by MongoDB to obtain the LDAP groups that the user\nbelongs to. The query is relative to the host or hosts\nspecified in  spec.security.authentication.ldap.servers .\nYou can use the following tokens in the template: Substitutes the authenticated username, or the\n transformed \nusername, into the LDAP query. Substitutes the supplied username, before either\nauthentication or LDAP transformation, into the LDAP query.\n( Available starting in MongoDB version 4.2 ) LDAP Query Templates  in the MongoDB Manual Type : string The Distinguished Name (DN) of the LDAP group to which the\nMongoDB Agent user belongs. This setting is required if: spec.security.authentication.ldap.authzQueryTemplate  is\npresent, and spec.security.authentication.agents.mode  is  LDAP  or\n X509 . Type : string Maps the username provided to  mongod  or\n mongos  for authentication to a LDAP Distinguished Name\n(DN). security.ldap.userToDNMapping  in the MongoDB Manual Type : integer Specifies how many seconds MongoDB waits to flush the LDAP user\ncache. Defaults to 30 seconds. Type : collection MongoDB Agent authentication configuration for the  Cloud Manager or Ops Manager  project. Type : string The authentication mechanism that the MongoDB Agents for\nyour MongoDB deployment use. Valid values are  SCRAM ,\n SCARM-SHA-1 ,  MONGODB-CR ,  X509 , and\n LDAP . The value you specify must also be present in\n spec.security.authentication.modes . We recommend\n SCRAM-SHA-256  ( SCRAM ) over  SCRAM-SHA-1 . If you specify\n SCRAM-SHA-1 , you must also specify  MONGODB-CR . This setting is required if you specified more than one value for\n spec.security.authentication.modes . Type : string Name of the user that the MongoDB Agents use to interact with your\nMongoDB deployment. The username is mapped to an LDAP Distinguished\nName (DN) according to\n spec.security.authentication.ldap.userToDNMapping . The\nresulting DN must already exist in your LDAP deployment. This setting is required if\n spec.security.authentication.agents.mode  is  LDAP . Type : collection Details of the  secret  that contains the password for the\n spec.security.authentication.agents.automationUserName \nuser. This setting is required if\n spec.security.authentication.agents.mode  is  LDAP . Type : string Name of the  secret  that contains the password for the\n spec.security.authentication.agents.automationUserName \nuser. You must create this secret in the same namespace to which you\ndeploy the  Kubernetes Operator : This secret must contain one key, the value of which matches the\npassword of the\n spec.security.authentication.agents.automationUserName  user\nin your LDAP deployment. This setting is required if\n spec.security.authentication.agents.mode  is  LDAP . Type : string Key in the\n spec.security.authentication.agents.automationPasswordSecretRef.name \n secret  that contains the password for the user in\n spec.security.authentication.agents.automationUserName . This setting is required if\n spec.security.authentication.agents.mode  is\n LDAP . Type : string Specifies the  secret  that contains the MongoDB Agent's\n TLS (Transport Layer Security)  certificate. If omitted, defaults to  agent-certs . This secret must contain the following keys, the\nvalues of which are  TLS (Transport Layer Security)  certificates that can be validated by the\nserver: You must create this secret in the same namespace to which you\ndeploy the  Kubernetes Operator : mms-automation-agent-pem mms-backup-agent-pem mms-monitoring-agent-pem Type : array Array that defines  User-defined roles  that give you\nfine-grained access control over your MongoDB deployment. To enable user-defined roles, the\n spec.security.authentication.enabled  must be  true . In this example, a user-defined role named  customRole  allows\nusers assigned this role to: Insert documents into the  cats  collection in the  pets \ndatabase, and Find and insert documents into the  dogs  collection in the\n pets  database. Type : string Name of the user-defined role. Type : string The database in which you want to store the user-defined role. admin Type : array Array that defines the IP address from which and to which users\nassigned this  spec.security.roles.role  can\nconnect. Type : array Array of IP addresses or CIDR blocks from which users assigned this\n spec.security.roles.role  can connect. MongoDB servers reject connection requests from users with this role\nif the requests come from a client that is not present in this array. Type : array Array of IP addresses or CIDR blocks to which users assigned this\n spec.security.roles.role  can connect. MongoDB servers reject connection requests from users with this role\nif the client requests to connect to a server that is not present in\nthis array. Type : array Array that describes the privileges that users granted this role\npossess. Type : array List of actions that users granted this role can perform. For a list\nof accepted values, see  Privilege Actions  in the\nMongoDB Manual for the MongoDB versions you deploy with the\n Kubernetes Operator . Type : collection Resources for which the privilege\n actions \napply. This collection must include either: The\n spec.security.roles.privileges.resource.database \nand\n spec.security.roles.privileges.resource.collection \nsettings, or The\n spec.security.roles.privileges.resource.cluster \nsetting with a value of  true . Type : string Database for which the privilege\n actions \napply. If you provide a value for this setting, you must also provide a value\nfor\n spec.security.roles.privileges.resource.collection . Type : string Collection in the\n database \nfor which the privilege\n actions \napply. If you provide a value for this setting, you must also provide a value\nfor\n spec.security.roles.privileges.resource.database . Type : boolean Default : False Flag that indicates that the privilege\n actions \napply to all databases and collections in the MongoDB deployment. If\nomitted, defaults to  false . If set to true, do not provide values for\n spec.security.roles.privileges.resource.database \nand\n spec.security.roles.privileges.resource.collection . The following example shows a resource specification for a\nstandalone deployment with every setting provided: The following example shows a resource specification for a\n replica set  with every setting provided: The following example shows a resource specification for a\n sharded cluster  with every setting provided: The following  StatefulSets  settings apply only to replica set and\nsharded cluster resource types. Type : collection Specification for the  StatefulSet  that the  MongoDB Enterprise Kubernetes Operator  creates\nfor  MongoDB  resources . Type : string Default :  <resource_name>-svc  and  <resource_name>-svc-external Name of the  Kubernetes  service to be created or used for a  StatefulSet .\nIf the service with this name already exists, the  MongoDB Enterprise Kubernetes Operator  doesn't\ndelete or recreate it. This setting lets you create your own custom\nservices and lets the  Kubernetes Operator  reuse them.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone\nspec:\n  version: \"4.4.0-ent\"\n  service: my-service\n\n  opsManager:\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  type: Standalone\n\n  persistent: true\n  agent:\n    startupOptions:\n      maxLogFiles: \"30\"\n      dialTimeoutSeconds: \"40\"\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "externalAccess:\n  externalService:\n    annotations:\n      # cloud-specific annotations for the service\n    spec:\n      type: NodePort # default is LoadBalancer\n      # you can specify other spec overrides if necessary"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  type: ReplicaSet\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:30907\"\n      - \"example-website\": \"web2.example.com:32350\"\n      - \"example-website\": \"web3.example.com:31185\"\n..."
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  memberConfig:\n    - votes: 1\n      priority: \"0.5\"\n      tags:\n        tag1: \"value1\"\n        environment: \"prod\"\n    - votes: 1\n      priority: \"1.5\"\n      tags:\n        tag2: \"value2\"\n        environment: \"prod\"\n    - votes: 0\n      priority: \"0.5\"\n      tags:\n        tag2: \"value2\"\n        environment: \"prod\""
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster-options\nspec:\n  version: \"4.4.0-ent\"\n  type: ShardedCluster\n  opsManager:\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  persistent: true\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 1\n\n  mongos:\n    agent:\n      startupOptions:\n        maxLogFiles: \"30\"\n\n  configSrv:\n     agent:\n       startupOptions:\n         dialTimeoutSeconds: \"40\"\n  shard:\n     agent:\n       startupOptions:\n         serverSelectionTimeoutSeconds: \"20\"\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster-options\nspec:\n  version: \"4.4.0-ent\"\n  type: ShardedCluster\n  opsManager:\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  persistent: true\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 1\n\n  mongos:\n    agent:\n      startupOptions:\n        maxLogFiles: \"30\"\n\n  configSrv:\n     agent:\n       startupOptions:\n         dialTimeoutSeconds: \"40\"\n  shard:\n     agent:\n       startupOptions:\n         serverSelectionTimeoutSeconds: \"20\"\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster-options\nspec:\n  version: \"4.4.0-ent\"\n  type: ShardedCluster\n  opsManager:\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  persistent: true\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 1\n\n  mongos:\n    agent:\n      startupOptions:\n        maxLogFiles: \"30\"\n\n  configSrv:\n     agent:\n       startupOptions:\n         dialTimeoutSeconds: \"40\"\n  shard:\n     agent:\n       startupOptions:\n         serverSelectionTimeoutSeconds: \"20\"\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  security:\n    authentication:\n      ldap:\n        servers:\n          - \"<hostname1>:<port1>\"\n          - \"<hostname2>:<port2>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic ldap-agent-user \\\n--from-literal=\"password=<password>\" -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic agent-certs \\\n--from-file=mms-automation-agent-pem=<automation-cert.pem> \\\n--from-file=mms-backup-agent-pem=<backup-cert.pem> \\\n--from-file=mms-monitoring-agent-pem=<monitoring-cert.pem> \\\n--namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  type: ReplicaSet\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  persistent: true\n  security:\n    authentication:\n      enabled: true\n      modes:\n        - \"SCRAM\"\n    roles:\n      - role: \"customRole\"\n        db: admin    \n        privileges:\n        - actions:\n          - insert\n          resource:\n            collection: cats\n            db: pets\n        - actions:\n          - insert\n          - find\n          resource:\n            collection: dogs\n            db: pets\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone\nspec:\n  version: \"4.2.2-ent\"\n  service: my-service\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  persistent: true\n  type: Standalone\n  additionalMongodConfig:\n    systemLog:\n      logAppend: true\n      verbosity: 4\n    operationProfiling:\n      mode: slowOp\n  podSpec:\n    persistence:\n      single:\n        storage: \"12Gi\"\n        storageClass: standard\n        labelSelector:\n          matchExpressions:\n          - {key: environment, operator: In, values: [dev]}\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: security\n                operator: In\n                values:\n                - S1\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/e2e-az-name\n                operator: In\n                values:\n                - e2e-az1\n                - e2e-az2\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - podAffinityTerm:\n                topologyKey: \"mykey\"\n              weight: 50\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: \"4.4.0-ent\"\n  service: my-service\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  persistent: true\n  type: ReplicaSet\n  podSpec:\n    persistence:\n      multiple:\n        data:\n          storage: \"10Gi\"\n        journal:\n          storage: \"1Gi\"\n          labelSelector:\n            matchLabels:\n              app: \"my-app\"\n        logs:\n          storage: \"500M\"\n          storageClass: standard\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: security\n                operator: In\n                values:\n                - S1\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/e2e-az-name\n                operator: In\n                values:\n                - e2e-az1\n                - e2e-az2\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - podAffinityTerm:\n                topologyKey: \"mykey\"\n              weight: 50\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n  security:\n    certsSecretPrefix: \"prefix\"\n    tls:\n      ca: custom-ca\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n  statefulSet:\n    spec:\n      serviceName: my-service\n  additionalMongodConfig:\n    net:\n      ssl:\n        mode: preferSSL\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.4.0-ent\"\n  service: my-service\n  type: ShardedCluster\n\n  ## Please Note: The default Kubernetes cluster name is\n  ## `cluster.local`.\n  ## If your cluster has been configured with another name, you can\n  ## specify it with the `clusterDomain` attribute.\n\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n\n  persistent: true\n  configSrvPodSpec:\n\n    # if \"persistence\" element is omitted then Operator uses the\n    # default size (5Gi) for mounting single Persistent Volume\n\n    podTemplate:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                  - key: security\n                    operator: In\n                    values:\n                      - S1\n              topologyKey: failure-domain.beta.kubernetes.io/zone\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: kubernetes.io/e2e-az-name\n                    operator: In\n                    values:\n                      - e2e-az1\n                      - e2e-az2\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            topologyKey: nodeId\n  mongosPodSpec:\n    podTemplate:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                  - key: security\n                    operator: In\n                    values:\n                      - S1\n              topologyKey: failure-domain.beta.kubernetes.io/zone\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: kubernetes.io/e2e-az-name\n                    operator: In\n                    values:\n                      - e2e-az1\n                      - e2e-az2\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            topologyKey: nodeId\n  shardPodSpec:\n    persistence:\n      multiple:\n        # if the child of \"multiple\" is omitted then the default size will be used.\n        # 16GB for \"data\", 1GB for \"journal\", 3GB for \"logs\"\n        data:\n          storage: \"20Gi\"\n        logs:\n          storage: \"4Gi\"\n          storageClass: standard\n    podTemplate:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                  - key: security\n                    operator: In\n                    values:\n                      - S1\n              topologyKey: failure-domain.beta.kubernetes.io/zone\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: kubernetes.io/e2e-az-name\n                    operator: In\n                    values:\n                      - e2e-az1\n                      - e2e-az2\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            topologyKey: nodeId\n  mongos:\n    additionalMongodConfig:\n      systemLog:\n        logAppend: true\n        verbosity: 4\n  configSrv:\n    additionalMongodConfig:\n      operationProfiling:\n        mode: slowOp\n  shard:\n    additionalMongodConfig:\n      storage:\n        journal:\n          commitIntervalMs: 50\n  security:\n    certsSecretPrefix: \"prefix\"\n    tls:\n     ca: custom-ca\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n  statefulSet:\n    spec:\n      serviceName: my-service\n...\n"
                }
            ],
            "preview": "Type: string",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/edit-deployment",
            "title": "Edit a Database Resource",
            "headings": [
                "Considerations",
                "Prerequisites",
                "Procedure",
                "Edit the Kubernetes resource specification file.",
                "Modify or add settings as needed.",
                "Save your specification file.",
                "Apply the file."
            ],
            "paragraphs": "You can modify a deployment's configuration and topology, including its\nMongoDB versions, storage engines, and numbers of hosts or shards. You\ncan make modifications at all levels of a deployment's topology from a\ntop-level  sharded cluster  or\n replica set  to lower levels, such as a replica set within a\nsharded cluster, or an individual process within a replica set. You can\nalso modify  standalone  processes. You can't change individual members of a replica set or sharded cluster.\nYou can change only the whole set or cluster. If a setting isn't available for a MongoDB resource, you can make changes\nusing only the  Ops Manager  or  Cloud Manager  application. MongoDB custom resources do not support all\n mongod  command line options. If you use an\nunsupported option in your object specification file, the backing\n MongoDB Agent \noverrides the unsupported options. For a complete list of options\nsupported by MongoDB custom resources, see  MongoDB Database Resource Specification . You can configure certain settings using only the  Kubernetes Operator . To\nreview the list of settings, see  MongoDB  Kubernetes Operator  Exclusive Settings . Before you update a MongoDB  object , complete the following procedures: Install and Configure the  Kubernetes Operator Create Credentials for the  Kubernetes Operator Create One Project using a ConfigMap Deploy a MongoDB Database Resource",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <standalone-conf>.yaml"
                }
            ],
            "preview": "You can modify a deployment's configuration and topology, including its\nMongoDB versions, storage engines, and numbers of hosts or shards. You\ncan make modifications at all levels of a deployment's topology from a\ntop-level sharded cluster or\nreplica set to lower levels, such as a replica set within a\nsharded cluster, or an individual process within a replica set. You can\nalso modify standalone processes.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/resize-pv-storage",
            "title": "Resize Storage for One Database Resource",
            "headings": [
                "Prerequisites",
                "Storage Class Must Support Resizing",
                "Procedure",
                "Create or identify a persistent custom resource.",
                "Insert data to the database that the resource serves.",
                "Patch each persistence volume.",
                "Remove the StatefulSets.",
                "Update the database resource with a new storage value.",
                "Update the pods in a rolling fashion.",
                "Validate data exists on the updated Persistent Volume Claim."
            ],
            "paragraphs": "Make sure the  StorageClass  and volume plugin provider that the  Persistent Volumes \nuse supports resize: If you don't have a StorageClass that supports resizing, ask your  Kubernetes \nadministrator to help. Use an existing database resource or create a new one with persistent\nstorage. Wait until the persistent volume gets to the  Running \nstate. A database resource with persistent storage would include: Start  mongo  in the  Kubernetes  cluster. Insert data into the  test  database. Invoke the following commands for the entire replica set: Wait until each  Persistent Volume Claim  gets to the following condition: Delete a  StatefulSet  resource. This step removes the  StatefulSet  only. The pods remain\nunchanged and running. Update the disk size. Open your preferred text editor and make\nchanges similar to this example: To update the disk size of the replica set to 2 GB, change the\n storage  value in database resource specification: Recreate a  StatefulSet  resource with the new volume size. Wait until this StatefulSet achieves the  Running  state. Invoke the following command: The new pods mount the resized volume. If the  Persistent Volumes  were reused, the data that you inserted in  Step\n2  can be found on the databases stored in  Persistent Volumes :",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl patch storageclass/<my-storageclass> --type='json' \\\n        -p='[{\"op\": \"add\", \"path\": \"/allowVolumeExpansion\", \"value\": true }]'"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.4.0\"\n  project: my-project\n  credentials: my-credentials\n  type: ReplicaSet\n  podSpec:\n    persistence:\n      single:\n        storage: \"1Gi\""
                },
                {
                    "lang": "sh",
                    "value": "$kubectl exec -it <my-replica-set>-0 \\\n         /var/lib/mongodb-mms-automation/mongodb-linux-x86_64-4.4.0/bin/mongo"
                },
                {
                    "lang": "javascript",
                    "value": "<my-replica-set>:PRIMARY> use test\n\nswitched to db test\n\n<my-replica-set>:PRIMARY> db.tmp.insertOne({\"foo\":\"bar\"})\n\n{\n  \"acknowledged\" : true,\n  \"insertedId\" : ObjectId(\"61128cb4a783c3c57ae5142d\")\n}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl patch pvc/\"data-<my-replica-set>-0\" -p='{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"2Gi\"}}}}'\nkubectl patch pvc/\"data-<my-replica-set>-1\" -p='{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"2Gi\"}}}}'\nkubectl patch pvc/\"data-<my-replica-set>-2\" -p='{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"2Gi\"}}}}'"
                },
                {
                    "lang": "yaml",
                    "value": "- lastProbeTime: null\n  lastTransitionTime: \"2019-08-01T12:11:39Z\"\n  message: Waiting for user to (re-)start a pod to finish file\n           system resize of volume on node.\n  status: \"True\"\n  type: FileSystemResizePending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete sts --cascade=false <my-replica-set>"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.4.0\"\n  project: my-project\n  credentials: my-credentials\n  type: ReplicaSet\n  podSpec:\n    persistence:\n      single:\n        storage: \"2Gi\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f my-replica-set-vol.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl rollout restart sts <my-replica-set>"
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl exec -it <my-replica-set>-1 \\\n          /var/lib/mongodb-mms-automation/mongodb-linux-x86_64-4.4.0/bin/mongo"
                },
                {
                    "lang": "javascript",
                    "value": "<my-replica-set>:PRIMARY> use test\nswitched to db test\n\n<my-replica-set>:PRIMARY> db.tmp.count()\n1"
                }
            ],
            "preview": "Make sure the StorageClass and volume plugin provider that the Persistent Volumes\nuse supports resize:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/create-project-using-configmap",
            "title": "Create One Project using a ConfigMap",
            "headings": [
                "Considerations",
                "Prerequisites",
                "Create One Project Using a ConfigMap",
                "Configure kubectl to default to your namespace.",
                "Invoke the following command to create a ConfigMap.",
                "Find the organization's ID.",
                "Copy and paste the orgId into the URL.",
                "Invoke the following Kubernetes command to verify your ConfigMap.",
                "Connect to HTTPS-enabled Ops Manager Using a Custom CA",
                "Create a ConfigMap for the certificate authority (CA) certificate.",
                "Copy the highlighted section of the following example ConfigMap.",
                "Add the highlighted section to your project's ConfigMap.",
                "Specify the TLS settings",
                "Save your updated ConfigMap.",
                "Invoke the Kubernetes command to verify your ConfigMap.",
                "Next Steps"
            ],
            "paragraphs": "The  MongoDB Enterprise Kubernetes Operator  uses a  Kubernetes   ConfigMap  to link to a single and unique\n Ops Manager   Project . If the\nreferenced project doesn't exist, the  Kubernetes Operator  creates it\naccording to the  projectName  that you provide in the ConfigMap. To create a  Kubernetes Operator  ConfigMap, you can edit a few lines of the\n example ConfigMap   YAML (Yet Another Markup Language)  file and apply\nthe ConfigMap. To view a full example, see the  project.yaml  file. Alternatively, you can use the  MongoDB Cloud Manager   UI  or the  Ops Manager \n UI  to create or\nchoose a project, and automatically generate the ConfigMap YAML file,\nwhich you can then apply to your Kubernetes environment. You can deploy only one MongoDB resource per project. This limit\napplies because  Ops Manager  supports only one authentication method for\ndatabase user access per project. To learn more, see\n Deploy a MongoDB Database Resource . You must pair a  MongoDB Enterprise Kubernetes Operator  deployment to a unique\n Ops Manager   Project .\nYou can either create a distinct ConfigMap for each  MongoDB Enterprise Kubernetes Operator  instance\nyou deploy, or you can reuse the same ConfigMap for any number of deployments\nby omitting the  data.projectName  from your ConfigMap, so that\nproject names align with deployed resource names. Where a project name is\nprovided in the config map, if that project does not\nexist yet it will be created. You can use the  Kubernetes Operator  to deploy MongoDB resources with\n Cloud Manager  and with  Ops Manager  version 5.0.x or later. You can use the  Atlas Operator \nto deploy MongoDB resources to  Atlas . Kubernetes version 1.11 or later or Openshift version\n3.11 or later. MongoDB Enterprise Kubernetes Operator  version 0.11 or later\n installed . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Key Type Description Example metadata.name string Name of the  Kubernetes   object . Resource names must be 44 characters or less. Kubernetes  documentation on  names .\nThis name must follow  RFC1123  naming\nconventions, using only lowercase alphanumeric\ncharacters,  -  or  . , and must start and end with an\nalphanumeric character. my-project metadata.namespace string Kubernetes   namespace  where the  Kubernetes Operator  creates this\n MongoDB  resource  and other  objects . mongodb data.projectName string Label for your  Ops Manager \n Project . The  Kubernetes Operator  creates the  Ops Manager  project if it does\nnot exist. If you omit the  projectName , the  Kubernetes Operator \ncreates a project with the same name as your\n Kubernetes  resource. To use an existing project in a  Cloud Manager or Ops Manager \norganization, locate\nthe  projectName  by clicking the  All Clusters \nlink at the top left of the  Cloud Manager or Ops Manager  page, and\nsearching by name in the  Search \nbox, or scrolling to find the name in the list.\nEach card in this list represents the\ncombination of one  Cloud Manager or Ops Manager   Organization  and  Project . myProjectName data.orgId string Required . 24 character hex string that uniquely\nidentifies your\n Cloud Manager or Ops Manager   Organization . Specify an  existing Organization : If you provide an empty string as your  orgId ,  Kubernetes Operator \ncreates an organization with the same name as your project. You can use the  Kubernetes Operator  to deploy MongoDB resources with\n Cloud Manager  and with  Ops Manager  version 5.0.x or later. You can use the  Atlas Operator \nto deploy MongoDB resources to  Atlas . Click  Settings  in the left navigation bar. Select your organization, view the current  URL (Uniform Resource Locator) \nin your browser and copy the value displayed in\nthe  <orgId>  placeholder as follows: https://ops.example.com:8443/ \n v2#/org/<orgId>/projects You must have the  Organization Project Creator \nrole to create a new project within an existing\n Cloud Manager or Ops Manager  organization. 5b890e0feacf0b76ff3e7183 data.baseUrl string URL (Uniform Resource Locator)  to your  Ops Manager Application  including the  FQDN (fully qualified domain name)  and port\nnumber. If you deploy  Ops Manager  with the  Kubernetes Operator  and  Ops Manager  will\nmanage MongoDB database resources deployed  outside  of the  Kubernetes \ncluster it's deployed to, you must set  data.baseUrl  to the same\nvalue of the\n spec.configuration.mms.centralUrl \nsetting in the  Ops Manager  resource specification. Managing External MongoDB Deployments If you're using  Cloud Manager , set the  data.baseUrl  value\nto  https://cloud.mongodb.com . https://ops.example.com:8443 This command returns a ConfigMap description in the shell: You might have chosen to use your own  TLS (Transport Layer Security)  certificate to enable\n HTTPS (Hypertext Transfer Protocol Secure)  for your  Ops Manager  instance. If you used a custom certificate,\nyou need to add the CA that signed that custom certificate to the\n Kubernetes Operator . To add your custom CA, complete the following: The  Kubernetes Operator  requires the root  CA (Certificate Authority)  certificate of the\n CA (Certificate Authority)  that issued the  Ops Manager  host's certificate. Run the following\ncommand to create a  ConfigMap  containing the root  CA (Certificate Authority)  in the\nsame namespace of your database Pods: The  Kubernetes Operator  requires that you name the  Ops Manager  resource's\ncertificate  mms-ca.crt  in the ConfigMap. Invoke the following command to edit your project's ConfigMap in\nthe default configured editor: Paste the highlighted section in the example  ConfigMap  at\nthe end of the project ConfigMap. Change the following  TLS (Transport Layer Security)  keys: Key Type Description Example sslMMSCAConfigMap string Name of the  ConfigMap  created in the first step\ncontaining the root  CA (Certificate Authority)  certificate used to sign the\n Ops Manager  host's certificate. This mounts the CA certificate\nto the  Kubernetes Operator  and database resources. my-root-ca sslRequireValidMMSServerCertificates boolean Forces the Operator to require a valid  TLS (Transport Layer Security)  certificate\nfrom  Ops Manager . The value must be enclosed in single quotes or the\noperator will throw an error. 'true' This command returns a ConfigMap description in the shell: Always include the namespace option with  kubectl .\n kubectl  defaults to an empty namespace if you don't specify the\n -n  option, resulting in deployment failures. You must specify\nthe value of the  <metadata.namespace>  field. The\n Kubernetes Operator ,  secret , and  MongoDB  resource s should run in the\nsame unique namespace. Now that you created your ConfigMap,  Create Credentials for the  Kubernetes Operator  before\nyou start  deploying MongoDB resources .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-project\n  namespace: mongodb\ndata:\n  projectName: myProjectName # this is an optional parameter; when omitted, the Operator creates a project with the resource name\n  orgId: 5b890e0feacf0b76ff3e7183 # this is a required parameter\n  baseUrl: https://ops.example.com:8443\n\nEOF"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe configmaps <configmap-name>"
                },
                {
                    "lang": "sh",
                    "value": "Name:           <configmap-name>\nNamespace:      <namespace>\nLabels:         <none>\nAnnotations:    <none>\n\nData\n====\nbaseUrl:\n----\n<myOpsManagerURL>\nEvents:  <none>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n <metadata.namespace> create configmap <root-ca-configmap-name> \\\n  --from-file=mms-ca.crt"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: <my-configmap>\n  namespace: <my-namespace>\ndata:\n  projectName: <my-ops-manager-project-name> # this is an optional parameter\n  orgId: <org-id> # this is a required parameter\n  baseUrl: https://<my-ops-manager-URL>\n\n"
                },
                {
                    "lang": "yaml",
                    "value": "  sslMMSCAConfigMap: <root-ca-configmap-name>\n  sslRequireValidMMSServerCertificates: \u2018true\u2019\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl edit configmaps <my-configmap> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe configmaps <my-configmap> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Name:           <my-configmap>\nNamespace:      <namespace>\nLabels:         <none>\nAnnotations:    <none>\n\nData\n====\nsslMMSCAConfigMap:\n----\n<root-ca-configmap-name>\nsslRequireValidMMSServerCertificates:\n----\ntrue\nEvents:  <none>"
                }
            ],
            "preview": "The MongoDB Enterprise Kubernetes Operator uses a Kubernetes ConfigMap to link to a single and unique\nOps Manager Project. If the\nreferenced project doesn't exist, the Kubernetes Operator creates it\naccording to the projectName that you provide in the ConfigMap.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "reference/k8s-operator-om-specification",
            "title": "Ops Manager Resource Specification",
            "headings": [
                "Examples",
                "Required Ops Manager Resource Settings",
                "Optional Ops Manager Resource Settings",
                "Prometheus Settings",
                "S3 Settings"
            ],
            "paragraphs": "The  MongoDB Enterprise Kubernetes Operator \ncreates a containerized  Ops Manager  deployment from specification files\nthat you write. After you create or update an  Ops Manager  resource specification, you\ndirect  MongoDB Enterprise Kubernetes Operator  to apply this specification to your  Kubernetes \nenvironment.  Kubernetes Operator  creates the services and custom  Kubernetes \nresources that  Ops Manager  requires, then deploys  Ops Manager  and its backing\napplication database in containers in your  Kubernetes  environment. Each  Ops Manager  resource uses an  object  specification in  YAML (Yet Another Markup Language)  to\ndefine the characteristics and settings of the deployment. The following examples show a resource specification for an  Ops Manager \ndeployment: This section describes settings that you must use for all  Ops Manager \nresources. Type : string Required . Version of the MongoDB  Kubernetes  resource schema. Type : string Required . Kind of MongoDB Kubernetes resource to create. Set this\nto  MongoDBOpsManager . Type : string Required . Name of the MongoDB  Kubernetes  resource you are creating. Resource names must be 44 characters or less. Type : integer Required . Number of  Ops Manager  instances to run in parallel.\nThe minimum accepted value is  1 . For high availability, set this value to more than  1 .\n Multiple Ops Manager instances \ncan read from the same Application Database, ensuring failover if\none instance is unavailable and enabling you to update the\n Ops Manager  resource without downtime. Type : string Required . Version of  Ops Manager  that you want to install\non this MongoDB  Kubernetes  resource. Type : string Required . Name of the  Kubernetes   secret  you created for\nthe  Ops Manager  admin user. When you deploy the  Ops Manager  resource,\n Kubernetes Operator  creates a user with these credentials. The admin user is granted the\n Global Owner \nrole. To avoid storing secrets in  Kubernetes , you can migrate all  secrets \nto a  secret storage tool . Type : string Required . Version of MongoDB installed on the  Ops Manager \n Application Database .\nYou must specify a compatible enterprise MongoDB version based on the tag in the\n container registry . For example,  6.0.0-ubi8 . Starting in  Kubernetes Operator  version 1.20, tags no longer end in  -ent . Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. If you update this value to a later version of MongoDB for the\nApplication Database, the Feature Compatibility Version (FCV)  does\nnot change  unless you also specify the\n featureCompatibilityVersion \nparameter under  spec.applicationDatabase . Ops Manager  resources can use the following settings: Type : collection Ops Manager   Application Database \nresource definition. The following settings from the\n replica set  resource specification are\noptional. All settings under  spec.applicationDatabase.agent  apply to both\nAutomation and Monitoring, unless you specify values for Automation\nand Monitoring separately in  spec.applicationDatabase.agent  and\n spec.applicationDatabase.monitoringAgent . Type : object MongoDB configuration object for rotating the MongoDB logs of a process. To use\nthe  agent.logRotate  settings, you must set\n systemLog.destination \nto  file  because you can't use the  agent.logRotate  settings if you're writing\nlogs to the host's  syslog  system. Type : integer Default :  0 Total number of log files that  Ops Manager  retains. If you don't change the default,\n Ops Manager  bases rotation on your other  agent.logRotate  settings. Type : integer Default :  5 Maximum number of total log files to leave uncompressed,\nincluding the current log file. Type : number Default :  0.02 Maximum percentage of total disk space that  Ops Manager  can use to\nstore the log files expressed as decimal. If this limit is\nexceeded,  Ops Manager  deletes compressed log files until it meets this\nlimit.  Ops Manager  deletes the oldest log files first. Type : number Required if rotating logs. Maximum size in MB for an individual log file before  Ops Manager \nrotates it.  Ops Manager  rotates the log file immediately if it meets\nthe value given in either this  sizeThresholdMB  or the\n logRotate.timeThresholdHrs  limit. Type : integer Required if rotating logs . Maximum duration in hours for an individual log file before the\nnext rotation. The time is since the last rotation.  Ops Manager  rotates the log file\nimmediately if it meets the value given in either this  timeThresholdHrs  or the\n logRotate.sizeThresholdM  limit. Type : boolean Set to  true  to have the Automation Agent rotate\nthe audit files along with MongoDB log files. Type : object MongoDB configuration object for startup options.\nSee  MongoDB Agent Settings  for available fields. Type : object MongoDB configuration object for configuring the  systemLog  options. Type : string Default:   /var/log/mongodb-mms-automation/mongodb.log The path of the log file to which  mongod  or  mongos  should send all diagnostic\nlogging information, rather than the standard output or the host's\n syslog .\nMongoDB creates the log file at the specified path. The Linux package init scripts don't expect  systemLog.path  to change from the defaults.\nIf you use the Linux packages and change  systemLog.path , you will have to use your own\ninit scripts and disable the built-in scripts. Type:  boolean Default:   false When  true ,  mongos  or  mongod  appends new entries to the end of the existing\nlog file when the  mongos  or  mongod  instance restarts. Without this option,  mongod \nwill back up the existing log and create a new file. Type:  string The destination to which MongoDB sends all log output. Specify either  file  or  syslog .\nIf you specify  file , you must also specify\n systemLog.path . If you don't specify\n systemLog.path , MongoDB\nsends all log output to standard output. The syslog daemon generates timestamps when it logs a message, not when MongoDB\nissues the message. This behavior can lead to misleading timestamps for log entries,\nespecially when the system is under heavy load. We recommend using the  file \noption for production systems to ensure accurate timestamps. Type : string Optional . The type of the  Kubernetes  deployment for the Application Database. To learn more, see the  example of the resource specification . The values are  SingleCluster  or  MultiCluster . If omitted,\nthe default value is  SingleCluster . If you specify  MultiCluster , you must specify\nthe  clusterSpecList  and\ninclude in it the  clusterName \nof each  Kubernetes  cluster included in your  multi-Kubernetes-cluster deployment , and the number\nof  members \nin each  Kubernetes  cluster. If you specify  MultiCluster , the  Kubernetes Operator  ignores values\nthat you set for the  spec.applicationDatabase.members \nfield, if specified. Type : collection Details of selected  Kubernetes  member clusters in a  multi-Kubernetes-cluster deployment \nthat serve as nodes that host the Application Database. Type : string Name of the member  Kubernetes  cluster in a  multi-Kubernetes-cluster deployment  where the  MongoDB Enterprise Kubernetes Operator \nschedules the  StatefulSet  for the Application Database. You can't convert a single cluster  Ops Manager  instance to a\n multi-Kubernetes-cluster deployment  instance by modifying the\n topology  and\nthe  clusterSpecList \nsettings in the CRD. Type : number Number of statefulSet nodes in the given member cluster. The member cluster\nis one of the member clusters that hosts the Application Database in\na  multi-Kubernetes-cluster deployment . Type : string Number that indicates the relative likelihood of an application database replica set member to become the  primary . For example, a member with a  memberConfig.priority  of  1.5  is more likely than a member with a  memberConfig.priority  of  0.5  to become the primary. A member with a  memberConfig.priority  of  0  is ineligible to become the primary. To learn more, see  Member Priority . To increase the relative likelihood that a replica set member becomes the primary, specify a higher  priority  value. To decrease the relative likelihood that a replica set member becomes the primary, specify a lower  priority  value. Type : map Map of  replica set tags  for directing\nread and write operations to specific members of your application database replica set. Type : number Determines whether an application database replica set member can vote in an  election . Set to  1  to allow the member to vote. Set to  0  to exclude the member from an election. Type : string Name of the  secret  that contains the\npassword for the  Ops Manager  database user  mongodb-ops-manager .\n Ops Manager  uses this password to  authenticate to the Application\nDatabase . Type : string Name of the field in the  secret  that\ncontains the password for the  Ops Manager  database user\n mongodb-ops-manager .  Ops Manager  uses this password to\n authenticate to the Application Database . The default value is  password . Type : string Text to prefix to the name of the secret that contains the\napplication database's  TLS (Transport Layer Security)  certificate. Name the secret\n <prefix>-<metadata.name>-db-cert . Type : string Name of the  Kubernetes   ConfigMap  containing the  CA (Certificate Authority)  file for\nthe application database. This  CA (Certificate Authority)  signs the certificates that: spec.applicationDatabase.security.tls.ca  is required\nif you use a custom  CA (Certificate Authority)  to sign your application database's\n TLS (Transport Layer Security)  certificates. The  Kubernetes Operator  requires that you name the application database's\ncertificate  ca-pem  in the ConfigMap. The  CA (Certificate Authority)  specified in this section is also used for\nconfiguring custom  TLS (Transport Layer Security)  certificates for  S3 (Simple Storage Service)  storage when either\n spec.backup.s3OpLogStores.customCertificate  or\n spec.backup.s3Stores.customCertificate  are set to\n true . the application database replica set members use to communicate\nwith one another, and Ops Manager  uses to communicate with the application database replica\nset. You must concatenate your custom  CA (Certificate Authority)  file and the entire\n TLS (Transport Layer Security)  certificate chain from  downloads.mongodb.com  to prevent\n Ops Manager  from becoming inoperable if the application database\nrestarts. Type : string Text to prefix to the  Kubernetes   secret  that you\ncreated that contains your application database's  TLS (Transport Layer Security)  key and\ncertificate. You must name your secret  <prefix>-<metadata.name>-db-cert . To learn how to configure your  Ops Manager  instance to run over\n HTTPS (Hypertext Transfer Protocol Secure) , see  Deploy an  Ops Manager  Resource . Encrypts communications using  TLS (Transport Layer Security)  certificates between  Ops Manager  and\nthe application database. spec.security.applicationDatabase.tls.enabled  is\ndeprecated and will be removed in a future release. To enable\n TLS (Transport Layer Security) , provide a value for the\n spec.security.applicationDatabase.certsSecretPrefix \nsetting. Type : array of strings A list of assignment labels for the  Backup Daemon Service  processes.\nUse assignment labels to identify that specific backup daemon\nprocesses are associated with particular projects. If you set assignment\nlabels using the  Kubernetes Operator , the values that you set in the  Kubernetes \nconfiguration file for assignment labels override the values defined\nin the  Ops Manager  UI. Assignment labels that you don't set using the\n Kubernetes Operator  continue to use the values set in the  Ops Manager  UI. Type : boolean Flag that enables Backup for your  Ops Manager  resource. When set to\n false , Backup is disabled. Default value is  true . Type : object Object that contains the backup encryption configuration settings. Type : object Object that contains the  KMIP (Key Management Interoperability)  backup encryption configuration\nsettings. To learn more, see  Configure KMIP Backup Encryption for Ops Manager . If you set this parameter, the API key linked with the value of\n spec.credentials  must have the  Global Owner  role. Type : object Object that contains the  KMIP (Key Management Interoperability)  backup encryption server\nconfiguration settings. Type : string Human-readable label that identifies the ConfigMap that contains an\nentry for the  CA (Certificate Authority)  certificate ( ca.pem ) to use for  KMIP (Key Management Interoperability) \nauthentication. Type : string URL (Uniform Resource Locator)  for the  KMIP (Key Management Interoperability)  server that uses the  hostname.port  format\n(for example,  192.168.1.3:5696  or\n my-kmip-server.mycorp.com:5696 ). Type : collection Configuration settings for the  head database .\n Kubernetes Operator  creates a  Persistent Volume Claim  with the specified configuration. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum size of  Persistent Volume  that should be mounted. This value is\nexpressed as an integer followed by a unit of storage in\n JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. Default value is  30Gi . Backup Daemon Hardware Requirements If the head database requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage specified in a  Persistent Volume Claim . You may create\nthis storage type as a  StorageClass  object before using it in this\n object  specification. Make sure to set the  StorageClass   reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a  Persistent Volume Claim  is removed. Type : array of strings Optional .  JVM (Java Virtual Machine)  parameters passed to the  Ops Manager  backup service\nin the container. This  Kubernetes Operator  parameter defaults to an empty list. Kubernetes Operator  calculates the  JVM (Java Virtual Machine)  memory heap values of the\nbackup service based on the container's memory. Changing the\n -Xms  and  -Xmx  values can cause issues with  Ops Manager . Type : integer Optional . Number of  backup daemon services \nto deploy in  Kubernetes . If not specified, defaults to  1 .\nTo ensure high availability for your backup service, deploy\n multiple backup daemons \nin  Ops Manager . Type : collection Required if you enable backup. Array of  oplog stores  used\nfor backup. Each item in the array references a MongoDB database\nresource deployed in the  Kubernetes  cluster by the  Kubernetes Operator . Type : array of strings A list of assignment labels for the  oplog store .\nUse assignment labels to identify that specific oplog stores are associated\nwith particular projects. If you set assignment labels using the  Kubernetes Operator ,\nthe values that you set in the  Kubernetes  configuration file for assignment\nlabels override the values defined in the  Ops Manager  UI. Assignment labels\nthat you don't set using the  Kubernetes Operator  continue to use the values\nset in the  Ops Manager  UI. Type : string Required if you enable Backup. Name of the oplog store. Once specified, don't edit the name of the oplog store. Type : string Required if you enable Backup. Name of the  MongoDB  resource  or the  MongoDBMultiCluster  resource  that you create to store\noplog slices. You must deploy this resource in the same namespace as\nthe  Ops Manager  resource. The Oplog database only supports the  SCRAM  authentication mechanism.\nYou cannot enable other authentication mechanisms. If you enable  SCRAM  authentication on the oplog database, you\nmust: Create a MongoDB user resource to connect  Ops Manager  to the oplog\ndatabase. Specify the  name \nof the user in the  Ops Manager  resource definition. If a MongoDB database resource with this name doesn't exist, the\n backup  resource enters a  Pending  state. The  Kubernetes Operator \nretries every 10 seconds until a MongoDB database resource with this\nname is created. The  Kubernetes Operator  begins to reconcile the  Ops Manager  resource\nautomatically when you make security changes to the database\nresources you reference in this setting. The  Kubernetes Operator  updates\nthe  mongoURI  and  ssl  flags in the  Ops Manager  configuration\nbased on your changes. Type : string Required if SCRAM authentication is enabled on the oplog\nstore database. Name of the MongoDB user resource used to connect to the oplog store\ndatabase. Deploy this user resource in the same\nnamespace as the  Ops Manager  resource and with all of the following roles: readWriteAnyDatabase dbAdminAnyDatabase clusterMonitor Type : collection Required if you enable Backup using a blockstore. Array of  blockstores  used\nfor Backup. Each item in the array references a MongoDB database\nresource deployed in the  Kubernetes  cluster by the  Kubernetes Operator . Type : array of strings A list of assignment labels for the  blockstore .\nUse assignment labels to identify that specific blockstores are\nassociated with particular projects. If you set assignment labels using\nthe  Kubernetes Operator , the values that you set in the  Kubernetes  configuration\nfile for assignment labels override the values defined in the  Ops Manager  UI.\nAssignment labels that you don't set using the  Kubernetes Operator  continue\nto use the values set in the  Ops Manager  UI. Type : string Required if you enable backup using a blockstore. Name of the blockstore. Once specified, don't edit the name of the  blockstore . Type : string Required if you enable backup using a blockstore. Name of the MongoDB database resource that you create for the\nblockstore. You must deploy this database resource in the same\nnamespace as the  Ops Manager  resource. The blockstore database only supports the  SCRAM  authentication\nmechanism. You cannot enable other authentication mechanisms. If you enable  SCRAM  authentication on the blockstore database,\nyou must: Create a MongoDB user resource to connect  Ops Manager  to the\nblockstore database. Specify the  name \nof the user in the  Ops Manager  resource definition. If a MongoDB database resource with this name doesn't exist, the\n backup  resource enters a  Pending  state. The  Kubernetes Operator \nretries every 10 seconds until a MongoDB database resource with this\nname is created. The  Kubernetes Operator  begins to reconcile the  Ops Manager  resource\nautomatically when you make security changes to the database\nresources you reference in this setting. The  Kubernetes Operator  updates\nthe  mongoURI  and  ssl  flags in the  Ops Manager  configuration\nbased on your changes. Type : string Required if SCRAM authentication is enabled on the blockstore database. Name of the MongoDB user resource used to connect to the blockstore\ndatabase. Deploy this user resource in the same\nnamespace as the  Ops Manager  resource and with all of the following roles: readWriteAnyDatabase dbAdminAnyDatabase clusterMonitor Type : string Name of the secret that contains the  queryable.pem \nfile from  Ops Manager  that you will use for accessing and querying backups\nbased on your deployment's  TLS (Transport Layer Security)  requirements.The PEM file contains\na public key certificate and its associated private key that are needed\nto access and run queries on backup snapshots in  Ops Manager .\nTo query backups, specify the value for this parameter. If not set,\nbackups are not affected, but you can't query them. Type : collection Specification for the  StatefulSet  that the  MongoDB Enterprise Kubernetes Operator  creates\nfor the  backup daemon service . To review which fields you can add to\n spec.backup.statefulSet.spec , see\n StatefulSetSpec v1 apps  in the  Kubernetes  documentation. Type : collection Template \nfor the  Kubernetes  Pods in the  StatefulSet  that the  MongoDB Enterprise Kubernetes Operator  creates\nfor the  backup daemon service . The  Kubernetes Operator  doesn't validate the fields you provide\nin  spec.backup.statefulSet.spec.template . Type : collection Metadata for the  Kubernetes  Pods in the  StatefulSet  that the\n MongoDB Enterprise Kubernetes Operator  creates for the  backup daemon service . To review which fields you can add to\n spec.backup.statefulSet.spec.template.metadata , see\nthe  Kubernetes documentation . Type : collection Specifications of the  Kubernetes  Pods in the  StatefulSet  that the\n MongoDB Enterprise Kubernetes Operator  creates for the  backup daemon service . To review the complete list of fields you can add to\n spec.backup.statefulSet.spec.template.spec , see the\n Kubernetes documentation . The following example  spec.backup.statefulSet.spec.template.spec \ndefines minimum and maximum CPU and memory capacity for one\n backup daemon service  container the  MongoDB Enterprise Kubernetes Operator  deploys: Type : collection List of containers that belong to the  Kubernetes  Pods in the\n StatefulSet  that the  MongoDB Enterprise Kubernetes Operator  creates for the\n backup daemon service . To modify the specifications of the  backup daemon service  container,\nyou must provide the exact name of the container using the  name \nfield, as shown in the following example: When you add containers to\n spec.backup.statefulSet.spec.template.spec.containers ,\nthe  Kubernetes Operator  adds them to the  Kubernetes  pod. These containers\nare appended to the  Backup Daemon Service  containers in the pod. Type : string Minimum CPU capacity that must be available on a  Kubernetes   node  to\nhost the  backup daemon service . The requested value must be less than or equal to\n spec.backup.statefulSet.spec.template.spec.containers.resources.limits.cpu . Type : string Maximum CPU capacity for the  node  being created to host\nthe  backup daemon service .\nIf omitted, this value is set to\n spec.backup.statefulSet.spec.template.spec.containers.resources.requests.cpu . Type : string Minimum memory capacity that must be available on a  Kubernetes   node \nto host the  backup daemon service  on  Kubernetes .\nThis value is expressed as an integer followed by a unit of memory\nin  JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. The requested value must be less than or equal to\n spec.backup.statefulSet.spec.template.spec.containers.resources.limits.memory . Set this value to at least  4.5Gi . Values of less than  4.5Gi \nmight result in an error. Type : string Maximum memory capacity for the  node  being created to host\nthe  backup daemon service . If omitted,\nthis value is set to  spec.backup.statefulSet.spec.template.spec.containers.resources.requests.memory . The  Kubernetes Operator  calculates and sets parameters for Java heap size\nbased on the container's memory. Setting this value to a value greater than 32 GB ( 32Gi ) can\ncause issues with the backup service. Excessive heaps can cause\nunpredictable results in  Ops Manager . Type : string Kubernetes  assigns each  Pod  a  FQDN (fully qualified domain name) . The  Kubernetes Operator  calculates\nthe  FQDN (fully qualified domain name)  for each  Pod  using a provided  clusterDomain .\n Kubernetes  doesn't provide an  API (Application Programming Interface)  to query these hostnames. Type : string Kubernetes  assigns each  Pod  a  FQDN (fully qualified domain name) . The  Kubernetes Operator  calculates\nthe  FQDN (fully qualified domain name)  for each  Pod  using a provided  clusterName .  Kubernetes \ndoesn't provide an  API (Application Programming Interface)  to query these hostnames. Use  spec.clusterDomain  instead. Type : collection Ops Manager  configuration properties.\nSee  Ops Manager Configuration Settings  for property names and descriptions.\nEach property takes a value of type  string . If  Ops Manager  will manage MongoDB resources deployed outside of the\n Kubernetes  cluster it's deployed to, you must add the  mms.centralUrl \nsetting to  spec.configuration . Set the value to the URL by which  Ops Manager  is exposed outside of the\n Kubernetes  cluster. Managing External MongoDB Deployments Type : string When set to  enabled , the MongoDB Agent requires signature files\nfor all MongoDB deployments that your  Ops Manager  instance manages. When you upgrade the MongoDB Agent with this option enabled, the current version\nof the MongoDB Agent requires signature files of the new MongoDB Agent binary. To learn more, see  Verify MongoDB Signatures . Type : string The  Kubernetes  service's  default server type . Accepted values are:  PRODUCTION_SERVER ,  TEST_SERVER ,  DEV_SERVER , and\n RAM_POOL . Type : collection Configuration object that enables external connectivity to  Ops Manager .\nIf provided, the  Kubernetes Operator  creates a  Kubernetes   service  that allows traffic\noriginating from outside of the  Kubernetes  cluster to reach the  Ops Manager \napplication. If not provided, the  Kubernetes Operator  doesn't create a  Kubernetes  service.\nYou must create one manually or use a third-party solution that\nenables you to route external traffic to the  Ops Manager Application  in your\n Kubernetes  cluster. Type : string The  Kubernetes  service  ServiceType \nthat exposes  Ops Manager  outside of  Kubernetes . Required  if  spec.externalConnectivity.type  is\npresent. Accepted values are:  LoadBalancer  and  NodePort .\n LoadBalancer  is recommended if your cloud provider supports it.\nUse  NodePort  for local deployments. Type : integer Value that indicates which port that a  Kubernetes  service exposes the\n Ops Manager Application  should use for external traffic. If  spec.externalConnectivity.type  is  NodePort : The  Kubernetes  service exposes the  Ops Manager Application  to external traffic\nthrough this port. If you don't provide a  spec.externalConnectivity.port \nvalue, the  Kubernetes  service routes traffic to the  Ops Manager Application  from an\navailable port selected randomly from the following default range:  30000 - 32767 . You must configure your network's firewall to allow traffic over\nthis port. If  spec.externalConnectivity.type  is  LoadBalancer : The load balancer resource that your cloud provider creates exposes\nthe  Ops Manager Application  through this port. If you don't provide a  spec.externalConnectivity.port \nvalue, the  Kubernetes  service exposes the  Ops Manager Application  to external\ntraffic through the default HTTP (8080) or HTTPS (8443) port. Type : string The IP address the  LoadBalancer   Kubernetes  service uses when the\n Kubernetes Operator  creates it. This setting can only be used if your cloud provider supports it and\n spec.externalConnectivity.type  is  LoadBalancer . To\nlearn more about the\n Type LoadBalancer , see the\n Kubernetes  documentation. Type : string Routing policy for external traffic to the  Ops Manager   Kubernetes  service.\nThe service routes external traffic to node-local or cluster-wide\nendpoints depending the value of this setting. Accepted values are:  Cluster  and  Local . To learn which of\nvalues meet your requirements, see  Source IPs in Kubernetes  in the  Kubernetes  documentation. If you select  Cluster , the  Source-IP  of your clients are\nlost during the network hops that happen at the  Kubernetes \nnetwork boundary. Type : collection Key-value pairs that allow you to provide cloud provider-specific\nconfiguration settings. To learn more about\n Annotations \nand\n TLS support on AWS ,\nsee the  Kubernetes  documentation. Type : array of strings Optional .  JVM (Java Virtual Machine)  parameters passed to the  Ops Manager Application  in the\ncontainer. Any parameters given replace the default  JVM (Java Virtual Machine)  parameters\nfor the  Ops Manager Application . This  Kubernetes Operator  parameter defaults to an empty list. Kubernetes Operator  calculates its  JVM (Java Virtual Machine)  memory heap values of the\n Ops Manager Application  based on the container's memory. Changing the\n -Xms  and  -Xmx  values can cause issues with  Ops Manager . Type : string Text to prefix to the  Kubernetes   secret  that you\ncreated that contain  Ops Manager 's  TLS (Transport Layer Security)  key and certificate. You must name your secret  <prefix>-<metadata.name>-cert . To learn how to configure your  Ops Manager  instance to run over\n HTTPS (Hypertext Transfer Protocol Secure) , see  Deploy an  Ops Manager  Resource . Name of the  Kubernetes   ConfigMap  that contains a custom  CA (Certificate Authority) \nfile for  Ops Manager . This  CA (Certificate Authority)  signs the certificates that: spec.security.tls.ca  is required if you use a custom\n CA (Certificate Authority)  to sign your  Ops Manager   TLS (Transport Layer Security)  certificates. The  Kubernetes Operator  requires that you name the certificate\nfor the  Ops Manager  resource  mms-ca.crt  in the ConfigMap. clients use to connect to the  Ops Manager Application , and agents in the application database  Pods  use to communicate\nwith  Ops Manager . You must concatenate your custom  CA (Certificate Authority)  file and the entire\n TLS (Transport Layer Security)  certificate chain from  downloads.mongodb.com  to prevent\n Ops Manager  from becoming inoperable if the application database\nrestarts. Encrypts communications using  TLS (Transport Layer Security)  certificates between clients and\n Ops Manager . spec.security.tls.enabled  is\ndeprecated and will be removed in a future release. To enable\n TLS (Transport Layer Security) , provide a value for the\n spec.security.certsSecretPrefix \nsetting. Type : collection Specification for the  StatefulSet  that the  MongoDB Enterprise Kubernetes Operator  creates\nfor  Ops Manager . To review which fields you can add to\n spec.statefulSet.spec , see\n StatefulSetSpec v1 apps \nin the  Kubernetes  documentation. Type : collection Template \nfor the  Kubernetes  Pods in the  StatefulSet  that the  MongoDB Enterprise Kubernetes Operator  creates\nfor the  Ops Manager . The  Kubernetes Operator  doesn't validate the fields you provide\nin  spec.statefulSet.spec.template . Type : collection Metadata for the  Kubernetes  Pods in the  StatefulSet  that the\n MongoDB Enterprise Kubernetes Operator  creates for the  Ops Manager . To review which fields you can add to\n spec.statefulSet.spec.template.metadata , see\nthe  Kubernetes documentation . Type : collection Specifications of the  Kubernetes  Pods in the  StatefulSet  that the\n MongoDB Enterprise Kubernetes Operator  creates for the  Ops Manager . To review the complete list of fields you can add to\n spec.statefulSet.spec.template.spec , see the\n Kubernetes documentation . The following example  spec.statefulSet.spec.template.spec  defines\nminimum and maximum CPU and memory capacity for one  Ops Manager \ncontainer the  MongoDB Enterprise Kubernetes Operator  deploys: Type : collection List of containers that belong to the  Kubernetes  Pods in the\n StatefulSet  that the  MongoDB Enterprise Kubernetes Operator  creates for the\n Ops Manager . To modify the specifications of the  Ops Manager  container,\nyou must provide the exact name of the container using the  name \nfield, as shown in the following example: When you add containers to\n spec.statefulSet.spec.template.spec.containers ,\nthe  Kubernetes Operator  adds them to the  Kubernetes  pod. These containers\nare appended to the  Ops Manager  containers in the pod. Type : string Minimum CPU capacity that must be available on a  Kubernetes   node  to\nhost the  Ops Manager . The requested value must be less than or equal to\n spec.statefulSet.spec.template.spec.containers.resources.limits.cpu . Type : string Maximum CPU capacity for the  node  being created to host\nthe  Ops Manager . If omitted, this value is set to\n spec.statefulSet.spec.template.spec.containers.resources.requests.cpu . Type : string Minimum memory capacity that must be available on a  Kubernetes   node \nto host the  Ops Manager  on  Kubernetes . This value is expressed as\nan integer followed by a unit of memory in  JEDEC (Joint Electron Device Engineering Council Solid State Technology Association)  notation. The requested value must be less than or equal to\n spec.statefulSet.spec.template.spec.containers.resources.limits.memory . If  Ops Manager  on  Kubernetes  requires 6 gigabytes of memory, set\nthis value to  6Gi . MongoDB recommends setting this value to at least  5Gi . Type : string Maximum memory capacity for the  node  being created to host\nthe  Ops Manager . If omitted, this value is set to\n spec.statefulSet.spec.template.spec.containers.resources.requests.memory . The  Kubernetes Operator  calculates and sets parameters for Java heap size\nbased on the container's memory. Setting this value to a value greater than 32 GB ( 32Gi ) can\ncause issues with the backup service. Excessive heaps can cause\nunpredictable results in  Ops Manager . The following settings apply when you use Prometheus with your\napplication database: Type : array Optional List that contains the parameters for exposing metrics to Prometheus. Type : string Optional Default :  \"/metrics\" Human-readable string that indicates the path to the metrics\nendpoint. If you don't specify this setting, the default applies. Type : object Conditional Object that contains the details of the  secret  for\nbasic HTTP authentication. If you want to use Prometheus with your\napplication database, you must specify this setting. Type : string Optional Default :  \"password\" Human-readable string that identifies the key in the  secret  that\nstores the password for basic HTTP authentication. If you don't specify\nthis setting, the default applies. Type : string Conditional Human-readable label that identifies the  secret  that contains\nthe password for basic HTTP authentication. If you want to use\nPrometheus with your application database, you must specify this\nsetting. Type : integer Optional Default:  9216 Number that identifies the port that the metrics endpoint will\nbind to. If you don't specify this setting, the default applies. Type : object Optional Object that contains the details of the  secret  for  TLS (Transport Layer Security) \nauthentication. Type : string Optional Default :  \"password\" Human-readable string that identifies the key in the  secret  that\nstores the password for  TLS (Transport Layer Security)  authentication. If you don't specify this\nsetting, the default applies. Type : string Conditional Human-readable label that identifies the  secret  that contains\nthe password for  TLS (Transport Layer Security)  authentication. If you want to use\nPrometheus with your application database and you want to use  TLS (Transport Layer Security) \nauthentication, you must specify this setting. Type : string Conditional Human-readable label that identifies the user for basic HTTP\nauthentication. If you want to use Prometheus with your application\ndatabase, you must specify this setting. You can configure  Ops Manager  to use  S3 (Simple Storage Service)  for storing oplogs and backup\nsnapshots, and secure connections to  S3 (Simple Storage Service)  with  TLS (Transport Layer Security)  using keys\nissued by custom  CA (Certificate Authority) . To configure custom CA keys, use the ConfigMap with which you\nconfigured  TLS (Transport Layer Security)  for your application database as described on\nthe  TLS-Encrypted Connection (HTTPS)  tab of\n Deploy an  Ops Manager  Resource .\nSet  spec.applicationDatabase.security.tls.ca  to this ConfigMap. You can use  TLS (Transport Layer Security)  for both  S3 (Simple Storage Service)  and your application database, or for\n S3 (Simple Storage Service)  only. To use  TLS (Transport Layer Security)  for both, get certificates for both purposes from the\nsame  ca-pem  referenced in the ConfigMap. To use  TLS (Transport Layer Security)  for  S3 (Simple Storage Service)  only, don't define\n spec.security.applicationDatabase.certsSecretPrefix  in\nyour ConfigMap. Type : array of strings A list of assignment labels for  S3 (Simple Storage Service)  oplog stores. Use assignment labels\nto identify that specific  S3 (Simple Storage Service)  oplog stores are associated with particular\nprojects. If you set assignment labels using the  Kubernetes Operator , the values\nthat you set in the  Kubernetes  configuration file for assignment labels override\nthe values defined in the  Ops Manager  UI. Assignment labels that you don't set\nusing the  Kubernetes Operator  continue to use the values set in the  Ops Manager  UI. Type : boolean Deprecated . Use\n spec.backup.s3OpLogStores.customCertificateSecretRefs \ninstead. Flag that indicates whether you use AppDB certificates\n( appdb-ca ) as the custom  TLS (Transport Layer Security)  certificate for your  S3 (Simple Storage Service)  oplog\nstore. The default is  False . Type : array of objects List of custom certificates for your  S3 (Simple Storage Service)  oplog store using  Kubernetes \n secrets . The base64-encoded x.509 certificate must already be\npresent in a  Kubernetes   secret  with a key and must be parsable by\nthe  Java CertifcateFactory .\nYou can't specify multiple certificates in a chain in one secret. If\nyou specify multiple certificates in a chain in one secret,\n Kubernetes Operator  uses only the first certificate in the chain. If you\nalso provide the\n customCertificate  setting,\n Kubernetes Operator  uses the\n spec.applicationDatabase.security.tls.ca  as the custom\ncertificate for backups. Each entry in the list specifies the\n name \nand the\n key .\nIf you specify multiple secrets,  Kubernetes Operator  uses all the\ncertificates in the specified secrets. If you don't provide this setting,  Ops Manager  uses the  JVM (Java\nVirtual Machine)  Default Trust Store used by  Ops Manager . Type : string Required to use custom certificates for your S3 oplog store. Kubernetes   secret  that contains the custom certificate. Type : string Required to use custom certificates for your S3 oplog store. File that represents the key in the  secret  that contains the\nbase64-encoded x.509 certificate. If you don't specify this setting,\n Kubernetes Operator  can't utilize the custom certificate for  S3 (Simple Storage Service)  oplog\nstore backups. Type : boolean Flag that enables using  AWS (Amazon Web Services)   IAM roles for service accounts \nin  AWS (Amazon Web Services)   EKS  to configure\nan  S3 (Simple Storage Service)  oplog store. The default is  False . If you aren't using\n AWS (Amazon Web Services)  EKS, this flag has no effect. When set to  False , using  AWS (Amazon Web Services) \nIAM roles for service accounts in EKS to configure an  S3 (Simple Storage Service)  oplog store\nis disabled. To learn more, see\n IAM roles for service accounts in EKS . Type : string Required to store the oplog using an S3 store. Name of the  S3 (Simple Storage Service)  oplog store. Type : string Name of the MongoDB database resource that you create to store\nmetadata for the  S3 (Simple Storage Service)  oplog store. You must deploy this database\nresource in the same namespace as the  Ops Manager  resource. If you enable  SCRAM  authentication on this database, you must: Omit this setting to use the application database to store\nmetadata for the  S3 (Simple Storage Service)  oplog store. If you omit this setting, you must also omit the\n spec.backup.s3OpLogStores.mongodbUserRef.name  setting.\nThe  Kubernetes Operator  handles  SCRAM  user authentication\ninternally. Create a MongoDB user resource to connect  Ops Manager  to the\ndatabase. Specify the\n name  of the\nuser in the  Ops Manager  resource definition. Type : string Required if you created a MongoDB database resource to store\nS3 oplog metadata and SCRAM is enabled on this database. Name of the MongoDB user resource used to connect to the metadata\ndatabase of the  S3 (Simple Storage Service)  oplog store. Deploy this user resource in the\nsame namespace as the  Ops Manager  resource and with all of the following roles: readWriteAnyDatabase dbAdminAnyDatabase clusterMonitor Once specified, don't edit the name of the  S3 (Simple Storage Service)  metadata oplog\nstore username. Type : string Required to store the oplog using an S3 store. Name of the secret that contains the  accessKey  and\n secretKey  fields. The  backup daemon service  uses\nthe values of these fields as credentials to access your\n AWS (Amazon Web Services)   S3 (Simple Storage Service)  or  S3 (Simple Storage Service) -compatible bucket. To configure the  S3 (Simple Storage Service)  oplog\nstore, you must specify both keys in the secret. Type : boolean Indicates the style of the bucket endpoint URL. Default value is  true . Value Description Example true Path-style URL s3.amazonaws.com/<bucket> false Virtual-host-style URL <bucket>.s3.amazonaws.com Type : string Required to store the oplog using an S3 store. URL of the  AWS (Amazon Web Services)   S3 (Simple Storage Service)  bucket or  S3 (Simple Storage Service) -compatible bucket that hosts the\noplog store. If your endpoint doesn't include a region in its  URL (Uniform Resource Locator) ,\nspecify the  s3RegionOverride \nfield. Type : string Required to store the oplog using an S3 store. Name of the  AWS (Amazon Web Services)   S3 (Simple Storage Service)  bucket or  S3 (Simple Storage Service) -compatible bucket that hosts\nthe oplog store. Type : string Region where your  S3 (Simple Storage Service) -compatible bucket resides. Use this\nfield only if your  S3 (Simple Storage Service)  oplog store's\n s3BucketEndpoint \ndoesn't support region scoping. Region scoping is when your endpoint doesn't include a region in its  URL (Uniform Resource Locator) . Don't use this field with  AWS (Amazon Web Services)   S3 (Simple Storage Service)  buckets. For more information, see\n S3 Blockstore Configuration . Type : array of strings A list of assignment labels for the  S3 (Simple Storage Service)  or  S3 (Simple Storage Service) -compatible buckets\nwhere  stores  the\ndatabase backup snapshots. Use assignment labels to identify that\nspecific  S3 (Simple Storage Service)  stores are associated with particular projects.\nIf you set assignment labels using the  Kubernetes Operator , the values that\nyou set in the  Kubernetes  configuration file for assignment labels override\nthe values defined in the  Ops Manager  UI. Assignment labels that you don't set\nusing the  Kubernetes Operator  continue to use the values set in the  Ops Manager  UI. Type : boolean Deprecated . Use\n spec.backup.s3Stores.customCertificateSecretRefs \ninstead. Flag that indicates whether you use Application Database's certificates\n( appdb-ca ) as the custom  TLS (Transport Layer Security)  certificate for your  S3 (Simple Storage Service)  backups.\nThe default is  False . Type : array of objects List of custom certificates for your  S3 (Simple Storage Service)  snapshot store using  Kubernetes \n secrets . The base64-encoded x.509 certificate must already be\npresent in a  Kubernetes   secret  with a key and must be parsable by\nthe  Java CertifcateFactory .\nYou can't specify multiple  certificates in a chain in one secret. If\nyou specify multiple certificates in a chain in one secret,\n Kubernetes Operator  uses only the first certificate in the chain. If you\nalso provide the  spec.backup.s3Stores.customCertificate \nsetting,  Kubernetes Operator  uses the\n spec.applicationDatabase.security.tls.ca  as the\ncustom certificate for backups. Each entry in the list specifies the\n name \nand the\n key .\nIf you specify multiple secrets,  Kubernetes Operator  uses all the\nspecified secrets. If you don't provide this setting, the  Kubernetes Operator  uses the\n JVM (Java Virtual Machine)  Default Trust Store used by\n Ops Manager  for backups. Type : string Required to use custom certificates for your S3 oplog store. Kubernetes   secret  that contains the custom certificate. Type : string Required to use custom certificates for your S3 oplog store. File that represents the key in the  secret  that contains the\nbase64-encoded x.509 certificate. If you don't specify this setting,\n Kubernetes Operator  can't utilize the custom certificate for  S3 (Simple Storage Service)  snapshot\nstore and defaults to the default  JVM {Java Virtual Machine) \ntrust store used by  Ops Manager . Type : boolean Flag that enables using  AWS (Amazon Web Services)   IAM roles for service accounts \nin  AWS (Amazon Web Services)   EKS  to configure\nan  S3 (Simple Storage Service)  snapshot store. The default is  False . If you aren't using\n AWS (Amazon Web Services)  EKS, this flag has no effect. When set to  False , using  AWS (Amazon Web Services) \nIAM roles for service accounts in EKS to configure an  S3 (Simple Storage Service)  snapshot\nstore is disabled. To learn more, see\n IAM roles for service accounts in EKS . Type : string Required to store the oplog using an S3 store. Name of the  S3 (Simple Storage Service)  snapshot store. Once specified, don't edit the name of the  S3 (Simple Storage Service)  snapshot store. This change will likely fail if\nbackups use the old name. The consequences of\na successful change are unpredictable. Type : string Name of the  MongoDB  resource  or  MongoDBMultiCluster  resource  that you create to store\nmetadata for the  S3 (Simple Storage Service)  snapshot store. You must deploy this database\nresource in the same namespace as the  Ops Manager  resource. If you enable  SCRAM  authentication on this database, you must: Omit this setting to use the application database to store\nmetadata for the  S3 (Simple Storage Service)  snapshot store. If you omit this setting, you must also omit the\n spec.backup.s3Stores.mongodbUserRef.name  setting.\nThe  Kubernetes Operator  handles  SCRAM  user authentication\ninternally. Create a MongoDB user resource to connect  Ops Manager  to the\ndatabase. Specify the\n name  of the\nuser in the  Ops Manager  resource definition. Once specified, don't edit the name of the  S3 (Simple Storage Service)  snapshot store.\nThis change will likely fail if backups use the old name. The\nconsequences of a successful change are unpredictable. If a MongoDB database resource with this name doesn't exist, the\n backup  resource enters a  Pending  state. The  Kubernetes Operator \nretries every 10 seconds until a MongoDB database resource with this\nname is created. The  Kubernetes Operator  begins to reconcile the  Ops Manager  resource\nautomatically when you make security changes to the database\nresources you reference in this setting. The  Kubernetes Operator  updates\nthe  mongoURI  and  ssl  flags in the  Ops Manager  configuration\nbased on your changes. Type : string Required if you created a MongoDB database resource to store\n|s3| snapshot metadata and SCRAM is enabled on this database. Name of the MongoDB user resource used to connect to the metadata\ndatabase of the  S3 (Simple Storage Service)  snapshot store. Deploy this user resource in the\nsame namespace as the  Ops Manager  resource and with all of the following roles: readWriteAnyDatabase dbAdminAnyDatabase clusterMonitor Once specified, don't edit the name of the  S3 (Simple Storage Service)  metadata snapshot\nstore username. Type : string Required if you enable Backup using an S3 store. Name of the secret that contains the  accessKey  and\n secretKey  fields. The  backup daemon service  uses\nthe values of these fields as credentials to access your\n AWS (Amazon Web Services)   S3 (Simple Storage Service)  or  S3 (Simple Storage Service) -compatible bucket. The  S3 (Simple Storage Service)  snapshot store\ncan't be configured if the secret is missing either key. Type : boolean Indicates the style of the bucket endpoint URL. Default value is  true . Value Description Example true Path-style URL s3.amazonaws.com/<bucket> false Virtual-host-style URL <bucket>.s3.amazonaws.com Type : string Required if you enable Backup using an S3 store. URL of the  AWS (Amazon Web Services)   S3 (Simple Storage Service)  bucket or  S3 (Simple Storage Service) -compatible bucket that hosts the\nsnapshot store. If your endpoint doesn't include a region in its  URL (Uniform Resource Locator) ,\nspecify the  s3RegionOverride \nfield. Type : string Required if you enable Backup using an S3 store. Name of the  AWS (Amazon Web Services)   S3 (Simple Storage Service)  bucket or  S3 (Simple Storage Service) -compatible bucket that hosts\nthe snapshot store. Type : string Region where your  S3 (Simple Storage Service) -compatible bucket resides. Use this\nfield only if your  S3 (Simple Storage Service)  store's\n s3BucketEndpoint \ndoesn't support region scoping. Region scoping is when your endpoint doesn't include a region in its  URL (Uniform Resource Locator) . Don't use this field with  AWS (Amazon Web Services)   S3 (Simple Storage Service)  buckets. For more information, see\n S3 Blockstore Configuration .",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: om\nspec:\n replicas: 1\n version: \"6.0.18\"\n adminCredentials: ops-manager-admin\n configuration:\n  mms.fromEmailAddr: admin@example.com\n  mms.security.allowCORS: \"false\"\n backup:\n  enabled: true\n  encryption:\n    kmip:\n      server:\n        url: kmip.corp.mongodb.com:5696\n        ca: mongodb-kmip-certificate-authority-pem\n  headDB:\n   storage: \"30Gi\"\n   labelSelector:\n    matchLabels:\n     app: my-app\n  opLogStores:\n   - name: oplog1\n                       # Sets labels for the oplog store.\n     assignmentLabels: [\"test1\", \"test2\"]\n     mongodbResourceRef:\n      name: my-oplog-db\n     mongodbUserRef:\n      name: my-oplog-user\n  s3Stores:\n   - name: s3store1\n                       # Sets labels for the S3 store.\n     assignmentLabels: [\"test1\", \"test2\"]\n                       \n     mongodbResourceRef:\n      name: my-s3-metadata-db\n     mongodbUserRef:\n      name: my-s3-store-user\n     s3SecretRef:\n       name: my-s3-credentials\n     pathStyleAccessEnabled: true\n     s3BucketEndpoint: s3.region.amazonaws.com\n     s3BucketName: my-bucket\n\n applicationDatabase:\n   passwordSecretKeyRef:\n    name: om-db-user-secret\n    key: password\n   members: 3\n   topology: SingleCluster\n   version: \"6.0.5-ubi8\"\n"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: om\nspec:\n replicas: 1\n version: \"6.0.18\"\n adminCredentials: ops-manager-admin\n configuration:\n  mms.fromEmailAddr: admin@example.com\n  mms.security.allowCORS: \"false\"\n backup:\n  enabled: true\n  encryption:\n    kmip:\n      server:\n        url: kmip.corp.mongodb.com:5696\n        ca: mongodb-kmip-certificate-authority-pem\n  headDB:\n   storage: \"30Gi\"\n   labelSelector:\n    matchLabels:\n     app: my-app\n  opLogStores:\n   - name: oplog1\n                       # Sets labels for the oplog store.\n     assignmentLabels: [\"test1\", \"test2\"]\n     mongodbResourceRef:\n      name: my-oplog-db\n     mongodbUserRef:\n      name: my-oplog-user\n  s3Stores:\n   - name: s3store1\n                       # Sets labels for the S3 store.\n     assignmentLabels: [\"test1\", \"test2\"]\n                       \n     mongodbResourceRef:\n      name: my-s3-metadata-db\n     mongodbUserRef:\n      name: my-s3-store-user\n     s3SecretRef:\n       name: my-s3-credentials\n     pathStyleAccessEnabled: true\n     s3BucketEndpoint: s3.region.amazonaws.com\n     s3BucketName: my-bucket\n\n applicationDatabase:\n   passwordSecretKeyRef:\n    name: om-db-user-secret\n    key: password\n    version: \"6.0.5-ubi8\"\n    topology: MultiCluster\n    clusterSpecList:\n    - clusterName: cluster1.example.com\n      members: 4\n    - clusterName: cluster2.example.com\n      members: 3\n    - clusterName: cluster3.example.com\n      members: 2\n    \n"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  backup:\n    jvmParameters: [\"-XX:+UseStringCache\"]"
                },
                {
                    "lang": "yaml",
                    "value": "statefulSet:\n  spec:\n    template:\n      spec:\n        containers:\n        - name: mongodb-backup-daemon\n          resources:\n            requests:\n              cpu: \"0.50\"\n              memory: \"4500M\"\n            limits:\n              cpu: \"1\"\n              memory: \"6000M\""
                },
                {
                    "lang": "yaml",
                    "value": "backup:\n statefulSet:\n   spec:\n     template:\n       spec:\n         containers:\n         - name: mongodb-backup-daemon"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  jvmParameters: [\"-XX:+HeapDumpOnOutOfMemoryError\",\"-XX:HeapDumpPath=/tmp\"]"
                },
                {
                    "lang": "yaml",
                    "value": "statefulSet:\n  spec:\n    template:\n      spec:\n        containers:\n          - name: mongodb-ops-manager\n            resources:\n              requests:\n                cpu: \"0.70\"\n                memory: \"6Gi\"\n              limits:\n                cpu: \"1\"\n                memory: \"7000M\""
                },
                {
                    "lang": "yaml",
                    "value": "backup:\n statefulSet:\n   spec:\n     template:\n       spec:\n         containers:\n         - name: mongodb-ops-manager"
                }
            ],
            "preview": "Type: string",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/secret-storage",
            "title": "Configure Secret Storage",
            "headings": [
                "Supported Secret Storage Tools",
                "Secrets You Can Store",
                "Limitations",
                "Set the Secret Storage Tool",
                "Prerequisites",
                "Procedure",
                "Add the Vault policies for the Kubernetes Operator and its components.",
                "Bind the Vault roles to the Vault policies for the Kubernetes Operator and its components.",
                "Add the annotations to the Kubernetes deployment file.",
                "Define the environment variable in Kubernetes.",
                "Create a file with the Vault configuration information.",
                "Replace the placeholders in the Vault configuration information.",
                "Create a ConfigMap with the Vault configuration.",
                "Manually migrate secrets that don't migrate automatically",
                "Next Steps"
            ],
            "paragraphs": "You can choose the  secret storage tool  for  Kubernetes Operator . The secret\nstorage tool is a secure place to store sensitive information for the components\nthat  Kubernetes Operator  manages. This includes secrets for MongoDB databases,  Ops Manager , and AppDB. Once you configure secret storage,  Kubernetes Operator  accesses the tool, retrieves\nthe secrets, and uses them to establish connections securely. Kubernetes Operator  supports the following  secret storage tools : Kubernetes : store sensitive information as  secrets  (the built-in secret\nstorage for  Kubernetes ).  Kubernetes   secrets \nstore authentication credentials so that only  Kubernetes  can access them. HashiCorp Vault : store sensitive information in  Vault , a third\nparty service for secret management. You can use any supported  secret storage tool  for any secret in the  MongoDB Enterprise Kubernetes Operator \ndocumentation except those listed in the  limitations . After configuration,  Kubernetes Operator  uses your selected  secret storage tool \nfor  all  secrets except those listed in the  limitations .\nYou can't mix and match  secret storage tools . The following limitations exist for the supported  secret storage tools : Some registries, such as OpenShift, require  imagePullSecrets \nto pull images from the repository. The  Kubernetes Operator  can't provide imagePullSecrets from\n HashiCorp Vault . You can  specify a kubelet image credential provider \nto retrieve credentials for a container image registry using  Kubernetes  instead. To set the  secret storage tool , select one of the following options: All tutorials in this  MongoDB Enterprise Kubernetes Operator  documentation use  Kubernetes   secrets  by default.\nTo use  Kubernetes   secrets  to store secrets for the  Kubernetes Operator , proceed with\ninstallation of the  Kubernetes Operator  and follow the steps in the tutorials. To use  HashiCorp Vault  to store secrets for the\n Kubernetes Operator , complete the following procedure. Before you begin, you must: Set up a  Vault  instance. The  Kubernetes  cluster where the\n Kubernetes Operator  is running must have access to the  Vault \ninstance. Ensure that  Vault  is  not  running in  dev mode \nand that your  Vault  installation follows any applicable\n configuration recommendations . Enable  Kubernetes Authentication \nfor the  Vault  instance. This allows you to authenticate with\n Vault . Deploy the Vault Agent sidecar injector \nin the  Kubernetes  cluster. This allows you to inject secrets from\n Vault  into your  Kubernetes  Pods. Download the four  Vault policy files \nfor the  Kubernetes Operator , MongoDB database,  Ops Manager , and AppDB. Write the policies for  Kubernetes Operator , MongoDB database,  Ops Manager , and AppDB resources\nto  Vault  using the following command, replacing the variables with the values in the table: Repeat the command for all the resources you're adding to  Vault . Placeholder Description {PolicyName} Human-readable label that identifies the policy you're creating in  Vault . {PathToPolicyFile} The absolute path to the policy file you downloaded. Bind  Vault  roles to the policies for  Kubernetes Operator , MongoDB database,\n Ops Manager , and AppDB resources using the following four commands, replacing the\nvariables with the values in the table: These commands ensure that each component's pods have only the access specified in their\npolicy. Placeholder Description {OperatorPolicyName} A human-readable label that identifies the  Kubernetes Operator  policy in  Vault . {DatabasePolicyName} A human-readable label that identifies the MongoDB database policy in  Vault . {OpsManagerPolicyName} A human-readable label that identifies the  Ops Manager  policy in  Vault . {AppDBPolicyName} A human-readable label that identifies the AppDB policy in  Vault . {ServiceAccountNamespace} Label that identifies the namespace for the service account bound to your pod. This step grants the  Kubernetes Operator  access to\n Vault . To use  Vault  with applications that the\n Kubernetes Operator  doesn't manage, you must write and bind  Vault  policies for those\napplications. You can adapt the commands in this step to bind other policies by\nreplacing the name of the  service accounts . To configure other\napplications to use  Vault , replace the\n{ServiceAccountName} in the following command with the service account used\nfor the application's pod: Add the following highlighted lines to the  spec.template.metadata.annotations  section of your\n Kubernetes Operator  deployment file. For most users, this file's name is  mongodb-enterprise.yaml  or\n mongodb-enterprise-openshift.yaml . If you're running  Vault  in  TLS (Transport Layer Security)  mode, you must also add the following\nhighlighted line to the file, replacing {TLSSecret} with the name of the secret\ncontaining a  ca.crt  entry. The content of the  ca.crt  entry must match\nthe certificate of the  CA (Certificate Authority)  used to generate the  Vault  TLS certificates. If you installed the  Kubernetes Operator  using Helm, the  Kubernetes Operator  already\nadded these annotations. You can proceed to the next step. Add the following highlighted lines to the  spec.env  section of your\n Kubernetes Operator  deployment file. For most users, this file's name is  mongodb-enterprise.yaml  or\n mongodb-enterprise-openshift.yaml . This  defines the environment variable \nfor  Vault  in  Kubernetes . Using your preferred text editing application, create a file named  config .\nPaste the following text into the file: The paths in this file are the default paths. You can replace them with your\nbase paths if you customized your  Kubernetes Operator  configuration. If you're running  Vault  in  TLS (Transport Layer Security)  mode, you must also add the following\nhighlighted line to the file: Replace the placeholders in the  config  file with these values. Save\nthe file with a  YAML (Yet Another Markup Language)  file type by replacing the  .txt  file extension with\n .yaml . Placeholder Description {Namespace} The  namespace you created \nfor the  Kubernetes Operator . The default namespace is  mongodb . {VaultServerAddress} The address that the  Kubernetes Operator  should use to connect to\n Vault . {TLSSecret} Name of a secret containing a  ca.crt  entry. The content of the\n ca.crt  entry must match the certificate of the  CA (Certificate Authority)  used to generate\nthe  Vault  TLS certificates. Issue the following command to create a  ConfigMap  containing the  Vault  information: This creates a  ConfigMap  named  secret-configuration . This\n ConfigMap  contains the contents of the  config  file. You must manually migrate the following secrets to store them in  Vault : To manually migrate or create new secrets,  add them to Vault .\nAfter you add them to  Vault , you can remove them from  Kubernetes . All other secrets that the  Kubernetes Operator  creates migrate automatically, and  Kubernetes Operator  uses\n Vault  for new secrets. User-created secrets must be  added to Vault . Any existing user-created secrets, including  Operator credentials stored as Kubernetes secrets ,\nif applicable The gen-key secret \n Kubernetes Operator  creates The  Ops Manager   admin credentials/admin key \n Kubernetes Operator  creates TLS secrets cert-manager automatically recreates the  Kubernetes   secrets  that\nit generates if you delete them from  Kubernetes . You must manually manage the\nremoval of these secrets or stop using cert-manager to avoid storing\nthe secrets in  Kubernetes . After you configure the  secret storage tool  for the  MongoDB Enterprise Kubernetes Operator , you can: Read the  Considerations . Complete the  Prerequisites . Install the Kubernetes Operator .",
            "code": [
                {
                    "lang": "sh",
                    "value": "vault policy write {PolicyName} {PathToPolicyFile}"
                },
                {
                    "lang": "sh",
                    "value": "vault write auth/kubernetes/role/{OperatorPolicyName}\nbound_service_account_names=enterprise-operator bound_service_account_namespaces={ServiceAccountNamespace}"
                },
                {
                    "lang": "sh",
                    "value": "vault write auth/kubernetes/role/{DatabasePolicyName}\nbound_service_account_names=mongodb-enterprise-database-pods bound_service_account_namespaces={ServiceAccountNamespace}"
                },
                {
                    "lang": "sh",
                    "value": "vault write auth/kubernetes/role/{OpsManagerPolicyName}\nbound_service_account_names=mongodb-enterprise-ops-manager bound_service_account_namespaces={ServiceAccountNamespace}"
                },
                {
                    "lang": "sh",
                    "value": "vault write auth/kubernetes/role/{AppDBPolicyName}\nbound_service_account_names=mongodb-enterprise-appdb bound_service_account_namespaces={ServiceAccountNamespace}"
                },
                {
                    "lang": "sh",
                    "value": "vault write auth/kubernetes/role/{PolicyName}\nbound_service_account_names={ServiceAccountName} bound_service_account_namespaces={ServiceAccountNamespace}"
                },
                {
                    "lang": "sh",
                    "value": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n   name: mongodb-enterprise-operator\n   namespace: production\nspec:\n   replicas: 1\n   template:\n      metadata:\n        annotations:\n          vault.hashicorp.com/agent-inject: \"true\"\n          vault.hashicorp.com/role: \"mongodbenterprise\""
                },
                {
                    "lang": "sh",
                    "value": "        annotations:\n          vault.hashicorp.com/agent-inject: \"true\"\n          vault.hashicorp.com/role: \"mongodbenterprise\"\n          vault.hashicorp.com/tls-secret: {TLSSecret}\n          vault.hashicorp.com/ca-cert: /vault/tls/ca.crt"
                },
                {
                    "lang": "sh",
                    "value": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n   name: mongodb-enterprise-operator\n   namespace: production\nspec:\n   env:\n   - name: OPERATOR_ENV\n     value: ENVIRONMENT_NAME\n   - name: SECRET_BACKEND\n     value: VAULT_BACKEND"
                },
                {
                    "lang": "sh",
                    "value": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: secret-configuration\n  namespace: {Namespace}\ndata:\n  VAULT_SERVER_ADDRESS: {VaultServerAddress}\n  OPERATOR_SECRET_BASE_PATH: mongodbenterprise/operator\n  DATABASE_SECRET_BASE_PATH: mongodbenterprise/database\n  OPS_MANAGER_SECRET_BASE_PATH: mongodbenterprise/opsmanager\n  APPDB_SECRET_BASE_PATH: mongodbenterprise/appdb"
                },
                {
                    "lang": "sh",
                    "value": "   OPS_MANAGER_SECRET_BASE_PATH: mongodbenterprise/opsmanager\n   APPDB_SECRET_BASE_PATH: mongodbenterprise/appdb\n   TLS_SECRET_REF: {TLSSecret}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap secret-configuration --from-file=config.yaml"
                }
            ],
            "preview": "You can choose the secret storage tool for Kubernetes Operator. The secret\nstorage tool is a secure place to store sensitive information for the components\nthat Kubernetes Operator manages. This includes secrets for MongoDB databases, Ops Manager, and AppDB.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/multi-cluster-edit-deployment",
            "title": "Edit a MongoDBMultiCluster Resource",
            "headings": [
                "Considerations",
                "Prerequisites",
                "Procedure",
                "Edit the Kubernetes MongoDBMultiCluster resource specification file.",
                "Modify or add settings as needed.",
                "Save your specification file.",
                "Apply the file."
            ],
            "paragraphs": "You can modify configuration and topology of the  MongoDBMultiCluster  resource , including\nits MongoDB versions, storage engines, and  Kubernetes  clusters. You can't change individual members of a replica set, you can change only\nthe whole set or  Kubernetes  cluster.\nFor a complete list of options supported by a  MongoDBMultiCluster  resource ,\nsee the  Multi-Kubernetes-Cluster Resource Specification . You can configure certain settings only using  Kubernetes Operator . To learn\nmore, see  MongoDB  Kubernetes Operator  Exclusive Settings . To update a  MongoDBMultiCluster  resource , you must have it installed and deployed. Multi-Kubernetes-Cluster Quick Start Deploy Replica Sets in a Multi-Kubernetes Cluster To learn about the settings you can change, see the  Multi-Kubernetes-Cluster Resource Specification .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <mongodb-multi-conf>.yaml"
                }
            ],
            "preview": "You can modify configuration and topology of the MongoDBMultiCluster resource, including\nits MongoDB versions, storage engines, and Kubernetes clusters.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/restricted-network-tutorial",
            "title": "Deploy in Restricted Networks",
            "headings": [
                "Procedure",
                "Mirror the Operator catalog and disable the default catalog.",
                "Install the Kubernetes Operator.",
                "Deploy Ops Manager in Local Mode.",
                "Deploy a MongoDB Database Resource."
            ],
            "paragraphs": "This tutorial demonstrates how to deploy MongoDB Enterprise Operator,\nan  Ops Manager  instance, and a MongoDB replica set using the\n Kubernetes Operator  in a restricted network OpenShift environment. Red Hat Operator catalogs contain metadata that  OLM\n(Operator Lifecycle Manager)  queries to install Operators\nand their dependencies on a cluster. You must create a copy of\nthe Operator catalog and then disable the default catalog as a\ncluster administrator. The  relatedImages  in the operator bundle lists the following\nimages: For  init  images, use only the version that is present in the\nlatest  Kubernetes Operator  version. The size of all related images referenced by the operator bundle\nis over 26 GB. Your mirroring command won't pick unsupported versions of the\nAppDB images. If you want to use any other unsupported version,\nyou must manually provide these images to the mirror registry. For\nother versions for a given operator version, see  image for Deployment . For the full list of images defined in the  RELATED_IMAGE_* \nenvironment variables, see the  CSV (cluster service versions) \n file . To learn more, see  Using Operator Lifecycle Manager on\nrestricted networks . Images directly used by the operator, which are the current\nversion of  mongodb-enterprise-init-database-ubi ,\n mongodb-enterprise-init-ops-manager-ubi ,\n mongodb-enterprise-init-appdb-ubi ,\n mongodb-enterprise-init-database-ubi . All currently supported images of  Ops Manager , AppDB and\n mongodb-agent-ubi  that can be configured in an  Ops Manager \ndeployment. To mirror, see  Mirroring images for a disconnected\ninstallation . When you run the  oc adm catalog mirror  command to mirror the\ncatalog, it generates the  imageContentSourcePolicy.yaml  file,\nwhich you must  apply \nto remap original source to mirrored images. For example: To learn more, see  Mirroring Operator catalogs for use\nwith disconnected clusters .\nFor a list of supported MongoDB versions for each  Ops Manager  version,\nsee  Compatible MongoDB Version . To disable the default catalog, add\n disableAllDefaultSources: true  to the  OperatorHub  object. To learn more, see: Plan your  MongoDB Enterprise Kubernetes Operator  Installation Install the  MongoDB Enterprise Kubernetes Operator To deploy  Ops Manager  in Local Mode, you must do the following: To learn more about deploying  Ops Manager  in Local Mode, see\n Configure an  Ops Manager  Resource to use Local Mode . Copy the following example  Ops Manager   Kubernetes   object  and save it as\na  .yaml  file. To learn more about the settings, see  Deploy an  Ops Manager  Resource . Use the  Ops Manager  configuration setting\n automation.versions.source: local  in\n spec.configuration  to enable Local Mode. Define a  Persistent Volume  for the  Ops Manager  StatefulSet to store the\nMongoDB installation archive. MongoDB Agents running in MongoDB\ndatabase resource containers that you create with the  Kubernetes Operator \ndownload the installation archives from  Ops Manager  instead of from the\nInternet. Configure  oc  to default to your namespace. Copy the following  Ops Manager  resource settings, paste into\nyour existing  Ops Manager  resource, and save your  Ops Manager  config\nfile. Copy the MongoDB installation archive to the  Ops Manager  Persistent\nVolume. To learn how to copy MongoDB installation archive, see step 10 in\nthe  Configure an Ops Manager Resource to use Local Mode  procedure. Create credentials and store them as a secret. Run the following command: Provide your public and private key values for the following\nparameters. To learn more, see  Create Credentials for the  Kubernetes Operator . Create a ConfigMap similar to the following: To learn more about the settings in the ConfigMap, see step 7 in\nthe  OpenShift Quick Start . You can  Deploy a Replica Set  or a  Deploy a Sharded Cluster .\nTo learn more, see  Deploy a MongoDB Database Resource .",
            "code": [
                {
                    "lang": "shell",
                    "value": "oc apply -f ./<output dir>/imageContentSourcePolicy.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: ops-manager-localmode\nspec:\n replicas: 2\n version: \"6.0.0\"\n adminCredentials: ops-manager-admin-secret\n configuration:\n   # this enables local mode in Ops Manager\n   automation.versions.source: local\n statefulSet:\n   spec:\n     # the Persistent Volume Claim will be created for each Ops Manager Pod\n     volumeClaimTemplates:\n      - metadata:\n          name: mongodb-versions\n        spec:\n          accessModes: [ \"ReadWriteOnce\" ]\n          resources:\n            requests:\n              storage: \"20Gi\"\n   template:\n     spec:\n       containers:\n         - name: mongodb-ops-manager\n           volumeMounts:\n           - name: mongodb-versions\n             # this is the directory in each Pod where all MongoDB\n             # archives must be put\n             mountPath: /mongodb-ops-manager/mongodb-releases\n backup:\n  enabled: false\n applicationDatabase:\n  members: 3\n  persistent: true"
                },
                {
                    "lang": "sh",
                    "value": "oc config set-context $(oc config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "configuration:\n  # this enables local mode in Ops Manager\n  automation.versions.source: local\nstatefulSet:\n  spec:\n    # the Persistent Volume Claim will be created for each Ops Manager Pod\n    volumeClaimTemplates:\n      - metadata:\n        name: mongodb-versions\n        spec:\n          accessModes: [ \"ReadWriteOnce\" ]\n          resources:\n            requests:\n              storage: \"20Gi\"\n  template:\n    spec:\n      containers:\n        - name: mongodb-ops-manager\n          volumeMounts:\n          - name: mongodb-versions\n            # this is the directory in each Pod where all MongoDB\n            # archives must be put\n            mountPath: /mongodb-ops-manager/mongodb-releases"
                },
                {
                    "lang": "sh",
                    "value": "oc \\\n  create secret generic ops-manager-admin-key \\\n  --from-literal=\"publicKey=<publicKey>\" \\\n  --from-literal=\"privateKey=<privateKey>\""
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | oc apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-project\n  namespace: mongodb\ndata:\n  projectName: myProjectName # this is an optional parameter\n  orgId: 5b890e0feacf0b76ff3e7183 # this is a required parameter\n  baseUrl: https://ops.example.com:8443\nEOF"
                }
            ],
            "preview": "This tutorial demonstrates how to deploy MongoDB Enterprise Operator,\nan Ops Manager instance, and a MongoDB replica set using the\nKubernetes Operator in a restricted network OpenShift environment.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/plan-k8s-op-considerations",
            "title": "Considerations",
            "headings": [
                "Deploy the Recommended Number of MongoDB Replica Sets",
                "Specify CPU and Memory Resource Requirements",
                "Co-locate mongos Pods with Your Applications",
                "Name Your MongoDB Service with its Purpose",
                "Use Labels to Differentiate Between Deployments",
                "Customize the CustomResourceDefinitions that the Kubernetes Operator Watches",
                "Ensure Proper Persistence Configuration",
                "Set CPU and Memory Utilization Bounds for the Kubernetes Operator Pod",
                "Set CPU and Memory Utilization Bounds for MongoDB Pods",
                "Use Multiple Availability Zones"
            ],
            "paragraphs": "This page details best practices and system configuration\nrecommendations for the  MongoDB Enterprise Kubernetes Operator  when running in production. We recommend that you use a single instance of the  Kubernetes Operator \nto deploy up to 20 replica sets in parallel. You  may  increase this number to 50 and expect a reasonable\nincrease in the time that the  Kubernetes Operator  takes to download,\ninstall, deploy, and reconcile its resources. For 50 replica sets, the time to deploy varies and might take up to\n40 minutes. This time depends on the network bandwidth of the  Kubernetes \ncluster and the time it takes each MongoDB Agent to download MongoDB\ninstallation binaries from the Internet for each MongoDB cluster member. To deploy more than 50 MongoDB replica sets in parallel,\nuse multiple instances of the  Kubernetes Operator . In  Kubernetes , each Pod includes parameters that allow you\nto specify  CPU resources \nand  memory resources  for each\ncontainer in the Pod. To indicate resource bounds,  Kubernetes  uses the  requests and limits \nparameters, where: The following sections illustrate how to: For the Pods hosting  Ops Manager , use the\n default resource limits configurations . The following considerations apply: All sizing and performance recommendations for common MongoDB deployments\nthrough the  Kubernetes Operator  in this section are subject to change. Do\nnot treat these recommendations as guarantees or limitations of any kind. These recommendations reflect performance testing findings and represent\nour suggestions for production deployments. We ran the tests on a cluster\ncomprised of seven AWS EC2 instances of type  t2.2xlarge  and a\nmaster node of type  t2.medium . The recommendations in this section don't discuss characteristics of\nany specific deployment. Your deployment's characteristics may differ\nfrom the assumptions made to create these recommendations. Contact\n MongoDB Support  for further help with sizings. request  indicates a lower bound of a resource. limit  indicates an upper bound of a resource. set CPU and Memory for the Operator Pod . set CPU and Memory for MongoDB Pods . You can run the lightweight  mongos  instance on the same  node \nas your apps using MongoDB. The  Kubernetes Operator  supports standard  Kubernetes \n node affinity and anti-affinity \nfeatures. Using these features, you can force install the  mongos \non the same node as your application. The following abbreviated example shows affinity and multiple\navailability zones configuration. The  podAffinity  key determines whether to install an application\non the same Pod, node, or data center as another application. To specify Pod affinity: See the full example of multiple availability zones and node affinity\nconfiguration in\n replica-set-affinity.yaml \nin the  Affinity Samples \ndirectory. This directory also contains sample affinity and multiple\nzones configurations for sharded clusters and standalone\nMongoDB deployments. Add a label and value in the  spec.podSpec.podTemplate.metadata.labels \n YAML (Yet Another Markup Language)  collection to tag the deployment. See\n spec.podSpec.podTemplate.metadata ,\nand the\n Kubernetes PodSpec v1 core API . Specify which label the  mongos  uses in the\n spec.mongosPodSpec.podAffinity \n .requiredDuringSchedulingIgnoredDuringExecution.labelSelector \n YAML (Yet Another Markup Language)  collection. The  matchExpressions  collection defines the\n label  that the  Kubernetes Operator  uses to identify the Pod for hosting\nthe  mongos . Assigning Pods to Nodes Node affinity and anti-affinity Kubernetes PodSpec v1 core API Set the  spec.service  parameter to a value that identifies\nthis deployment's purpose, as illustrated in the following example. spec.service Use the  Pod affinity \n Kubernetes  feature to: Separate different MongoDB resources, such as  test ,  staging ,\nand  production  environments. Place  Pods  on some specific nodes to take advantage of\nfeatures such as  SSD (Solid State Disk)  support. Pod affinity You can specify which custom resources you want the  Kubernetes Operator \nto watch. This allows you to install the CustomResourceDefinition for\nonly the resources that you want the  Kubernetes Operator  to manage. You must use  helm  to configure the  Kubernetes Operator  to watch only the\ncustom resources you specify. Follow the relevant  helm \n installation instructions ,\nbut make the following adjustments: Decide which CustomResourceDefinitions you want to install. You can\ninstall any number of the following: Value Description mongodb Install the CustomResourceDefinitions for database resources\nand watch those resources. mongodbusers Install the CustomResourceDefinitions for MongoDB user resources\nand watch those resources. opsmanagers Install the CustomResourceDefinitions for  Ops Manager  resources\nand watch those resources. Install the Helm Chart and specify which\nCustomResourceDefinitions you want the\n Kubernetes Operator  to watch. Separate each custom resource with a comma: The  Kubernetes  deployments orchestrated by the  Kubernetes Operator  are\nstateful. The  Kubernetes  container uses  Persistent Volumes  to maintain the\ncluster state between restarts. To satisfy the statefulness requirement, the  Kubernetes Operator  performs\nthe following actions: To meet your MongoDB cluster's storage needs, make the following\nchanges in your configuration for each replica set deployed with\nthe  Kubernetes Operator : The following abbreviated example shows recommended persistent storage\nsizes. For a full example of persistent volumes configuration, see\n replica-set-persistent-volumes.yaml \nin the  Persistent Volumes Samples  directory. This\ndirectory also contains sample persistent volumes configurations for\nsharded clusters and standalone deployments. Creates  Persistent Volumes  for your MongoDB deployment. Mounts storage devices to one or more directories\ncalled mount points. Creates one persistent volume for each MongoDB mount point. Sets the default path in each  Kubernetes  container to  /data . Verify that persistent volumes are enabled in\n spec.persistent . This setting defaults to  true . Specify a sufficient amount of storage for the  Kubernetes Operator \nto allocate for each of the volumes. The volumes store the data\nand the logs. To set multiple volumes, each for data, logs, and the  oplog , use\n spec.podSpec.persistence.multiple.data . To set a single volume to store data, logs, and the  oplog ,\nuse  spec.podSpec.persistence.single . spec.persistent spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data When you deploy replica sets with the  Kubernetes Operator , CPU usage for\nPod used to host the  Kubernetes Operator  is initially high during the\nreconciliation process, however, by the time the deployment completes,\nit lowers. For production deployments, to satisfy deploying up to 50 MongoDB\nreplica sets or sharded clusters in parallel with the  Kubernetes Operator ,\nset the CPU and memory resources and limits for the  Kubernetes Operator  Pod\nas follows: If you don't include the unit of measurement for CPUs,  Kubernetes  interprets\nit as the number of cores. If you specify  m , such as 500m,  Kubernetes \ninterprets it as  millis . To learn more, see\n Meaning of CPU . The following abbreviated example shows the configuration with\nrecommended CPU and memory bounds for the  Kubernetes Operator  Pod in your\ndeployment of 50 replica sets or sharded clusters. If you are\ndeploying fewer than 50 MongoDB clusters, you may use lower\nnumbers in the configuration file for the  Kubernetes Operator  Pod. For a full example of CPU and memory utilization resources and limits\nfor the  Kubernetes Operator  Pod that satisfy parallel deployment of up to\n50 MongoDB replica sets, see the  mongodb-enterprise.yaml \nfile. spec.template.spec.containers.resources.requests.cpu  to 500m spec.template.spec.containers.resources.limits.cpu  to 1100m spec.template.spec.containers.resources.requests.memory  to 200Mi spec.template.spec.containers.resources.limits.memory  to 1Gi Monitoring tools report the size of the  node  rather than the\nactual size of the container. Requests and Limits Assign CPU Resources to Containers and Pods The values for Pods hosting replica sets or sharded clusters map\nto the  requests field \nfor CPU and memory for the created Pod. These values are consistent\nwith  considerations \nstated for MongoDB hosts. The  Kubernetes Operator  uses its allocated memory for processing, for the\nWiredTiger cache, and for storing packages during the deployments. For production deployments, set the CPU and memory resources and limits\nfor the MongoDB Pod as follows: If you don't include the unit of measurement for CPUs,  Kubernetes  interprets\nit as the number of cores. If you specify  m , such as 500m,  Kubernetes \ninterprets it as  millis . To learn more, see\n Meaning of CPU . The following abbreviated example shows the configuration with\nrecommended CPU and memory bounds for each Pod hosting a MongoDB\nreplica set member in your deployment. For a full example of CPU and memory utilization resources and limits\nfor Pods hosting MongoDB replica set members, see the\n replica-set-podspec.yaml \nfile in the  MongoDB Podspec Samples  directory. This directory also contains sample CPU and memory limits\nconfigurations for Pods used for: spec.podSpec.podTemplate.spec.containers.resources.requests.cpu  to 0.25 spec.podSpec.podTemplate.spec.containers.resources.limits.cpu  to 0.25 spec.podSpec.podTemplate.spec.containers.resources.requests.memory  to 512M spec.podSpec.podTemplate.spec.containers.resources.limits.memory  to 512M A sharded cluster, in the  sharded-cluster-podspec.yaml . Standalone MongoDB deployments, in the  standalone-podspec.yaml . spec.podSpec.podTemplate.spec Requests and Limits Assign CPU Resources to Containers and Pods Set the  Kubernetes Operator  and  StatefulSets  to distribute all members\nof one replica set to different  nodes  to ensure high\navailability. The following abbreviated example shows affinity and multiple\navailability zones configuration. In this example, the  Kubernetes Operator  schedules the Pods deployment to\nthe nodes which have the label  kubernetes.io/e2e-az-name  in  e2e-az1  or\n e2e-az2  availability zones. Change  nodeAffinity  to\nschedule the deployment of Pods to the desired availability zones. See the full example of multiple availability zones configuration in\n replica-set-affinity.yaml \nin the  Affinity Samples \ndirectory. This directory also contains sample affinity and multiple zones\nconfigurations for sharded clusters and standalone MongoDB deployments. Running in Multiple Zones Node affinity",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: 4.2.1-ent\n  service: my-service\n\n  ...\n    podTemplate:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                  - key: security\n                    operator: In\n                    values:\n                      - S1\n              topologyKey: failure-domain.beta.kubernetes.io/zone\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                  - key: kubernetes.io/e2e-az-name\n                    operator: In\n                    values:\n                      - e2e-az1\n                      - e2e-az2\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            topologyKey: nodeId"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: \"4.4.0-ent\"\n  service: drilling-pumps-geosensors\n  featureCompatibilityVersion: \"4.0\""
                },
                {
                    "lang": "yaml",
                    "value": "mongosPodSpec:\n  podAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n        matchExpressions:\n        - key: security\n          operator: In\n          values:\n          - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone"
                },
                {
                    "lang": "sh",
                    "value": "helm install <deployment-name> mongodb/enterprise-operator \\\n  --set operator.watchedResources=\"{mongodb,mongodbusers}\" \\\n     --skip-crds"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-cluster\nspec:\n\n  ...\n  persistent: true\n\n\n  shardPodSpec:\n  ...\n    persistence:\n      multiple:\n        data:\n          storage: \"20Gi\"\n        logs:\n          storage: \"4Gi\"\n          storageClass: standard"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n name: mongodb-enterprise-operator\n namespace: mongodb\nspec:\n replicas: 1\n selector:\n  matchLabels:\n     app.kubernetes.io/component: controller\n     app.kubernetes.io/name: mongodb-enterprise-operator\n     app.kubernetes.io/instance: mongodb-enterprise-operator\n template:\n  metadata:\n   labels:\n     app.kubernetes.io/component: controller\n     app.kubernetes.io/name: mongodb-enterprise-operator\n     app.kubernetes.io/instance: mongodb-enterprise-operator\n   spec:\n     serviceAccountName: mongodb-enterprise-operator\n     securityContext:\n       runAsNonRoot: true\n       runAsUser: 2000\n     containers:\n     - name: mongodb-enterprise-operator\n       image: quay.io/mongodb/mongodb-enterprise-operator:1.9.2\n       imagePullPolicy: Always\n       args:\n        - \"-watch-resource=mongodb\"\n        - \"-watch-resource=opsmanagers\"\n        - \"-watch-resource=mongodbusers\"\n       command:\n        - \"/usr/local/bin/mongodb-enterprise-operator\"\n       resources:\n         limits:\n           cpu: 1100m\n           memory: 1Gi\n         requests:\n           cpu: 500m\n           memory: 200Mi"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\nname: my-replica-set\nspec:\n  members: 3\n  version: 4.0.0-ent\n  service: my-service\n  ...\n\n  persistent: true\n  podSpec:\n    podTemplate:\n      spec:\n        containers:\n        - name: mongodb-enterprise-database\n          resources:\n            limits:\n              cpu: \"0.25\"\n              memory: 512M"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: 4.2.1-ent\n  service: my-service\n  ...\n    podAntiAffinityTopologyKey: nodeId\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n        matchExpressions:\n        - key: security\n          operator: In\n          values:\n          - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n\n    nodeAffinity:\n       requiredDuringSchedulingIgnoredDuringExecution:\n         nodeSelectorTerms:\n         - matchExpressions:\n           - key: kubernetes.io/e2e-az-name\n           operator: In\n           values:\n           - e2e-az1\n           - e2e-az2"
                }
            ],
            "preview": "This page details best practices and system configuration\nrecommendations for the MongoDB Enterprise Kubernetes Operator when running in production.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/manage-database-users-ldap",
            "title": "Manage Database Users Using LDAP Authentication",
            "headings": [
                "Considerations",
                "Supported Authentication Mechanisms",
                "Prerequisites",
                "Add a Database User",
                "Configure kubectl to default to your namespace.",
                "Copy the following example CustomResourceDefinition.",
                "Open your preferred text editor and paste the example CustomResourceDefinition into a new text file.",
                "Change the lines for the following parameters, as needed.",
                "Add any additional roles for the user to the CustomResourceDefinition.",
                "Create the user.",
                "View the newly created user in Cloud Manager or Ops Manager.",
                "Delete a Database User"
            ],
            "paragraphs": "The  Kubernetes Operator  supports managing database users for deployments\nrunning with  TLS (Transport Layer Security)  and LDAP cluster authentication enabled. The configuration for users authenticated through  LDAP (Lightweight Directory Access Protocol)  relies on the\nLDAP Query Templates and the mappings that MongoDB establishes. To learn more, see the following sections in the MongoDB Server\ndocumentation: LDAP Authorization LDAP Query Templates security.ldap.userToDNMapping The  Kubernetes Operator  supports SCRAM, LDAP, and X.509 authentication\nmechanisms in deployments it creates. In an  Kubernetes Operator -created\ndeployment, you cannot use  Ops Manager  to: Configure other authentication mechanisms for deployments. Manage users  not  using SCRAM, LDAP, or X.509 authentication. Before managing database users, you must deploy a\n replica set  or\n sharded cluster  with  LDAP (Lightweight Directory Access Protocol)  enabled.\nenabled. Optionally, you can enable  TLS (Transport Layer Security) . To learn more, see\n Secure a Database Resource . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Use the following table to guide you through changing the relevant\nlines in the  MongoDB User Resource Specification . For a full list of LDAP user settings, see\n security settings  in the  Kubernetes Operator \nMongoDB resource specification. Key Type Description Example metadata.name string The name of the resource for the MongoDB database user. Resource names must be 44 characters or less. ldap-user-1 spec.db string The name of the MongoDB database where users will be added. This\nvalue must be  $external . $external spec.mongodbResourceRef.name string The name of the  MongoDB resource \nto which this user is associated. my-resource spec.opsManager.configMapRef.name string The name of the project containing the MongoDB database\nwhere the user will be added. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. my-project spec.roles.db string The database the  role  can act on. admin spec.roles.name string The name of the  role  to grant the database user.\nThe role name can be any  built-in MongoDB role \nor  custom role  that\nexists in  Cloud Manager or Ops Manager . readWriteAnyDatabase spec.username string The authenticated username that is mapped to an LDAP Distinguished\nName (DN) according to\n spec.security.authentication.ldap.userToDNMapping .\nThe DN must already exist in your LDAP deployment.\nThis username must comply with the  RFC 2253 \nLDAPv3 Distinguished Name standard.  transformed To learn more, see\n LDAP Query Templates  in the\nMongoDB Manual. uid=mdb0,dc=example,dc=org You may grant additional roles to this user using the format defined\nin the following example: Invoke the following  Kubernetes  command to create your database user: The following examples illustrate the connection string formats that you\ncan use when enabling authentication with LDAP in  Kubernetes Operator  MongoDB\ndeployments. These examples use the  mongodb  namespace and a replica\nset deployment named  replica-set-ldap . The examples are similar for\nsharded clusters. Using the previously-shown formats, you can connect to the MongoDB\ndatabase with the MongoDB Shell ( mongosh ), as in the following\nexample: You can use these credentials to\n connect to a MongoDB Database Resource from Inside Kubernetes . connectionString.standard : Standard connection string \nthat can connect you to the database as this database user. connectionString.standardSrv :  DNS seed list connection string  that\ncan connect you to the database as this database user. You can view the newly-created user in  Cloud Manager or Ops Manager : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. To delete a database user, pass the  metadata.name  from the user\n ConfigMap  to the following command:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: ldap-user-1\nspec:\n  username: \"uid=mdb0,dc=example,dc=org\"\n  db: \"$external\"\n  mongodbResourceRef:\n    name: ldap-replica-set\n  roles:\n    - db: \"admin\"\n      name: \"clusterAdmin\"\n    - db: \"admin\"\n      name: \"readWriteAnyDatabase\"\n    - db: \"admin\"\n      name: \"dbAdminAnyDatabase\"\n\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: ldap-user-1\nspec:\n  username: \"uid=mdb0,dc=example,dc=org\"\n  db: \"$external\"\n  mongodbResourceRef:\n    name: ldap-replica-set\n  roles:\n  - db: \"admin\"\n    name: \"clusterAdmin\"\n  - db: \"admin\"\n    name: \"readWriteAnyDatabase\"\n  - db: \"admin\"\n    name: \"dbAdminAnyDatabase\"\n\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "mongosh <connection-string> \\\n  --host <my-replica-set>/web1.example.com \\\n  --port 30907 \\\n  --authenticationMechanism PLAIN \\\n  --username cn=rob,cn=Users,dc=ldaps-01,dc=myteam,dc=com"
                },
                {
                    "lang": "sh",
                    "value": "mongodb://replica-set-ldap-0-0-svc.mongodb.svc.cluster.local/?connectTimeoutMS=20000&replicaSet=replica-set-ldap&serverSelectionTimeoutMS=20000&ssl=true&authSource=$external"
                },
                {
                    "lang": "sh",
                    "value": "mongodb+srv://replica-set-ldap-svc.mongodb.svc.cluster.local/?connectTimeoutMS=20000&replicaSet=replica-set-ldap&serverSelectionTimeoutMS=20000&ssl=true&authSource=$external"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mdbu <metadata.name>"
                }
            ],
            "preview": "The Kubernetes Operator supports managing database users for deployments\nrunning with TLS (Transport Layer Security) and LDAP cluster authentication enabled.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/manage-database-users-x509",
            "title": "Manage Database Users Using X.509 Authentication",
            "headings": [
                "Supported Authentication Mechanisms",
                "Prerequisites",
                "Add a Database User",
                "Configure kubectl to default to your namespace.",
                "Copy the following example CustomResourceDefinition.",
                "Open your preferred text editor and paste the example CustomResourceDefinition into a new text file.",
                "Change the lines for the following parameters, as needed.",
                "Add any additional roles for the user to the CustomResourceDefinition.",
                "Create the user.",
                "View the newly created user in Cloud Manager or Ops Manager.",
                "Delete a Database User"
            ],
            "paragraphs": "The  Kubernetes Operator  supports managing database users for deployments\nrunning with  TLS (Transport Layer Security)  and X.509 internal cluster authentication enabled. After enabling X.509 authentication, you can add X.509 users using the  Ops Manager  interface or the  CustomResourceDefinition . The  Kubernetes Operator  supports SCRAM, LDAP, and X.509 authentication\nmechanisms in deployments it creates. In an  Kubernetes Operator -created\ndeployment, you cannot use  Ops Manager  to: Configure other authentication mechanisms for deployments. Manage users  not  using SCRAM, LDAP, or X.509 authentication. Before managing database users, you must deploy a\n replica set  or\n sharded cluster  with  TLS (Transport Layer Security)  and X.509\nenabled. If you need to generate X.509 certificates for your MongoDB users,\nsee  Generate X.509 Client Certificates . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Use the following table to guide you through changing the relevant\nlines in the  MongoDB User Resource Specification : Key Type Description Example metadata.name string The name of the database user resource. Resource names must be 44 characters or less. mms-user-1 spec.username string The subject line of the x509 client certificate signed\nby the  Kubernetes   CA (Certificate Authority)  (Kube CA). To get the subject line of the X.509 certificate, run the\nfollowing command: The username must comply with the\n RFC 2253 \nLDAPv3 Distinguished Name standard. CN=mms-user,U=My Organizational Unit,O=My Org,L=New York,ST=New York,C=US spec.opsManager.configMapRef.name string The name of the project containing the MongoDB database\nwhere user will be added. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. my-project spec.roles.db string The database the  role  can act on. admin spec.mongodbResourceRef.name string The name of the  MongoDB resource  to\nwhich this user is associated. my-resource spec.roles.name string The name of the  role  to grant the database\nuser. The role name can be any\n built-in MongoDB role  or\n custom role  that exists\nin  Cloud Manager or Ops Manager . readWriteAnyDatabase You may grant additional roles to this user using the format defined\nin the following example: Invoke the following  Kubernetes  command to create your database user: You can use these credentials to  Connect to a MongoDB Database Resource from Inside Kubernetes . When you create a new MongoDB database user,  Kubernetes Operator  automatically\ncreates a new  Kubernetes   secret . The  Kubernetes   secret \ncontains the following information about the new database user: username : Username for the database user password : Password for the database user connectionString.standard :  Standard connection string \nthat can connect you to the database as this database user. connectionString.standardSrv :  DNS seed list connection string  that can\nconnect you to the database as this database user. Alternatively, you can specify an optional\n spec.connectionStringSecretName  field in the\n MongoDB User Resource Specification  to specify\nthe name of the connection string secret that the\n Kubernetes Operator  creates. You can view the newly-created user in  Cloud Manager or Ops Manager : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. To delete a database user, pass the  metadata.name  from the user\n ConfigMap  to the following command:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: <resource-name>\nspec:\n  username: <rfc2253-subject>\n  db: \"$external\"\n  mongodbResourceRef:\n    name: '<MongoDB-Resource-name>'\n  roles:\n    - db: <database-name>\n      name: <role-name>\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "openssl x509 -noout \\\n  -subject -in <my-cert.pem> \\\n  -nameopt RFC2253"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: mms-user-1\nspec:\n  username: CN=mms-user,U=My Organizational Unit,O=My Org,L=New York,ST=New York,C=US\n  project: my-project\n  db: \"$external\"\n  roles:\n    - db: admin\n      name: backup\n    - db: admin\n      name: restore\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mdbu <metadata.name>"
                }
            ],
            "preview": "The Kubernetes Operator supports managing database users for deployments\nrunning with TLS (Transport Layer Security) and X.509 internal cluster authentication enabled.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "reference/k8s-operator-multi-cluster-specification",
            "title": "Multi-Kubernetes-Cluster Resource Specification",
            "headings": [
                "Example",
                "Required MongoDBMultiCluster Resource Settings",
                "Optional MongoDBMultiCluster Resource Settings"
            ],
            "paragraphs": "The  MongoDBMultiCluster  resource  defines your  multi-Kubernetes-cluster deployment  and gives the  MongoDB Enterprise Kubernetes Operator  the information it needs to create or update your clusters,  Ops Manager  deployment,  StatefulSets , services, and other  Kubernetes  resources. The following example shows a resource specification for a  multi-Kubernetes-cluster deployment : This section describes settings that you must use for your  MongoDBMultiCluster  resource . Type : string Version of the MongoDB  Kubernetes  resource schema. Type : string Kind of MongoDB  Kubernetes  resource to create. Set this to  MongoDBMultiCluster . Type : string Name of the MongoDB  Kubernetes  resource you are creating. Resource names must be 44 characters or less. Type : string Name of the secret you  created  as  Ops Manager   API (Application Programming Interface) \nauthentication credentials for the  Kubernetes Operator  to communicate with  Ops Manager . The  Ops Manager   Kubernetes   Secret  object\nholding the Credentials must exist on the  same  Namespace as\nthe resource you want to create. The  Kubernetes Operator  tracks any changes to the Secret and\nreconciles the state of the  MongoDB  resource . Type : string Type of MongoDB  Kubernetes  resource to create. The only accepted value for a  multi-Kubernetes-cluster deployment  is  ReplicaSet . Type : string Version of MongoDB installed for this  MongoDBMultiCluster  resource . Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. If you update this value to a later version, consider setting\n spec.featureCompatibilityVersion  to give yourself the\noption to downgrade if necessary. MongoDBMultiCluster  resources  can use the following settings: Type : collection Additional  configuration options  with\nwhich you want to start MongoDB processes. The  Kubernetes Operator  supports all configuration options that the MongoDB\nversion you deploy through the MongoDB Agent supports, except that the\n Kubernetes Operator  overrides values that you provide for any of the\nfollowing options: To learn more about the configuration options that the  Kubernetes Operator \nowns, see  MongoDB  Kubernetes Operator  Exclusive Settings . To learn which configuration options you can use, see\n Advanced Options for MongoDB Deployments  in the  Ops Manager \ndocumentation. net.port net.tls.certificateKeyFile net.tls.clusterFile net.tls.PEMKeyFile replication.replSetName security.clusterAuthMode sharding.clusterRole storage.dbPath systemLog.destination systemLog.path Type : collection MongoDB Agent configuration settings for the MongoDB database resource. Type : collection MongoDB Agent settings with which you want to start the MongoDB database resource. You must provide MongoDB Agent settings as key-value pairs. The values\nmust be strings.\nFor a list of supported MongoDB Agent settings, see: MongoDB Agent Settings \nfor  Cloud Manager  projects. MongoDB Agent Settings  for the  Ops Manager  version you\ndeployed with the  Kubernetes Operator . Type : collection The collection container for  spec.backup.mode ,\nwhich enables continuous backups for MongoDB resources in  Kubernetes Operator . Type : array A list of assignment labels for the  Backup Daemon Service  processes.\nUse assignment labels to identify that specific backup daemon\nprocesses are associated with particular projects. If you set assignment\nlabels using the  Kubernetes Operator , the values that you set in the  Kubernetes \nconfiguration file for assignment labels override the values defined\nin the  Ops Manager  UI. Assignment labels that you don't set using the\n Kubernetes Operator  continue to use the values set in the  Ops Manager  UI. Type : boolean Flag that indicates whether the  Kubernetes Operator  stops and terminates the\nbackup when you delete a  MongoDBMultiCluster  resource . The default value is  false .\nSetting this flag to  true  is useful when you want to delete the\n MongoDBMultiCluster  resource  while the  spec.backup.mode  setting\nis set to  enabled . Type : object Object that contains the backup encryption configuration settings. Type : object Object that contains the  KMIP (Key Management Interoperability)  backup encryption configuration\nsettings. To learn more, see  Configure KMIP Backup Encryption for Ops Manager . Type : object Object that contains the  KMIP (Key Management Interoperability)  backup encryption client configuration\nsettings. Type : string Enables continuous backups for a  MongoDBMultiCluster  resource .\nPossible values are  enabled ,  disabled , and\n terminated . After you enable continuous backups for your MongoDB resource with\n spec.backup.mode , you can  check the backup status . The  spec.backup.mode \nsetting relies on  Backup  that is\nenabled in  Ops Manager  and requires that the\n spec.backup.enabled \nvalue in the  Ops Manager   resource specification  is set to  true . Type : collection Collection container for snapshot schedule settings for\ncontinuous backups for MongoDB resources in  Kubernetes Operator . Type : number Number of days to keep daily snapshots. You can set a value\nbetween  1  and  365 , inclusive. Setting the value to  0 \ndisables this rule. Type : string Day of the week when  Ops Manager  takes a full snapshot. This setting\nensures a recent complete backup.  Ops Manager  sets the default value to\n SUNDAY . Type : number Number of months to keep monthly snapshots. You can set a value\nbetween  1  and  36 , inclusive. Setting the value to  0 \ndisables this rule. Type : number Number of hours in the past for which you can create a point-in-time\nsnapshot. Type : number UTC (Coordinated Universal Time)  hour of the day to schedule snapshots using a 24 hour clock.\nYou can set a value between  0  and  23 , inclusive. Type : number UTC (Coordinated Universal Time)  minute of the hour to schedule snapshots. You can\nset a value between  0  and  59 , inclusive. Type : number Number of hours between snapshots. You can set a value of  6 ,\n 8 ,  12 , or  24 . Type : number Number of days to keep recent snapshots. You can set a value\nbetween  2  and  5 , inclusive. Type : number Number of weeks to keep weekly snapshots. You can set a value\nbetween  1  and  52 , inclusive. Setting the value to  0 \ndisables this rule. Type : string Alias for  spec.opsManager.configMapRef.name . Type : collection List of specifications for each  Kubernetes  cluster in a  MongoDBMultiCluster  resource . Type : string Name of the cluster where the  MongoDB Enterprise Kubernetes Operator  schedules the  StatefulSet . When the  Kubernetes Operator  deploys this  MongoDBMultiCluster  resource , it creates  a service account . This name is what the service account in the central cluster uses to communicate with the workload clusters. Type : string An external domain used to externally expose your replica set deployment. By default, each replica set member uses the  Kubernetes  Pod's  FQDN (fully qualified domain name) \n( *.svc.cluster.local ) as the default hostname. However, if you add an\nexternal domain to this setting, the replica set uses a hostname that is a\nsubdomain of the specified domain instead. This hostname uses the following\nformat: For example: After you deploy the replica set with this setting, the\n Kubernetes Operator  uses the hostname with the external domain to override\nthe  processes[n].hostname  field in the  Ops Manager   automation configuration . Then, the MongoDB Agent uses this hostname to\nconnect to  mongod . To specify other hostnames for connecting to the replica set, you can use the\n spec.connectivity.replicaSetHorizons  setting. However, the following\nconnections still use the hostname with the external domain: <replica-set-name>-<cluster-idx>-<pod-idx>.<externalDomain> multi-replica-set-0-1.cluster-0.example.com The MongoDB Agent to connect to  mongod . mongod  to connect to other  mongod  instances. Specifying this field changes how  Ops Manager  registers  mongod  processes.\nYou can specify this field only for new replica set deployments starting in  Kubernetes Operator \nversion 1.19. You can't change the value of this field or any  processes[n].hostname  fields\nin the  Ops Manager   automation configuration  for a running\nreplica set deployment. Use this setting only when deploying a  multi-Kubernetes-cluster deployment  replica set  without\na service mesh . See  Deploy Replica Sets in a Multi-Kubernetes Cluster without a Service Mesh . Type : collection Configuration for externally exposing a specific cluster in your  multi-Kubernetes-cluster deployment . These settings\noverride the global  spec.externalAccess.externalService  settings. When you set the  spec.externalAccess  setting, the  Kubernetes Operator \nautomatically creates an external load balancer service with  default values .\nYou can override certain values or add new values depending on your needs.\nFor example, if you intend to create  NodePort services \nand don't need a load balancer, you must configure overrides in your\n Kubernetes  specification: For more information about the  Kubernetes  specification, see  ServiceSpec \nin the  Kubernetes  documentation. Type : collection Key-value pairs that let you add cloud provider-specific\nconfiguration settings to a specific cluster in your  multi-Kubernetes-cluster deployment . This setting overrides the global\nsetting,  spec.externalAccess.externalService.annotations . To learn more, see\n Annotations \nin the  Kubernetes  documentation and the documentation for the cloud provider you use\nfor  Kubernetes  deployments. Type : collection Configuration for the  ServiceSpec .\nTo learn more, see  spec.clusterSpecList.externalAccess.externalService . Type : collection Specification for each MongoDB replica set and its\nmembers in your  multi-Kubernetes-cluster deployment . The order of the elements in the object for each replica set\nmust reflect the order of members in the replica set. For example,\nthe first element affects the Pod at index  0 , the second\nelement affects index  1 , and so on. Consider the following example specification for a\n multi-Kubernetes-cluster deployment  with three replica sets: Type : string Number that indicates the relative likelihood of a MongoDB replica set member to become the  primary . For example, a member with a  memberConfig.priority  of  1.5  is more likely than a member with a  memberConfig.priority  of  0.5  to become the primary. A member with a  memberConfig.priority  of  0  is ineligible to become the primary. To learn more, see  Member Priority . To increase the relative likelihood that a replica set member becomes the primary, specify a higher  priority  value. To decrease the relative likelihood that a replica set member becomes the primary, specify a lower  priority  value. Type : map Map of  replica set tags  for directing\nread and write operations to specific members of your MongoDB replica set. Type : number Determines whether a MongoDB replica set member can vote in an  election . Set to  1  to allow the member to vote. Set to  0  to exclude the member from an election. Type : number Number of members in the MongoDB replica set. Type : string Default : <resource_name>+\"-service\" Name of the  Kubernetes  service you want to create or use for a\n StatefulSet . If a service with this name already exists, the\n MongoDB Enterprise Kubernetes Operator  does not delete or recreate it. This setting lets\nyou create your own custom services and lets the  Kubernetes Operator \nreuse them. Type : collection Provides the configuration for the  StatefulSet  override for each of\nthe cluster's StatefulSets in a  multi-Kubernetes-cluster deployment . To set the global configuration that\napplies to all clusters in your  multi-Kubernetes-cluster deployment , see  spec.statefulSet.spec . This setting applies only to replica set resource types in  multi-Kubernetes-cluster deployments . Type : collection Allows you to provide different  DNS (Domain Name System)  settings for client\napplications and the MongoDB Agents. The  Kubernetes Operator  uses split\nhorizon  DNS (Domain Name System)  for replica set members. This feature allows\ncommunication both within the  Kubernetes  cluster and from outside  Kubernetes . You can add multiple external mappings per host. In this example, clients communicate with the replica set using the  example-website \nhorizon. Make sure that each value in this array is unique. Make sure that the number of entries in this array matches the\nvalue given in  spec.clusterSpecList.members . Provide a value for the\n spec.security.certsSecretPrefix  setting to\nenable  TLS (Transport Layer Security) . This method to use split horizons requires the\nServer Name Indication extension of the  TLS (Transport Layer Security)  protocol. Configure the routing for external hostnames . Type : boolean Default : true Specifies whether the  Kubernetes Operator  duplicates a Pod's service mesh object in each cluster to allow  DNS (Domain Name System)  resolution. Set to  false  if you configure a DNS proxy for your service mesh. For example, see  DNS  Proxying  in the Istio  documentation. Type : collection Specification to expose your  multi-Kubernetes-cluster deployment  for external connections.\nTo learn how to connect to your  multi-Kubernetes-cluster deployment  from outside\nof the  Kubernetes  cluster, see  Connect to Multi-Cluster Resource from Outside Kubernetes . These settings apply to services across all clusters.\nTo override these global settings in a specific cluster, use\n spec.clusterSpecList.externalAccess.externalService . If you add  spec.externalAccess , the  Kubernetes Operator  creates an external service\nfor each Pod in a replica set. External services provide an external entry point\nfor each MongoDB database Pod in a cluster. Each external service has\n selectors \nthat match the external service to a specific Pod. If you add this setting without any values, the  Kubernetes Operator  creates\nan external service with the following default values: Field Value Description Name <pod-name>-svc-external Name of the external service. You can't change this value. Type LoadBalancer Creates an external  LoadBalancer  service. Port <Port Number> A port for  mongod . publishNotReadyAddress true Specifies that  DNS records \nare created even if the Pod isn't ready.\nDo not set to  false  for any database Pod. If you set  spec.clusterSpecList.externalAccess.externalDomain ,\nthe external service adds another port ( Port Number + 1 ) for backups. Type : collection Specification for overriding the default values in  spec.externalAccess . When you set the  spec.externalAccess  setting, the  Kubernetes Operator \nautomatically creates an external load balancer service with  default values .\nYou can override certain values or add new values depending on your needs.\nFor example, if you intend to create  NodePort services \nand don't need a load balancer, you must configure overrides in your\n Kubernetes  specification: For more information about the  Kubernetes  specification, see  ServiceSpec \nin the  Kubernetes  documentation. Type : collection Key-value pairs that let you add cloud provider-specific\nconfiguration settings to all clusters in your  multi-Kubernetes-cluster deployment . For cluster-specific overrides,\nsee  spec.clusterSpecList.externalAccess.externalService.annotations . To learn more, see\n Annotations \nin the  Kubernetes  documentation and the documentation for the cloud provider you use\nfor  Kubernetes  deployments. Type : collection Configuration for the  ServiceSpec .\nTo learn more, see  spec.externalAccess.externalService . Type : number Limits changes to data that occur with an upgrade to a\nnew major version. This allows you to downgrade to the previous major\nversion. To learn more about feature compatibility, see  setFeatureCompatibilityVersion \nin the MongoDB Manual. Type : string Configures the level of Automation Agent logging inside the\n Pod . Accepted values include: DEBUG INFO WARN ERROR FATAL Type : string Name of the  ConfigMap  with the  Cloud Manager or Ops Manager  connection\nconfiguration. The  spec.cloudManager.configMapRef.name \nsetting is an alias for this setting and can be used in its place. This value must exist on the  same  namespace as the resource\nyou want to create. The  Kubernetes Operator  tracks any changes to the ConfigMap and\nreconciles the state of the  MongoDB  resource . Type : boolean Default : true Grant your containers permission to write to your  Persistent Volume .\nThe  Kubernetes Operator  sets  fsGroup = 2000 ,  runAsUser = 2000 ,\nand  runAsNonRoot = true  in  securityContext .  Kubernetes Operator \nsets  fsgroup  equal to  runAsUser  to make the volume writable\nfor a user that runs the main process in the container. To learn\nmore, see  Configure a\nSecurity Context for a Pod or Container  and the related\n discussion  in\nthe  Kubernetes  documentation. If redeploying the resource doesn't fix\nissues with your Persistent Volume, contact  MongoDB Support . If you do not use  Persistent Volumes , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. Type : collection Authentication specifications for your  multi-Kubernetes-cluster deployment . Type : collection MongoDB Agent authentication configuration for the  Cloud Manager or Ops Manager  project. Type : string The Distinguished Name (DN) of the LDAP group to which the\nMongoDB Agent user belongs. This setting is required if: spec.security.authentication.ldap.authzQueryTemplate  is\npresent, and spec.security.authentication.agents.mode  is  LDAP  or\n X509 . Type : collection Details of the  secret  that contains the password for the\n spec.security.authentication.agents.automationUserName \nuser. This setting is required if\n spec.security.authentication.agents.mode  is  LDAP . Type : string Key in the\n spec.security.authentication.agents.automationPasswordSecretRef.name \n secret  that contains the password for the user in\n spec.security.authentication.agents.automationUserName . This setting is required if\n spec.security.authentication.agents.mode  is\n LDAP . Type : string Name of the  secret  that contains the password for the\n spec.security.authentication.agents.automationUserName \nuser. You must create this secret in the same namespace to which you\ndeploy the  Kubernetes Operator : This secret must contain one key, the value of which matches the\npassword of the\n spec.security.authentication.agents.automationUserName  user\nin your LDAP deployment. This setting is required if\n spec.security.authentication.agents.mode  is  LDAP . Type : boolean Specifies whether these options are required or optional: spec.security.authentication.agents.automationPasswordSecretRef.key spec.security.authentication.agents.automationPasswordSecretRef.name Type : string Name of the user that the MongoDB Agents use to interact with your\n multi-Kubernetes-cluster deployment . The username is mapped to an LDAP Distinguished\nName (DN) according to\n spec.security.authentication.ldap.userToDNMapping . The\nresulting DN must already exist in your LDAP deployment. This setting is required if\n spec.security.authentication.agents.mode  is  LDAP . Type : string Default : agent-certs Specifies the  secret  that contains the MongoDB Agent's\n TLS (Transport Layer Security)  certificate. This secret must contain the following keys, the\nvalues of which are  TLS (Transport Layer Security)  certificates that can be validated by the\nserver: You must create this secret in the same namespace to which you\ndeploy the  Kubernetes Operator : mms-automation-agent-pem mms-backup-agent-pem mms-monitoring-agent-pem Type : boolean Default : false Specifies whether authentication is enabled on the  Cloud Manager or Ops Manager \nproject. If set to  true , you must set an authentication mechanism in  spec.security.authentication.modes . The  Kubernetes Operator  manages authentication for this MongoDB\nresource if you include this setting, even if it is set to\n false . You can't configure authentication for this\nresource using the  Cloud Manager or Ops Manager  user interface or APIs while this\nsetting exists in the resource specification. Omit this setting if you want to manage authentication using the\n Cloud Manager or Ops Manager  user interface or APIs. Type : string The authentication mechanism that the MongoDB Agents for\nyour  multi-Kubernetes-cluster deployment  use. Valid values are  SCRAM ,  SCRAM-SHA-1 ,  MONGODB-CR ,  X509 , and  LDAP .\nThe value you specify must also be present in\n spec.security.authentication.modes .\nWe recommend  SCRAM-SHA-256 (SCRAM)  over  SCRAM-SHA-1 . If you specify  SCRAM-SHA-1 , you must also specify  MONGODB-CR . This setting is required if you specified more than one value for\n spec.security.authentication.modes . Type : boolean Default : false Determines whether you can modify database users that were not\nconfigured through the  Kubernetes Operator , or the  Cloud Manager or Ops Manager  user interface. To manage database users directly through the  mongod  or  mongos , set to  true . Type : string Specifies whether  X.509 internal cluster authentication  is enabled. To enable X.509 internal cluster authentication, set to  \"X509\" .\nRequires that the following settings be specified: The  Kubernetes Operator  accepts the following values: spec.security.authentication.modes   : [\"X509\"] spec.security.certsSecretPrefix [\"X509\"] : X.509 internal cluster authentication is enabled. \"\"  or omitted: internal cluster authentication is not enabled. After you enable internal cluster authentication, you can't disable it. Type : collection Required for LDAP authentication. Configures  LDAP (Lightweight Directory Access Protocol)  authentication for the  Cloud Manager or Ops Manager  project. To enable  LDAP (Lightweight Directory Access Protocol)  authentication, set\n spec.security.authentication.modes  to  [\"LDAP\"] . Type : string Required for LDAP authorization. An  RFC4515  and  RFC4516  LDAP-formatted query URL\ntemplate executed by MongoDB to obtain the LDAP groups that the user\nbelongs to. The query is relative to the host or hosts\nspecified in  spec.security.authentication.ldap.servers .\nYou can use the following tokens in the template: For more details, see  LDAP Query Templates  in the MongoDB Manual. Substitutes the authenticated username, or the\n transformed \nusername, into the LDAP query. Substitutes the supplied username, before either\nauthentication or LDAP transformation, into the LDAP query.\n( Available starting in MongoDB version 4.2 ). Type : collection Required for LDAP authentication. Specifies the  secret  that contains the password with which\nMongoDB binds when connecting to the  LDAP (Lightweight Directory Access Protocol)  server. Type : string Required for LDAP authentication. Name of the  secret  that contains the password with which MongoDB\nbinds when connecting to the  LDAP (Lightweight Directory Access Protocol)  server. The  secret  must contain only one  password  field which stores\nthe password. Type : string Required for LDAP authentication. LDAP (Lightweight Directory Access Protocol)  Distinguished Name to which MongoDB binds when connecting to\nthe  LDAP (Lightweight Directory Access Protocol)  server. Type : collection Required for LDAP authentication with TLS. ConfigMap  that contains a  CA (Certificate Authority)  which validates the  LDAP (Lightweight Directory Access Protocol) \nserver's  TLS (Transport Layer Security)  certificate. Type : string Required for LDAP authentication with TLS. Field name that stores the  CA (Certificate Authority)  which validates the  LDAP (Lightweight Directory Access Protocol) \nserver's  TLS (Transport Layer Security)  certificate. Type : string Required for LDAP authentication with TLS. Name of the  ConfigMap  that contains a  CA (Certificate Authority)  which validates\nthe  LDAP (Lightweight Directory Access Protocol)  server's  TLS (Transport Layer Security)  certificate. Type : boolean Specifies whether these options are required or optional: spec.security.authentication.ldap.caConfigMapRef.key spec.security.authentication.ldap.caConfigMapRef.name Type : array of strings Required for LDAP authentication. List of hostnames and ports of the  LDAP (Lightweight Directory Access Protocol)  servers. Specify\nhostnames with their respective ports in the following format: Type : integer Specifies how many milliseconds an authentication request should\nwait before timing out. Type : string Required for LDAP authentication. Specifies whether the  LDAP (Lightweight Directory Access Protocol)  server accepts  TLS (Transport Layer Security) . If the  LDAP (Lightweight Directory Access Protocol)  server accepts  TLS (Transport Layer Security) , set the value to  tls . If\nthe  LDAP (Lightweight Directory Access Protocol)  server doesn't accept  TLS (Transport Layer Security) , leave this value blank or set\nthe value to  none . If you specify a string other than  none  or  tls ,\n Kubernetes Operator  still sets the setting to  tls . Type : integer Specifies how many seconds MongoDB waits to flush the  LDAP (Lightweight Directory Access Protocol)  user\ncache. Defaults to 30 seconds. Type : string Maps the username provided to  mongod  or\n mongos  for authentication to an LDAP Distinguished Name\n(DN). For more details, see  security.ldap.userToDNMapping  in the MongoDB Manual. Type : array Specifies the authentication mechanism that your  multi-Kubernetes-cluster deployment  uses. Valid values are  SCRAM ,  SCRAM-SHA-1 ,  MONGODB-CR ,  X509 , and  LDAP . We recommend  SCRAM-SHA-256 (SCRAM)  over  SCRAM-SHA-1 . If you specify  SCRAM-SHA-1 , you must also specify  MONGODB-CR . If you provide more than one value for\n spec.security.authentication.modes , you must also specify a\nvalue for  spec.security.authentication.agents.mode . To enable  X.509 internal cluster authentication  for the  Cloud Manager or Ops Manager  project, set this\nvalue to  [\"X509\"]  and specify the following settings: spec.security.authentication.internalCluster   : \"X509\" provide a value for the  spec.security.certsSecretPrefix  setting. Type : boolean Default : false Specifies whether the MongoDB host requires clients to connect using a\n TLS (Transport Layer Security)  certificate. Defaults to  true  if you enable  TLS (Transport Layer Security) \nauthentication. To enable  TLS (Transport Layer Security)  authentication, provide a value for the\n spec.security.certsSecretPrefix  setting. Type : string Text to prefix to the  Kubernetes   secrets  that you\ncreated that contain your replica set's  TLS (Transport Layer Security) \nkeys and certificates. To learn more about naming the secrets that contain your  TLS (Transport Layer Security) \ncertificates, see the topic in  Multi-Kubernetes-Cluster Quick Start  that applies to your\ndeployment. You must prefix your secrets with  <prefix>-<metadata.name> . If you call your deployment  my-deployment  and you set the\nprefix to  mdb , you must name the  TLS (Transport Layer Security)  secret for the\nclient  TLS (Transport Layer Security)  communications  mdb-my-deployment-cert . Also,\nyou must name the  TLS (Transport Layer Security)  secret for internal cluster authentication\n(if enabled)  mdb-my-deployment-clusterfile . Type : array Array that defines  User-defined roles  that give you\nfine-grained access control over your  multi-Kubernetes-cluster deployment . To enable user-defined roles, the\n spec.security.authentication.enabled  must be  true . In this example, a user-defined role named  customRole  allows\nusers assigned this role to: Insert documents into the  cats  collection in the  pets \ndatabase, and Find and insert documents into the  dogs  collection in the\n pets  database. Type : array Array that defines the IP address from and to which users\nassigned this  spec.security.roles.role  can\nconnect. Type : array Array of IP addresses or CIDR blocks from which users assigned this\n spec.security.roles.role  can connect. MongoDB servers reject connection requests from users with this role\nif the requests come from a client that is not present in this array. Type : array Array of IP addresses or CIDR blocks to which users assigned this\n spec.security.roles.role  can connect. MongoDB servers reject connection requests from users with this role\nif the client requests to connect to a server that is not present in\nthis array. Type : string The database in which to store the user-defined role. admin Type : array Array that describes the privileges that users granted this role\npossess. Type : array List of actions that users granted this role can perform. For a list\nof accepted values, see  Privilege Actions  in the\nMongoDB Manual for the MongoDB versions you deploy with the\n Kubernetes Operator . Type : collection Resources for which the privilege\n spec.security.roles.privileges.actions \napply. This collection must include either: The  spec.security.roles.privileges.resource.db  and  spec.security.roles.privileges.resource.collection  settings, or The  spec.security.roles.privileges.resource.cluster  setting with a value of  true . Type : boolean Default : false Flag that indicates that the privilege\n spec.security.roles.privileges.actions \napply to all databases and collections in the MongoDB deployment. If set to true, do not provide values for\n spec.security.roles.privileges.resource.db \nand\n spec.security.roles.privileges.resource.collection . Type : string Collection in the\n spec.security.roles.privileges.resource.db \nfor which the privilege\n spec.security.roles.privileges.actions \napply. If you provide a value for this setting, you must also provide a value\nfor\n spec.security.roles.privileges.resource.db . Type : string Database for which the privilege\n spec.security.roles.privileges.actions \napply. If you provide a value for this setting, you must also provide a value\nfor\n spec.security.roles.privileges.resource.collection . Type : string Name of the user-defined role. Type : collection List of every domain that should be added to  TLS (Transport Layer Security)  certificates to\neach Pod in this deployment. When you set this parameter, every  CSR (Certificate Signing Request) \nthat the  Kubernetes Operator  transforms into a  TLS (Transport Layer Security)  certificate includes\na  SAN (Subject Alternative Name)  in the form  <pod name>.<additional cert domain> . Replica set resources don't need this parameter. Use\n spec.connectivity.replicaSetHorizons  instead. If you add this parameter to a  TLS (Transport Layer Security) -enabled resource,  Kubernetes \ndisplays an error when the resource reaches the  Pending  state.\nThis error displays:  Please manually remove the |csr| in order\nto proceed.  To remedy this issue: Remove any existing  CSR (Certificate Signing Request) s so that  Kubernetes  can generate new\n CSR (Certificate Signing Request) s. To learn how to delete a resource, see the\n deleting resources  in the\n Kubernetes  documentation. Approve the  CSR (Certificate Signing Request) s after  Kubernetes  generates them. Type : string Provide the name of the  ConfigMap  that stores the  CA (Certificate Authority) . If you use a custom  CA (Certificate Authority)  to sign your  TLS (Transport Layer Security)  certificates for\nthe  MongoDBMultiCluster  resource , you must specify this parameter. The  Kubernetes Operator  requires that you name the certificate for the\n MongoDBMultiCluster  resource   ca-pem  in the ConfigMap. Type : boolean Encrypts communications using TLS certificates between: spec.security.tls.enabled  is deprecated starting in  Kubernetes Operator  version 1.19\nand will be removed in a future  Kubernetes Operator  release. To enable  TLS (Transport Layer Security) , provide a value for\nthe  spec.security.certsSecretPrefix  setting. MongoDB hosts in a replica set or sharded cluster configuration Clients ( mongo  shell, drivers,  MongoDB Compass , and others) and the MongoDB deployment Type : collection Global specification for the  StatefulSet  that the  MongoDB Enterprise Kubernetes Operator  creates\nfor your  multi-Kubernetes-cluster deployment . To review which fields you can add to\n spec.statefulSet.spec , see\n StatefulSetSpec v1 apps \nin the  Kubernetes  documentation.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "# This example provides statefulSet overrides per cluster.\n\napiVersion: mongodb.com/v1\nkind: MongoDBMultiCluster\nmetadata:\n  name: multi-replica-set\nspec:\n  version: 4.4.0-ent\n  type: ReplicaSet\n  duplicateServiceObjects: false\n  credentials: my-credentials\n  opsManager:\n    configMapRef:\n      name: my-project\n  clusterSpecList:\n    - clusterName: cluster1.example.com\n      members: 2\n      statefulSet:\n        spec:\n          template:\n            spec:\n              containers:\n                # Example of custom sidecar containers. Remove it before using the file in production.\n                - name: sidecar1\n                  image: busybox\n                  command: [ \"sleep\" ]\n                  args: [ \"infinity\" ]\n          # Use the following settings to override the default storage size of the \"data\" Persistent Volume.\n          volumeClaimTemplates:\n            - metadata:\n                name: data\n              spec:\n                resources:\n                  requests:\n                    storage: 1Gi\n    - clusterName: cluster2.example.com\n      members: 1\n      statefulSet:\n        spec:\n          template:\n            spec:\n              containers:\n                # Example of custom sidecar containers. Remove it before using the file in production.\n                - name: sidecar2\n                  image: busybox\n                  command: [ \"sleep\" ]\n                  args: [ \"infinity\" ]\n          volumeClaimTemplates:\n            - metadata:\n                name: data\n              spec:\n                resources:\n                  requests:\n                    storage: 1Gi\n    - clusterName: cluster3.example.com\n      members: 1\n      statefulSet:\n        spec:\n          template:\n            spec:\n              containers:\n                # Example of custom sidecar containers. Remove it before using the file in production.\n                - name: sidecar3\n                  image: busybox\n                  command: [ \"sleep\" ]\n                  args: [ \"infinity\" ]\n          volumeClaimTemplates:\n            - metadata:\n                name: data\n              spec:\n                resources:\n                  requests:\n                    storage: 1Gi\n\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "externalAccess:\n  externalService:\n    annotations:\n      # cloud-specific annotations for the service\n    spec:\n      type: NodePort # default is LoadBalancer\n      # you can specify other spec overrides if necessary"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBMultiCluster\nmetadata:\n  name: multi-replica-set\nspec:\n  version: 4.4.0-ent\n  type: ReplicaSet\n  duplicateServiceObjects: false\n  credentials: my-credentials\n  opsManager:\n    configMapRef:\n      name: my-project\n  clusterSpecList:\n    - clusterName: cluster1.example.com\n      members: 2\n      memberConfig:\n          - votes: 1\n            priority: \"0.5\"\n            tags:\n                tag1: \"value1\"\n                environment: \"prod\"\n          - votes: 1\n            priority: \"1.5\"\n            tags:\n                  tag2: \"value2\"\n                  environment: \"prod\"\n    - clusterName: cluster2.example.com\n      members: 1\n      memberConfig:\n          - votes: 1\n            priority: \"0.5\"\n            tags:\n                tag1: \"value1\"\n                environment: \"prod\"\n    - clusterName: cluster3.example.com\n      members: 1\n      memberConfig:\n          - votes: 1\n            priority: \"0.5\"\n            tags:\n                tag1: \"value1\"\n                environment: \"prod\""
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:30907\"\n      - \"example-website\": \"web2.example.com:32350\"\n      - \"example-website\": \"web3.example.com:31185\"\n..."
                },
                {
                    "lang": "yaml",
                    "value": "externalAccess:\n  externalService:\n    annotations:\n      # cloud-specific annotations for the service\n    spec:\n      type: NodePort # default is LoadBalancer\n      # you can specify other spec overrides if necessary"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic ldap-agent-user \\\n--from-literal=\"password=<password>\" -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic agent-certs \\\n--from-file=mms-automation-agent-pem=<automation-cert.pem> \\\n--from-file=mms-backup-agent-pem=<backup-cert.pem> \\\n--from-file=mms-monitoring-agent-pem=<monitoring-cert.pem> \\\n--namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n   security:\n      authentication:\n      ldap:\n         servers:\n            - \"<hostname1>:<port1>\"\n            - \"<hostname2>:<port2>\""
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    authentication:\n      enabled: true\n      modes:\n        - \"SCRAM\"\n    roles:\n      - role: \"customRole\"\n        db: admin    \n        privileges:\n        - actions:\n          - insert\n          resource:\n            collection: cats\n            db: pets\n        - actions:\n          - insert\n          - find\n          resource:\n            collection: dogs\n            db: pets\n...\n"
                }
            ],
            "preview": "The MongoDBMultiCluster resource defines your multi-Kubernetes-cluster deployment and gives the MongoDB Enterprise Kubernetes Operator the information it needs to create or update your clusters, Ops Manager deployment, StatefulSets, services, and other Kubernetes resources.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/plan-k8s-op-container-images",
            "title": "Container Images",
            "headings": [],
            "paragraphs": "When you install the  Kubernetes Operator , it pulls the images from the Quay.io\ncontainer registry. The  Kubernetes Operator  images are based on the\n Red Hat UBI 8 \noperating system. MongoDB rebuilds  Kubernetes Operator  images daily for the\nlatest operating system and supporting library updates. Official images provide the following advantages: To view all available versions for each image, see the following links. They are rebuilt daily for the latest upstream vulnerability fixes. MongoDB tests, maintains, and supports them. Image Name Description mongodb-agent-ubi MongoDB Agent image. mongodb-enterprise-server The Enterprise MongoDB image used for the Application Database. mongodb-enterprise-init-appdb-ubi initContainer  image that contains the Application Database\nstart-up scripts and the readiness probe. mongodb-enterprise-database-ubi MongoDB Database environment image. mongodb-enterprise-init-database-ubi initContainer  image that contains the MongoDB Agent start-up\nscripts and the readiness probe. mongodb-enterprise-ops-manager-ubi Ops Manager  image. mongodb-enterprise-init-ops-manager-ubi initContainer  image that contains the  Ops Manager  start-up\nscripts and the readiness probe.",
            "code": [],
            "preview": "When you install the Kubernetes Operator, it pulls the images from the Quay.io\ncontainer registry. The Kubernetes Operator images are based on the\nRed Hat UBI 8\noperating system. MongoDB rebuilds Kubernetes Operator images daily for the\nlatest operating system and supporting library updates.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/multi-cluster-connect-from-outside-k8s",
            "title": "Connect to Multi-Cluster Resource from Outside Kubernetes",
            "headings": [
                "Prerequisite",
                "Compatible MongoDB Versions",
                "Considerations",
                "Configure Readiness Probe Overrides",
                "Procedure",
                "Deploy a Multi-Kubernetes Cluster Replica Set.",
                "Secure the Multi-Kubernetes Cluster with TLS.",
                "Create an external service for the MongoDB Pods.",
                "Optional: Configure an external service for cluster members.",
                "Add Subject Alternate Names to your TLS (Transport Layer Security) certificate.",
                "Verify the external services.",
                "Update your replica set resource YAML (Yet Another Markup Language) file.",
                "Apply the updated replica set file.",
                "Test the connection to the replica set."
            ],
            "paragraphs": "The following procedure describes how to connect to a  MongoDBMultiCluster  resource \ndeployed in  Kubernetes  from outside of the  Kubernetes  cluster. Databases that run MongoDB 4.2.3 or later allow you to access them outside\nof the  Kubernetes  cluster. If you create custom services that require external access to MongoDB custom\nresources deployed by the  Kubernetes Operator  and use readiness probes\nin  Kubernetes , set the  publishNotReadyAddresses  setting in  Kubernetes  to  true . The  publishNotReadyAddresses  setting indicates that an agent that\ninteracts with endpoints for this service should disregard the service's\n ready \nstate. Setting  publishNotReadyAddresses  to  true  overrides the\nbehavior of the readiness probe configured for the Pod hosting your service. By default, the  publishNotReadyAddresses  setting is set to  false .\nIn this case, when the Pods that host the MongoDB custom resources in the\n Kubernetes Operator  lose connectivity to  Cloud Manager  or  Ops Manager , the\nreadiness probes configured for these Pods fail.\nHowever, when you set the   publishNotReadyAddresses  setting to  true : Kubernetes  does not shut down the service whose readiness probe fails. Kubernetes  considers all endpoints as  ready \neven if the probes for the Pods hosting the services for these endpoints\nindicate that they aren't ready. MongoDB custom resources are still available for read and write operations. Kubernetes API Reference  and search for  publishNotReadyAddresses DNS for Services in Pods Configure Readiness Probes To connect to your  Kubernetes Operator -deployed replica set with a  MongoDBMultiCluster  resource \nfrom outside of the  Kubernetes  cluster: Provide values for: The  TLS (Transport Layer Security)  secret in  spec.security.certsSecretPrefix . The custom  CA (Certificate Authority)  certificate in  spec.security.tls.ca . To connect to your multi-Kubernetes-cluster deployment from an external resource, configure the\n spec.externalAccess  setting: This setting instructs the  Kubernetes Operator  to create an external  LoadBalancer  service for the MongoDB Pods in your\nmulti-Kubernetes-cluster deployment. The external service provides an entry point for external connections.\nAdding this setting with no values creates an external service with the following default\nvalues: Optionally, if you need to add values to the service or override the default values,\nspecify: For example, the following settings override the default values for the external service\nto configure your multi-Kubernetes-cluster deployment to create  NodePort services  that expose the MongoDB Pods: Field Value Description Name <pod-name>-svc-external Name of the external service. You can't change this value. Type LoadBalancer Creates an external  LoadBalancer  service. Port <Port Number> A port for  mongod . publishNotReadyAddress true Specifies that  DNS records \nare created even if the Pod isn't ready.\nDo not set to  false  for any database Pod. Annotations specific to your cloud provider, in  spec.externalAccess.externalService.annotations Overrides for the service specification, in  spec.externalAccess.externalService.spec . To learn more, see\n Annotations \nand  ServiceSpec \nin the  Kubernetes  documentation. If you need to configure settings for a specific cluster member,\nsuch as when you're hosting members on different cloud providers,\nyou can override the global  spec.externalAccess \nsettings for a specific member by using the  spec.clusterSpecList.externalAccess.externalService  setting. To add values to the service or override the default values for a\ncluster member, specify: For example, the following file configures your  multi-Kubernetes-cluster deployment  to\ncreate load balancer services that expose the  multi-Kubernetes-cluster deployment  for\ncluster members deployed in  GKE (Google Kubernetes Engine)  and  AWS (Amazon Web Services)   EKS . Annotations specific to the cloud provider for the cluster member, in\n spec.clusterSpecList.externalAccess.externalService.annotations . Overrides specific to the cluster member, in  spec.clusterSpecList.externalAccess.externalService.spec . The following example doesn't configure overrides, so the external services\nuse the default values from the  spec.externalAccess \nsetting. Add each external  DNS (Domain Name System)  name to the certificate  SAN (Subject Alternative Name) . In each cluster, run the following command to verify that the\n Kubernetes Operator  created the external service for your deployment. The command returns a list of services similar to the following output.\nFor each database Pod in the cluster, the  Kubernetes Operator  creates an external service\nnamed <pod-name>-<cluster-idx>-<pod-idx>-svc-external. This service is configured according to the values\nand overrides you provide in the  external service specification . Depending on your cluster configuration or cloud provider,\nthe IP address of the LoadBalancer service is an externally\naccessible IP address or  FQDN (fully qualified domain name) . You can use the IP address or  FQDN (fully qualified domain name) \nto route traffic from your external domain. Set the hostnames and ports in  spec.connectivity.replicaSetHorizons \nto the external service values that you created in the previous step. Confirm that you specified the correct external hostnames. External\nhostnames should match the  DNS (Domain Name System)  names of  Kubernetes  worker nodes.\nThese can be  any  nodes in the  Kubernetes  cluster. If the Pod runs on another\nnode,  Kubernetes  nodes use internal routing. In each cluster, run this command to apply the updated replica set file: In the development environment, for each host in a replica set, run\nthe following command: In production, for each host in a replica set, specify the  TLS (Transport Layer Security) \ncertificate and the  CA (Certificate Authority)  to securely connect to client tools or\napplications: If the connection succeeds, you should see: Don't use the  --sslAllowInvalidCertificates  flag in production.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "externalAccess: {}"
                },
                {
                    "lang": "yaml",
                    "value": "externalAccess:\n  externalService:\n    annotations:\n      # cloud-specific annotations for the service\n    spec:\n      type: NodePort # default is LoadBalancer\n      port: 27017\n      # you can specify other spec overrides if necessary"
                },
                {
                    "lang": "yaml",
                    "value": "clusterSpecList:\n  - clusterName: gke-cluster-0.mongokubernetes.com\n    members: 2\n    externalAccess:\n      externalService:\n        annotations:\n          \"cloud.google.com/l4-rbs\": \"enabled\"\n  - clusterName: eks-cluster-1.mongokubernetes.com\n    members: 2\n    externalAccess:\n      externalService:\n        annotations:\n          \"service.beta.kubernetes.io/aws-load-balancer-type\": \"external\",\n          \"service.beta.kubernetes.io/aws-load-balancer-nlb-target-type\": \"instance\",\n          \"service.beta.kubernetes.io/aws-load-balancer-scheme\": \"internet-facing\""
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl get services"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                  TYPE         CLUSTER-IP   EXTERNAL-IP       PORT(S)           AGE\n<my-replica-set>-0-0-svc-external   LoadBalancer   10.102.27.116    <lb-ip-or-fqdn>   27017:27017/TCP    8m30s"
                },
                {
                    "lang": "sh",
                    "value": "apiVersion: mongodb.com/v1\n kind: MongoDBMultiCluster\n metadata:\n  name: multi-cluster-replica-set\n  namespace: mongodb\n spec:\n  clusterSpecList:\n   - clusterName: e2e.cluster1.example.com\n     members: 1\n   - clusterName: e2e.cluster2.example.com\n     members: 1\n   - clusterName: e2e.cluster3.example.com\n     members: 1\n  connectivity:\n   replicaSetHorizons:\n   - sample-horizon: web1.example.com:30907\n   - sample-horizon: web2.example.com:30907\n   - sample-horizon: web3.example.com:30907\n  credentials: my-credentials\n  duplicateServiceObjects: false\n  opsManager:\n   configMapRef:\n    name: my-project\n  persistent: true\n  security:\n   certsSecretPrefix: clustercert\n   tls:\n     ca: ca-issuer\n  type: ReplicaSet\n  version: 4.4.0-ent\""
                },
                {
                    "lang": "sh",
                    "value": "$ Kubectl apply -f <file_name.yaml>"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host <my-replica-set>/web1.example.com \\\n      --port 30907\n      --ssl \\\n      --sslAllowInvalidCertificates"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host <my-replica-set>/web1.example.com \\\n  --port 30907 \\\n  --tls \\\n  --tlsCertificateKeyFile server.pem \\\n  --tlsCAFile ca-pem"
                },
                {
                    "lang": "javascript",
                    "value": "Enterprise <my-replica-set> [primary]"
                }
            ],
            "preview": "The following procedure describes how to connect to a MongoDBMultiCluster resource\ndeployed in Kubernetes from outside of the Kubernetes cluster.",
            "tags": "split-horizon DNS",
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/cert-manager-integration",
            "title": "Set Up a cert-manager Integration",
            "headings": [
                "Procedure",
                "Create a CA (Certificate Authority) secret.",
                "Add additional certificates to custom CA (Certificate Authority) certificates.",
                "Configure a cert-manager CA (Certificate Authority) issuer",
                "Create a CA (Certificate Authority) ConfigMap",
                "Create certificates for your MongoDB resources",
                "Create certificates for Ops Manager and AppDB with TLS",
                "Renewing Certificates"
            ],
            "paragraphs": "cert-manager  simplifies and automates\nthe management of security certificates for Kubernetes. The following\nprocedure describes how to configure  cert-manager  to generate\ncertificates for MongoDB Kubernetes Operator resources. Create a secret to store your  CA (Certificate Authority)  data: The following steps assume that you have already created a custom\n CA (Certificate Authority)  along with the corresponding  tls.key  private key\nand  tls.crt  signed certificate. If your  Ops Manager   TLS (Transport Layer Security)  certificate is signed by a custom  CA (Certificate Authority) ,\nthe  CA (Certificate Authority)  certificate must also contain\nadditional certificates that allow  Ops Manager  Backup Daemon to download\nMongoDB binaries from the internet. To create the  TLS (Transport Layer Security) \ncertificate(s), create a  ConfigMap  to hold the  CA (Certificate Authority) \ncertificate: The  Kubernetes Operator  requires that your  Ops Manager  certificate is named\n mms-ca.crt  in the ConfigMap. Obtain the entire  TLS (Transport Layer Security)  certificate chain for  Ops Manager  from\n downloads.mongodb.com . The following  openssl  command\noutputs the certificate in the chain to your current working\ndirectory, in  .crt  format: Concatenate your  CA (Certificate Authority) 's certificate file for  Ops Manager \nwith the entire  TLS (Transport Layer Security)  certificate chain from\n downloads.mongodb.com  that\nyou obtained in the previous step: Create the  ConfigMap  for  Ops Manager : Create a  CA (Certificate Authority)  issuer that references your  CA (Certificate Authority)  secret: Verify that the issuer is ready: The  READY  field in the output should have a value of  True . Create a ConfigMap containing your  CA (Certificate Authority) . It must have two\nfields,  ca-pem  and  mms-ca.crt , both pointing to your\n CA (Certificate Authority)  certificate. Replace  <CA-certificate>  with the path to your\n CA (Certificate Authority)  certificate. To secure a MongoDB resource with your generated certification, you\nmust create certificates for both the resource itself and the MongoDB\nagent. Create the MongoDB resource certificate. The following example\nassumes a replica set named  my-replica-set  with three members: For sharded clusters, you must create one certificate for each\n StatefulSet . To learn more about sharded cluster\nconfiguration, see  Deploy a Sharded Cluster . The  spec.issuerRef.name  parameter references the previously\ncreated  CA (Certificate Authority)  ConfigMap. Create the MongoDB agent certificate: The  spec.issuerRef.name  parameter references the previously\ncreated  CA (Certificate Authority)  ConfigMap. Create the MongoDB resource: If you leave the  spec.security.tls.ca \nparameter unspecified, it defaults to  {replica-set}-ca . To secure an  Ops Manager  resource, you must first create certificates\nfor  Ops Manager  and AppDB, then create the  Ops Manager  resource. Create the Ops Manager certificate: The  spec.issuerRef.name  parameter references the previously\ncreated  CA (Certificate Authority)  ConfigMap. Create the AppDB certificate: The  spec.issuerRef.name  parameter references the previously\ncreated  CA (Certificate Authority)  ConfigMap. Create the  Ops Manager  resource: cert-manager will renew certificates under the following circumstances: The certificate expires according to its  spec.duration  and\n spec.renewBefore  fields. You delete the secret holding a certificate. In this case,\ncert-manager recreates the secret according to the configuration\nin your certificate custom resource. You alter the configuration of the certificate custom resource.\nIn this case, cert-manager recreates the secret that contains the\ncertificate when it detects the changes to its configuration.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: ca-key-pair\n  namespace: <namespace>\ndata:\n  tls.crt: <your-CA-certificate>\n  tls.key: <your-CA-private-key>"
                },
                {
                    "lang": "sh",
                    "value": "openssl s_client -showcerts -verify 2 \\\n-connect downloads.mongodb.com:443 -servername downloads.mongodb.com < /dev/null \\\n| awk '/BEGIN/,/END/{ if(/BEGIN/){a++}; out=\"cert\"a\".crt\"; print >out}'"
                },
                {
                    "lang": "sh",
                    "value": "cat cert1.crt cert2.crt cert3.crt cert4.crt  >> mms-ca.crt"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap om-http-cert-ca --from-file=\"mms-ca.crt\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: ca-issuer\n  namespace: <namespace>\nspec:\n  ca:\n    secretName: ca-key-pair"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get issuer ca-issuer"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create cm ca-issuer --from-file=ca-pem=<CA-certificate> \\\n--from-file=mms-ca.crt=<CA-certificate>"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: my-replica-set-certificate\n  namespace: mongodb\nspec:\n  dnsNames:\n  - my-replica-set-0\n  - my-replica-set-0.my-replica-set-svc.mongodb.svc.cluster.local\n  - my-replica-set-1\n  - my-replica-set-1.my-replica-set-svc.mongodb.svc.cluster.local\n  - my-replica-set-2\n  - my-replica-set-2.my-replica-set-svc.mongodb.svc.cluster.local\n  duration: 240h0m0s\n  issuerRef:\n    name: ca-issuer\n  renewBefore: 120h0m0s\n  secretName: mdb-my-replica-set-cert\n  usages:\n  - server auth\n  - client auth"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: agent-certs\n  namespace: mongodb\nspec:\n  commonName: automation\n  dnsNames:\n  - automation\n  duration: 240h0m0s\n  issuerRef:\n    name: ca-issuer\n  renewBefore: 120h0m0s\n  secretName: mdb-my-replica-set-agent-certs\n  usages:\n  - digital signature\n  - key encipherment\n  - client auth\n  subject:\n    countries:\n    - US\n    localities:\n    - NY\n    organizationalUnits:\n    - a-1635241837-m5yb81lfnrz\n    organizations:\n    - cluster.local-agent\n    provinces:\n    - NY"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\n  namespace: mongodb\nspec:\n  type: ReplicaSet\n\n  members: 3\n  version: 4.0.4-ent\n\n  opsManager:\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n\n  security:\n    certsSecretPrefix: mdb\n    authentication:\n      enabled: true\n      modes:\n      - X509\n    tls:\n      ca: ca-issuer\n      enabled: true"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: cert-for-ops-manager\n  namespace: mongodb\nspec:\n  dnsNames:\n  - om-with-https-svc.mongodb.svc.cluster.local\n  duration: 240h0m0s\n  issuerRef:\n    name: ca-issuer\n  renewBefore: 120h0m0s\n  secretName: mdb-om-with-https-cert\n  usages:\n  - server auth\n  - client auth"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: appdb-om-with-https-db-cert\n  namespace: mongodb\nspec:\n  dnsNames:\n  - om-with-https-db-0\n  - om-with-https-db-0.om-with-https-db-svc.mongodb.svc.cluster.local\n  - om-with-https-db-1\n  - om-with-https-db-1.om-with-https-db-svc.mongodb.svc.cluster.local\n  - om-with-https-db-2\n  - om-with-https-db-2.om-with-https-db-svc.mongodb.svc.cluster.local\n  duration: 240h0m0s\n  issuerRef:\n    name: ca-issuer\n  renewBefore: 120h0m0s\n  secretName: appdb-om-with-https-db-cert\n  usages:\n  - server auth\n  - client auth"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: om-with-https\n  namespace: mongodb\nspec:\n  adminCredentials: ops-manager-admin-secret\n  applicationDatabase:\n    members: 3\n    security:\n      certsSecretPrefix: appdb\n      tls:\n        ca: ca-issuer\n    version: 6.0.0-ubi8\n  replicas: 1\n  security:\n    certsSecretPrefix: mdb\n    tls:\n      ca: ca-issuer"
                }
            ],
            "preview": "cert-manager simplifies and automates\nthe management of security certificates for Kubernetes. The following\nprocedure describes how to configure cert-manager to generate\ncertificates for MongoDB Kubernetes Operator resources.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/create-operator-credentials",
            "title": "Create Credentials for the Kubernetes Operator",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Create a new Kubernetes secret",
                "Verify your new Kubernetes secret"
            ],
            "paragraphs": "For the  Kubernetes Operator  to create or update  objects  in your  Cloud Manager or Ops Manager \nProject, you need to store your  Programmatic API Key  in your  secret storage tool . Multiple secrets can exist in the same namespace. Each user should\nhave their own secret. You can follow the  Procedure  below to\nmanually store the  Programmatic API Key  as a  Kubernetes   secret . Alternatively, you can: Use the  MongoDB Cloud Manager   UI  or the\n Ops Manager   UI  to\nautomatically generate the Kubernetes secret YAML file, which you can\nthen apply to your Kubernetes environment. Store the  Programmatic API Key  as a\n Vault  secret using the procedure to  Create a Vault Secret .\nTo use  Vault , you must also  configure the secret storage . To create credentials for the  Kubernetes Operator , you must: Have or create an  Ops Manager \n Organization . Have or generate a\n Programmatic API Key . Grant this new  Programmatic API Key : The  Organization Owner  or  Global Owner \nrole, if you want the  Kubernetes Operator  to create projects and manage\nthem. The  Project Owner  role, if you want the  Kubernetes Operator \nto only manage projects. You must grant the  Programmatic API Key  the\n Organization Owner  or  Global Owner  role.\nIf you want to grant the  Programmatic API Key  only the  Project Owner  role, you must first create the project and then the\n Programmatic API Key  for the project with the  Project Owner \nrole. The  Kubernetes Operator  can't create projects if the\n Programmatic API Key  has only the  Project Owner  role. Add the  IP (Internet Protocol)  or  CIDR (Classless Inter-Domain Routing)  block of any hosts that serve the\n Kubernetes Operator  to the\n API Access List . To create your  Kubernetes  secret: Ensure you have the Public and Private Keys for your desired\n Ops Manager   Programmatic API Key . Invoke the following  Kubernetes  command to create your secret: The  -n  flag limits the  namespace  to which this secret applies.\nAll MongoDB  Kubernetes  resources must exist in the same namespace as the\n secrets  and  ConfigMaps . The  Kubernetes Operator  doesn't use\neither the secrets or ConfigMaps. The deprecated version of this command specifies a  user  and  publicApiKey \ninstead of a  publicKey  and  privateKey .  Kubernetes Operator  accepts\neither version for authentication. Invoke the following  Kubernetes  command to verify your secret: This command returns a secret description in the shell:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl -n <metadata.namespace> \\\ncreate secret generic <mycredentials> \\\n--from-literal=\"publicKey=<publicKey>\" \\\n--from-literal=\"privateKey=<privateKey>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe secrets/<mycredentials> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Name:         <mycredentials>\nNamespace:    <metadata.namespace>\nLabels:       <none>\nAnnotations:  <none>\n\nType:  Opaque\n\nData\n====\nprivateKey:  31 bytes\npublicKey:          22 bytes"
                }
            ],
            "preview": "For the Kubernetes Operator to create or update objects in your Cloud Manager or Ops Manager\nProject, you need to store your Programmatic API Key in your secret storage tool.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/mdb-resources-arch",
            "title": "MongoDB Database Architecture in Kubernetes",
            "headings": [
                "The MongoDB Custom Resource Definition",
                "Standalone",
                "Replica Set",
                "Sharded Cluster",
                "Reconciling the MongoDB Custom Resource",
                "Diagram of a Replica Set Reconciliation",
                "Diagram of a Sharded Cluster Reconciliation",
                "Reconciliation Workflow",
                "Reconciling the MongoDBUser Custom Resource"
            ],
            "paragraphs": "You can use the  Kubernetes Operator  and  Cloud Manager or Ops Manager  to deploy MongoDB database\nresources to a  Kubernetes  cluster. You can use an existing  Cloud Manager or Ops Manager , or deploy\n Ops Manager  in  Kubernetes  to manage your databases. The  Kubernetes Operator  uses  Cloud Manager or Ops Manager  to manage the following MongoDB database custom resources: Your  custom resource  specifications define these resources in the  Kubernetes Operator .\nThe  Kubernetes Operator  monitors these resources. When you update the\nresource's specification, the  Kubernetes Operator  pushes these changes to\n Cloud Manager or Ops Manager , which make changes to the MongoDB deployment's configuration. This section is for single  Kubernetes  cluster deployments only. For\n multi-Kubernetes-cluster deployments , see  Architecture, Capabilities, and Limitations . MongoDB MongoDBUser The  Kubernetes Operator  manages MongoDB database deployments\nwhich are defined by MongoDB custom resources. The MongoDB database  custom resource  specification defines the\nfollowing types of the MongoDB database custom resources: The following diagram illustrates the composition of each type of the MongoDB\nresource in the  Kubernetes Operator . Standalone ReplicaSet ShardedCluster Kubernetes Operator  doesn't support  arbiter nodes . For the  Standalone  type of the MongoDB database resource, the  Kubernetes Operator \ndeploys a replica set with a single member to the  Kubernetes  cluster as a\n StatefulSet . The  Kubernetes Operator  creates the StatefulSet, which contains the Pod\nspecification with the number of Pods to create. The  Kubernetes Operator \nrelies on the  Kubernetes  StatefulSet Controller to create a Pod for this\nstandalone MongoDB database instance. In  Kubernetes , a  Standalone  resource is equivalent to a  ReplicaSet \nresource with only one member. We recommend that you deploy\na  ReplicaSet  with one member instead of a  Standalone \nbecause a replica set allows you to add members to it in\nthe future. For the  ReplicaSet  type of the MongoDB resource, the  Kubernetes Operator \ndeploys a replica set to the  Kubernetes  cluster as a  StatefulSet , with\na number of members equal to the value of  spec.members . The  Kubernetes Operator  relies on the  Kubernetes  StatefulSet Controller to create\none Pod in the StatefulSet for each member of the replica set. Each Pod in the StatefulSet runs a MongoDB Agent instance. The  ShardedCluster  type of the MongoDB resource consists of one or more\nConfig Servers,  mongos  instances, and shard members. For the  ShardedCluster  resource, the  Kubernetes Operator  deploys: The  Kubernetes Operator  relies on the  Kubernetes  StatefulSet Controller to\ncreate one Pod in each of the StatefulSets created for the sharded cluster. One StatefulSet for all Config Servers One StatefulSet for all  mongos  instances One StatefulSet for each Shard Member When you apply a MongoDB custom resource specification,\nthe  Kubernetes Operator  deploys each resource as a  StatefulSet \nto the  Kubernetes  cluster. The  Kubernetes Operator : Watches the custom resource's specification and associated\n ConfigMap  or secrets stored in your\n secret storage tool . Validates the changes when the specification file, the ConfigMap,\nor the secret change. Makes the appropriate updates to the MongoDB database resources\nin the  Kubernetes  cluster. Pushes the changes to  Cloud Manager or Ops Manager , which make changes to the MongoDB\ndeployment's configuration. The following diagram describes how the  Kubernetes Operator  behaves\nif you make changes to a replica set's: MongoDB  custom resource specifications Associated  ConfigMap Associated secrets stored in your\n secret storage tool The following diagram describes how the  Kubernetes Operator  behaves if\nyou make changes to a sharded cluster's: MongoDB  custom resource specifications Associated  ConfigMap Associated secrets stored in your\n secret storage tool When you create or change a MongoDB resource specification, or when you make\nchanges to an associated  ConfigMap  or secret, the  Kubernetes Operator \nperforms the following actions to reconcile the changes: Reads the required organization and project configuration\nfrom the  ConfigMap \nthat you used to create or connect to a project in the  Kubernetes Operator . If you change your resource specification, the  Kubernetes Operator  identifies\nthat the change took place, and checks the specification for the ConfigMap\nspecified in  spec.opsManager.configMapRef.name . When you configure the  Kubernetes Operator  for MongoDB resources,\nyou  create a ConfigMap  to connect\nor create your  Cloud Manager or Ops Manager  project. The MongoDB Agent uses this ConfigMap\nto start or make changes to the deployment for the MongoDB resource. Reads the authentication configuration for  Cloud Manager or Ops Manager  from\nthe secret specified in either: This secret stores the\n Cloud Manager API keys \nor the  Ops Manager API Keys \nrequired for the  Kubernetes Operator  to authenticate to  Cloud Manager or Ops Manager . spec.credentials  in the resource specification Your  secret storage tool When you configure the  Kubernetes Operator  for MongoDB resources,\nyou either  create this secret in Kubernetes  or store this\nsecret in your  secret storage tool . The  Kubernetes Operator  connects to  Cloud Manager or Ops Manager  and performs the following actions: Reads the organization from the  orgId  field in the ConfigMap.\nYou must provide a value in the  orgId  field. Reads a project name specified in the  projectName  field in the\nConfigMap, or, if you didn't specify a value for this optional field,\ncreates this project in  Cloud Manager or Ops Manager  if it doesn't exist. Checks that the  <project-id>-group-secret  secret\ncreated by the  Kubernetes Operator  for the MongoDB Agent exists.\nThe  Kubernetes Operator  reads the secret from your  secret storage tool ,\nor creates it with\n Ops Manager API keys \nor  Cloud Manager API keys . Registers itself as a watcher of the ConfigMap and this secret.\nThis enables the  Kubernetes Operator  to react to changes that you make\nto the ConfigMap or the secret. The  Kubernetes Operator  verifies any  TLS and X.509 certificates . If  TLS  is enabled for a replica set, the\n Kubernetes Operator  looks for certificates provided in the\n <prefix>-<resource-name>-cert  secret or your\n secret storage tool . If  TLS  is enabled for a sharded cluster, the\n Kubernetes Operator  looks for certificates in these secrets: <prefix>-<resource-name>-x-cert  for each shard member. <prefix>-<resource-name>-config-cert  for all config servers. <prefix>-<resource-name>-mongos-cert  for all  mongos  instances. Your  secret storage tool . If  X.509  or\n internal authentication with X.509 and  TLS \nare enabled, the  Kubernetes Operator  checks that their certificates\ncontain the required configuration. The  Kubernetes Operator  locates and updates the necessary StatefulSets,\nor creates new StatefulSets if they don't exist. The number of\nStatefulSets depends on the type of the MongoDB resource. For  ReplicaSet  or  Standalone  resources,\nthe  Kubernetes Operator  creates a single StatefulSet. For a  ShardedCluster  resource, the  Kubernetes Operator  creates: At this point, each Pod runs at least one MongoDB Agent instance,\nbut does not yet contain  mongod  instances. One StatefulSet for all config servers. One StatefulSet for all  mongos  instances. One StatefulSet for each shard member. Each MongoDB Agent instance starts polling  Cloud Manager or Ops Manager  to receive the\nMongoDB automation configuration. When the MongoDB Agent receives the configuration for the first\ntime, it downloads the MongoDB binaries with the version\nspecified in  spec.version  from the Internet, or\nfrom  Ops Manager , if the MongoDB Agent is configured in the local mode. After the MongoDB Agent receives the automation configuration, it starts a\n mongod  instance on the corresponding Pod. For each Pod of each StatefulSet that the MongoDB custom resource creates,\nexcept for  mongos  StatefulSets, the  Kubernetes Operator  generates a  Persistent Volume Claim .\nYou can override this behavior by setting  spec.persistent  to\n false  in the resource specification. The  Kubernetes Operator  updates the automation configuration it received from the\nMongoDB Agent with changes from the specifications and sends it to  Cloud Manager or Ops Manager . Each MongoDB Agent for each Pod polls  Cloud Manager or Ops Manager  again and receives the\nupdated automation configuration. If you change any field in the specification, the  Kubernetes Operator \nperforms a  rolling update  of the StatefulSets to start new Pods\nmatching the new specification. The  Kubernetes Operator  waits for each MongoDB Agent to report that it reached\nthe ready state. If you change the  security configuration \nof a database resource, or  scale down \nan existing StatefulSet, the  Kubernetes Operator  runs step 6 before it\nruns  step 5. The  Kubernetes Operator  updates the  Kubernetes  services, or for a new MongoDB\nresource, creates the services required for each new StatefulSet. For the  ServiceType   ClusterIP , the  Kubernetes Operator  sets\n ClusterIP  to  None , and performs these actions: Creates this service if it doesn't exist. For  ReplicaSet  or  Standalone  resources, the  Kubernetes Operator \nnames the service with the custom resource's name  with  -svc \nappended to it. For a  ShardedCluster  resource, the  Kubernetes Operator  uses these\nnaming conventions: For  mongos  instances, the  Kubernetes Operator  uses the name specified in\n spec.service , or the resource's name  with  -svc \nappended to it. For the config servers, the  Kubernetes Operator  uses the resource's name\nwith  -cs  appended to it. For each shard, the  Kubernetes Operator  uses the resource's name\nwith  -sh  appended to it. For the port, the  Kubernetes Operator  uses the default port 27017, or\nthe  .net.port \nspecified in  spec.additionalMongodConfig . If the user authentication method is set to  SCRAM ,\nthe  MongoDB User Resource Specification  depends\non the  secret storage tool  that stores the\nuser credentials.  If you are using a  Kubernetes   secret , you specify the\nsecret in the  spec.passwordSecretKeyRef  settings in the  MongoDBUser \nresource specification. The  Kubernetes Operator  watches the secret for changes. If you make changes\nto the secret's configuration, the  Kubernetes Operator  reconciles the\nchanges. It takes the following actions: The following diagram describes how the  Kubernetes Operator  behaves if you make\nchanges to the user secret or the  MongoDB User Resource Specification . Determines the MongoDB user's resource based on the value\nspecified in the  spec.MongoDBResourceRef.name  setting in the\n MongoDB User Resource Specification . Connects to  Cloud Manager or Ops Manager : Reads the organization from the  orgId  in the ConfigMap. Reads a project's name from  projectName  in the ConfigMap,\nor creates this project in  Cloud Manager or Ops Manager  if it doesn't exist. Checks that the  <project-id>-group-secret  created by the\n Kubernetes Operator  for the MongoDB Agent exists.\nThe  Kubernetes Operator  reads the secret from your  secret storage tool ,\nor creates it with\n Ops Manager API keys \nor  Cloud Manager API keys . Updates the user's credentials in  Cloud Manager or Ops Manager , or creates a new user if it doesn't exist. If the user authentication method is  SCRAM ,\nreads the password from the secret. Reads the user name. If the user name has changed, the  Kubernetes Operator \nremoves the old name and adds a new one. Ensures that the user exists in  Cloud Manager or Ops Manager .",
            "code": [],
            "preview": "You can use the Kubernetes Operator and Cloud Manager or Ops Manager to deploy MongoDB database\nresources to a Kubernetes cluster. You can use an existing Cloud Manager or Ops Manager, or deploy\nOps Manager in Kubernetes to manage your databases.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/install-k8s-operator",
            "title": "Install the MongoDB Enterprise Kubernetes Operator",
            "headings": [
                "Prerequisites and Considerations",
                "Install with Kubernetes",
                "Clone the MongoDB Enterprise Kubernetes Operator repository.",
                "Navigate to the directory in which you cloned the MongoDB Enterprise Kubernetes Operator repository.",
                "Install the CustomResourceDefinitions for MongoDB deployments using the following kubectl command:",
                "Optional: Customize the Kubernetes Operator YAML (Yet Another Markup Language) before installing it.",
                "Install the Kubernetes Operator using the following kubectl command:",
                "Add the MongoDB Helm Charts for Kubernetes repository to Helm.",
                "Install the Kubernetes Operator:",
                "Use docker to request the files on a host connected to the Internet.",
                "Export the Kubernetes Operator images as .tar archive files:",
                "Copy these .tar files to the host running the Kubernetes docker daemon.",
                "Import the .tar files into docker.",
                "Add the MongoDB Helm Charts for Kubernetes repository to Helm.",
                "Install the Kubernetes Operator with modified pull policy values using the following helm command:",
                "Install with OpenShift",
                "Clone the MongoDB Enterprise Kubernetes Operator repository.",
                "Navigate to the directory in which you cloned the MongoDB Enterprise Kubernetes Operator repository.",
                "Install the CustomResourceDefinitions for MongoDB deployments.",
                "Optional: Customize the Kubernetes Operator YAML (Yet Another Markup Language) before installing it.",
                "Add your <openshift-pull-secret> to the ServiceAccount definitions in the Kubernetes Operator YAML (Yet Another Markup Language) before installing it.",
                "Install the Kubernetes Operator using the following oc command:",
                "Add the MongoDB Helm Charts for Kubernetes repository to Helm.",
                "Install the Kubernetes Operator using helm.",
                "Use docker to request the files on a host connected to the Internet.",
                "Export the Kubernetes Operator images as .tar archive files:",
                "Copy these .tar files to the host running the Kubernetes docker daemon.",
                "Import the .tar files into docker.",
                "Add the MongoDB Helm Charts for Kubernetes repository to Helm.",
                "Install the Kubernetes Operator with modified pull policy values.",
                "Verify the Installation",
                "Install a Specific Daily Build with Helm",
                "Next Steps"
            ],
            "paragraphs": "Before you install the  Kubernetes Operator , make sure you\n plan for your installation : Choose one of the following installation procedures\nto install the  Kubernetes Operator : Choose a  deployment topology . Read the  Considerations . Complete the  Prerequisites . This tutorial presumes some knowledge of  Kubernetes  and links to\nrelevant  Kubernetes  documentation. If you are unfamiliar\nwith  Kubernetes , please review that documentation first. Install with  Kubernetes Install with OpenShift The installation procedure varies based on how you want to configure your\nenvironment: The following examples assume that you created a  namespace \nusing the default  Kubernetes Operator  namespace of  mongodb .\nIf you specified a different label for your namespace when\nyou  created it , change all\nvalues for  metadata.namespace  to that namespace. To change the label for the namespace for the following\ndeployment to  production , edit all values for\n metadata.namespace  in  mongodb-enterprise.yaml : Invoke the following  git  command: For example, if you cloned the repository in your home directory, run: Invoke the following  kubectl  command: To learn about optional  Kubernetes Operator  installation settings,\nsee  Operator kubectl and oc Installation Settings . Invoke the following  kubectl  command: Use the  MongoDB Helm Charts for Kubernetes . You can install the  Kubernetes Operator  with  Helm 3 . Install  MongoDB Helm Charts for Kubernetes .\nThe following command installs the  CustomResourceDefinitions  and the  Kubernetes Operator \nin the current namespace named  default . By default, the\n Kubernetes Operator  uses the  default  namespace. The following command installs the  Kubernetes Operator   in the  mongodb \nnamespace with the optional  --create-namespace  option. To learn about optional  Kubernetes Operator  installation settings, see\n Operator Helm Installation Settings . Use the  MongoDB Helm Charts for Kubernetes . To install the  Kubernetes Operator  on a host not connected to the Internet: You can install the  Kubernetes Operator  with  Helm 3 . Replace  <om-version>  with the  Ops Manager  version that you're installing. Replace  <om-version>  with the  Ops Manager  version that you're installing. Replace  <om-version>  with the  Ops Manager  version that you're installing. Install  MongoDB Helm Charts for Kubernetes \nand set the value of  registry.pullPolicy  to  IfNotPresent .\nTo learn about optional  Kubernetes Operator  installation settings, see\n Operator Helm Installation Settings . Before you begin, ensure that the  MANAGED_SECURITY_CONTEXT  flag is set\nto  true  when you deploy the  Kubernetes Operator  to OpenShift. This value is\npre-defined in the  values-openshift.yaml  file. The installation procedure varies based on how you want to configure your\nenvironment: The following examples assume that you created a  namespace \nusing the default  Kubernetes Operator  namespace of  mongodb .\nIf you specified a different label for your namespace when\nyou  created it , change all\nvalues for  metadata.namespace  to that namespace. To change the label for the namespace for the following\ndeployment to  production , edit all values for\n metadata.namespace  in  mongodb-enterprise-openshift.yaml : Invoke the following  git  command: For example, if you cloned the repository in your home directory, run: Invoke the following  oc  command: To learn about optional  Kubernetes Operator  installation settings,\nsee  Operator kubectl and oc Installation Settings . To learn more, see the  registry.imagePullSecrets  setting in the\n Helm installation settings . Invoke the following  oc  command: Use the  MongoDB Helm Charts for Kubernetes . You can install the  Kubernetes Operator  with  Helm 3 . Install  MongoDB Helm Charts for Kubernetes : Use the  values-openshift.yaml \nsettings. To learn about optional  Kubernetes Operator  installation settings,\nsee  Operator Helm Installation Settings . Use the  MongoDB Helm Charts for Kubernetes . To install the  Kubernetes Operator  on a host not connected to the Internet: You can install the  Kubernetes Operator  with  Helm 3 . Replace  <om-version>  with the  Ops Manager  version that you're installing. Replace  <om-version>  with the  Ops Manager  version that you're installing. Replace  <om-version>  with the  Ops Manager  version that you're installing. Install  MongoDB Helm Charts for Kubernetes : Use the  values-openshift.yaml \nsettings,  registry.pullPolicy=IfNotPresent , and\n registry.imagePullSecrets=<openshift-pull-secret> . To learn\nabout optional  Kubernetes Operator  installation settings, see\n Operator Helm Installation Settings . To verify that the  Kubernetes Operator  installed correctly, run the\nfollowing command and verify the output: By default, deployments exist in the  mongodb  namespace. If the\nfollowing error message appears, ensure you use the correct\nnamespace: To troubleshoot your  Kubernetes Operator , see  Review Logs from the  Kubernetes Operator \nand other  troubleshooting topics . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . MongoDB rebuilds  Kubernetes Operator  images every day to integrate the\nlatest security and OS updates. By default,  helm  installs the latest build for the version of\nthe  Kubernetes Operator  you specify. To install an earlier build, specify the build ID as a parameter with\n --set build=<build-id> . Build IDs are always in the format\n -b<YYYYMMDD>T000000Z , where  <YYYYMMDD>  is the date that the\nbuild you want to use was created. This example shows how to install the  Kubernetes Operator  with the latest\nimage: This example shows how to install the  Kubernetes Operator  with the image\ncreated at midnight on February 5th, 2021: MongoDB recommends using the default (latest) build. After installing the  MongoDB Enterprise Kubernetes Operator , you can: Create an instance of Ops Manager Configure the Kubernetes Operator to deploy MongoDB resources",
            "code": [
                {
                    "lang": "yaml",
                    "value": "##---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-operator\n  namespace: production\n##---\n# Source: mongodb-enterprise-operator/templates/operator.yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-enterprise-operator\n  namespace: production\n\n---\n# Example truncated\n---\n..."
                },
                {
                    "lang": "sh",
                    "value": "git clone https://github.com/mongodb/mongodb-enterprise-kubernetes.git"
                },
                {
                    "lang": "sh",
                    "value": "cd ~/mongodb-enterprise-kubernetes"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f crds.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f mongodb-enterprise.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm repo add mongodb https://mongodb.github.io/helm-charts"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator \\\n  --namespace mongodb \\\n  --create-namespace"
                },
                {
                    "lang": "sh",
                    "value": "docker pull quay.io/mongodb/mongodb-enterprise-operator-ubi:1.24.0; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-database-ubi:1.24.0; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-ops-manager-ubi:<om-version>; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-init-ops-manager-ubi:1.24.0; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-init-appdb-ubi:1.24.0; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-init-database-ubi:1.24.0;"
                },
                {
                    "lang": "sh",
                    "value": "docker save quay.io/mongodb/mongodb-enterprise-operator-ubi:1.24.0 -o mongodb-enterprise-operator.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-database-ubi:1.24.0 -o mongodb-enterprise-database.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-ops-manager-ubi:<om-version> -o mongodb-enterprise-ops-manager.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-init-ops-manager-ubi:1.24.0 -o mongodb-enterprise-init-ops-manager.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-init-appdb-ubi:1.24.0 -o mongodb-enterprise-init-appdb.tar;\ndocker save quay.io/mongodb/mongodb-enterprise-init-database-ubi:1.24.0 -o mongodb-enterprise-init-database.tar;"
                },
                {
                    "lang": "sh",
                    "value": "docker load -i mongodb-enterprise-operator.tar; \\\ndocker load -i mongodb-enterprise-database.tar; \\\ndocker load -i mongodb-enterprise-ops-manager.tar; \\\ndocker load -i mongodb-enterprise-appdb.tar; \\\ndocker load -i mongodb-enterprise-init-ops-manager.tar; \\\ndocker load -i mongodb-enterprise-init-appdb.tar; \\\ndocker load -i mongodb-enterprise-init-database.tar;"
                },
                {
                    "lang": "sh",
                    "value": "helm repo add mongodb https://mongodb.github.io/helm-charts"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator \\\n  --set registry.pullPolicy='IfNotPresent'"
                },
                {
                    "lang": "yaml",
                    "value": "##---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n  namespace: production\n##---\n# Source: mongodb-enterprise-operator/templates/operator.yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: enterprise-operator\n  namespace: production\n\n---\n# Example truncated\n---\n..."
                },
                {
                    "lang": "sh",
                    "value": "git clone https://github.com/mongodb/mongodb-enterprise-kubernetes.git"
                },
                {
                    "lang": "sh",
                    "value": "cd ~/mongodb-enterprise-kubernetes"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-appdb\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-database-pods\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f mongodb-enterprise-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm repo add mongodb https://mongodb.github.io/helm-charts"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator \\\n  --values https://raw.githubusercontent.com/mongodb/helm-charts/main/charts/enterprise-operator/values-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "docker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-operator:1.24.0; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-database:1.24.0; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager:<om-version>; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb:10.2.15.5958-1_4.2.11-ent; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-init-ops-manager:1.24.0; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-init-appdb:1.24.0; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-init-database:1.24.0;"
                },
                {
                    "lang": "sh",
                    "value": "docker save quay.io/mongodb/mongodb-enterprise-operator-ubi:1.24.0 -o mongodb-enterprise-operator.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-database-ubi:1.24.0 -o mongodb-enterprise-database.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-ops-manager-ubi:<om-version> -o mongodb-enterprise-ops-manager.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-init-ops-manager-ubi:1.24.0 -o mongodb-enterprise-init-ops-manager.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-init-appdb-ubi:1.24.0 -o mongodb-enterprise-init-appdb.tar;\ndocker save quay.io/mongodb/mongodb-enterprise-init-database-ubi:1.24.0 -o mongodb-enterprise-init-database.tar;"
                },
                {
                    "lang": "sh",
                    "value": "docker load -i mongodb-enterprise-operator.tar; \\\ndocker load -i mongodb-enterprise-database.tar; \\\ndocker load -i mongodb-enterprise-ops-manager.tar; \\\ndocker load -i mongodb-enterprise-appdb.tar; \\\ndocker load -i mongodb-enterprise-init-ops-manager.tar; \\\ndocker load -i mongodb-enterprise-init-appdb.tar; \\\ndocker load -i mongodb-enterprise-init-database.tar;"
                },
                {
                    "lang": "sh",
                    "value": "helm repo add mongodb https://mongodb.github.io/helm-charts"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator \\\n  --set registry.pullPolicy='IfNotPresent' \\\n  --set registry.imagePullSecrets='<openshift-pull-secret>' \\\n  --values https://raw.githubusercontent.com/mongodb/helm-charts/main/charts/enterprise-operator/values-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "Error from server (NotFound): deployments.apps \"mongodb-enterprise-operator\" not found"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe deployments mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "oc describe deployments mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator \\\n  --set build=-b20210205T000000Z"
                }
            ],
            "preview": "Before you install the Kubernetes Operator, make sure you\nplan for your installation:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/deploy-replica-set",
            "title": "Deploy a Replica Set",
            "headings": [
                "Considerations",
                "Prerequisites",
                "Deploy a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Create the secret for your replica set's TLS (Transport Layer Security) certificate.",
                "Create the secret for your agent's TLS certificate.",
                "Create the ConfigMap to link your CA (Certificate Authority) with your deployment.",
                "Copy the sample replica set resource.",
                "Paste the copied example to create a new replica set resource.",
                "Change the settings to your preferred values.",
                "Configure the TLS settings for your replica set resource using a custom certificate authority (CA).",
                "Optional: Configure ACME (Automatic Certificate Management Environment) based TLS (Transport Layer Security) certificates for your replica set resource.",
                "Add any additional accepted settings for a replica set deployment.",
                "Save this replica set config file with a .yaml extension.",
                "Start your replica set deployment.",
                "Track the status of your replica set deployment.",
                "Renew TLS Certificates for a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Renew the secret for your TLS certificates.",
                "Configure kubectl to default to your namespace.",
                "Copy the sample replica set resource.",
                "Paste the copied example to create a new replica set resource.",
                "Change the settings to your preferred values.",
                "Add any additional accepted settings for a replica set deployment.",
                "Save this replica set config file with a .yaml extension.",
                "Start your replica set deployment.",
                "Track the status of your replica set deployment."
            ],
            "paragraphs": "A  replica set  is a group of MongoDB deployments that maintain\nthe same data set. Replica sets provide redundancy and high\navailability and are the basis for all production deployments. To learn more about replica sets, see the\n Replication Introduction  in\nthe MongoDB manual. Use this procedure to deploy a new replica set that  Ops Manager  manages.\nAfter deployment, use  Ops Manager  to manage the replica set, including such\noperations as adding, removing, and reconfiguring members. At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . You can use the  Kubernetes Operator  to deploy MongoDB resources with\n Cloud Manager  and with  Ops Manager  version 5.0.x or later. You can use the  Atlas Operator \nto deploy MongoDB resources to  Atlas . Kubernetes Operator  doesn't support  arbiter nodes . When you deploy your replica set via the  Kubernetes Operator , you must\nchoose whether to encrypt connections using  TLS (Transport Layer Security)  certificates. The following procedure for  TLS-Encrypted  connections: The following procedure for  Non-Encrypted Connections : To set up  TLS (Transport Layer Security)  encryption for a sharded cluster, see\n Deploy a Sharded Cluster . Select the appropriate tab based on whether you want to encrypt your\nreplica set connections with  TLS (Transport Layer Security) . Establishes  TLS (Transport Layer Security) -encrypted connections between MongoDB hosts in the\nreplica set. Establishes  TLS (Transport Layer Security) -encrypted connections between client applications\nand MongoDB deployments. Requires valid certificates for  TLS (Transport Layer Security)  encryption. Doesn't encrypt connections between MongoDB hosts in the\nreplica set. Doesn't encrypt connections between client applications\nand MongoDB deployments. Has fewer setup requirements than a deployment with  TLS (Transport Layer Security) -encrypted\nconnections. To deploy a  replica set  using an  object , you must: Have or create an  Ops Manager instance  or a  Cloud Manager organization . Have or install the  MongoDB Enterprise Kubernetes Operator . Create or generate a  Kubernetes Operator ConfigMap . Create  credentials for the Kubernetes Operator  or\nconfigure  a different secret storage tool . To avoid storing secrets in  Kubernetes , you can migrate all  secrets \nto a  secret storage tool . Generate one  TLS (Transport Layer Security)  certificate for each of the following components: Your replica set. Ensure that you add  SAN (Subject Alternative Name) s for each  Kubernetes  pod\nthat hosts a member of your replica set to the certificate. In your  TLS (Transport Layer Security)  certificate, the  SAN (Subject Alternative Name)  for each pod must use the\nfollowing format: To use an  ACME (Automatic Certificate Management Environment)  based certificate, you must configure\nthe certificate for your replica set resource.\nTo learn more, see the step about  ACME (Automatic Certificate Management Environment)  based  TLS (Transport Layer Security) \ncertificates in the  procedure . If you're using an  ACME (Automatic Certificate Management Environment)  based service provider such as  Let's Encrypt  to issue  TLS (Transport Layer Security)  certificates, the provider\nmight prohibit you from adding the Pod's default  FQDN (fully qualified domain name) s ( *.svc.cluster.local )\nto  SAN (Subject Alternative Name) s in the certificate. Your project's MongoDB Agent. For the MongoDB Agent certificate,\nensure that you meet the following requirements: The Common Name in the  TLS (Transport Layer Security)  certificate is not empty. The combined Organization and Organizational Unit in each  TLS (Transport Layer Security) \ncertificate differs from the Organization and\nOrganizational Unit in the  TLS (Transport Layer Security)  certificate for your\nreplica set members. You must have the  CA (Certificate Authority)  certificate file and name it  ca-pem . You must have the key that you used to sign your  TLS (Transport Layer Security)  certificates. The  Kubernetes Operator  uses  kubernetes.io/tls  secrets\nto store  TLS (Transport Layer Security)  certificates and private keys for  Ops Manager  and MongoDB\nresources. Starting in  Kubernetes Operator  version 1.17.0, the\n Kubernetes Operator  doesn't support concatenated  PEM (Privacy-Enhanced Mail)  files stored as\n Opaque secrets . To deploy a  replica set  using an  object , you must: Have or create an  Ops Manager instance  or a  Cloud Manager organization . Have or install the  MongoDB Enterprise Kubernetes Operator . Create or generate a  Kubernetes Operator ConfigMap . Create  credentials for the Kubernetes Operator  or\nconfigure  a different secret storage tool . To avoid storing secrets in  Kubernetes , you can migrate all  secrets \nto a  secret storage tool . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Run this  kubectl  command to create a new  secret  that stores\nthe replica set's certificate: You must prefix your secrets with  <prefix>-<metadata.name> . If you call your deployment  my-deployment  and you set the\nprefix to  mdb , you must name the  TLS (Transport Layer Security)  secret for the\nclient  TLS (Transport Layer Security)  communications  mdb-my-deployment-cert . Also,\nyou must name the  TLS (Transport Layer Security)  secret for internal cluster authentication\n(if enabled)  mdb-my-deployment-clusterfile . If you're using  HashiCorp Vault  as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. To learn about your options for secret\nstorage, see  Configure Secret Storage . Run this  kubectl  command to create a new  secret  that stores\nthe agent's TLS certificate: If you're using  HashiCorp Vault  as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. Run this  kubectl  command to link your  CA (Certificate Authority)  to your replica\nset and specify the  CA (Certificate Authority)  certificate file. The  Kubernetes Operator  requires that the certificate for the  MongoDB  resource \nis named  ca-pem  in the ConfigMap. Change the settings of this  YAML (Yet Another Markup Language)  file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the  object  specification\ninto a new text file. Key Type Description Example metadata.name string Label for this  Kubernetes   replica set   object . Resource names must be 44 characters or less. metadata.name Kubernetes documentation on\n names . myproject spec.members integer Number of members of the  replica set . 3 spec.version string Version of MongoDB that this  replica set  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n MongoDB Versioning  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. 4.4.0-ent string Name of the  ConfigMap  with the  Ops Manager  connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value must exist on the  same  namespace as the resource\nyou want to create. The  Kubernetes Operator  tracks any changes to the ConfigMap and\nreconciles the state of the  MongoDB  resource . <myconfigmap> spec.credentials string Name of the secret you\n created  as  Ops Manager   API (Application Programming Interface) \nauthentication credentials for the  Kubernetes Operator  to\ncommunicate with  Ops Manager . The  Ops Manager   Kubernetes   Secret  object\nholding the Credentials must exist on the  same  Namespace as\nthe resource you want to create. The  Kubernetes Operator  tracks any changes to the Secret and\nreconciles the state of the  MongoDB  resource . <mycredentials> spec.type string Type of  MongoDB  resource  to create. ReplicaSet spec.persistent string Optional. Flag indicating if this  MongoDB  resource  should use  Persistent Volumes  for\nstorage. Persistent volumes are not deleted when the\n MongoDB  resource  is stopped or restarted. If this value is  true , then\n spec.podSpec.persistence.single  is set to its\ndefault value of  16Gi . To change your  Persistent Volume Claims  configuration, configure the\nfollowing collections to meet your deployment requirements: If you want one  Persistent Volume  for each  Pod , configure the\n spec.podSpec.persistence.single  collection. If you want separate  Persistent Volumes  for data, journals, and\nlogs for each  Pod , configure the following\ncollections: spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs Grant your containers permission to write to your  Persistent Volume .\nThe  Kubernetes Operator  sets  fsGroup = 2000 ,  runAsUser = 2000 ,\nand  runAsNonRoot = true  in  securityContext .  Kubernetes Operator \nsets  fsgroup  equal to  runAsUser  to make the volume writable\nfor a user that runs the main process in the container. To learn\nmore, see  Configure a\nSecurity Context for a Pod or Container  and the related\n discussion  in\nthe  Kubernetes  documentation. If redeploying the resource doesn't fix\nissues with your Persistent Volume, contact  MongoDB Support . If you do not use  Persistent Volumes , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true To enable  TLS (Transport Layer Security)  in your deployment, configure the following\nsettings in your  Kubernetes  object: Key Type Necessity Description Example string Required Add the  ConfigMap 's name that stores the custom  CA (Certificate Authority) \nthat you used to sign your deployment's  TLS (Transport Layer Security)  certificates. <custom-ca> string Required Add the  <prefix>  of the secret\nname that contains your MongoDB deployment's  TLS (Transport Layer Security)  certificates. If you call your deployment  my-deployment  and you set the\nprefix to  mdb , you must name the  TLS (Transport Layer Security)  secret for the\nclient  TLS (Transport Layer Security)  communications  mdb-my-deployment-cert . Also,\nyou must name the  TLS (Transport Layer Security)  secret for internal cluster authentication\n(if enabled)  mdb-my-deployment-clusterfile . devDb To configure a certificate that doesn't contain the pod's  FQDN (fully qualified domain name) s: If you're using an  ACME (Automatic Certificate Management Environment)  based service provider such as  Let's Encrypt  to issue  TLS (Transport Layer Security)  certificates, the provider\nmight prohibit you from adding the Pod's default  FQDN (fully qualified domain name) s ( *.svc.cluster.local )\nto  SAN (Subject Alternative Name) s in the certificate. Issue the certificate for an external domain. For more information, see the\n Let's Encrypt documentation  or\nthe documentation for your provider. Ensure that your certificate contains all hostnames that you plan to deploy in the\nreplica set. Alternatively, you can issue a wildcard certificate for  *.<externalDomain> . To use a certificate containing only external domains for your replica set deployment,\nyou must change the default hostname used by the replica set: If you prefer to configure the hostname while creating your  Kubernetes  cluster, change the\n default domain \nfrom  cluster.local  to the external domain when creating or recreating your  Kubernetes  cluster.\nThen, set this domain in your MongoDB resource by using the  spec.clusterDomain  setting. Otherwise, create your MongoDB deployment with the following settings configured in your  Kubernetes \nobject: Key Type Necessity Description string Required An external domain used to externally expose your replica set deployment. By default, each replica set member uses the  Kubernetes  Pod's  FQDN (fully qualified domain name) \n( *.svc.cluster.local ) as the default hostname. However, if you add an\nexternal domain to this setting, the replica set uses a hostname that is a\nsubdomain of the specified domain instead. This hostname uses the following\nformat: For example: After you deploy the replica set with this setting, the\n Kubernetes Operator  uses the hostname with the external domain to override\nthe  processes[n].hostname  field in the  Ops Manager   automation configuration . Then, the MongoDB Agent uses this hostname to\nconnect to  mongod . To specify other hostnames for connecting to the replica set, you can use the\n spec.connectivity.replicaSetHorizons  setting. However, the following\nconnections still use the hostname with the external domain: <replica-set-name>-<pod-idx>.<externalDomain> replica-set-1.example.com The MongoDB Agent to connect to  mongod . mongod  to connect to other  mongod  instances. Specifying this field changes how  Ops Manager  registers  mongod  processes.\nYou can specify this field only for new replica set deployments starting in  Kubernetes Operator \nversion 1.19. You can't change the value of this field or any  processes[n].hostname  fields\nin the  Ops Manager   automation configuration  for a running\nreplica set deployment. collection Optional Configuration for the  ServiceSpec . When you set the  spec.externalAccess  setting, the  Kubernetes Operator \nautomatically creates an external load balancer service with  default values .\nYou can override certain values or add new values depending on your needs.\nFor example, if you intend to create  NodePort services \nand don't need a load balancer, you must configure overrides in your\n Kubernetes  specification: For more information about the  Kubernetes  specification, see  ServiceSpec \nin the  Kubernetes  documentation. collection Optional Key-value pairs that let you add cloud provider-specific\nconfiguration settings to all clusters in your deployment. To learn more about annotations, see the\n Kubernetes documentation \nand the documentation for your  Kubernetes  cloud provider. You can also add any of the following optional settings to the\n object  specification file for a  replica set  deployment: spec.additionalMongodConfig spec.backup.assignmentLabels spec.backup.mode spec.backup.snapshotSchedule.snapshotIntervalHours spec.backup.snapshotSchedule.snapshotRetentionDays spec.backup.snapshotSchedule.dailySnapshotRetentionDays spec.backup.snapshotSchedule.weeklySnapshotRetentionWeeks spec.backup.snapshotSchedule.monthlySnapshotRetentionMonths spec.backup.snapshotSchedule.pointInTimeWindowHours spec.backup.snapshotSchedule.referenceHourOfDay spec.backup.snapshotSchedule.referenceMinuteOfHour spec.backup.snapshotSchedule.fullIncrementalDayOfWeek spec.clusterDomain spec.connectivity.replicaSetHorizons spec.featureCompatibilityVersion spec.logLevel spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs spec.podSpec.podTemplate.affinity.podAffinity spec.podSpec.podTemplate.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution.topologyKey spec.podSpec.podTemplate.affinity.nodeAffinity spec.podSpec.podTemplate.metadata spec.podSpec.podTemplate.spec You must set  spec.clusterDomain  if your  Kubernetes  cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n Kubernetes Operator  might not function as expected. In any directory, invoke the following  Kubernetes  command to create your\n replica set : To check the status of your  MongoDB  resource , use the following\ncommand: With the  -w  (watch) flag set, when the configuration changes, the output\nrefreshes immediately until the status phase achieves the  Running  state.\nTo learn more about resource deployment statuses, see  Troubleshoot the  Kubernetes Operator . After you encrypt your database resource with  TLS (Transport Layer Security) , you can secure the\nfollowing: Client authentication with LDAP Client authentication with X.509 Internal authentication with X.509 Renew your  TLS (Transport Layer Security)  certificates periodically\nusing the following procedure: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Run this  kubectl  command to renew an existing  secret  that\nstores the replica set's certificates: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Change the settings of this  YAML (Yet Another Markup Language)  file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the  object  specification\ninto a new text file. Key Type Description Example metadata.name string Label for this  Kubernetes   replica set   object . Resource names must be 44 characters or less. metadata.name Kubernetes documentation on\n names . myproject spec.members integer Number of members of the  replica set . 3 spec.version string Version of MongoDB that this  replica set  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n MongoDB Versioning  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. 4.4.0-ent string Name of the  ConfigMap  with the  Ops Manager  connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value must exist on the  same  namespace as the resource\nyou want to create. The  Kubernetes Operator  tracks any changes to the ConfigMap and\nreconciles the state of the  MongoDB  resource . <myconfigmap> spec.credentials string Name of the secret you\n created  as  Ops Manager   API (Application Programming Interface) \nauthentication credentials for the  Kubernetes Operator  to\ncommunicate with  Ops Manager . The  Ops Manager   Kubernetes   Secret  object\nholding the Credentials must exist on the  same  Namespace as\nthe resource you want to create. The  Kubernetes Operator  tracks any changes to the Secret and\nreconciles the state of the  MongoDB  resource . <mycredentials> spec.type string Type of  MongoDB  resource  to create. ReplicaSet spec.persistent string Optional. Flag indicating if this  MongoDB  resource  should use  Persistent Volumes  for\nstorage. Persistent volumes are not deleted when the\n MongoDB  resource  is stopped or restarted. If this value is  true , then\n spec.podSpec.persistence.single  is set to its\ndefault value of  16Gi . To change your  Persistent Volume Claims  configuration, configure the\nfollowing collections to meet your deployment requirements: If you want one  Persistent Volume  for each  Pod , configure the\n spec.podSpec.persistence.single  collection. If you want separate  Persistent Volumes  for data, journals, and\nlogs for each  Pod , configure the following\ncollections: spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs Grant your containers permission to write to your  Persistent Volume .\nThe  Kubernetes Operator  sets  fsGroup = 2000 ,  runAsUser = 2000 ,\nand  runAsNonRoot = true  in  securityContext .  Kubernetes Operator \nsets  fsgroup  equal to  runAsUser  to make the volume writable\nfor a user that runs the main process in the container. To learn\nmore, see  Configure a\nSecurity Context for a Pod or Container  and the related\n discussion  in\nthe  Kubernetes  documentation. If redeploying the resource doesn't fix\nissues with your Persistent Volume, contact  MongoDB Support . If you do not use  Persistent Volumes , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n object  specification file for a  replica set  deployment: spec.additionalMongodConfig spec.backup.assignmentLabels spec.backup.mode spec.backup.snapshotSchedule.snapshotIntervalHours spec.backup.snapshotSchedule.snapshotRetentionDays spec.backup.snapshotSchedule.dailySnapshotRetentionDays spec.backup.snapshotSchedule.weeklySnapshotRetentionWeeks spec.backup.snapshotSchedule.monthlySnapshotRetentionMonths spec.backup.snapshotSchedule.pointInTimeWindowHours spec.backup.snapshotSchedule.referenceHourOfDay spec.backup.snapshotSchedule.referenceMinuteOfHour spec.backup.snapshotSchedule.fullIncrementalDayOfWeek spec.clusterDomain spec.connectivity.replicaSetHorizons spec.featureCompatibilityVersion spec.logLevel spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs spec.podSpec.podTemplate.affinity.podAffinity spec.podSpec.podTemplate.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution.topologyKey spec.podSpec.podTemplate.affinity.nodeAffinity spec.podSpec.podTemplate.metadata spec.podSpec.podTemplate.spec You must set  spec.clusterDomain  if your  Kubernetes  cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n Kubernetes Operator  might not function as expected. In any directory, invoke the following  Kubernetes  command to create your\n replica set : To check the status of your  MongoDB  resource , use the following\ncommand: With the  -w  (watch) flag set, when the configuration changes, the output\nrefreshes immediately until the status phase achieves the  Running  state.\nTo learn more about resource deployment statuses, see  Troubleshoot the  Kubernetes Operator .",
            "code": [
                {
                    "lang": "none",
                    "value": "<pod-name>.<metadata.name>-svc.<namespace>.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-cert \\\n  --cert=<replica-set-tls-cert> \\\n  --key=<replica-set-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap custom-ca --from-file=ca-pem=<your-custom-ca-file>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n           # Must match metadata.name in ConfigMap file\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      ca: <custom-ca>\n    certsSecretPrefix: <prefix>\n..."
                },
                {
                    "lang": "yaml",
                    "value": "externalAccess:\n  externalService:\n    annotations:\n      # cloud-specific annotations for the service\n    spec:\n      type: NodePort # default is LoadBalancer\n      # you can specify other spec overrides if necessary"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-cert \\\n  --cert=<replica-set-tls-cert> \\\n  --key=<replica-set-tls-key> \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n           # Must match metadata.name in ConfigMap file\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "A replica set is a group of MongoDB deployments that maintain\nthe same data set. Replica sets provide redundancy and high\navailability and are the basis for all production deployments.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/secure-client-connections",
            "title": "Secure Client Connections",
            "headings": [],
            "paragraphs": "Configure  LDAP (Lightweight Directory Access Protocol)  for client authentication. Configure X.509 for client authentication. Configure X.509 for internal authentication.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/deploy-om-container-remote-mode",
            "title": "Configure an Ops Manager Resource to use Remote Mode",
            "headings": [
                "Prerequisites and Considerations",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Create a ConfigMap for Nginx.",
                "Deploy Nginx to your Kubernetes cluster.",
                "Create a Kubernetes service to make Nginx accessible from other pods in your cluster.",
                "Copy and update the highlighted fields of this Ops Manager resource.",
                "Paste the copied example section into your existing Ops Manager resource.",
                "Save your Ops Manager config file.",
                "Apply changes to your Ops Manager deployment.",
                "Track the status of your Ops Manager instance.",
                "Deploy a MongoDB Database Resource."
            ],
            "paragraphs": "In a default configuration, the MongoDB Agents and Backup Daemons\naccess MongoDB installation archives over the Internet from  MongoDB, Inc. You can configure  Ops Manager  to run in  Remote Mode  with the\n Kubernetes Operator  if the nodes in your  Kubernetes  cluster don't have access to\nthe Internet. The Backup Daemons and managed MongoDB resources download\ninstallation archives only from  Ops Manager , which proxies download\nrequests to an HTTP endpoint on a local web server or S3-compatible\nstore deployed to your  Kubernetes  cluster. This procedure covers deploying an Nginx HTTP server to your  Kubernetes \ncluster to host the MongoDB installation archives. Deploy an  Ops Manager  Resource . The following procedure shows you how to\nupdate your  Ops Manager   Kubernetes   object  to enable Remote Mode. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . The ConfigMap in this tutorial configures Nginx to: Run an HTTP server named  localhost  listening on port  80  on a\nnode in your  Kubernetes  cluster, and Route HTTP requests for specific resources to locations that serve\nthe the MongoDB Server and MongoDB Database Tools installation\narchives. Paste the following example Nginx ConfigMap into a text editor: Save this file with a  .yaml  file extension. Create the Nginx ConfigMap by invoking the following\n kubectl  command on the ConfigMap file you created: The Nginx resource configuration in this tutorial: Deploys one Nginx replica, Creates volume mounts to store MongoDB Server and MongoDB Database\nTools installation archives, and Defines  init containers  that use  curl \ncommands to download the installation archives that Nginx serves to\nMongoDB Database resources you deploy in your  Kubernetes  cluster. Paste the following example Nginx resource configuration\ninto a text editor: Modify the lines highlighted in the example to specify the\nMongoDB Server versions that you want to install. For example, to replace MongoDB version  4.0.2  with\na different database version, update the following block: Update this block to modify the MongoDB Database Tools\nversion: to the appropriate initContainer for each version you want\nNginx to serve. For example, to configure Nginx to serve MongoDB  5.0.12 \nand  6.0.1 : Save this file with a  .yaml  file extension. Deploy Nginx by invoking the following  kubectl \ncommand on the Nginx resource file you created: Paste the following example Nginx resource configuration\ninto a text editor: Modify the lines highlighted in the example to specify the\nMongoDB Server versions that you want to install. For example, to replace MongoDB version  4.0.2  with\na different database version, update the following block: Update this block to modify the MongoDB Database Tools\nversion: To load multiple versions, append  curl  commands\nto the appropriate initContainer for each version you want\nNginx to serve. For example, to configure Nginx to serve MongoDB  4.2.0 \nand  4.4.0 : Save this file with a  .yaml  file extension. Deploy Nginx by invoking the following  oc \ncommand on the Nginx resource file you created: The service in this tutorial exposes Nginx to traffic from other nodes\nin your  Kubernetes  cluster over port  80 . This allows the MongoDB\nDatabase resource pods you deploy using the  Kubernetes Operator  to download\nthe installation archives from Nginx. Run the following command to create a service your Nginx deployment: Paste the following example service into a text editor: Save this file with a  .yaml  file extension. Create the service by invoking the following\n kubectl  command on the service file you created: The highlighted section uses the following  Ops Manager  configuration\nsettings: automation.versions.source: remote  in\n spec.configuration  to enable Remote Mode. automation.versions.download.baseUrl  in\n spec.configuration  to provide the base URL of the\nHTTP resources that serve the MongoDB installation archives. Update this line to replace  <namespace>  with the namespace to\nwhich you deploy resources with the  Kubernetes Operator . automation.versions.download.baseUrl.allowOnlyAvailableBuilds:\n\"false\"  in  spec.configuration  to help ensure\nenterprise builds have no issues. Open your preferred text editor and paste the  object \nspecification into the appropriate location in your resource file. Invoke the following  kubectl  command on the filename of the\n Ops Manager  resource  definition: To check the status of your  Ops Manager  resource, invoke the following\ncommand: See  Troubleshoot the  Kubernetes Operator  for information about the\nresource deployment statuses. After the  Ops Manager  resource completes the  Pending  phase, the\ncommand returns output similar to the following: Copy the value of the  status.opsManager.url  field, which states\nthe resource's connection  URL (Uniform Resource Locator) . You use this value when you create a\n ConfigMap  later in the procedure. MongoDB Agents running in MongoDB database resource containers that\nyou create with the  Kubernetes Operator  download the installation archives\nfrom  Ops Manager  via Nginx instead of from the Internet. If you have not done so already, complete the following\nprerequisites: Create Credentials for the  Kubernetes Operator Create One Project using a ConfigMap Deploy a  MongoDB Database resource \nin the same namespace to which you deployed  Ops Manager .\nEnsure that you: Match the  spec.opsManager.configMapRef.name  of the resource\nto the  metadata.name  of your ConfigMap. Match the  spec.credentials  of the resource to the name of\nthe secret you created that contains an  Ops Manager  programmatic\nAPI key pair.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-conf\ndata:\n  nginx.conf: |\n    events {}\n    http {\n      server {\n        server_name localhost;\n        listen 80;\n        location /linux/ {\n          alias /mongodb-ops-manager/mongodb-releases/linux/;\n        }\n        location /tools/ {\n          alias /tools/;\n        }\n      }\n    }\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <nginix-configmap>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - image: nginx:1.14.2\n          imagePullPolicy: IfNotPresent\n          name: nginx\n          ports:\n            - containerPort: 80\n          volumeMounts:\n            - mountPath: /mongodb-ops-manager/mongodb-releases/linux\n              name: mongodb-versions\n            - mountPath: /tools/db/\n              name: mongodb-tools\n            - name: nginx-conf\n              mountPath: /etc/nginx/nginx.conf\n              subPath: nginx.conf\n      initContainers:\n        - name: setting-up-rhel-mongodb\n          image: curlimages/curl:latest\n          command:\n            - curl\n            - -L\n            - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel80-6.0.1.tgz\n            - -o\n            - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel80-6.0.1.tgz\n          volumeMounts:\n            - name: mongodb-versions\n              mountPath: /mongodb-ops-manager/mongodb-releases/linux\n        - name: setting-up-rhel-mongodb-tools\n          image: curlimages/curl:latest\n          command:\n            - curl\n            - -L\n            - https://fastdl.mongodb.org/tools/db/mongodb-database-tools-rhel80-x86_64-100.6.0.tgz\n            - -o\n            - /tools/db/mongodb-database-tools-rhel80-x86_64-100.6.0.tgz\n          volumeMounts:\n            - name: mongodb-tools\n              mountPath: /tools/db/\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      volumes:\n        - name: mongodb-versions\n          emptyDir: {}\n        - name: mongodb-tools\n          emptyDir: {}\n        - configMap:\n            name: nginx-conf\n          name: nginx-conf\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "- name: setting-up-rhel-mongodb\n  image: curlimages/curl:latest\n  command:\n    - curl\n    - -L\n    - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel80-6.0.1.tgz\n    - -o\n    - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel80-6.0.1.tgz"
                },
                {
                    "lang": "yaml",
                    "value": "- name: setting-up-rhel-mongodb-tools\n  image: curlimages/curl:latest\n  command:\n    - curl\n    - -L\n    - https://fastdl.mongodb.org/tools/db/mongodb-database-tools-rhel80-x86_64-100.6.0.tgz\n    - -o\n    - /tools/db/mongodb-database-tools-rhel80-x86_64-100.6.0.tgz"
                },
                {
                    "lang": "yaml",
                    "value": "- name: setting-up-rhel-mongodb\n  image: curlimages/curl:latest\n  command:\n    - curl\n    - -L\n    - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel80-5.0.12.tgz\n    - -o\n    - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel80-5.0.12.tgz\n    - &&\n    - curl\n    - -L\n    - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel80-6.0.1.tgz\n    - -o\n    - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel80-6.0.1.tgz"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <nginix>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - image: nginx:1.14.2\n          imagePullPolicy: IfNotPresent\n          name: nginx\n          ports:\n            - containerPort: 80\n          volumeMounts:\n            - mountPath: /mongodb-ops-manager/mongodb-releases/linux\n              name: mongodb-versions\n            - mountPath: /tools/db/\n              name: mongodb-tools\n            - name: nginx-conf\n              mountPath: /etc/nginx/nginx.conf\n              subPath: nginx.conf\n      initContainers:\n        - name: setting-up-rhel-mongodb\n          image: curlimages/curl:latest\n          command:\n            - curl\n            - -L\n            - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel<version>-4.2.0.tgz\n            - -o\n            - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel<version>-4.2.0.tgz\n          volumeMounts:\n            - name: mongodb-versions\n              mountPath: /mongodb-ops-manager/mongodb-releases/linux\n        - name: setting-up-rhel-mongodb-tools\n          image: curlimages/curl:latest\n          command:\n            - curl\n            - -L\n            - https://fastdl.mongodb.org/tools/db/mongodb-database-tools-rhel<version>-x86_64-100.1.0.tgz\n            - -o\n            - /tools/db/mongodb-database-tools-rhel<version>-x86_64-100.1.0.tgz\n          volumeMounts:\n            - name: mongodb-tools\n              mountPath: /tools/db/\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      volumes:\n        - name: mongodb-versions\n          emptyDir: {}\n        - name: mongodb-tools\n          emptyDir: {}\n        - configMap:\n            name: nginx-conf\n          name: nginx-conf\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "- name: setting-up-rhel-mongodb\n  image: curlimages/curl:latest\n  command:\n    - curl\n    - -L\n    - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel<version>-4.2.0.tgz\n    - -o\n    - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel<version>-4.2.0.tgz"
                },
                {
                    "lang": "yaml",
                    "value": "- name: setting-up-rhel-mongodb-tools\n  image: curlimages/curl:latest\n  command:\n    - curl\n    - -L\n    - https://fastdl.mongodb.org/tools/db/mongodb-database-tools-rhel<version>-x86_64-100.1.0.tgz\n    - -o\n    - /tools/db/mongodb-database-tools-rhel<version>-x86_64-100.1.0.tgz"
                },
                {
                    "lang": "yaml",
                    "value": "- name: setting-up-rhel-mongodb\n  image: curlimages/curl:latest\n  command:\n    - curl\n    - -L\n    - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel80-5.0.12.tgz\n    - -o\n    - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel80-5.0.12.tgz\n    - &&\n    - curl\n    - -L\n    - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel80-6.0.1.tgz\n    - -o\n    - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel80-6.0.1.tgz"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f <nginix>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: Service\nmetadata:\n name: nginx-svc\n labels:\n   app: nginx\nspec:\n ports:\n - port: 80\n   protocol: TCP\n selector:\n   app: nginx\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <nginix-service>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: ops-manager-localmode\nspec:\n replicas: 1\n version: \"6.0.0\"\n adminCredentials: ops-manager-admin-secret\n configuration:\n   # this enables remote mode in Ops Manager\n   automation.versions.source: remote\n   automation.versions.download.baseUrl: \"http://nginx-svc.<namespace>.svc.cluster.local:8080\"\n\n backup:\n   enabled: false\n\n applicationDatabase:\n   members: 3\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2020-05-15T16:20:22Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: \"4.4.5-ubi8\"\n  backup:\n    phase: \"\"\n  opsManager:\n    lastTransition: \"2020-05-15T16:20:26Z\"\n    phase: Running\n    replicas: 1\n    url: http://ops-manager-localmode-svc.mongodb.svc.cluster.local:8080\n    version: \"5.0.0\"\n"
                }
            ],
            "preview": "In a default configuration, the MongoDB Agents and Backup Daemons\naccess MongoDB installation archives over the Internet from MongoDB, Inc.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/modify-resource-image",
            "title": "Modify Ops Manager or MongoDB Kubernetes Resource Containers",
            "headings": [
                "Define a Volume Mount for a MongoDB Kubernetes Resource",
                "Tune MongoDB Kubernetes Resource Docker Images with an InitContainer",
                "Build Custom Images with Dockerfile Templates",
                "MongoDB Dockerfile Templates",
                "Context Images",
                "docker build Example"
            ],
            "paragraphs": "You can modify the containers in the  Pods  in which  Ops Manager  and\nMongoDB database resources run using the  template  or\n podTemplate  setting that applies to your deployment: To review which fields you can add to a  template  or a\n podTemplate , see the  Kubernetes documentation . When you create containers with a  template  or  podTemplate , the\n Kubernetes Operator  handles container creation differently based on the\n name  you provide for each container in the  containers  array: MongoDB database:  spec.podSpec.podTemplate Ops Manager :  spec.statefulSet.spec.template Backup Daemon Service :  spec.backup.statefulSet.spec.template If the  name  field  matches  the name of the applicable resource\nimage, the  Kubernetes Operator  updates the  Ops Manager  or MongoDB database\ncontainer in the  Pod  to which the  template  or\n podTemplate  applies: Ops Manager :  mongodb-enterprise-ops-manager Backup Daemon Service :  mongodb-backup-daemon MongoDB database:  mongodb-enterprise-database Application Database:  mongodb-enterprise-appdb If the  name  field  does not match  the name of the applicable\nresource image, the  Kubernetes Operator  creates a new container in each\n Pod  to which the  template  or  podTemplate  applies. On-disk files in containers in  Pods  don't survive container\ncrashes or restarts. Using the  spec.podSpec.podTemplate \nsetting, you can add a  volume mount \nto persist data in a MongoDB database resource for the life of the\n Pod . To create a volume mount for a MongoDB database resource: Update the MongoDB database resource definition to include a volume\nmount for containers in the database pods that the  Kubernetes Operator \ncreates. Use  spec.podSpec.podTemplate  to define a volume mount: Apply the updated resource definition: MongoDB  resource  Docker images run on RHEL and use RHEL's default\nsystem configuration. To tune the underlying RHEL system\nconfiguration in the  MongoDB  resource  containers, add a privileged\nInitContainer\n init container \nusing one of the following settings: To tune Docker images for a MongoDB database resource container: Kubernetes  adds a privileged InitContainer to each  Pod  that the\n Kubernetes Operator  creates using the  MongoDB  resource  definition. Open a shell session to a running container in your database resource\n Pod  and verify your changes. spec.podSpec.podTemplate : add a privileged InitContainer\nto a MongoDB database resource container. spec.statefulSet.spec.template : add a privileged\nInitContainer to an  Ops Manager  resource container. MongoDB database resource Docker images use the RHEL default\n keepalive  time of  7200 . MongoDB recommends a shorter\n keepalive  time of  120  for database deployments. You can tune the  keepalive  time in the database resource Docker\nimages if you experience network timeouts or socket errors in\ncommunication between clients and the database resources. Does TCP keepalive time affect MongoDB Deployments?  in the\nMongoDB Manual Update the MongoDB database resource definition to append a\nprivileged InitContainer to the database pods that the\n Kubernetes Operator  creates. Change  spec.podSpec.podTemplate  the  keepalive \nvalue to the recommended value of  120 : Apply the updated resource definition: To follow the previous  keepalive  example, invoke the following\ncommand to get the current  keepalive  value: Operating System Configuration  in the\nMongoDB Manual. You can modify MongoDB Dockerfile templates to create custom\n Kubernetes Operator  images that suit your use case. To build a\ncustom image, you need: Your custom Dockerfile, modified from a MongoDB template. The MongoDB-provided context image for your template. The Dockerfiles used to build container images are publicly\navailable from the\n MongoDB Enterprise Kubernetes GitHub repository . The Dockerfile directory is organized by resource name, version and\ndistribution: Copy the template you want to use to your own Dockerfile and modify as\ndesired. To build an image from any MongoDB Dockerfile template, you must supply\nits context image. Each Dockerfile template has one associated context image, retrievable\nfrom the same  Quay.io  registry as the original\nimages. Context image are always tagged in the format\n quay.io/mongodb/<resource-name>:<image-version>-context . To supply a context image to  docker build , include the\n --build-arg  option with the  imagebase  variable set to a\nQuay.io tag, where  <resource-name>  and  <image-version>  match\nyour Dockerfile template. If you want to build the  mongodb-enterprise-database  version\n2.0.0 image for any distribution, include: The Ubuntu distribution for  mongodb-enterprise-operator  version\n1.9.1 is based on  ubuntu:1604  by default. In this example, that\nbase Dockerfile template is modified to use  ubuntu:1804  and\nsaved as  myDockerfile . The following command builds the custom image and gives it the tag\n 1.9.1-ubuntu-1804 : Include a hyphen ( - ) at the end of  docker build  to read\nthe output of  cat myDockerfile  instead of providing a\nlocal directory as build context. To learn more about  docker build , see the\n Docker documentation .",
            "code": [
                {
                    "lang": "yaml",
                    "value": "podSpec:\n  podTemplate:\n    spec:\n      containers:\n      - name: mongodb-enterprise-database\n        volumeMounts:\n        - mountPath: </new/mount/path>\n          name: survives-restart\n      volumes:\n      - name: survives-restart\n        emptyDir: {}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-resource-conf>.yaml -n <metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  podSpec:\n    podTemplate:\n      spec:\n        initContainers:\n        - name: \"adjust-tcp-keepalive\"\n          image: \"busybox:latest\"\n          securityContext:\n            privileged: true\n          command: [\"sysctl\", \"-w\", \"net.ipv4.tcp_keepalive_time=120\"]"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-resource-conf>.yaml -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "> kubectl exec -n <metadata.namespace> -it <pod-name> -- cat /proc/sys/net/ipv4/tcp_keepalive_time\n\n\n> 120"
                },
                {
                    "lang": "sh",
                    "value": "\u251c\u2500\u2500 <resource name>\n\u2502   \u2514\u2500\u2500 <image version>\n\u2502       \u2514\u2500\u2500 <base distribution>\n\u2502           \u2514\u2500\u2500 Dockerfile template"
                },
                {
                    "lang": "sh",
                    "value": "--build-arg imagebase=quay.io/mongodb/mongodb-enterprise-database:2.0.0-context"
                },
                {
                    "lang": "sh",
                    "value": "cat myDockerfile | docker build --build-arg imagebase=quay.io/mongodb/mongodb-enterprise-operator:1.9.1-context \\\n--tag mongodb-enterprise-operator:1.9.1-ubuntu-1804 -"
                }
            ],
            "preview": "You can modify the containers in the Pods in which Ops Manager and\nMongoDB database resources run using the template or\npodTemplate setting that applies to your deployment:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/scale-resources",
            "title": "Scale a Deployment",
            "headings": [
                "Considerations",
                "Procedure",
                "Adjust the spec.members setting from 3 to 4:",
                "Reapply the configuration to Kubernetes:",
                "Adjust the following settings to the desired values:",
                "Reapply the configuration to Kubernetes:"
            ],
            "paragraphs": "You can scale your  replica set  and  sharded cluster \ndeployments up or down to match your desired configuration. Scaling up\nincreases the number of members and/or shards in the deployment,\nthereby improving your deployment's redundancy and availability. The\nscale of your deployment is configured in its corresponding\n custom resource . To scale your replica set deployment, set the  spec.members \nsetting to the desired number of replica set members. To learn more\nabout replication, see  Replication  in the\nMongoDB manual. To scale your sharded cluster deployment, set the following settings\nas desired: To learn more about sharded cluster configurations, see\n Sharded Cluster Components  in the MongoDB manual. Setting Description spec.shardCount Number of  shards  in the sharded cluster. spec.mongodsPerShardCount Number of members per shard. spec.mongosCount Number of Shard Routers. spec.configServerCount Number of members in the Config Server. The  Kubernetes Operator  does not support modifying deployment types.\nFor example, you cannot convert a standalone deployment to a\nreplica set. To modify the type of a deployment,\nwe recommend the following procedure: Create the new deployment with the desired configuration. Back up the data  from\nyour current deployment. Restore the data  from your current\ndeployment to the new deployment. Test your application connections to the new deployment as needed. Once you have verified that the new deployment contains the\nrequired data and can be reached by your application(s), bring\ndown the old deployment. To scale up your deployment, select the desired tab based on the deployment\nconfiguration you want to scale: spec.shardCount spec.mongodsPerShardCount spec.mongosCount spec.configServerCount",
            "code": [
                {
                    "lang": "yaml",
                    "value": "spec:\n  members: 4"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n           # Must match metadata.name in ConfigMap file\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <repl-set-config>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  shardCount: 3\n  mongodsPerShardCount: 3\n  mongosCount: 3\n  configServerCount: 4"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-secure-sharded-cluster>\nspec:\n  shardCount: 3\n  mongodsPerShardCount: 3\n  mongosCount: 3\n  configServerCount: 4\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-config>.yaml"
                }
            ],
            "preview": "You can scale your replica set and sharded cluster\ndeployments up or down to match your desired configuration. Scaling up\nincreases the number of members and/or shards in the deployment,\nthereby improving your deployment's redundancy and availability. The\nscale of your deployment is configured in its corresponding\ncustom resource.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/connect-from-outside-k8s",
            "title": "Connect to a MongoDB Database Resource from Outside Kubernetes",
            "headings": [
                "Prerequisite",
                "Compatible MongoDB Versions",
                "Considerations",
                "Configure Readiness Probe Overrides",
                "Procedure",
                "Deploy a standalone resource with the Kubernetes Operator.",
                "Create an external service for the MongoDB Pod.",
                "Verify the external services.",
                "Test the connection to the standalone resource.",
                "Deploy a replica set with the Kubernetes Operator.",
                "Create an external service for the MongoDB Pods.",
                "Add Subject Alternate Names to your TLS (Transport Layer Security) certificates.",
                "Verify the external services.",
                "Open your replica set resource YAML (Yet Another Markup Language) file.",
                "Copy the sample replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Change the highlighted settings to your preferred values.",
                "Confirm the external hostnames and external service values in your replica set resource.",
                "Save your replica set config file.",
                "Update and restart your replica set deployment.",
                "Test the connection to the replica set.",
                "Deploy a replica set with the Kubernetes Operator.",
                "Configure services to ensure connectivity.",
                "Configure routes to ensure TLS (Transport Layer Security) terminination passthrough.",
                "Add Subject Alternate Names to your TLS (Transport Layer Security) certificates.",
                "Open your replica set resource YAML (Yet Another Markup Language) file.",
                "Configure your replica set resource YAML (Yet Another Markup Language) file.",
                "Change the settings to your preferred values.",
                "Save your replica set config file.",
                "Create the necessary TLS (Transport Layer Security) certificates and Kubernetes secrets.",
                "Update and restart your replica set deployment.",
                "Test the connection to the replica set.",
                "Deploy a sharded cluster with the Kubernetes Operator.",
                "Create an external service for the mongos Pods.",
                "Add Subject Alternate Names to your TLS (Transport Layer Security) certificates.",
                "Verify the external services.",
                "Test the connection to the sharded cluster."
            ],
            "paragraphs": "The following procedure describes how to connect to a MongoDB resource\ndeployed in  Kubernetes  from outside of the  Kubernetes  cluster. For your databases to be accessed outside of  Kubernetes , they must run\nMongoDB 4.2.3 or later. If you create custom services that require external access to MongoDB custom\nresources deployed by the  Kubernetes Operator  and use readiness probes\nin  Kubernetes , set the  publishNotReadyAddresses  setting in  Kubernetes  to  true . The  publishNotReadyAddresses  setting indicates that an agent that\ninteracts with endpoints for this service should disregard the service's\n ready \nstate. Setting  publishNotReadyAddresses  to  true  overrides the\nbehavior of the readiness probe configured for the Pod hosting your service. By default, the  publishNotReadyAddresses  setting is set to  false .\nIn this case, when the Pods that host the MongoDB custom resources in the\n Kubernetes Operator  lose connectivity to  Cloud Manager  or  Ops Manager , the\nreadiness probes configured for these Pods fail.\nHowever, when you set the   publishNotReadyAddresses  setting to  true : Kubernetes  does not shut down the service whose readiness probe fails. Kubernetes  considers all endpoints as  ready \neven if the probes for the Pods hosting the services for these endpoints\nindicate that they aren't ready. MongoDB custom resources are still available for read and write operations. Kubernetes API Reference  and search for  publishNotReadyAddresses DNS for Services in Pods Configure Readiness Probes The following procedure walks you through the process of configuring external\nconnectivity for your deployment by using the built-in configuration options in\nthe  Kubernetes Operator . How you connect to a MongoDB resource that the  Kubernetes Operator  deployed\nfrom outside of the  Kubernetes  cluster depends on the resource. To connect to your  Kubernetes Operator -deployed MongoDB\nstandalone resource from outside of the  Kubernetes  cluster: If you haven't deployed a standalone resource, follow the instructions to\n deploy one . This procedure uses the following example: To connect to your standalone resource from an external resource, configure the\n spec.externalAccess  setting: This setting instructs the  Kubernetes Operator  to create an external  LoadBalancer  service for the MongoDB Pod in your\nstandalone resource. The external service provides an entry point for external connections.\nAdding this setting with no values creates an external service with the following default\nvalues: Optionally, if you need to add values to the service or override the default values,\nspecify: For example, the following settings override the default values for the external service\nto configure your standalone resource to create  NodePort services  that expose the MongoDB Pod: Field Value Description Name <pod-name>-svc-external Name of the external service. You can't change this value. Type LoadBalancer Creates an external  LoadBalancer  service. Port <Port Number> A port for  mongod . publishNotReadyAddress true Specifies that  DNS records \nare created even if the Pod isn't ready.\nDo not set to  false  for any database Pod. Annotations specific to your cloud provider, in  spec.externalAccess.externalService.annotations Overrides for the service specification, in  spec.externalAccess.externalService.spec . To learn more, see\n Annotations \nand  ServiceSpec \nin the  Kubernetes  documentation. In your standalone resource, run the following command to verify that the\n Kubernetes Operator  created the external service for your deployment. The command returns a list of services similar to the following output.\nFor each database Pod in the cluster, the  Kubernetes Operator  creates an external service\nnamed <pod-name>-0-svc-external. This service is configured according to the values\nand overrides you provide in the  external service specification . Depending on your cluster configuration or cloud provider,\nthe IP address of the LoadBalancer service is an externally\naccessible IP address or  FQDN (fully qualified domain name) . You can use the IP address or  FQDN (fully qualified domain name) \nto route traffic from your external domain. To connect to your deployment from outside of the  Kubernetes  cluster,\nuse the MongoDB Shell ( mongosh ) and specify the MongoDB Pod address\nthat you've exposed through the external domain. If you have an external  FQDN (fully qualified domain name)  of  <my-standalone>.<external-domain> , you can\nconnect to this sharded cluster instance from outside of the  Kubernetes \ncluster by using the following command: To connect to your  Kubernetes Operator -deployed MongoDB replica\nset resource from outside of the  Kubernetes  cluster: This procedure explains the least complicated way to\nenable external connectivity. Other utilities can be\nused in production. If you haven't deployed a replica set, follow the instructions to\n deploy one . You must enable  TLS (Transport Layer Security)  for the replica set by providing a value for\nthe  spec.security.certsSecretPrefix  setting. The replica\nset must use a custom  CA (Certificate Authority)  certificate stored with\n spec.security.tls.ca . To connect to your replica set from an external resource, configure the\n spec.externalAccess  setting: This setting instructs the  Kubernetes Operator  to create an external  LoadBalancer  service for the MongoDB Pods in your\nreplica set. The external service provides an entry point for external connections.\nAdding this setting with no values creates an external service with the following default\nvalues: Optionally, if you need to add values to the service or override the default values,\nspecify: For example, the following settings override the default values for the external service\nto configure your replica set to create  NodePort services  that expose the MongoDB Pods: Field Value Description Name <pod-name>-svc-external Name of the external service. You can't change this value. Type LoadBalancer Creates an external  LoadBalancer  service. Port <Port Number> A port for  mongod . publishNotReadyAddress true Specifies that  DNS records \nare created even if the Pod isn't ready.\nDo not set to  false  for any database Pod. Annotations specific to your cloud provider, in  spec.externalAccess.externalService.annotations Overrides for the service specification, in  spec.externalAccess.externalService.spec . To learn more, see\n Annotations \nand  ServiceSpec \nin the  Kubernetes  documentation. Add each external  DNS (Domain Name System)  name to the certificate  SAN (Subject Alternative Name) . In your replica set, run the following command to verify that the\n Kubernetes Operator  created the external service for your deployment. The command returns a list of services similar to the following output.\nFor each database Pod in the cluster, the  Kubernetes Operator  creates an external service\nnamed <pod-name>-<pod-idx>-svc-external. This service is configured according to the values\nand overrides you provide in the  external service specification . Depending on your cluster configuration or cloud provider,\nthe IP address of the LoadBalancer service is an externally\naccessible IP address or  FQDN (fully qualified domain name) . You can use the IP address or  FQDN (fully qualified domain name) \nto route traffic from your external domain. Change the settings of this  YAML (Yet Another Markup Language)  file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the  object  specification\nat the end of your resource file in the  spec  section. Key Type Necessity Description Example collection Conditional Add this parameter and values if you need your database to be\naccessed outside of  Kubernetes . This setting allows you to provide\ndifferent  DNS (Domain Name System)  settings within the  Kubernetes  cluster and to the\n Kubernetes  cluster. The  Kubernetes Operator  uses split horizon  DNS (Domain Name System)  for\nreplica set members. This feature allows communication both\nwithin the  Kubernetes  cluster and from outside  Kubernetes . You may add multiple external mappings per host. Make sure that each value in this array is unique. Make sure that the number of entries in this array matches\nthe value given in  spec.members . Provide a value for the\n spec.security.certsSecretPrefix  setting to\nenable  TLS (Transport Layer Security) . This method to use split horizons requires the\nServer Name Indication extension of the  TLS (Transport Layer Security)  protocol. See Setting string Required Add the  <prefix>  of the secret\nname that contains your MongoDB deployment's  TLS (Transport Layer Security)  certificates. devDb Confirm that the external hostnames in the\n spec.connectivity.replicaSetHorizons  setting are correct. External hostnames should match the  DNS (Domain Name System)  names of  Kubernetes  worker nodes.\nThese can be  any  nodes in the  Kubernetes  cluster.  Kubernetes  nodes use internal\nrouting if the pod runs on another node. Set the ports in  spec.connectivity.replicaSetHorizons  to\nthe external service values. In any directory, invoke the following  Kubernetes  command to update and\nrestart your  replica set : In the development environment, for each host in a replica set, run\nthe following command: In production, for each host in a replica set, specify the  TLS (Transport Layer Security) \ncertificate and the  CA (Certificate Authority)  to securely connect to client tools or\napplications: If the connection succeeds, you should see: Don't use the  --sslAllowInvalidCertificates  flag in production. To connect to your  Kubernetes Operator -deployed MongoDB replica\nset resource from outside of the  Kubernetes  cluster with OpenShift: If you haven't deployed a replica set, follow the instructions to\n deploy one . You must enable  TLS (Transport Layer Security)  for the replica set by providing a value for\nthe  spec.security.certsSecretPrefix  setting. The replica\nset must use a custom  CA (Certificate Authority)  certificate stored with\n spec.security.tls.ca . Paste the following example services into a text editor: If the  spec.selector  has entries that target headless\nservices or applications, OpenShift may create a software\nfirewall rule explicitly dropping connectivity. Review the\nselectors carefully and consider targeting the stateful set pod\nmembers directly as seen in the example. Routes in OpenShift\noffer port 80 or port 443. This example service uses\nport 443. Change the settings to your preferred values. Save this file with a  .yaml  file extension. To create the services, invoke the following  kubectl  command\non the services file you created: Paste the following example routes into a text editor: To ensure the  TLS (Transport Layer Security)   SNI (Server Name Indication)  negotiation with  mongod  necessary\nfor  mongod  to respond with the correct horizon replica set\ntopology for the drivers to use, you must set  TLS (Transport Layer Security) \ntermination passthrough. Change the settings to your preferred values. Save this file with a  .yaml  file extension. To create the routes, invoke the following  kubectl  command on\nthe routes file you created: Add each external  DNS (Domain Name System)  name to the certificate  SAN (Subject Alternative Name) . Use the following example to edit your replica set resource  YAML (Yet Another Markup Language) \nfile: OpenShift clusters require localhost horizons if you intend to use\nthe  Kubernetes Operator  to create each  CSR (Certificate Signing Request) . If you manually create\nyour  TLS (Transport Layer Security)  certificates, ensure you include localhost in\nthe  SAN (Subject Alternative Name)  list. Key Type Necessity Description Example collection Conditional Add this parameter and values if you need your database to be\naccessed outside of  Kubernetes . This setting allows you to provide\ndifferent  DNS (Domain Name System)  settings within the  Kubernetes  cluster and to the\n Kubernetes  cluster. The  Kubernetes Operator  uses split horizon  DNS (Domain Name System)  for\nreplica set members. This feature allows communication both\nwithin the  Kubernetes  cluster and from outside  Kubernetes . You may add multiple external mappings per host. Make sure that each value in this array is unique. Make sure that the number of entries in this array matches\nthe value given in  spec.members . Provide a value for the\n spec.security.certsSecretPrefix  setting to\nenable  TLS (Transport Layer Security) . This method to use split horizons requires the\nServer Name Indication extension of the  TLS (Transport Layer Security)  protocol. See Setting string Required Add the  <prefix>  of the secret\nname that contains your MongoDB deployment's  TLS (Transport Layer Security)  certificates. devDb Configure TLS for your replica set . Create one secret for the MongoDB replica set\nand one for the certificate authority. The  Kubernetes Operator  uses these\nsecrets to place the  TLS (Transport Layer Security)  files in the pods for MongoDB to use. In any directory, invoke the following  Kubernetes  command to update and\nrestart your  replica set : The  Kubernetes Operator  should deploy the MongoDB replica set,\nconfigured with the horizon routes created for ingress. After\nthe  Kubernetes Operator  completes the deployment, you may connect with the\nhorizon using  TLS (Transport Layer Security)  connectivity.  If the certificate authority is\nnot present on your workstation, you can view and copy it from a\nMongoDB pod using the following command: To test the connections, run the following command: In production, for each host in a replica set, specify the  TLS (Transport Layer Security) \ncertificate and the  CA (Certificate Authority)  to securely connect to client tools or\napplications: If the connection succeeds, you should see: In the following example, for each member of the replica set, use\nyour replica set names and replace  {redacted}  with the domain\nthat you manage. Don't use the  --tlsAllowInvalidCertificates  flag in production. To connect to your  Kubernetes Operator -deployed MongoDB sharded\ncluster resource from outside of the  Kubernetes  cluster: If you haven't deployed a sharded cluster, follow the instructions to\n deploy one . You must enable  TLS (Transport Layer Security)  for the sharded cluster by configuring the\nfollowing settings: Key Type Necessity Description Example string Required Add the  <prefix>  of the secret\nname that contains your MongoDB deployment's  TLS (Transport Layer Security)  certificates. devDb collection Optional List of every domain that should be added to  TLS (Transport Layer Security)  certificates\nto each pod in this deployment. When you set this parameter,\nevery  CSR (Certificate Signing Request)  that the  Kubernetes Operator  transforms into a  TLS (Transport Layer Security) \ncertificate includes a  SAN (Subject Alternative Name)  in the form  <pod\nname>.<additional cert domain> . example.com To connect to your sharded cluster from an external resource, configure the\n spec.externalAccess  setting: This setting instructs the  Kubernetes Operator  to create an external  LoadBalancer  service for the  mongos  Pods in your\nsharded cluster. The external service provides an entry point for external connections.\nAdding this setting with no values creates an external service with the following default\nvalues: Optionally, if you need to add values to the service or override the default values,\nspecify: For example, the following settings override the default values for the external service\nto configure your sharded cluster to create  NodePort services  that expose the  mongos  Pods: Field Value Description Name <pod-name>-svc-external Name of the external service. You can't change this value. Type LoadBalancer Creates an external  LoadBalancer  service. Port <Port Number> A port for  mongod . publishNotReadyAddress true Specifies that  DNS records \nare created even if the Pod isn't ready.\nDo not set to  false  for any database Pod. Annotations specific to your cloud provider, in  spec.externalAccess.externalService.annotations Overrides for the service specification, in  spec.externalAccess.externalService.spec . To learn more, see\n Annotations \nand  ServiceSpec \nin the  Kubernetes  documentation. Add each external  DNS (Domain Name System)  name to the certificate  SAN (Subject Alternative Name) . Each MongoDB host uses the following  SAN (Subject Alternative Name) s: The  mongos  instance uses the following  SAN (Subject Alternative Name) : Configure the  spec.security.tls.additionalCertificateDomains  setting similar\nto the following example. Each  TLS (Transport Layer Security)  certificate that you use must include the\ncorresponding  SAN (Subject Alternative Name)  for the shard, config server, or  mongos  instance.\nThe  Kubernetes Operator  validates your configuration. In your sharded cluster, run the following command to verify that\nthe  Kubernetes Operator  created the external services for your deployment. The command returns a list of services similar to the following output.\nFor each  mongos  instance in the cluster, the  Kubernetes Operator  creates an external service\nnamed  <pod-name>-<pod-idx>-svc-external . This service is configured according to the values\nand overrides you provide in the  external service specification . Depending on your cluster configuration or cloud provider, the IP address of the\nLoadBalancer service is an externally accessible IP address or  FQDN (fully qualified domain name) . You can use\nthe IP address or  FQDN (fully qualified domain name)  to route traffic from your external domain. This example\nhas two  mongos  instances, therefore the  Kubernetes Operator  creates two external services. To connect to your deployment from outside of the  Kubernetes  cluster,\nuse the MongoDB Shell ( mongosh ) and specify the addresses for the  mongos  instances\nthat you've exposed through the external domain. If you have external  FQDN (fully qualified domain name)  of  <my-sharded-cluster>-mongos-0-svc-external.<external-domain>  and  <my-sharded-cluster>-mongos-1-svc-external.<external-domain>  addressCommand: mongodb://<my-sharded-cluster>-mongos-0-svc-external.<external-domain>,<my-sharded-cluster>-mongos-1-svc-external.<external-domain>, you can\nconnect to this sharded cluster instance from outside of the  Kubernetes \ncluster by using the following command:",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\nspec:\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: Standalone\n..."
                },
                {
                    "lang": "yaml",
                    "value": "externalAccess: {}"
                },
                {
                    "lang": "yaml",
                    "value": "externalAccess:\n  externalService:\n    annotations:\n      # cloud-specific annotations for the service\n    spec:\n      type: NodePort # default is LoadBalancer\n      port: 27017\n      # you can specify other spec overrides if necessary"
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl get services"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                  TYPE         CLUSTER-IP   EXTERNAL-IP       PORT(S)           AGE\n<my-standalone>-0-svc-external   LoadBalancer   10.102.27.116    <lb-ip-or-fqdn>   27017:27017/TCP    8m30s"
                },
                {
                    "lang": "sh",
                    "value": "mongosh \"mongodb://<my-standalone>.<external-domain>\""
                },
                {
                    "lang": "yaml",
                    "value": "externalAccess: {}"
                },
                {
                    "lang": "yaml",
                    "value": "externalAccess:\n  externalService:\n    annotations:\n      # cloud-specific annotations for the service\n    spec:\n      type: NodePort # default is LoadBalancer\n      port: 27017\n      # you can specify other spec overrides if necessary"
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl get services"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                  TYPE         CLUSTER-IP   EXTERNAL-IP       PORT(S)           AGE\n<my-replica-set>-0-svc-external   LoadBalancer   10.102.27.116    <lb-ip-or-fqdn>   27017:27017/TCP    8m30s"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  type: ReplicaSet\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:30907\"\n      - \"example-website\": \"web2.example.com:32350\"\n      - \"example-website\": \"web3.example.com:31185\"\n..."
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:30907\"\n      - \"example-website\": \"web2.example.com:32350\"\n      - \"example-website\": \"web3.example.com:31185\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host <my-replica-set>/web1.example.com \\\n      --port 30907\n      --ssl \\\n      --sslAllowInvalidCertificates"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host <my-replica-set>/web1.example.com \\\n  --port 30907 \\\n  --tls \\\n  --tlsCertificateKeyFile server.pem \\\n  --tlsCAFile ca-pem"
                },
                {
                    "lang": "javascript",
                    "value": "Enterprise <my-replica-set> [primary]"
                },
                {
                    "lang": "",
                    "value": "---\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-external-0\nspec:\n  ports:\n    - name: mongodb\n      protocol: TCP\n      port: 443\n      targetPort: 27017\n  selector:\n    statefulset.kubernetes.io/pod-name: my-external-0\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-external-1\nspec:\n  ports:\n    - name: mongodb\n      protocol: TCP\n      port: 443\n      targetPort: 27017\n  selector:\n    statefulset.kubernetes.io/pod-name: my-external-1\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-external-2\nspec:\n  ports:\n    - name: mongodb\n      protocol: TCP\n      port: 443\n      targetPort: 27017\n  selector:\n    statefulset.kubernetes.io/pod-name: my-external-2\n\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <my-external-services>.yaml"
                },
                {
                    "lang": "",
                    "value": "---\napiVersion: v1\nkind: Route\nmetadata:\n  name: my-external-0\nspec:\n  host: my-external-0.{redacted}\n  to:\n    kind: Service\n    name: my-external-0\n  tls:\n    termination: passthrough\n---\napiVersion: v1\nkind: Route\nmetadata:\n  name: my-external-1\nspec:\n  host: my-external-1.{redacted}\n  to:\n    kind: Service\n    name: my-external-1\n  tls:\n    termination: passthrough\n---\napiVersion: v1\nkind: Route\nmetadata:\n  name: my-external-2\nspec:\n  host: my-external-2.{redacted}\n  to:\n    kind: Service\n    name: my-external-2\n  tls:\n    termination: passthrough\n\n...\n "
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <my-external-routes>.yaml"
                },
                {
                    "lang": "",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-external\n  namespace: mongodb\nspec:\n  type: ReplicaSet\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: {redacted}\n  credentials: {redacted}\n  persistent: false\n  security:\n    tls:\n      # TLS must be enabled to allow external connectivity\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"SCRAM\",\"X509\"]\n  connectivity:\n    # The \"localhost\" routes are included to enable the creation of localhost\n    # TLS SAN in the CSR, per OpenShift route requirements.\n    # \"ocroute\" is the configured route in OpenShift.\n    replicaSetHorizons:\n      - \"ocroute\": \"my-external-0.{redacted}:443\"\n        \"localhost\": \"localhost:27017\"\n      - \"ocroute\": \"my-external-1.{redacted}:443\"\n        \"localhost\": \"localhost:27018\"\n      - \"ocroute\": \"my-external-2.{redacted}:443\"\n        \"localhost\": \"localhost:27019\"\n\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "oc exec -it my-external-0 -- cat /mongodb-automation/ca.pem"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host my-external/my-external-0.{redacted} \\\n      --port 443\n      --ssl \\\n      --tlsAllowInvalidCertificates"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host my-external/my-external-0.{redacted} \\\n  --port 443 \\\n  --tls \\\n  --tlsCertificateKeyFile server.pem \\\n  --tlsCAFile ca-pem"
                },
                {
                    "lang": "javascript",
                    "value": "Enterprise <my-replica-set> [primary]"
                },
                {
                    "lang": "yaml",
                    "value": "externalAccess: {}"
                },
                {
                    "lang": "yaml",
                    "value": "externalAccess:\n  externalService:\n    annotations:\n      # cloud-specific annotations for the service\n    spec:\n      type: NodePort # default is LoadBalancer\n      port: 27017\n      # you can specify other spec overrides if necessary"
                },
                {
                    "lang": "sh",
                    "value": "<my-sharded-cluster>-<shard>-<pod-index>.<external-domain>\n<my-sharded-cluster>-config-<pod-index>.<external-domain>\n<my-sharded-cluster>-mongos-<pod-index>.<external-domain>"
                },
                {
                    "lang": "sh",
                    "value": "<my-sharded-cluster>-mongos-<pod-index>-svc-external.<external-domain>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  credentials: my-secret\n  type: ShardedCluster\n  externalAccess: {}\n  security:\n    tls:\n      certsSecretPrefix: <prefix>\n      additionalCertificateDomains:\n         - \"<external-domain>\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl get services"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                              TYPE         CLUSTER-IP     EXTERNAL-IP       PORT(S)           AGE\n<my-sharded-cluster>-mongos-0-svc-external    LoadBalancer   10.102.27.116  <lb-ip-or-fqdn>   27017:27017/TCP    8m30s\n<my-sharded-cluster>-mongos-1-svc-external    LoadBalancer   10.102.27.116  <lb-ip-or-fqdn>   27017:27017/TCP    8m30s"
                },
                {
                    "lang": "sh",
                    "value": "mongosh \"\""
                }
            ],
            "preview": "The following procedure describes how to connect to a MongoDB resource\ndeployed in Kubernetes from outside of the Kubernetes cluster.",
            "tags": "split-horizon DNS",
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/plan-k8s-install-single-or-multi-clusters",
            "title": "Choose Kubernetes Operator Installation Mode: Single- or Multi-Kubernetes Clusters",
            "headings": [
                "Kubernetes Operator Watches Single-Kubernetes Cluster Resources",
                "Kubernetes Operator Watches Multi-Kubernetes Cluster Resources",
                "Kubernetes Operator Watches Resources in a Single- and Multi-Kubernetes Cluster",
                "Next Steps"
            ],
            "paragraphs": "The  Kubernetes Operator  can manage custom resources for single- and multi- Kubernetes \nclusters. Before you install the  Kubernetes Operator , decide which type of\n Kubernetes  cluster deployment you want to support, single- or multi- Kubernetes  cluster. You can configure the  Kubernetes Operator  to watch  Ops Manager  resources  and  MongoDB  resources \nfor a replica set or a sharded cluster in a single  Kubernetes  cluster.\nFor steps, see  Install the Operator . You can configure the  Kubernetes Operator  to watch  Ops Manager  resources ,  MongoDB  resources ,\nand  MongoDBMultiCluster  resources  for a replica set in a multi- Kubernetes  cluster. For steps,\nsee  Multi-Kubernetes-Cluster Quick Start . You can configure the  Kubernetes Operator  to watch the following types of  CustomResourceDefinitions : Depending on the watched resources, the  Kubernetes Operator  reconciles resources\nbased on the given  CustomResourceDefinition . To support custom resources deployed in single- and multi- Kubernetes  clusters,\nset up one instance of the  Kubernetes Operator  that will watch for and reconcile\ncustom resources for a single  Kubernetes  cluster and a  multi-Kubernetes-cluster deployment .\nUse different non-overlapping subsets of namespaces for each type of resource. Set  .Values.operator.watchedResources  as follows: MongoDB  resources Ops Manager  resources MongoDBUsers  resources MongoDBMultiCluster  resources Install and set up a single  Kubernetes Operator  instance and configure it\nto watch one, many, or all custom resources in different, non-overlapping\nsubsets of namespaces. See also  Does MongoDB support running more than one  Kubernetes Operator  instance? After deciding how you want to install the  Kubernetes Operator , you can: Set the  scope of your deployments  for\nsingle  Kubernetes  clusters, or  set the multi-Kubernetes cluster deployment's scope . Install single- Kubernetes  cluster. For single- Kubernetes  clusters, review the\n considerations , complete the  prerequisites \nand  install the Kubernetes Operator . Install the  Kubernetes Operator  in a  multi-Kubernetes-cluster deployment . See the  Multi-Kubernetes-Cluster Quick Start .",
            "code": [
                {
                    "lang": "yaml",
                    "value": "-watch-resource=MongoDB \\\n-watch-resource=OpsMnagers \\\n-watch-resource=MongoDBusers \\\n-watch-resource=MongoDBMultiCluster"
                }
            ],
            "preview": "The Kubernetes Operator can manage custom resources for single- and multi-Kubernetes\nclusters. Before you install the Kubernetes Operator, decide which type of\nKubernetes cluster deployment you want to support, single- or multi-Kubernetes cluster.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/back-up-mdb-resources",
            "title": "Configure MongoDB Database Backups",
            "headings": [
                "Considerations",
                "Prerequisites",
                "Procedure",
                "Enable backups for your MongoDB database deployment.",
                "Optional: Set the snapshot schedule.",
                "Optional: Set the backup assignment labels.",
                "Check that the backup status is STARTED."
            ],
            "paragraphs": "You can configure continuous, automated backups for the MongoDB\ndatabases that the  Kubernetes Operator  manages using the MongoDB custom\nresource. If you already configured backups for your MongoDB resources during\n MongoDB database resource deployment , you don't need to complete the\nfollowing procedure. Before you configure backups for MongoDB resources, see the following\nconsiderations: If you set assignment labels using the  Kubernetes Operator , the values that\nyou set in the  Kubernetes  configuration file for assignment labels override\nthe values defined in the  Ops Manager  UI. Assignment labels that you don't set\nusing the  Kubernetes Operator  continue to use the values set in the  Ops Manager  UI. If you set a snapshot schedule field using the  Kubernetes Operator , the value\nthat you set in the  Kubernetes  configuration file for that field overrides\nthe value defined in the  Ops Manager  UI. Snapshot schedule fields that you\ndon't set using the  Kubernetes Operator  continue to use the value set in the\n Ops Manager  UI. Ops Manager  uses the following values for your deployment: You set  backup.snapshotSchedule.snapshotIntervalHours=6  in\nthe  Kubernetes Operator . You set the following values in the UI: Snapshot Inverval: 10 Snapshot Retention Days: 5 Snapshot Inverval: 6 Snapshot Retention Days: 5 If you enable backups for your MongoDB database deployment using the\n Kubernetes Operator , but you don't set a snapshot schedule using the\n Kubernetes Operator ,  Ops Manager  uses the snapshot schedule you set in the  Ops Manager  UI. If you enable backups for your MongoDB deployment, but you don't set\na snapshot schedule at all,  Ops Manager  uses the default snapshot schedule. Before you configure continuous backups for MongoDB resources, complete\nthe following tasks: Install the Kubernetes Operator . Deploy the Ops Manager application . Configure backups for the Ops Manager resource . In the linked procedure, see the\nsteps for configuring backups. Continuous backups for MongoDB\ndatabases require that you set the  spec.backup.enabled \nvalue in the  Ops Manager   resource specification  to  true . Deploy a  replica set  or a\n sharded cluster . Add the  spec.backup.mode  setting to the config file for\nyour MongoDB database deployment and set its value to  enabled  as\nshown in the following replica set example: To learn more about creating or editing a config file, see\n deploy a replica set  or\n deploy a sharded cluster . Add any of the following snapshot schedule settings to the specification\nfile for the deployment. To learn how  Ops Manager  determines the snapshot\nschedule if you don't set a snapshot schedule field using the  Kubernetes Operator ,]\nsee the  considerations . spec.backup.snapshotSchedule.snapshotIntervalHours spec.backup.snapshotSchedule.snapshotRetentionDays spec.backup.snapshotSchedule.dailySnapshotRetentionDays spec.backup.snapshotSchedule.weeklySnapshotRetentionWeeks spec.backup.snapshotSchedule.monthlySnapshotRetentionMonths spec.backup.snapshotSchedule.pointInTimeWindowHours spec.backup.snapshotSchedule.referenceHourOfDay spec.backup.snapshotSchedule.referenceMinuteOfHour spec.backup.snapshotSchedule.fullIncrementalDayOfWeek spec.backup.snapshotSchedule.clusterCheckpointIntervalMin Add one or more  spec.backup.assignmentLabels  to the specification\nfile for the deployment. Use assignment labels to identify that specific\nbackup stores are associated with particular projects. Setting labels\nin  Kubernetes Operator  overrides labels that you set in  Ops Manager . To learn more,\nsee  considerations . See the example of the specification file with assignment labels earlier\nin this procedure. Run the following command to check the status of the backups: The  status.backup.statusname  field indicates the status of the\nbackup. The status displays  STARTED  when you successfully\nconfigure backups.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.4.0-ent\"\n  type: ReplicaSet\n  opsManager:\n    configMapRef:\n      name: <my-project>\n  credentials: <my-credentials>\n  backup:\n                      # Sets labels for the Backup Daemon.\n    assignmentLabels: [\"test1\", \"test2\"]\n    mode: enabled"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <metadata.namespace> -o yaml"
                }
            ],
            "preview": "You can configure continuous, automated backups for the MongoDB\ndatabases that the Kubernetes Operator manages using the MongoDB custom\nresource.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/secure-ldap-auth",
            "title": "Secure Client Authentication with LDAP",
            "headings": [
                "Considerations",
                "General Prerequisites",
                "Configure LDAP Client Authentication for a Replica Set",
                "Copy the sample replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the LDAP settings for your replica set resource.",
                "Configure the LDAP settings for the MongoDB Agent.",
                "Save your replica set config file.",
                "Apply your changes to your replica set deployment.",
                "Track the status of your deployment.",
                "Configure LDAP Client Authentication for a Sharded Cluster",
                "Copy the sample sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the LDAP settings for your sharded cluster resource.",
                "Configure the LDAP settings for the MongoDB Agent.",
                "Save your sharded cluster config file.",
                "Apply your changes to your sharded cluster deployment.",
                "Track the status of your deployment."
            ],
            "paragraphs": "You can use the  Kubernetes Operator  to configure LDAP to authenticate your\nclient applications that connect to your MongoDB deployments. This guide\ndescribes how to configure LDAP authentication from client applications\nto your MongoDB deployments. MongoDB Enterprise \nsupports: To learn more, see the  LDAP Proxy Authentication \nand  LDAP Authorization  sections\nin the MongoDB Server documentation. Proxying authentication requests to a Lightweight Directory Access\nProtocol (LDAP) service. Simple and SASL binding to LDAP servers. MongoDB Enterprise can bind\nto an LDAP server via  saslauthd  or through the operating system\nlibraries. To configure  LDAP (Lightweight Directory Access Protocol)  in  CustomResourceDefinitions , use the parameters under the\n spec.security.authentication.ldap  and other\n security LDAP settings  specific to the\nMongoDB Agent, from the  Kubernetes Operator  MongoDB resource specification.\nThe procedures in this section describe the required settings and\nprovide examples of LDAP configuration. To improve security, consider deploying a\n TLS-encrypted replica set  or a\n TLS-encrypted sharded cluster .\nEncryption with  TLS (Transport Layer Security)  is optional. By default,  LDAP (Lightweight Directory Access Protocol)  traffic is sent\nas plain text. This means that username and password are exposed to\nnetwork threats. Many modern directory services, such as Microsoft\nActive Directory, require encrypted connections. Consider using\n LDAP (Lightweight Directory Access Protocol)  over  TLS (Transport Layer Security) / SSL (Secure Sockets Layer)  to encrypt authentication requests in your\n Kubernetes Operator  MongoDB deployments. Before you configure LDAP authentication for your MongoDB deployments,\ncomplete the following tasks: Ensure that you deploy the MongoDB Enterprise database resource.\nMongoDB Community databases don't support LDAP authentication. Deploy the replica set  or\n deploy the sharded cluster \nwhose client authentication you want to secure with  LDAP (Lightweight Directory Access Protocol) . Change the settings of this  YAML (Yet Another Markup Language)  file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the  object  specification\nat the end of your resource file in the  spec  section. To enable  LDAP (Lightweight Directory Access Protocol)  in your deployment, configure the following\nsettings in your  Kubernetes  object: The resulting configuration may look similar to the following\nexample: For a full list of LDAP settings, see  security settings  in the  Kubernetes Operator  MongoDB resource specification.\nAlso see the  spec.security.authentication.agents.automationUserName \nsetting for the MongoDB Agent user in your LDAP-enabled  Kubernetes Operator \ndeployment. Key Type and necessity Description Example Set to  true  to enable LDAP authentication. true Specify the LDAP Distinguished Name to which MongoDB binds when\nconnecting to the LDAP server. cn=admin,dc=example,dc=org Specify the name of the  secret  that contains the\nLDAP Bind Distinguished Name's password with which MongoDB binds\nwhen connecting to an LDAP server. <secret-name> Add the  ConfigMap 's name that stores the custom  CA (Certificate Authority) \nthat you used to sign your deployment's  TLS (Transport Layer Security)  certificates. <configmap-name> Add the field name that stores the  CA (Certificate Authority)  which validates the\nLDAP server's  TLS (Transport Layer Security)  certificate. <configmap-key> Specify the list of  hostname:port  combinations of one or more\nLDAP servers. For each server, use a separate line. <example.com:636> Set to  tls  to use LDAPS (LDAP over  TLS (Transport Layer Security) ). Leave blank if\nyour LDAP server doesn't accept TLS. You must enable TLS when you\ndeploy the database resource to use this setting. tls Specify the mapping that maps the username provided to\n mongod  or  mongos  for authentication\nto an LDAP Distinguished Name (DN). To learn more, see  security.ldap.userToDNMapping \nand  LDAP Query Templates  in the\nMongoDB Server documentation. <match: \"(.+)\",substitution: \"uid={0},ou=groups,dc=example,dc=org\"> Set to  LDAP  to enable authentication through LDAP. LDAP Update your MongoDB resource \nwith  security settings  specific to the Agent,\nfrom the  Kubernetes Operator  MongoDB resource specification. The resulting\nconfiguration may look similar to the following example: Invoke the following  Kubernetes  command to update your\n replica set : To check the status of your  MongoDB  resource , use the following\ncommand: With the  -w  (watch) flag set, when the configuration changes, the output\nrefreshes immediately until the status phase achieves the  Running  state.\nTo learn more about resource deployment statuses, see  Troubleshoot the  Kubernetes Operator . Change the settings of this  YAML (Yet Another Markup Language)  file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the  object  specification\nat the end of your resource file in the  spec  section. To enable  LDAP (Lightweight Directory Access Protocol)  in your deployment, configure the following\nsettings in your  Kubernetes  object: The resulting configuration may look similar to the following\nexample: For a full list of LDAP settings, see  security settings  in the  Kubernetes Operator  MongoDB resource specification.\nAlso see the  spec.security.authentication.agents.automationUserName \nsetting for the MongoDB Agent user in your LDAP-enabled  Kubernetes Operator \ndeployment. Key Type and necessity Description Example Set to  true  to enable LDAP authentication. true Specify the LDAP Distinguished Name to which MongoDB binds when\nconnecting to the LDAP server. cn=admin,dc=example,dc=org Specify the name of the  secret  that contains the\nLDAP Bind Distinguished Name's password with which MongoDB binds\nwhen connecting to an LDAP server. <secret-name> Add the  ConfigMap 's name that stores the custom  CA (Certificate Authority) \nthat you used to sign your deployment's  TLS (Transport Layer Security)  certificates. <configmap-name> Add the field name that stores the  CA (Certificate Authority)  which validates the\nLDAP server's  TLS (Transport Layer Security)  certificate. <configmap-key> Specify the list of  hostname:port  combinations of one or more\nLDAP servers. For each server, use a separate line. <example.com:636> Set to  tls  to use LDAPS (LDAP over  TLS (Transport Layer Security) ). Leave blank if\nyour LDAP server doesn't accept TLS. You must enable TLS when you\ndeploy the database resource to use this setting. tls Specify the mapping that maps the username provided to\n mongod  or  mongos  for authentication\nto an LDAP Distinguished Name (DN). To learn more, see  security.ldap.userToDNMapping \nand  LDAP Query Templates  in the\nMongoDB Server documentation. <match: \"(.+)\",substitution: \"uid={0},ou=groups,dc=example,dc=org\"> Set to  LDAP  to enable authentication through LDAP. LDAP Update your MongoDB resource \nwith  security settings  specific to the Agent,\nfrom the  Kubernetes Operator  MongoDB resource specification. The resulting\nconfiguration may look similar to the following example: Invoke the following  Kubernetes  command to update your\n sharded cluster : To check the status of your  MongoDB  resource , use the following\ncommand: With the  -w  (watch) flag set, when the configuration changes, the output\nrefreshes immediately until the status phase achieves the  Running  state.\nTo learn more about resource deployment statuses, see  Troubleshoot the  Kubernetes Operator .",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n            # Must match metadata.name in ConfigMap file\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      ca: <custom-ca>\n    certsSecretPrefix: <prefix>\n..."
                },
                {
                    "lang": "yaml",
                    "value": "security:\n authentication:\n   enabled: true\n   # Enabled LDAP Authentication Mode\n   modes:\n     - \"LDAP\"\n     - \"SCRAM\"\n     # LDAP related configuration\n   ldap:\n   # Specify the hostname:port combination of one or\n   # more LDAP servers\n     servers:\n       - \"ldap1.example.com:636\"\n       - \"ldap2.example.com:636\"\n\n   # Set to \"tls\" to use LDAP over TLS. Leave blank if\n   # the LDAP server doesn't accept TLS. You must enable TLS when you deploy the database resource to use this setting.\n   transportSecurity: \"tls\"\n\n   # If TLS is enabled, add a reference to a ConfigMap that\n   # contains a CA certificate that validates the LDAP server's\n   # TLS certificate.\n   caConfigMapRef:\n     name: \"<configmap-name>\"\n     key: \"<configmap-entry-key>\"\n\n   # Specify the LDAP Distinguished Name to which\n   # MongoDB binds when connecting to the LDAP server\n   bindQueryUser: \"cn=admin,dc=example,dc=org\"\n\n   # Specify the password with which MongoDB binds\n   # when connecting to an LDAP server. This is a\n   # reference to a Secret Kubernetes Object containing\n   # one \"password\" key.\n   bindQueryPasswordSecretRef:\n     name: \"<secret-name>\""
                },
                {
                    "lang": "yaml",
                    "value": "security:\n  authentication:\n    agents:\n      automationPasswordSecretRef:\n        key: automationConfigPassword\n        name: automation-config-password\n      automationUserName: mms-automation-agent\n      clientCertificateSecretRef:\n        name: agent-client-cert\n      mode: LDAP\n    enabled: true\n    ldap:\n      bindQueryPasswordSecretRef:\n        name: bind-query-password\n      bindQueryUser: cn=admin,dc=example,dc=org\n      servers:\n        - openldap.namespace.svc.cluster.local:389\n      userToDNMapping: '[{match: \"(.+)\",substitution: \"uid={0},ou=groups,dc=example,dc=org\"}]'\n    modes:\n      - LDAP\n      - SCRAM\n    requireClientTLSAuthentication: false"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      ca: <custom-ca>\n    certsSecretPrefix: <prefix>\n..."
                },
                {
                    "lang": "yaml",
                    "value": "security:\n authentication:\n   enabled: true\n   # Enabled LDAP Authentication Mode\n   modes:\n     - \"LDAP\"\n     - \"SCRAM\"\n     # LDAP related configuration\n   ldap:\n   # Specify the hostname:port combination of one or\n   # more LDAP servers\n     servers:\n       - \"ldap1.example.com:636\"\n       - \"ldap2.example.com:636\"\n\n   # Set to \"tls\" to use LDAP over TLS. Leave blank if\n   # the LDAP server doesn't accept TLS. You must enable TLS when you deploy the database resource to use this setting.\n   transportSecurity: \"tls\"\n\n   # If TLS is enabled, add a reference to a ConfigMap that\n   # contains a CA certificate that validates the LDAP server's\n   # TLS certificate.\n   caConfigMapRef:\n     name: \"<configmap-name>\"\n     key: \"<configmap-entry-key>\"\n\n   # Specify the LDAP Distinguished Name to which\n   # MongoDB binds when connecting to the LDAP server\n   bindQueryUser: \"cn=admin,dc=example,dc=org\"\n\n   # Specify the password with which MongoDB binds\n   # when connecting to an LDAP server. This is a\n   # reference to a Secret Kubernetes Object containing\n   # one \"password\" key.\n   bindQueryPasswordSecretRef:\n     name: \"<secret-name>\""
                },
                {
                    "lang": "yaml",
                    "value": "security:\n  authentication:\n    agents:\n      automationPasswordSecretRef:\n        key: automationConfigPassword\n        name: automation-config-password\n      automationUserName: mms-automation-agent\n      clientCertificateSecretRef:\n        name: agent-client-cert\n      mode: LDAP\n    enabled: true\n    ldap:\n      bindQueryPasswordSecretRef:\n        name: bind-query-password\n      bindQueryUser: cn=admin,dc=example,dc=org\n      servers:\n        - openldap.namespace.svc.cluster.local:389\n      userToDNMapping: '[{match: \"(.+)\",substitution: \"uid={0},ou=groups,dc=example,dc=org\"}]'\n    modes:\n      - LDAP\n      - SCRAM\n    requireClientTLSAuthentication: false"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "You can use the Kubernetes Operator to configure LDAP to authenticate your\nclient applications that connect to your MongoDB deployments. This guide\ndescribes how to configure LDAP authentication from client applications\nto your MongoDB deployments.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/upgrade-mdb-version",
            "title": "Upgrade MongoDB Version and FCV",
            "headings": [
                "Overview",
                "Procedure",
                "Change the settings in the MongoDB Database Resource Specification as shown in the following example:",
                "Reapply the configuration to Kubernetes."
            ],
            "paragraphs": "You can upgrade the major, minor, or feature compatibility versions of\nyour MongoDB resource. Configure these settings in your  MongoDB Database Resource Specification . To upgrade your resource's major or minor versions, set the\n spec.version  setting to the desired MongoDB version. To modify your resource's\n feature compatibility version ,\nset the  spec.featureCompatibilityVersion  setting to the desired\nversion. If you update  spec.version  to a later version, consider setting\n spec.featureCompatibilityVersion  to the current working\nMongoDB version to give yourself the option to downgrade if\nnecessary. To learn more about feature compatibility, see\n setFeatureCompatibilityVersion  in the MongoDB Server\nDocumentation. To upgrade the standalone deployment's MongoDB version from  4.2.2-ent \nto  4.4.18-ent , complete the steps in the following syntactic example. If you update  spec.version  to a later version without setting the\n spec.featureCompatibilityVersion  to any value, the Feature\nCompatibility Version (FCV) upgrades to the  same version  that you specify\nin  spec.version . However, you can explicitly specify a previous\nversion for the FCV. The following example illustrates this use case.\nIt sets  spec.version  to  4.4.18-ent  and\n spec.featureCompatibilityVersion  to  4.2 . The following example shows the result of this change: Set  spec.version  to the desired MongoDB version. Set  spec.featureCompatibilityVersion  to the current\nworking MongoDB version: Kubernetes  automatically reconfigures your deployment with the new\nspecifications. You can see these changes reflected in your  Ops Manager  or\n Cloud Manager  application.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n name: my-standalone-downgrade\nspec:\n version: \"4.4.18-ent\"\n featureCompatibilityVersion: \"4.2\"\n type: Standalone\n project: my-project\n credentials: my-credentials\n persistent: false\n..."
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  version: \"4.4.18-ent\"\n  featureCompatibilityVersion: \"4.2\""
                },
                {
                    "lang": "none",
                    "value": "kubectl apply -f <standalone-config>.yaml"
                }
            ],
            "preview": "You can upgrade the major, minor, or feature compatibility versions of\nyour MongoDB resource. Configure these settings in your MongoDB Database Resource Specification.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/create-x509-client-certs",
            "title": "Generate X.509 Client Certificates",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Generate an X.509 client certificate.",
                "Configure kubectl to default to your namespace.",
                "Copy and save the following example ConfigMap.",
                "Create the X.509 MongoDB user.",
                "Verify your newly created user",
                "Find the mount location of the CA (Certificate Authority).",
                "Use your X.509 user to connect to the MongoDB deployment"
            ],
            "paragraphs": "The  MongoDB Enterprise Kubernetes Operator  can deploy MongoDB instances with\n X.509 authentication \nenabled. If X.509 authentication has been enabled for the deployment,\nyou must generate and use an X.509 certificate to connect to the\ndeployment. This new client certificate must be signed by the\nsame  CA (Certificate Authority)  that signs the server certificates for the MongoDB\ndeployment to accept it. Use the procedure outlined in this document to use an X.509 certificate\nto connect to your X.509-enabled MongoDB deployment. If you're using  HashiCorp Vault  as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. To automate certificate renewal for  Ops Manager  deployments, consider setting up the  cert-manager integration . A full description of Transport Layer Security (TLS), Public Key Infrastructure (PKI)\ncertificates, and Certificate Authorities is beyond the scope of this\ndocument. This page assumes prior knowledge of  TLS (Transport Layer Security)  and\nX.509 authentication. To complete this tutorial, you must have the  MongoDB Enterprise Kubernetes Operator \ninstalled. For instructions on installing the  Kubernetes Operator ,\nsee  Install the  MongoDB Enterprise Kubernetes Operator . This tutorial assumes you have a MongoDB deployment which\nrequires X.509 authentication. For instructions on deploying\nMongoDB resources, see  Deploy a MongoDB Database Resource . First create the client certificate. Then create a MongoDB user\nand connect to the X.509-enabled deployment. For production use, your MongoDB deployment should use valid\ncertificates generated and signed by a  CA (Certificate Authority) . You or your\norganization can generate and maintain an independent  CA (Certificate Authority)  using\n Kubernetes -native tools such as\n cert-manager . Obtaining and managing certificates is beyond the scope of this\ndocumentation. To learn about the properties that your client certificates must have,\nsee  Client Certificate Requirements \nin the MongoDB Manual. You must concatenate your client's  TLS (Transport Layer Security)  certificate and the\ncertificate's key in a  .pem  file. You must present this\n .pem  file when you connect to your X.509-enabled MongoDB\ndeployment. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Save the following ConfigMap as  x509-mongodb-user.yaml : This ConfigMap  .yaml  file describes a  MongoDBUser  custom object. You\ncan use these custom objects to create MongoDB users. To learn more, see  MongoDB User Resource Specification . In this example, the ConfigMap describes the user as an X.509\nuser that the client can use to connect to MongoDB with the\ncorresponding X.509 certificate. Run the following command to apply the ConfigMap and create the\nX.509 MongoDB user: You should see an output similar to the following: Run the following command to check the state of the  new-x509-user : You should see an output similar to the following: Run the following command to find where in each pod the  Kubernetes Operator \nmounted the  CA (Certificate Authority)  secret: In the output, find the  secret-ca  mount: In the following step when you connect to your database deployment,\nappend  secret-ca  to the  mountPath , which forms the full path: Once you have created your X.509 user, try to connect to the\ndeployment using the MongoDB Shell ( mongosh ):",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "none",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: new-x509-user\nspec:\n  username: \"CN=my-x509-authenticated-user,OU=organizationalunit,O=organization\"\n  db: \"$external\"\n  mongodbResourceRef:\n    name: '<name of the MongoDB resource>'\n  roles:\n    - db: \"admin\"\n      name: \"readWriteAnyDatabase\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f x509-mongodb-user.yaml"
                },
                {
                    "lang": "sh",
                    "value": "mongodbuser.mongodb.com/new-x509-user created"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdbu/new-x509-user -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "NAME            CREATED AT\nnew-x509-user   8m"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulset <metadata.name> -o yaml"
                },
                {
                    "lang": "yaml",
                    "value": "volumeMounts:\n  - mountPath: /opt/scripts\n    name: database-scripts\n    readOnly: true\n  - mountPath: /var/lib/mongodb-automation/secrets/ca\n    name: secret-ca\n    readOnly: true\n  - mountPath: /var/lib/mongodb-automation/secrets/certs\n    name: secret-certs\n    readOnly: true"
                },
                {
                    "lang": "sh",
                    "value": "/var/lib/mongodb-automation/secrets/ca/secret-ca"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host {host} --port {port} --tls \\\n  --tlsCAFile </path/to/secret-ca> \\\n  --tlsCertificateKeyFile <your-cert>.pem \\\n  --authenticationMechanism MONGODB-X509  \\\n  --authenticationDatabase '$external'"
                },
                {
                    "lang": "sh",
                    "value": "mongosh --host {host} --port {port} --ssl \\\n  --sslCAFile </path/to/secret-ca> \\\n  --sslPEMKeyFile <your-cert>.pem \\\n  --authenticationMechanism MONGODB-X509 \\\n  --authenticationDatabase '$external'"
                }
            ],
            "preview": "The MongoDB Enterprise Kubernetes Operator can deploy MongoDB instances with\nX.509 authentication\nenabled. If X.509 authentication has been enabled for the deployment,\nyou must generate and use an X.509 certificate to connect to the\ndeployment. This new client certificate must be signed by the\nsame CA (Certificate Authority) that signs the server certificates for the MongoDB\ndeployment to accept it.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/multi-cluster-secure-client-connections",
            "title": "Secure Client Connections",
            "headings": [],
            "paragraphs": "Configure  LDAP (Lightweight Directory Access Protocol)  for client authentication in  multi-Kubernetes-cluster deployments . Configure X.509 for client authentication in  multi-Kubernetes-cluster deployments . Configure X.509 for internal authentication in  multi-Kubernetes-cluster deployments .",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/multi-cluster-secure-x509",
            "title": "Secure Deployments with X.509",
            "headings": [
                "Prerequisites",
                "Enable X.509 Authentication for a MongoDBMultiCluster Resource",
                "Create the secret for your agent's X.509 certificate of your MongoDBMultiCluster resource.",
                "Update your MongoDBMultiCluster resource to enable X509 authentication.",
                "Verify that the MongoDBMultiCluster resources are running.",
                "Renew X.509 Certificates for a MongoDBMultiCluster Resource",
                "Renew the secret for a MongoDBMultiCluster resource.",
                "Renew the secret for your agent's X.509 certificates."
            ],
            "paragraphs": "You can configure the  Kubernetes Operator  to use X.509 certificates to authenticate\nyour client applications in a  multi-Kubernetes-cluster deployment . To secure your  multi-Kubernetes-cluster deployment  with X.509 certificates, you run all actions on\nthe  central cluster .\nThe  Kubernetes Operator  propagates the X.509 configuration to each member cluster\nand updates the  Kubernetes Operator  configuration on each member cluster. Before you secure your  multi-Kubernetes-cluster deployment  using  TLS (Transport Layer Security)  encryption, complete the following tasks: Follow the steps in the  Multi-Cluster Quick Start Prerequisites . Deploy a  TLS-encrypted multi-cluster . Create credentials for the Kubernetes Operator  for the  Kubernetes Operator . Enabling X.509 authentication at the project level configures all\nagents to use X.509 client authentication when communicating with\nMongoDB deployments. X.509 client authentication requires one of the following: Cloud Manager Ops Manager  5.0.7 or later Run the  kubectl  command to create a new secret that stores the agent's X.509 certificate: Update your MongoDBMultiCluster custom resource \nwith security settings from the  Kubernetes Operator \n MongoDBMultiCluster resource specification .\nThe resulting configuration may look similar to the following example: For member clusters, run the following commands to verify that\nthe MongoDB Pods are in the running state: In the central cluster, run the following command to verify that\nthe  MongoDBMultiCluster  resource  is in the running state: If you have already created X.509 certificates, renew them periodically using\nthe following procedure. Run this  kubectl  command to renew an existing  secret  that stores the certificates for the  MongoDBMultiCluster  resource : Run the  kubectl  command to renew an existing secret that stores\nthe  MongoDBMultiCluster  resource  agent certificates:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key>"
                },
                {
                    "lang": "yaml",
                    "value": "  apiVersion: mongodb.com/v1\n  kind: MongoDBMultiCluster\n  metadata:\n   name: multi-replica-set\n  spec:\n   version: 5.0.0-ent\n   type: ReplicaSet\n   persistent: false\n   duplicateServiceObjects: true\n   credentials: my-credentials\n   opsManager:\n     configMapRef:\n       name: my-project\n   security:\n     tls:\n       ca: custom-ca\n     certsSecretPrefix: <prefix>\n   authentication:\n     enabled: true\n     modes: [\"X509\"]\n     agents:\n       mode: \"X509\"\n   clusterSpecList:\n     - clusterName: ${MDB_CLUSTER_1_FULL_NAME}\n       members: 3\n     - clusterName: ${MDB_CLUSTER_2_FULL_NAME}\n       members: 2\n     - clusterName: ${MDB_CLUSTER_3_FULL_NAME}\n       members: 3\n\nThe |k8s-op-short| copies the ConfigMap with the |certauth| created in\nthe central cluster to each member cluster, generates a concatenated\n|pem| secret, and distributes it to the member clusters."
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_1_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_2_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_3_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context=$MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace mongodb \\\n  get mdbmc multi-replica-set -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n--namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-cert \\\n--cert=<resource-tls-cert> \\\n--key=<resource-tls-key> \\\n--dry-run=client \\\n-o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key> \\\n  --dry-run=client \\\n  -o yaml | kubectl apply -f -"
                }
            ],
            "preview": "You can configure the Kubernetes Operator to use X.509 certificates to authenticate\nyour client applications in a multi-Kubernetes-cluster deployment.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/migrate-k8s-images",
            "title": "Migrate MongoDB Enterprise Kubernetes Operator from Ubuntu-based Images to UBI-based Images",
            "headings": [],
            "paragraphs": "To migrate  MongoDB Enterprise Kubernetes Operator  from Ubuntu-based images to UBI-based images, edit\nyour Kubernetes Operator\n configuration file  to pull\nimages from the appropriate UBI repositories by suffixing the existing\nimage repository path with  -ubi . You don't need to perform this\nprocedure if you are using OpenShift, as you are already using UBI\nimages. The following example compares a default configuration for the\n INIT_APPDB_IMAGE_REPOSITORY  setting with an updated configuration\nthat pulls a UBI image. After saving the changes, reapply your configuration file. For users running vanilla Kubernetes: For users running OpenShift: Repeat this procedure for the following repository\nconfigurations by applying the same  -ubi  suffix, saving the changes\nand reapplying the configuration each time to migrate the images\nseparately: After adding the necessary suffix, your configuration should match the\nconfiguration below: AGENT_IMAGE INIT_DATABASE_IMAGE_REPOSITORY INIT_OPS_MANAGER_IMAGE_REPOSITORY MONGODB_ENTERPRISE_DATABASE_IMAGE OPS_MANAGER_IMAGE_REPOSITORY",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f mongodb-enterprise.yaml"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f mongodb-enterprise-openshift.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_APPDB_IMAGE_REPOSITORY\n              value: quay.io/mongodb/mongodb-enterprise-init-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n        - name: mongodb-enterprise-operator\n          image: <operatorVersionUrl>\n          imagePullPolicy: <policyChoice>\n          env:\n            - name: INIT_APPDB_IMAGE_REPOSITORY\n              value: quay.io/mongodb/mongodb-enterprise-init-appdb-ubi"
                }
            ],
            "preview": "To migrate MongoDB Enterprise Kubernetes Operator from Ubuntu-based images to UBI-based images, edit\nyour Kubernetes Operator\nconfiguration file to pull\nimages from the appropriate UBI repositories by suffixing the existing\nimage repository path with -ubi. You don't need to perform this\nprocedure if you are using OpenShift, as you are already using UBI\nimages.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/secure-internal-auth",
            "title": "Secure Internal Authentication with X.509",
            "headings": [
                "General Prerequisites",
                "Configure X.509 Internal Authentication for a Replica Set",
                "Prerequisites",
                "Enable X.509 Internal Authentication",
                "Create the secret for your X.509 certificate.",
                "Copy the sample replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the general X.509 settings for your replica set resource.",
                "Configure the internal X.509 settings for your replica set resource.",
                "Save your replica set config file.",
                "Apply your changes to your replica set deployment.",
                "Track the status of your deployment.",
                "Renew Internal Authentication X.509 Certificates for a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Renew the secret for your TLS certificates.",
                "Renew the secret for your X.509 certificate.",
                "Renew the secret for your agents' X.509 certificates.",
                "Configure X.509 Internal Authentication for a Sharded Cluster",
                "Prerequisites",
                "Enable X.509 Internal Authentication",
                "Create the secret for your Shards' X.509 certificates.",
                "Create the secret for your config servers' X.509 certificate.",
                "Create the secret for your mongos server's X.509 certificates.",
                "Copy the sample sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the general X.509 settings for your sharded cluster resource.",
                "Configure the internal X.509 settings for your sharded cluster resource.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Track the status of your deployment.",
                "Renew Internal Authentication X.509 Certificates for a Sharded Cluster",
                "Configure kubectl to default to your namespace.",
                "Renew the secret for your Shards' TLS certificates.",
                "Renew the secret for your config server's TLS certificates.",
                "Renew the secret for your mongos server's TLS certificates.",
                "Renew the secret for your Shards' X.509 certificates.",
                "Renew the secret for your config servers' X.509 certificate.",
                "Renew the secret for your mongos server's X.509 certificates.",
                "Renew the secret for your agents' X.509 certificates."
            ],
            "paragraphs": "This guide instructs you on how to configure: The  Kubernetes Operator  doesn't support other authentication schemes between\nMongoDB nodes in a cluster. X.509 internal authentication between MongoDB nodes in a cluster. X.509 authentication from clients to your MongoDB instances. Before you secure any of your MongoDB deployments using  TLS (Transport Layer Security) \nencryption, complete the following: Install the Kubernetes Operator Create Credentials for the Kubernetes Operator Enabling X.509 authentication at the project level configures all\nagents to use X.509 client authentication when communicating with\nMongoDB deployments. X.509 client authentication requires one of the following: Cloud Manager Ops Manager  4.1.7 or later Ops Manager  4.0.11 or later Before you secure your replica set using X.509,\n deploy a TLS-encrypted replica set . Run this  kubectl  command to create a new  secret  that stores\nthe replica set's certificate: You must prefix your secrets with  <prefix>-<metadata.name> . If you call your deployment  my-deployment  and you set the\nprefix to  mdb , you must name the  TLS (Transport Layer Security)  secret for the\nclient  TLS (Transport Layer Security)  communications  mdb-my-deployment-cert . Also,\nyou must name the  TLS (Transport Layer Security)  secret for internal cluster authentication\n(if enabled)  mdb-my-deployment-clusterfile . Change the settings of this  YAML (Yet Another Markup Language)  file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the  object  specification\nat the end of your resource file in the  spec  section. To enable  TLS (Transport Layer Security)  and X.509 in your deployment, configure the following\nsettings in your  Kubernetes  object: Key Type Necessity Description Example boolean Required Set this value to  true  to enable authentication on the\nMongoDB deployment. true array Conditional Set this value to  [\"X509\"] . [\"X509\"] To enable  TLS (Transport Layer Security)  and X.509 in your deployment, configure the following\nsettings in your  Kubernetes  object: Key Type Necessity Description Example string Required Use this setting to enable\n X.509 internal cluster authentication . Once internal cluster authentication is enabled, it can't\nbe disabled. X509 Invoke the following  Kubernetes  command to update your\n replica set : To check the status of your  MongoDB  resource , use the following\ncommand: With the  -w  (watch) flag set, when the configuration changes, the output\nrefreshes immediately until the status phase achieves the  Running  state.\nTo learn more about resource deployment statuses, see  Troubleshoot the  Kubernetes Operator . If you have already created certificates, we recommend that you renew\nthem periodically using the following procedure. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Run this  kubectl  command to renew an existing  secret  that\nstores the replica set's certificates: Run this  kubectl  command to renew an existing  secret  that\nstores the replica set's certificate: Run this  kubectl  command to renew an existing  secret  that\nstores the agents' X.509 certificates: Before you secure your sharded cluster using X.509,\n deploy a TLS-encrypted sharded cluster . Run this  kubectl  command to create a new  secret  that stores\nthe sharded cluster shards' certificates: Run this  kubectl  command to create a new  secret  that stores\nthe sharded cluster config server's certificates: Run this  kubectl  command to create a new  secret  that stores\nthe sharded cluster  mongos  certificates: Change the settings of this  YAML (Yet Another Markup Language)  file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the  object  specification\nat the end of your resource file in the  spec  section. To enable  TLS (Transport Layer Security)  and X.509 in your deployment, configure the following\nsettings in your  Kubernetes  object: Key Type Necessity Description Example boolean Required Set this value to  true  to enable authentication on the\nMongoDB deployment. true array Conditional Set this value to  [\"X509\"] . [\"X509\"] To enable  TLS (Transport Layer Security)  and X.509 in your deployment, configure the following\nsettings in your  Kubernetes  object: Key Type Necessity Description Example string Required Use this setting to enable\n X.509 internal cluster authentication . Once internal cluster authentication is enabled, it can't\nbe disabled. X509 In any directory, invoke the following  Kubernetes  command to update and\nrestart your  sharded cluster : To check the status of your  MongoDB  resource , use the following\ncommand: With the  -w  (watch) flag set, when the configuration changes, the output\nrefreshes immediately until the status phase achieves the  Running  state.\nTo learn more about resource deployment statuses, see  Troubleshoot the  Kubernetes Operator . If you have already created certificates, we recommend that you renew\nthem periodically using the following procedure. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Run this  kubectl  command to renew an existing  secret  that\nstores the sharded cluster shards' certificates: Run this  kubectl  command to renew an existing  secret  that\nstores the sharded cluster config server's certificates: Run this  kubectl  command to renew an existing  secret  that\nstores the sharded cluster  mongos  certificates: Run this  kubectl  command to renew an existing  secret  that stores\nthe sharded cluster shards' certificates: Run this  kubectl  command to renew an existing  secret  that stores\nthe sharded cluster config servers' certificate: Run this  kubectl  command to renew an existing  secret  that stores\nthe sharded cluster  mongos  certificates: Run this  kubectl  command to renew an existing  secret  that\nstores the agents' X.509 certificates:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-clusterfile \\\n  --cert=<replica-set-clusterfile-tls-cert> \\\n  --key=<replica-set-clusterfile-tls-key>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n            # Must match metadata.name in ConfigMap file\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      ca: <custom-ca>\n    certsSecretPrefix: <prefix>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-cert \\\n  --cert=<replica-set-tls-cert> \\\n  --key=<replica-set-tls-key> \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-clusterfile \\\n  --cert=<replica-set-clusterfile-tls-cert> \\\n  --key=<replica-set-clusterfile-tls-key> \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key> \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-0-clusterfile \\\n  --cert=<shard-0-clusterfile-tls-cert> \\\n  --key=<shard-0-clusterfile-tls-cert>\n\nkubectl -n mongodb create secret tls <prefix>-<metadata.name>-1-clusterfile \\\n  --cert=<shard-1-clusterfile-tls-cert> \\\n  --key=<shard-1-clusterfile-tls-cert>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-config-clusterfile \\\n  --cert=<config-clusterfile-tls-cert> \\\n  --key=<config-clusterfile-tls-cert>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-mongos-clusterfile \\\n  --cert=<mongos-clusterfile-tls-cert> \\\n  --key=<mongos-clusterfile-tls-cert>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      ca: <custom-ca>\n    certsSecretPrefix: <prefix>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-0-cert \\\n  --cert=<shard-0-tls-cert> \\\n  --key=<shard-0-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -\n\nkubectl -n mongodb create secret tls <prefix>-<metadata.name>-1-cert \\\n  --cert=<shard-1-tls-cert> \\\n  --key=<shard-1-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-config-cert \\\n  --cert=<config-tls-cert> \\\n  --key=<config-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-mongos-cert \\\n  --cert=<mongos-tls-cert> \\\n  --key=<mongos-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-0-clusterfile \\\n  --cert=<shard-0-clusterfile-tls-cert> \\\n  --key=<shard-0-clusterfile-tls-cert> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -\n\nkubectl -n mongodb create secret tls <prefix>-<metadata.name>-1-clusterfile \\\n  --cert=<shard-1-clusterfile-tls-cert> \\\n  --key=<shard-1-clusterfile-tls-cert> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-config-clusterfile \\\n  --cert=<config-clusterfile-tls-cert> \\\n  --key=<config-clusterfile-tls-cert> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-mongos-clusterfile \\\n  --cert=<mongos-clusterfile-tls-cert> \\\n  --key=<mongos-clusterfile-tls-cert> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key> \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                }
            ],
            "preview": "This guide instructs you on how to configure:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/om-arch",
            "title": "Ops Manager Architecture in Kubernetes",
            "headings": [
                "The MongoDBOpsManager Custom Resource Definition",
                "Application Database",
                "Ops Manager Application",
                "Backup Daemon",
                "Reconciling the MongoDBOpsManager Custom Resource"
            ],
            "paragraphs": "You can use the  Kubernetes Operator  to deploy  Ops Manager  and MongoDB resources\nto a  Kubernetes  cluster. The  Kubernetes Operator  manages the lifecycle of each of\nthese deployments differently. All  Kubernetes   secrets  that the  Kubernetes Operator \ncreates can later be migrated to a different  secret storage tool \nto avoid storing secrets in  Kubernetes . This section is for single  Kubernetes  cluster deployments only. For\n multi-Kubernetes-cluster deployments , see  Architecture, Capabilities, and Limitations . The  Kubernetes Operator  manages  Ops Manager  deployments using the\n MongoDBOpsManager   custom resource . The  Kubernetes Operator  watches\nthe custom resource's specification for changes. When the\nspecification changes, the  Kubernetes Operator  validates the changes and\nmakes the appropriate updates to the resources in the  Kubernetes  cluster. MongoDBOpsManager   custom resource s specification defines the\nfollowing  Ops Manager  components: the Application Database, the  Ops Manager  application, and the Backup Daemon. For the Application Database, the  Kubernetes Operator  deploys a MongoDB\nreplica set as a  StatefulSet  to the  Kubernetes  cluster.  Kubernetes  creates\none Pod in the StatefulSet for each member\nthat comprises your Application Database replica set. Each Pod in\nthe StatefulSet runs a  mongod  and the MongoDB Agent. To enable each MongoDB Agent to start  mongod  on its\nPod in the StatefulSet, you must specify a specific MongoDB Server\nversion for the Application Database using the\n spec.applicationDatabase.version  setting. The version\nthat you specify in this setting must correspond to the tag in the\n container registry . Each MongoDB Agent\nstarts the Application Database with the specified version on its Pod\nin the StatefulSet. After each MongoDB Agent starts  mongod s on its Application Database\nPod, the MongoDB Agents add all  mongod  processes to the Application\nDatabase replica set. You configure the number of replicas in and other\nconfiguration options for the Application Database replica set in the\n spec.applicationDatabase  collection in the\n MongoDBOpsManager  custom resource. The  Kubernetes Operator  passes\nthis configuration to the MongoDB Agents using a  secret  that the\n Kubernetes Operator  mounts to each Pod in the Application Database StatefulSet. Each time that you update\nthe  spec.applicationDatabase  collection, the\n Kubernetes Operator  applies the changes to the MongoDB Agent configuration and\nthe StatefulSet specification, if applicable. If the StatefulSet\nspecification changes,  Kubernetes  upgrades the Pods in a rolling\nfashion and restarts each Pod. The  Kubernetes Operator  creates a  service  with  clusterIp=none  to\nprovide connectivity to each Application Database Pod from within the\n Kubernetes  cluster. You can customize the  Persistent Volume Claims  for the Application Database Pods using\nthe  spec.applicationDatabase.podSpec.persistence.single  or\n spec.applicationDatabase.podSpec.persistence.multiple  options. Depending on the  StorageClass  or the environment to which you deploy the\n Kubernetes Operator ,  Kubernetes  might create the  Persistent Volumes  using\n dynamic volume provisioning . After the Application Database reaches a  Running  state, the\n Kubernetes Operator  starts the  Ops Manager Application . For  Ops Manager , the\n Kubernetes Operator  deploys a StatefulSet to the  Kubernetes  cluster.  Kubernetes \ncreates one Pod in the StatefulSet for each  Ops Manager  replica that\nyou want to deploy. Each Pod contains one  Ops Manager Application  process. The  Kubernetes Operator  creates a  service  with  clusterIp=none  to\nallow clients deployed to the  Kubernetes  cluster to connect to  Ops Manager . To\nallow clients external to the  Kubernetes  cluster to connect to  Ops Manager ,\nconfigure the  spec.externalConnectivity  collection in the\nspecification for your  Ops Manager  deployment. Deploy  multiple  Ops Manager  replicas to\nmake your deployment highly available in the event of an  Ops Manager  Pod\nfailure. If  spec.backup.enabled  is  true , the  Kubernetes Operator \nstarts the Backup Daemon after the  Ops Manager Application  reaches a  Running \nstage. For the Backup Daemon,  Kubernetes Operator  deploys a StatefulSet\nto the  Kubernetes  cluster.  Kubernetes  creates one pod in the\nStatefulSet for the Backup Daemon. If you enable backup, you must provide additional fields in the\n spec.backup  collection to configure:\nthe  oplog store  and a  blockstore  or an  S3 (Simple Storage Service)   snapshot store . You can also\n encrypt backup jobs , but\n limitations  apply to deployments where the same\n Kubernetes Operator  instance is not managing both the\n MongoDBOpsManager  and  MongoDB \ncustom resources. If you enable backup, the  Kubernetes Operator  creates a  Persistent Volume Claim  for the\nBackup Daemon's  head database . You can\nconfigure the head database using the  spec.backup.headDB \nsetting. The  Kubernetes Operator  invokes  Ops Manager  APIs to ensure that the\n Ops Manager Application 's backup configuration matches the one that you define in\nthe custom resource definition. The following diagram describes how the  Kubernetes Operator  reconciles\nchanges to the  MongoDBOpsManager   CustomResourceDefinition . The  Kubernetes Operator  creates or updates the\n <om_resource_name>-db-config  secret. This secret contains\nthe configurations that the MongoDB Agent uses to start the\nApplication Database replica set. The  Kubernetes Operator  creates or updates the  <om_resource_name>-db \nApplication Database StatefulSet. This StatefulSet contains at\nleast three  Pods . Each Pod runs one MongoDB Agent instance. Each MongoDB Agent starts a\n mongod  instance on its Pod. The  Kubernetes Operator  mounts the  <om_resource_name>-db-config \nsecret to each Pod. The MongoDB Agent uses this secret to\nconfigure the Application Database replica set. The  Kubernetes Operator  creates or updates the  <om_resource_name> \nStatefulSet. This StatefulSet contains one Pod for each\n Ops Manager  replica. Each  Ops Manager  replica connects to the Application\nDatabase. Most changes to the  MongoDBOpsManager   custom resource \ntrigger a rolling upgrade of the Pods in the\n <om_resource_name>  StatefulSet.  Enabling TLS for the\nApplication Database  also triggers a rolling\nrestart because the connection string to the Application Database\nchanges. Changes to the  spec.backup \n MongoDBOpsManager   custom resource  collection don't\ntrigger a rolling upgrade. The  Kubernetes Operator  invokes  Ops Manager  APIs to create an admin user.\nThe  Kubernetes Operator  saves this admin user's credentials in the\n <om_resource_name>-admin-key  secret. The  Kubernetes Operator \nuses these credentials for all other  Ops Manager  API invocations. This reconciliation step happens only once: when you use the\n Kubernetes Operator  to create an  Ops Manager  resource. The\n Kubernetes Operator  skips this step when it updates the resource. The  Kubernetes Operator  performs a rolling upgrade of the Pods in the\n <om_resource_name>-db  Application Database StatefulSet\nto enable  Ops Manager  to monitor it. This reconciliation step happens only when you enable Monitoring\nfor an application database for the first time. This happens most\noften when you deploy a new  Ops Manager  resource. If  spec.backup.enabled  is  true , the  Kubernetes Operator \ncreates the  <om_resource_name>-backup-daemon  StatefulSet or\nverifies that it is running. The  Kubernetes Operator  mounts a  Persistent Volume  for\nthe head database. The Backup Daemon connects to the same Application Database as the\n Ops Manager  deployment. If  spec.backup.enabled  is  true , the  Kubernetes Operator \ninvokes  Ops Manager  APIs to ensure that the  Ops Manager Application 's backup\nconfiguration matches the one that you define in the custom resource\ndefinition.",
            "code": [],
            "preview": "You can use the Kubernetes Operator to deploy Ops Manager and MongoDB resources\nto a Kubernetes cluster. The Kubernetes Operator manages the lifecycle of each of\nthese deployments differently.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/deploy-standalone",
            "title": "Deploy a Standalone MongoDB Instance",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the following example standalone Kubernetes object.",
                "Open your preferred text editor and paste the object specification into a new text file.",
                "Configure the settings highlighted in the preceeding step as follows.",
                "Add any additional accepted settings for a Standalone deployment.",
                "Save this file with a .yaml file extension.",
                "Start your Standalone deployment.",
                "Track the status of your standalone deployment."
            ],
            "paragraphs": "You can deploy a  standalone  MongoDB instance for  Ops Manager  to\nmanage. Use standalone instances for testing and development.\n Do not  use these deployments for production systems as they lack\nreplication and high availability. For all production deployments\nuse replica sets. To learn about replica sets, see\n Deploy a Replica Set . At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . You can use the  Kubernetes Operator  to deploy MongoDB resources with\n Cloud Manager  and with  Ops Manager  version 5.0.x or later. You can use the  Atlas Operator \nto deploy MongoDB resources to  Atlas . To deploy a  standalone  using an  object , you must: Have or create an  Ops Manager instance  or a  Cloud Manager organization . Have or install the  MongoDB Enterprise Kubernetes Operator . Create or generate a  Kubernetes Operator ConfigMap . Create  credentials for the Kubernetes Operator  or\nconfigure  a different secret storage tool . To avoid storing secrets in  Kubernetes , you can migrate all  secrets \nto a  secret storage tool . To troubleshoot your sharded cluster, see: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . This is a  YAML (Yet Another Markup Language)  file that you can modify to meet your desired\nconfiguration. Change the highlighted settings to match your desired\nstandalone configuration. Key Type Description Example metadata.name string Label for this  Kubernetes  standalone  object . Resource names must be 44 characters or less. metadata.name Kubernetes  documentation on  names . my-project spec.version string Version of MongoDB that is installed on this\nstandalone. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n MongoDB Versioning  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. For best results, use the\n latest available enterprise MongoDB version  that is  compatible  with your\n Ops Manager  version. string Name of the  ConfigMap  with the  Ops Manager  connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value must exist on the  same  namespace as the resource\nyou want to create. <myproject> spec.credentials string Name of the secret you\n created  as  Ops Manager   API (Application Programming Interface) \nauthentication credentials for the  Kubernetes Operator  to\ncommunicate with  Ops Manager . The  Ops Manager   Kubernetes   Secret  object\nholding the Credentials must exist on the  same  Namespace as\nthe resource you want to create. The  Kubernetes Operator  tracks any changes to the Secret and\nreconciles the state of the  MongoDB  resource . <mycredentials> spec.type string Type of  MongoDB  resource  to create. Standalone spec.persistent string Optional. If this value is  true , then\n spec.podSpec.persistence.single  is set to its\ndefault value of  16Gi . To change your  Persistent Volume Claims  configuration, configure the\nfollowing collections to meet your deployment requirements: If you want one  Persistent Volume  for each  Pod , configure the\n spec.podSpec.persistence.single  collection. If you want separate  Persistent Volumes  for data, journals, and\nlogs for each  Pod , configure the following\ncollections: spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs Grant your containers permission to write to your  Persistent Volume .\nThe  Kubernetes Operator  sets  fsGroup = 2000 ,  runAsUser = 2000 ,\nand  runAsNonRoot = true  in  securityContext .  Kubernetes Operator \nsets  fsgroup  equal to  runAsUser  to make the volume writable\nfor a user that runs the main process in the container. To learn\nmore, see  Configure a\nSecurity Context for a Pod or Container  and the related\n discussion  in\nthe  Kubernetes  documentation. If redeploying the resource doesn't fix\nissues with your Persistent Volume, contact  MongoDB Support . If you do not use  Persistent Volumes , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n object  specification file for a Standalone deployment: spec.additionalMongodConfig spec.logLevel spec.featureCompatibilityVersion spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs spec.podSpec.podTemplate.affinity.podAffinity spec.podSpec.podTemplate spec.podSpec.podTemplate.affinity.nodeAffinity Invoke the following  Kubernetes  command to create your standalone: To check the status of your  MongoDB  resource , use the following\ncommand: With the  -w  (watch) flag set, when the configuration changes, the output\nrefreshes immediately until the status phase achieves the  Running  state.\nTo learn more about resource deployment statuses, see  Troubleshoot the  Kubernetes Operator . Find a Specific Pod Review Logs from a Specific Pod",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\nspec:\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <standalone-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "You can deploy a standalone MongoDB instance for Ops Manager to\nmanage. Use standalone instances for testing and development.\nDo not use these deployments for production systems as they lack\nreplication and high availability. For all production deployments\nuse replica sets. To learn about replica sets, see\nDeploy a Replica Set.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/upgrade-om-version",
            "title": "Upgrade Ops Manager and Backing Database Versions",
            "headings": [
                "Prerequisites",
                "Procedure",
                "In your Ops Manager Resource Specification, the settings as shown in the following example:",
                "Reapply the configuration to Kubernetes."
            ],
            "paragraphs": "Update the major and minor versions of your  Ops Manager  instance and  backing databases \nin the  Ops Manager Resource Specification  that the  Kubernetes Operator  uses to manage your deployment. To maintain existing settings and availability, back up the following in your current  Ops Manager  instance: Your  conf-mms.properties  to a secure location. The  conf-mms.properties \nstores settings for the  Ops Manager  instance. Your  gen.key  files to a secure location. The  gen.key \nprovides details to encrypt and decrypt  Ops Manager 's backing databases\nand user credentials.  Ops Manager  might delete these files as part of the upgrade process. Your  application database . If the upgrade fails,\nyou need a current backup to restore your  Ops Manager  instance. Upgrade  Ops Manager  by following the considerations, prerequisites, and procedure in  Upgrade Ops Manager . Reference  Use a Compatible MongoDB Version  to ensure your  backing databases \nuse a MongoDB version that is compatible with the new  Ops Manager  version. If you need to upgrade your backing databases to a compatible MongoDB version,\nsee  Upgrade MongoDB Version and FCV . To update  Ops Manager  from 5.0 to 6.0 and the application database to MongoDB\n 4.4.18-ent , complete the following steps: Set  spec.version  to the new  Ops Manager  version. If you upgraded your  application database , set\n spec.applicationDatabase.version  to the compatible MongoDB\nversion. (Optional)  If you might need to downgrade, set\n spec.featureCompatibilityVersion . Kubernetes  automatically reconfigures your deployment with the new\nspecifications. You can see these changes reflected in your  Ops Manager  or\n Cloud Manager  application.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: om\nspec:\n replicas: 1\n version: \"6.0.18\"\n adminCredentials: ops-manager-admin\n configuration:\n  mms.fromEmailAddr: admin@example.com\n  mms.security.allowCORS: \"false\"\n backup:\n  enabled: true\n  encryption:\n    kmip:\n      server:\n        url: kmip.corp.mongodb.com:5696\n        ca: mongodb-kmip-certificate-authority-pem\n  headDB:\n   storage: \"30Gi\"\n   labelSelector:\n    matchLabels:\n     app: my-app\n  opLogStores:\n   - name: oplog1\n                       # Sets labels for the oplog store.\n     assignmentLabels: [\"test1\", \"test2\"]\n     mongodbResourceRef:\n      name: my-oplog-db\n     mongodbUserRef:\n      name: my-oplog-user\n  s3Stores:\n   - name: s3store1\n                       # Sets labels for the S3 store.\n     assignmentLabels: [\"test1\", \"test2\"]\n                       \n     mongodbResourceRef:\n      name: my-s3-metadata-db\n     mongodbUserRef:\n      name: my-s3-store-user\n     s3SecretRef:\n       name: my-s3-credentials\n     pathStyleAccessEnabled: true\n     s3BucketEndpoint: s3.region.amazonaws.com\n     s3BucketName: my-bucket\n\n applicationDatabase:\n   passwordSecretKeyRef:\n    name: om-db-user-secret\n    key: password\n   members: 3\n   topology: SingleCluster\n   version: \"6.0.5-ubi8\"\n"
                },
                {
                    "lang": "none",
                    "value": "kubectl apply -f <om-resource-specification>.yaml"
                }
            ],
            "preview": "Update the major and minor versions of your Ops Manager instance and backing databases\nin the Ops Manager Resource Specification that the Kubernetes Operator uses to manage your deployment.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/configure-file-store",
            "title": "Configure File System Backup Store with Kubernetes Operator",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Create a PersistentVolumeClaim object.",
                "Create and apply an Ops Manager Custom Resource Definition specifying your PersistentVolumeClaim.",
                "Apply changes to your Ops Manager deployment.",
                "Configure your oplog store.",
                "Create a File System Snapshot Store in Ops Manager."
            ],
            "paragraphs": "Kubernetes Operator  supports storage of filesystem snapshots. To configure file system snapshot storage, your  Kubernetes Operator \ndeployment must have a\n storage class \nconfigured with the  ReadWriteMany  method. To configure file system snapshot storage: Create a  PersistentVolumeClaim \nobject, and allocate storage as needed. Set\n accessModes \nto  ReadWriteMany : Create an  Ops Manager   CustomResourceDefinition  that specifies your\n PersistentVolumeClaim  object and the  backup.fileSystemStores \nfield, which is the name of your file system snapshot store. The following example creates a  CustomResourceDefinition  file named\n ops-manager-fs.yaml , for the MongoDB\n oplog store  with a  kube-user . Invoke the following  kubectl  command on the filename of your\n Ops Manager  resource definition: Wait for your  Ops Manager  object to report its state as  Running , then\nconfigure your oplog store as described in the\n Configure Backup Settings  step of the\n Deploy an Ops Manager Resource  procedure. Log into your  Ops Manager  instance and navigate to:  Admin \n   Backup   \n Snapshot Store   \n Create New File System Store . Set the name to the value you set for  backup.fileSystemStores \nin your  CustomResourceDefinition . Set the other values as appropriate, then click\n Create . Your  Ops Manager  object will report a  BACKUP  state of  Pending \nafter you create the new file system store.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: snapshot-store-ops-manager\nspec:\n  storageClassName: managed-nfs-storage #SC that supports(RWX)\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 10G\n    ..."
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\n  kind: MongoDBOpsManager\n  metadata:\n    name: ops-manager\n  spec:\n    replicas: 1\n    version: 6.0.7\n    adminCredentials: ops-manager-admin-secret\n    statefulSet:\n      spec:\n        template:\n          spec:\n            volumes:\n              - name: snapshot-store\n                persistentVolumeClaim:\n                  claimName: snapshot-store-ops-manager\n            containers:\n              - name: mongodb-ops-manager\n                volumeMounts:\n                  - name: snapshot-store\n                    mountPath: /snapshot_store\n    backup:\n      enabled: true\n      fileSystemStores:\n        - name: filesystem1\n       assignmentLabels: [\"test1\", \"test2\"]\n      opLogStores:\n        assignmentLabels: [\"test1\", \"test2\"]\n        - name: oplog1\n          mongodbResourceRef:\n            name: oplog-db\n          # mongodbUserRef:\n          #   name: kube-user\n      statefulSet:\n        spec:\n          template:\n            spec:\n              volumes:\n                - name: snapshot-store\n                  persistentVolumeClaim:\n                    claimName: snapshot-store-ops-manager\n              containers:\n                - name: mongodb-backup-daemon\n                  volumeMounts:\n                    - name: snapshot-store\n                      mountPath: /snapshot_store\n    applicationDatabase:\n      members: 3\n      version: 5.0.7-ubi8"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                }
            ],
            "preview": "Kubernetes Operator supports storage of filesystem snapshots.",
            "tags": "backup, snapshot storage",
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/plan-k8s-operator-install",
            "title": "Plan your MongoDB Enterprise Kubernetes Operator Installation",
            "headings": [],
            "paragraphs": "Use the  MongoDB Enterprise Kubernetes Operator  to deploy: To deploy MongoDB resources with the  Kubernetes Operator , you need an\n Ops Manager  instance. Deploy this instance to  Kubernetes  using the Operator or\noutside  Kubernetes  using\n traditional installation methods . The\nOperator uses  Ops Manager   API (Application Programming Interface)  methods to deploy and then manage MongoDB\nresources. This section is for single  Kubernetes  cluster deployments only. For\n multi-Kubernetes-cluster deployments , see  Overview . Ops Manager resources MongoDB standalone, replica set, and sharded cluster resources Review the architecture of the custom resources in the  Kubernetes Operator :\nthe  Ops Manager  and the MongoDB database. Review compatible versions of  Kubernetes , OpenShift, MongoDB, and  Ops Manager . Review container image details. Decide whether to set up single or multiple  Kubernetes  clusters for your custom\nMongoDB resources. Set the scope for the  Kubernetes Operator  deployment by configuring which\ntype of namespace the  Kubernetes Operator  should use. Review  Kubernetes Operator  deployment scopes and other preparation\ninformation. Review the prerequisites before you install the  Kubernetes Operator .",
            "code": [],
            "preview": "Use the MongoDB Enterprise Kubernetes Operator to deploy:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/deploy-om-container-local-mode",
            "title": "Configure an Ops Manager Resource to use Local Mode",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Delete the StatefulSet that manages your Ops Manager Pods.",
                "Copy the fields of this Ops Manager resource.",
                "Paste the copied example section into your existing Ops Manager resource.",
                "Save your Ops Manager config file.",
                "Apply changes to your Ops Manager deployment.",
                "In a rolling fashion, delete your old Ops Manager Pods.",
                "Track the status of your Ops Manager instance.",
                "Download the MongoDB installation archive to your local machine.",
                "Copy the MongoDB archive to the Ops Manager Persistent Volume.",
                "Deploy a MongoDB Database Resource."
            ],
            "paragraphs": "In a default configuration, the MongoDB Agents and Backup Daemons\naccess MongoDB installation archives over the Internet from  MongoDB, Inc. You can configure  Ops Manager  to run in  Local Mode  with the\n Kubernetes Operator  if the nodes in your  Kubernetes  cluster don't have access to\nthe Internet. The Backup Daemons and managed MongoDB resources download\ninstallation archives only from a  Persistent Volume  that you create for\nthe  Ops Manager  StatefulSet. This procedure covers uploading installation archives to  Ops Manager . If  Ops Manager  has no internet access, see\n Configure Deployment to Have Limited Internet Access . For compatbility, see  MongoDB Enterprise Kubernetes Operator  Compatibility . To view all available\nversions for each image, see  Container Images . Configuring  Ops Manager  to use Local Mode in  Kubernetes  is not recommended.\nConsider  configuring Ops Manager to use Remote Mode  instead. Deploy an  Ops Manager  Resource . The following procedure shows you how to\nupdate your  Ops Manager   Kubernetes   object  to enable Local Mode. To avoid downtime when you enable Local Mode, ensure that you set\n spec.replicas  to a value greater than  1  in your\n Ops Manager  resource definition. If you updated your  Ops Manager  resource definition to make  Ops Manager \nhighly available, apply your changes before you begin this tutorial: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . In this tutorial, you update the StatefulSet that manages the  Ops Manager \nPods in your  Kubernetes  cluster. You must first delete the  Ops Manager  StatefulSet so that  Kubernetes  can apply\nthe updates that Local Mode requires. Find the name of your  Ops Manager  StatefulSet: The entry in the response that matches the\n metadata.name  of your Your  Ops Manager  StatefulSet is the entry in the response that matches\nthe  metadata.name  in your  Ops Manager  resource\ndefinition. Delete the  Ops Manager  StatefulSet: Ensure that you include the  --cascade=false  flag when you\ndelete your  Ops Manager  StatefulSet. If you don't include this\nflag,  Kubernetes  also deletes your  Ops Manager  Pods. Copy Lines 9-31 of this example to: Use the  Ops Manager  configuration setting\n automation.versions.source: local  in\n spec.configuration  to enable Local Mode. Define a  Persistent Volume  for the  Ops Manager  StatefulSet to store the\nMongoDB installation archive. MongoDB Agents running in MongoDB\ndatabase resource containers that you create with the  Kubernetes Operator \ndownload the installation archives from  Ops Manager  instead of from the\nInternet. Open your preferred text editor and paste the  object \nspecification into the appropriate location in your resource file. Invoke the following  kubectl  command on the filename of the\n Ops Manager  resource definition: Kubernetes  creates a new  Ops Manager  StatefulSet when you apply the changes\nto your  Ops Manager  resource definition. Before proceeding to the next\nstep, run the following command to ensure that the  Ops Manager  StatefulSet\nexists: The new  Ops Manager  StatefulSet should show 0 members ready: List the  Ops Manager  Pods in your  Kubernetes  cluster: Delete one  Ops Manager  Pod: Kubernetes  recreates the  Ops Manager  Pod you deleted. Continue to get the\nstatus of the new Pod until it is ready: When the new Pod is initializing, the output is similar to the\nfollowing example: When the new Pod is ready, the output is similar to the following example: Repeat Steps  b  and  c  until you've deleted all of your\n Ops Manager  Pods and confirmed that all of the new Pods are ready. To check the status of your  Ops Manager  resource, invoke the following\ncommand: See  Troubleshoot the  Kubernetes Operator  for information about the\nresource deployment statuses. After the  Ops Manager  resource completes the  Pending  phase, the\ncommand returns output similar to the following: Copy the value of the  status.opsManager.url  field, which states\nthe resource's connection  URL (Uniform Resource Locator) . You use this value when you create a\n ConfigMap  later in the procedure. The installers that you download depend on the environment to which\nyou deployed the operator: Download the RHEL installation tarball for the MongoDB Server version\nyou want the  Kubernetes Operator  to deploy. For example, to download the\n 6.0.1  release: The following example includes a link that allows you to download\nthe specified version of MongoDB Community Edition.\nTo download any other version of MongoDB Community Edition, visit the  MongoDB Community Edition Download Center .\nTo download MongoDB Enterprise Edition, visit the  MongoDB Enterprise Download Center . Copy the MongoDB archive for each MongoDB version you intend to deploy\nto the  Ops Manager  Persistent Volume. The commands that you use depend on the environment to which you\ndeployed the  Kubernetes Operator : If you deployed more than one  Ops Manager \n replica , copy only the MongoDB\ninstallation  tarball  packages to  Replica 1  and\nbeyond. To copy the MongoDB installation archive to the\n Ops Manager  PersistentVolume: Copy the MongoDB Server installation tarball to the\n Ops Manager  PersistentVolume. For example, to copy the  6.0.1 \nrelease: To copy the MongoDB installation archive to the\n Ops Manager  PersistentVolume, copy the MongoDB Server installation  tarball  to the\n Ops Manager  PersistentVolume. For example, to copy the  6.0.1 \nrelease: MongoDB Agents run in MongoDB database resource containers that\nyou create with the  Kubernetes Operator . Download the installation archives\nfrom  Ops Manager  instead of downloading them from the Internet. If you have not done so already, complete the following\nprerequisites: Create Credentials for the  Kubernetes Operator Create One Project using a ConfigMap Deploy a  MongoDB database resource \nin the same namespace to which you deployed  Ops Manager .\nEnsure that you: Match the  spec.opsManager.configMapRef.name  of the resource\nto the  metadata.name  of your ConfigMap. Match the  spec.credentials  of the resource to the name of\nthe secret you created that contains an  Ops Manager  programmatic\nAPI key pair.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulsets"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulsets -n mongodb\nNAME                       READY   AGE\nops-manager-localmode      2/2     2m31s\nops-manager-localmode-db   3/3     4m46s"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete statefulset --cascade=false <ops-manager-statefulset>"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: ops-manager-localmode\nspec:\n replicas: 2\n version: \"6.0.0\"\n adminCredentials: ops-manager-admin-secret\n configuration:\n   # this enables local mode in Ops Manager\n   automation.versions.source: local\n statefulSet:\n   spec:\n     # the Persistent Volume Claim will be created for each Ops Manager Pod\n     volumeClaimTemplates:\n      - metadata:\n          name: mongodb-versions\n        spec:\n          accessModes: [ \"ReadWriteOnce\" ]\n          resources:\n            requests:\n              storage: \"20Gi\"\n     template:\n       spec:\n         containers:\n           - name: mongodb-ops-manager\n             volumeMounts:\n             - name: mongodb-versions\n               # this is the directory in each Pod where all MongoDB\n               # archives must be put\n               mountPath: /mongodb-ops-manager/mongodb-releases\n backup:\n  enabled: false\n applicationDatabase:\n  members: 3"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulsets"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulsets -n mongodb\nNAME                       READY   AGE         ops-manager-localmode      0/2     2m31s\nops-manager-localmode-db   3/3     4m46s"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete pod <om-pod-0>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                          READY   STATUS    RESTARTS   AGE\nmongodb-enterprise-operator-5648d4c86-k5brh   1/1     Running   0          5m24s\nops-manager-localmode-0                       0/1     Running   0          0m55s\nops-manager-localmode-1                       1/1     Running   0          5m45s\nops-manager-localmode-db-0                    1/1     Running   0          5m19s\nops-manager-localmode-db-1                    1/1     Running   0          4m54s\nops-manager-localmode-db-2                    1/1     Running   0          4m12s"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                          READY   STATUS    RESTARTS   AGE\nmongodb-enterprise-operator-5648d4c86-k5brh   1/1     Running   0          5m24s\nops-manager-localmode-0                       1/1     Running   0          3m55s\nops-manager-localmode-1                       1/1     Running   0          5m45s\nops-manager-localmode-db-0                    1/1     Running   0          5m19s\nops-manager-localmode-db-1                    1/1     Running   0          4m54s\nops-manager-localmode-db-2                    1/1     Running   0          4m12s"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2020-05-15T16:20:22Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: \"4.4.5-ubi8\"\n  backup:\n    phase: \"\"\n  opsManager:\n    lastTransition: \"2020-05-15T16:20:26Z\"\n    phase: Running\n    replicas: 1\n    url: http://ops-manager-localmode-svc.mongodb.svc.cluster.local:8080\n    version: \"5.0.0\"\n"
                },
                {
                    "lang": "sh",
                    "value": "curl -OL https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel80-6.0.1.tgz"
                },
                {
                    "lang": "sh",
                    "value": "kubectl cp mongodb-linux-x86_64-rhel80-6.0.1.tgz \\\n\"ops-manager-localmode-0:/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-rhel80-6.0.1.tgz\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl cp mongodb-linux-x86_64-rhel80-6.0.1.tgz \\\n\"ops-manager-localmode-1:/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-rhel80-6.0.1.tgz\""
                },
                {
                    "lang": "sh",
                    "value": "oc rsync  \"ops-manager-localmode-0:/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-rhel80-6.0.1.tgz\" \\\nmongodb-linux-x86_64-rhel80-6.0.1.tgz"
                },
                {
                    "lang": "sh",
                    "value": "oc rsync  \"ops-manager-localmode-1:/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-rhel80-6.0.1.tgz\" \\\nmongodb-linux-x86_64-rhel80-6.0.1.tgz"
                }
            ],
            "preview": "In a default configuration, the MongoDB Agents and Backup Daemons\naccess MongoDB installation archives over the Internet from MongoDB, Inc.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/plan-k8s-op-compatibility",
            "title": "MongoDB Enterprise Kubernetes Operator Compatibility",
            "headings": [
                "Kubernetes and OpenShift Versions",
                "Supported Kubernetes Cluster Naming Conventions",
                "Other Distributions of Kubernetes",
                "Supported Platforms and MongoDB Versions",
                "Supported Hardware Architectures",
                "Cloud Manager and Ops Manager Versions"
            ],
            "paragraphs": "The  Kubernetes Operator  doesn't support  Kubernetes  versions that have reached\nEnd of Life (EOL). The  Kubernetes Operator  is compatible with the following  Kubernetes  and OpenShift\nversions. Unless otherwise noted, each  Kubernetes Operator  version listed\nspans the full release series starting from the listed version. Kubernetes Operator  Release Series Kubernetes  Version OpenShift Version 1.24.x 1.26, 1.27, 1.28 4.11-4.13 1.23.x 1.26, 1.27, 1.28 4.11-4.13 1.22.x 1.25, 1.26, 1.27 4.11-4.13 1.21.x 1.25, 1.26, 1.27 4.11-4.13 1.20.x 1.25, 1.26, 1.27 4.11-4.13 The  Kubernetes Operator  supports only  Kubernetes  cluster names that consist of\nalphanumeric characters, dashes, underscores, and periods. If your cluster name contains unsupported characters, you must  rename your cluster \nto a supported name. For example, if you are deploying the  Kubernetes Operator  in an  AWS (Amazon Web Services) \n EKS  cluster, you must rename the\ncluster as the default name contains unsupported characters. The  Kubernetes Operator  is also compatible with all\n CNCF-certified distributions \nof  Kubernetes  and core  Kubernetes  features. The distribution version\nmust match one of the base  Kubernetes  versions supported by the  Kubernetes Operator . To learn which  Kubernetes  versions are supported by your  Kubernetes Operator  series,\nsee the preceding  version table . The  Kubernetes Operator  is compatible with different versions of MongoDB\ndepending on the base image of the MongoDB database resource. The  Kubernetes Operator  is compatible with actively supported versions of\nMongoDB. The  Kubernetes Operator  isn't compatible with MongoDB versions that\nhave reached end of life, or that are listed on the  MongoDB Alerts  page. To learn more about\nwhich MongoDB versions your base image supports, see\n Platform Support \nin the MongoDB Manual. Unless otherwise noted, each  Kubernetes Operator  version listed spans the\nfull release series starting from the listed version. Starting with the  Kubernetes Operator  1.17 release, Ubuntu base images are\ndeprecated. Ubuntu base images are removed in the  Kubernetes Operator \n1.19 release. MongoDB continues to support the Red Hat UBI 8 base images.\nTo provide feedback about this change, use the  MongoDB Feedback Engine . Kubernetes Operator  Release Series Base Image 1.24.x Red Hat UBI 8 Base Image 1.23.x Red Hat UBI 8 Base Image 1.22.x Red Hat UBI 8 Base Image 1.21.x Red Hat UBI 8 Base Image 1.20.x Red Hat UBI 8 Base Image Kubernetes  nodes must be running processors with the x86-64/AMD64 architecture. The  Kubernetes Operator  is compatible with  Cloud Manager  and with the\nfollowing  Ops Manager  versions. To learn about MongoDB Agent compatibility,\nsee  Agent Compatibility . Unless otherwise noted, each  Kubernetes Operator \nversion listed spans the full release series starting from the listed\nversion. Kubernetes Operator  Release Series Ops Manager  Version 1.24.x 6.0, 7.0 1.23.x 6.0 1.22.x 6.0 1.21.x 6.0 1.20.x 5.0, 6.0 End of Life (EOL) Dates for the MongoDB Kubernetes Operator .",
            "code": [],
            "preview": "The Kubernetes Operator is compatible with the following Kubernetes and OpenShift\nversions. Unless otherwise noted, each Kubernetes Operator version listed\nspans the full release series starting from the listed version.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/deploy-sharded-cluster",
            "title": "Deploy a Sharded Cluster",
            "headings": [
                "Considerations",
                "Do Not Deploy Monitoring Agents Inside and Outside Kubernetes",
                "Choose Whether to Encrypt Connections",
                "Prerequisites",
                "Deploy a Sharded Cluster",
                "Configure kubectl to default to your namespace.",
                "Create the secret for your Shards' TLS certificates.",
                "Create the secret for your config servers' TLS certificate.",
                "Create the secret for your mongos servers' TLS certificate.",
                "Create the secret for your agent's TLS certificate.",
                "Create the ConfigMap to link your CA (Certificate Authority) with your deployment.",
                "Copy the sample sharded cluster resource.",
                "Paste the copied example to create a new sharded cluster resource.",
                "Configure the settings highlighted in the preceding step as follows.",
                "Configure the TLS settings for your sharded cluster resource using a custom certificate authority (CA).",
                "Add any additional accepted settings for a sharded cluster deployment.",
                "Save this file with a .yaml file extension.",
                "Start your sharded cluster deployment.",
                "Track the status of your sharded cluster deployment.",
                "Renew TLS Certificates for a Sharded Cluster",
                "Configure kubectl to default to your namespace.",
                "Renew the secret for your Shards' TLS certificates.",
                "Renew the secret for your config server's TLS certificates.",
                "Renew the secret for your mongos server's TLS certificates.",
                "Configure kubectl to default to your namespace.",
                "Copy the sample sharded cluster resource.",
                "Paste the copied example to create a new sharded cluster resource.",
                "Configure the settings highlighted in the preceding step as follows.",
                "Add any additional accepted settings for a sharded cluster deployment.",
                "Save this file with a .yaml file extension.",
                "Start your sharded cluster deployment.",
                "Track the status of your sharded cluster deployment."
            ],
            "paragraphs": "Sharded clusters  provide horizontal scaling\nfor large data sets and enable high throughput operations by\ndistributing the data set across a group of servers. To learn more about sharding, see\n Sharding Introduction  in the\nMongoDB manual. Use this procedure to deploy a new sharded cluster that  Ops Manager  manages.\nLater, you can use  Ops Manager  to add shards and perform other maintenance\noperations on the cluster. At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . You can use the  Kubernetes Operator  to deploy MongoDB resources with\n Cloud Manager  and with  Ops Manager  version 5.0.x or later. You can use the  Atlas Operator \nto deploy MongoDB resources to  Atlas . Kubernetes Operator  doesn't support  arbiter nodes . Due to  Kubernetes  network translation, a monitoring agent outside  Kubernetes \ncannot monitor MongoDB instances inside  Kubernetes . For this reason, k8s\nand non-k8s deployments in the same project are not supported. Use\nseparate projects. When you deploy your sharded cluster via the  Kubernetes Operator , you must\nchoose whether to encrypt connections using  TLS (Transport Layer Security)  certificates. The following procedure for  TLS-Encrypted  connections: The following procedure for  Non-Encrypted Connections : To set up  TLS (Transport Layer Security)  encryption for a replica set, see\n Deploy a Replica Set . Select the appropriate tab based on whether you want to encrypt your\nreplica set connections with  TLS (Transport Layer Security) . Establishes  TLS (Transport Layer Security) -encrypted connections between cluster shards. Establishes  TLS (Transport Layer Security) -encrypted connections between client applications\nand MongoDB deployments. Requires valid certificates for  TLS (Transport Layer Security)  encryption. Doesn't encrypt connections between cluster shards. Doesn't encrypt connections between client applications\nand MongoDB deployments. Has fewer setup requirements than a deployment with  TLS (Transport Layer Security) -encrypted\nconnections. To deploy a  sharded cluster  using an  object , you must: Have or create an  Ops Manager instance  or a  Cloud Manager organization . Have or install the  MongoDB Enterprise Kubernetes Operator . Create or generate a  Kubernetes Operator ConfigMap . Create  credentials for the Kubernetes Operator  or\nconfigure  a different secret storage tool . To avoid storing secrets in  Kubernetes , you can migrate all  secrets \nto a  secret storage tool . Generate one  TLS (Transport Layer Security)  certificate for each of the following components: Each shard in your sharded cluster. Ensure that you add  SAN (Subject Alternative Name) s for\neach  Kubernetes  pod that hosts a shard member to the certificate. In your  TLS (Transport Layer Security)  certificates, the  SAN (Subject Alternative Name) \nfor each shard pod must use the following format: Your config servers. Ensure that you add  SAN (Subject Alternative Name) s for\neach  Kubernetes  pod that hosts your config servers to the certificate. In your  TLS (Transport Layer Security)  certificates, the  SAN (Subject Alternative Name) \nfor each config server pod must use the following format: Your  mongos  instances. Ensure that you add  SAN (Subject Alternative Name) s for\neach  Kubernetes  pod that hosts a  mongos  to the certificate. In your  TLS (Transport Layer Security)  certificates, the  SAN (Subject Alternative Name)  for each  mongos  pod must use this\nformat: Your project's MongoDB Agent. For the MongoDB Agent certificate,\nensure that you meet the following requirements: The Common Name in the  TLS (Transport Layer Security)  certificate is not empty. The combined Organization and Organizational Unit in each  TLS (Transport Layer Security) \ncertificate differs from the Organization and\nOrganizational Unit in the  TLS (Transport Layer Security)  certificate for your\nreplica set members. You must possess the  CA (Certificate Authority)  certificate and the key that you used to\nsign your  TLS (Transport Layer Security)  certificates. The  Kubernetes Operator  uses  kubernetes.io/tls  secrets\nto store  TLS (Transport Layer Security)  certificates and private keys for  Ops Manager  and MongoDB\nresources. Starting in  Kubernetes Operator  version 1.17.0, the\n Kubernetes Operator  doesn't support concatenated  PEM (Privacy-Enhanced Mail)  files stored as\n Opaque secrets . To deploy a  sharded cluster  using an  object , you must: Have or create an  Ops Manager instance  or a  Cloud Manager organization . Have or install the  MongoDB Enterprise Kubernetes Operator . Create or generate a  Kubernetes Operator ConfigMap . Create  credentials for the Kubernetes Operator  or\nconfigure  a different secret storage tool . To avoid storing secrets in  Kubernetes , you can migrate all  secrets \nto a  secret storage tool . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Run this  kubectl  command to create a new  secret  that stores\nthe sharded cluster shards' certificates: If you're using  HashiCorp Vault  as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. Run this  kubectl  command to create a new secret that stores\nthe sharded cluster config servers' certificate: If you're using  HashiCorp Vault  as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. Run this  kubectl  command to create a new secret that stores\nthe sharded cluster  mongos  certificate: If you're using  HashiCorp Vault  as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. Run this  kubectl  command to create a new  secret  that stores\nthe agent's TLS certificate: If you're using  HashiCorp Vault  as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. Run this  kubectl  command to link your  CA (Certificate Authority)  to your sharded\ncluster and specify the  CA (Certificate Authority)  certificate file that you must always\nname  ca-pem  for the  MongoDB  resource : Change the settings of this  YAML (Yet Another Markup Language)  file to match your\ndesired  sharded cluster  configuration. Change the settings to match your desired\n sharded cluster  configuration. Open your preferred text editor and paste the  object  specification\ninto a new text file. Key Type Description Example metadata.name string Label for this  Kubernetes   sharded cluster   object . Resource names must be 44 characters or less. metadata.name Kubernetes  documentation on  names . myproject spec.shardCount integer Number of shards to deploy. 2 spec.mongodsPerShardCount integer Number of shard members per shard. 3 spec.mongosCount integer Number of shard routers to deploy. 2 spec.configServerCount integer Number of members of the config server replica set. 3 spec.version string Version of MongoDB that this  sharded cluster  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n MongoDB Versioning  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. For best results, use the\n latest available enterprise MongoDB version  that is  compatible  with your\n Ops Manager  version. spec.opsManager.configMapRef.name string Name of the  ConfigMap  with the  Ops Manager  connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value must exist on the  same  namespace as the resource\nyou want to create. The  Kubernetes Operator  tracks any changes to the ConfigMap and\nreconciles the state of the  MongoDB  resource . <myproject> spec.credentials string Name of the secret you\n created  as  Ops Manager   API (Application Programming Interface) \nauthentication credentials for the  Kubernetes Operator  to\ncommunicate with  Ops Manager . The  Ops Manager   Kubernetes   Secret  object\nholding the Credentials must exist on the  same  Namespace as\nthe resource you want to create. The  Kubernetes Operator  tracks any changes to the Secret and\nreconciles the state of the  MongoDB  resource . <mycredentials> spec.type string Type of  MongoDB  resource  to create. ShardedCluster spec.persistent string Optional. Flag indicating if this  MongoDB  resource  should use  Persistent Volumes  for\nstorage. Persistent volumes are not deleted when the\n MongoDB  resource  is stopped or restarted. If this value is  true , then the following values are set\nto their default value of  16Gi : To change your  Persistent Volume Claims  configuration, configure the\nfollowing collections to meet your deployment requirements: spec.shardPodSpec.persistence.single spec.configSrvPodSpec.persistence.single If you want one  Persistent Volume  for each  Pod , configure the\n spec.shardPodSpec.persistence.single  and\n spec.configSrvPodSpec.persistence.single \ncollections. If you want separate  Persistent Volumes  for data, journals, and\nlogs for each  Pod , configure the following\ncollections: In the  spec.configSrvPodSpec.persistence.multiple \ncollection:\n-  .data \n-  .journal \n-  .logs In the  spec.configSrvPodSpec.persistence.multiple  collection:\n-  .data \n-  .journal \n-  .logs Grant your containers permission to write to your  Persistent Volume .\nThe  Kubernetes Operator  sets  fsGroup = 2000 ,  runAsUser = 2000 ,\nand  runAsNonRoot = true  in  securityContext .  Kubernetes Operator \nsets  fsgroup  equal to  runAsUser  to make the volume writable\nfor a user that runs the main process in the container. To learn\nmore, see  Configure a\nSecurity Context for a Pod or Container  and the related\n discussion  in\nthe  Kubernetes  documentation. If redeploying the resource doesn't fix\nissues with your Persistent Volume, contact  MongoDB Support . If you do not use  Persistent Volumes , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true To enable  TLS (Transport Layer Security)  in your deployment, configure the following\nsettings in your  Kubernetes  object: Key Type Necessity Description Example string Required Add the  ConfigMap 's name that stores the custom  CA (Certificate Authority) \nthat you used to sign your deployment's  TLS (Transport Layer Security)  certificates. <custom-ca> string Required Add the  <prefix>  of the secret\nname that contains your MongoDB deployment's  TLS (Transport Layer Security)  certificates. If you call your deployment  my-deployment  and you set the\nprefix to  mdb , you must name the  TLS (Transport Layer Security)  secret for the\nclient  TLS (Transport Layer Security)  communications  mdb-my-deployment-cert . Also,\nyou must name the  TLS (Transport Layer Security)  secret for internal cluster authentication\n(if enabled)  mdb-my-deployment-clusterfile . devDb You can also add any of the following optional settings to the\n object  specification file for a  sharded cluster \ndeployment: For config server For shard routers For shard members spec.backup.assignmentLabels spec.backup.mode spec.backup.snapshotSchedule.snapshotIntervalHours spec.backup.snapshotSchedule.snapshotRetentionDays spec.backup.snapshotSchedule.dailySnapshotRetentionDays spec.backup.snapshotSchedule.weeklySnapshotRetentionWeeks spec.backup.snapshotSchedule.monthlySnapshotRetentionMonths spec.backup.snapshotSchedule.pointInTimeWindowHours spec.backup.snapshotSchedule.referenceHourOfDay spec.backup.snapshotSchedule.referenceMinuteOfHour spec.backup.snapshotSchedule.fullIncrementalDayOfWeek spec.backup.snapshotSchedule.clusterCheckpointIntervalMin spec.clusterDomain spec.connectivity.replicaSetHorizons spec.featureCompatibilityVersion spec.logLevel You must set  spec.clusterDomain  if your  Kubernetes  cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n Kubernetes Operator  might not function as expected. spec.configSrv.additionalMongodConfig spec.configSrvPodSpec.persistence.single spec.configSrvPodSpec.persistence.multiple.data spec.configSrvPodSpec.persistence.multiple.journal spec.configSrvPodSpec.persistence.multiple.logs spec.configSrvPodSpec.podTemplate.affinity.podAffinity spec.configSrvPodSpec.podTemplate.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution.topologyKey spec.configSrvPodSpec.podTemplate.affinity.nodeAffinity spec.configSrvPodSpec.podTemplate.metadata spec.configSrvPodSpec.podTemplate.spec spec.mongos.additionalMongodConfig spec.mongosPodSpec.podTemplate.affinity.podAffinity spec.mongosPodSpec.podTemplate.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution.topologyKey spec.mongosPodSpec.podTemplate.affinity.nodeAffinity spec.mongosPodSpec.podTemplate.metadata spec.mongosPodSpec.podTemplate.spec spec.shard.additionalMongodConfig spec.shardPodSpec.persistence.single spec.shardPodSpec.persistence.multiple.data spec.shardPodSpec.persistence.multiple.journal spec.shardPodSpec.persistence.multiple.logs spec.shardPodSpec.podTemplate.affinity.podAffinity spec.shardPodSpec.podTemplate.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution.topologyKey spec.shardPodSpec.podTemplate.affinity.nodeAffinity spec.shardPodSpec.podTemplate.metadata spec.shardPodSpec.podTemplate.spec spec.shardSpecificPodSpec Invoke the following  Kubernetes  command to create your\n sharded cluster : Check the log  after running this\ncommand. If the creation was successful, you should see a message\nsimilar to the following: To check the status of your  MongoDB  resource , use the following\ncommand: With the  -w  (watch) flag set, when the configuration changes, the output\nrefreshes immediately until the status phase achieves the  Running  state.\nTo learn more about resource deployment statuses, see  Troubleshoot the  Kubernetes Operator . After you encrypt your database resource with  TLS (Transport Layer Security) , you can secure the\nfollowing: Client authentication with LDAP Client authentication with X.509 Internal authentication with X.509 Renew your  TLS (Transport Layer Security)  certificates periodically\nusing the following procedure: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Run this  kubectl  command to renew an existing  secret  that\nstores the sharded cluster shards' certificates: Run this  kubectl  command to renew an existing  secret  that\nstores the sharded cluster config server's certificates: Run this  kubectl  command to renew an existing  secret  that\nstores the sharded cluster  mongos  certificates: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Change the settings of this  YAML (Yet Another Markup Language)  file to match your\ndesired  sharded cluster  configuration. This is a  YAML (Yet Another Markup Language)  file that you can modify to meet your desired\nconfiguration. Change the settings to match your desired\n sharded cluster  configuration. Open your preferred text editor and paste the  object  specification\ninto a new text file. Key Type Description Example metadata.name string Label for this  Kubernetes   sharded cluster   object . Resource names must be 44 characters or less. metadata.name Kubernetes  documentation on  names . myproject spec.shardCount integer Number of shards to deploy. 2 spec.mongodsPerShardCount integer Number of shard members per shard. 3 spec.mongosCount integer Number of shard routers to deploy. 2 spec.configServerCount integer Number of members of the config server replica set. 3 spec.version string Version of MongoDB that this  sharded cluster  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n MongoDB Versioning  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. For best results, use the\n latest available enterprise MongoDB version  that is  compatible  with your\n Ops Manager  version. spec.opsManager.configMapRef.name string Name of the  ConfigMap  with the  Ops Manager  connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value must exist on the  same  namespace as the resource\nyou want to create. The  Kubernetes Operator  tracks any changes to the ConfigMap and\nreconciles the state of the  MongoDB  resource . <myproject> spec.credentials string Name of the secret you\n created  as  Ops Manager   API (Application Programming Interface) \nauthentication credentials for the  Kubernetes Operator  to\ncommunicate with  Ops Manager . The  Ops Manager   Kubernetes   Secret  object\nholding the Credentials must exist on the  same  Namespace as\nthe resource you want to create. The  Kubernetes Operator  tracks any changes to the Secret and\nreconciles the state of the  MongoDB  resource . <mycredentials> spec.type string Type of  MongoDB  resource  to create. ShardedCluster spec.persistent string Optional. Flag indicating if this  MongoDB  resource  should use  Persistent Volumes  for\nstorage. Persistent volumes are not deleted when the\n MongoDB  resource  is stopped or restarted. If this value is  true , then the following values are set\nto their default value of  16Gi : To change your  Persistent Volume Claims  configuration, configure the\nfollowing collections to meet your deployment requirements: spec.shardPodSpec.persistence.single spec.configSrvPodSpec.persistence.single If you want one  Persistent Volume  for each  Pod , configure the\n spec.shardPodSpec.persistence.single  and\n spec.configSrvPodSpec.persistence.single \ncollections. If you want separate  Persistent Volumes  for data, journals, and\nlogs for each  Pod , configure the following\ncollections: In the  spec.configSrvPodSpec.persistence.multiple \ncollection:\n-  .data \n-  .journal \n-  .logs In the  spec.configSrvPodSpec.persistence.multiple  collection:\n-  .data \n-  .journal \n-  .logs Grant your containers permission to write to your  Persistent Volume .\nThe  Kubernetes Operator  sets  fsGroup = 2000 ,  runAsUser = 2000 ,\nand  runAsNonRoot = true  in  securityContext .  Kubernetes Operator \nsets  fsgroup  equal to  runAsUser  to make the volume writable\nfor a user that runs the main process in the container. To learn\nmore, see  Configure a\nSecurity Context for a Pod or Container  and the related\n discussion  in\nthe  Kubernetes  documentation. If redeploying the resource doesn't fix\nissues with your Persistent Volume, contact  MongoDB Support . If you do not use  Persistent Volumes , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n object  specification file for a  sharded cluster \ndeployment: For config server For shard routers For shard members spec.backup.assignmentLabels spec.backup.mode spec.backup.snapshotSchedule.snapshotIntervalHours spec.backup.snapshotSchedule.snapshotRetentionDays spec.backup.snapshotSchedule.dailySnapshotRetentionDays spec.backup.snapshotSchedule.weeklySnapshotRetentionWeeks spec.backup.snapshotSchedule.monthlySnapshotRetentionMonths spec.backup.snapshotSchedule.pointInTimeWindowHours spec.backup.snapshotSchedule.referenceHourOfDay spec.backup.snapshotSchedule.referenceMinuteOfHour spec.backup.snapshotSchedule.fullIncrementalDayOfWeek spec.backup.snapshotSchedule.clusterCheckpointIntervalMin spec.clusterDomain spec.connectivity.replicaSetHorizons spec.featureCompatibilityVersion spec.logLevel You must set  spec.clusterDomain  if your  Kubernetes  cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n Kubernetes Operator  might not function as expected. spec.configSrv.additionalMongodConfig spec.configSrvPodSpec.persistence.single spec.configSrvPodSpec.persistence.multiple.data spec.configSrvPodSpec.persistence.multiple.journal spec.configSrvPodSpec.persistence.multiple.logs spec.configSrvPodSpec.podTemplate.affinity.podAffinity spec.configSrvPodSpec.podTemplate.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution.topologyKey spec.configSrvPodSpec.podTemplate.affinity.nodeAffinity spec.configSrvPodSpec.podTemplate.metadata spec.configSrvPodSpec.podTemplate.spec spec.mongos.additionalMongodConfig spec.mongosPodSpec.podTemplate.affinity.podAffinity spec.mongosPodSpec.podTemplate.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution.topologyKey spec.mongosPodSpec.podTemplate.affinity.nodeAffinity spec.mongosPodSpec.podTemplate.metadata spec.mongosPodSpec.podTemplate.spec spec.shard.additionalMongodConfig spec.shardPodSpec.persistence.single spec.shardPodSpec.persistence.multiple.data spec.shardPodSpec.persistence.multiple.journal spec.shardPodSpec.persistence.multiple.logs spec.shardPodSpec.podTemplate.affinity.podAffinity spec.shardPodSpec.podTemplate.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution.topologyKey spec.shardPodSpec.podTemplate.affinity.nodeAffinity spec.shardPodSpec.podTemplate.metadata spec.shardPodSpec.podTemplate.spec spec.shardSpecificPodSpec Invoke the following  Kubernetes  command to create your\n sharded cluster : Check the log  after running this\ncommand. If the creation was successful, you should see a message\nsimilar to the following: To check the status of your  MongoDB  resource , use the following\ncommand: With the  -w  (watch) flag set, when the configuration changes, the output\nrefreshes immediately until the status phase achieves the  Running  state.\nTo learn more about resource deployment statuses, see  Troubleshoot the  Kubernetes Operator .",
            "code": [
                {
                    "lang": "none",
                    "value": "<pod-name>.<metadata.name>-sh.<namespace>.svc.cluster.local"
                },
                {
                    "lang": "none",
                    "value": "<pod-name>.<metadata.name>-cs.<namespace>.svc.cluster.local"
                },
                {
                    "lang": "none",
                    "value": "<pod-name>.<metadata.name>-svc.<namespace>.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-0-cert \\\n  --cert=<shard-0-tls-cert> \\\n  --key=<shard-0-tls-key>\n\nkubectl -n mongodb create secret tls <prefix>-<metadata.name>-1-cert \\\n  --cert=<shard-1-tls-cert> \\\n  --key=<shard-1-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-config-cert \\\n  --cert=<config-tls-cert> \\\n  --key=<config-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-mongos-cert \\\n  --cert=<mongos-tls-cert> \\\n  --key=<mongos-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap custom-ca --from-file=ca-pem=<your-custom-ca-file>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      ca: <custom-ca>\n    certsSecretPrefix: <prefix>\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "2018-06-26T10:30:30.346Z INFO operator/shardedclusterkube.go:52 Created! {\"sharded cluster\": \"my-sharded-cluster\"}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-0-cert \\\n  --cert=<shard-0-tls-cert> \\\n  --key=<shard-0-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -\n\nkubectl -n mongodb create secret tls <prefix>-<metadata.name>-1-cert \\\n  --cert=<shard-1-tls-cert> \\\n  --key=<shard-1-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-config-cert \\\n  --cert=<config-tls-cert> \\\n  --key=<config-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-mongos-cert \\\n  --cert=<mongos-tls-cert> \\\n  --key=<mongos-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "2018-06-26T10:30:30.346Z INFO operator/shardedclusterkube.go:52 Created! {\"sharded cluster\": \"my-sharded-cluster\"}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "Sharded clusters provide horizontal scaling\nfor large data sets and enable high throughput operations by\ndistributing the data set across a group of servers.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/multi-cluster-secure-internal-auth",
            "title": "Secure Internal Authentication with X.509",
            "headings": [
                "Prerequisites",
                "Configure X.509 Internal Authentication for a MongoDBMultiCluster Resource",
                "Create the secret for your agent's X.509 certificate of your MongoDBMultiCluster resource.",
                "Create the secret for the member cluster's internal X.509 certificate.",
                "Update your MongoDBMultiCluster resource to enable X509 authentication.",
                "Verify that the MongoDBMultiCluster resources are running.",
                "Renew Internal Authentication X.509 Certificates for a MongoDBMultiCluster Resource",
                "Renew the secret for a MongoDBMultiCluster resource.",
                "Renew the secret for your agent's X.509 certificates.",
                "Renew the secret for internal members's X.509 certificates of the MongoDBMultiCluster resource."
            ],
            "paragraphs": "This guide instructs you on how to configure: X.509 internal authentication between MongoDB nodes in each cluster in\nyour  multi-Kubernetes-cluster deployments . X.509 authentication from clients to your MongoDB instances. Before you secure your  multi-Kubernetes-cluster deployment  using  TLS (Transport Layer Security) \nencryption, complete the following tasks: Follow the steps in the  Multi-Cluster Quick Start Prerequisites . Deploy a   TLS-encrypted multi-Kubernetes-cluster . Create credentials  for the  Kubernetes Operator . Enabling X.509 authentication at the project level configures all\nagents to use X.509 client authentication when communicating with\nMongoDB deployments. X.509 client authentication requires one of the following: Cloud Manager Ops Manager  5.0.7 or later Run the  kubectl  command to create a new secret that stores the agent's X.509 certificate: Run the  kubectl  command to create a new secret that stores the internal\ncluster member's X.509 certificate. The member clusters are defined in\nyour  MongoDBMultiCluster  resource . Update your MongoDBMultiCluster custom resource \nwith security settings from the  Kubernetes Operator \n MongoDBMultiCluster resource specification .\nAdd the  internalCluster  setting, under  spec.authentication , and set it\nto  \"X509\" . The resulting configuration may look similar to the following example: For member clusters, run the following commands to verify that\nthe MongoDB Pods are in the running state: In the central cluster, run the following command to verify that\nthe  MongoDBMultiCluster  resource  is in the running state: If you have already created certificates, renew them periodically using\nthe following procedure. Run this  kubectl  command to renew an existing  secret  that stores the certificates for the  MongoDBMultiCluster  resource : Run the  kubectl  command to renew an existing secret that stores\nthe  MongoDBMultiCluster  resource  agent certificates: Run the  kubectl  command to renew an existing secret that stores\nX.509 certificates for internal members of the  MongoDBMultiCluster  resource :",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-clusterfile \\\n  --cert=<resource-clusterfile-tls-cert> \\\n  --key=<resource-clusterfile-tls-key>"
                },
                {
                    "lang": "yaml",
                    "value": "  apiVersion: mongodb.com/v1\n  kind: MongoDBMultiCluster\n  metadata:\n   name: multi-replica-set\n  spec:\n   version: 5.0.0-ent\n   type: ReplicaSet\n   persistent: false\n   duplicateServiceObjects: true\n   credentials: my-credentials\n   opsManager:\n     configMapRef:\n       name: my-project\n   security:\n     tls:\n        a: custom-ca\n     certsSecretPrefix: <prefix>\n   authentication:\n     enabled: true\n     modes: [\"X509\"]\n     agents:\n       mode: \"X509\"\n     internalCluster: \"X509\"\n   clusterSpecList:\n     - clusterName: ${MDB_CLUSTER_1_FULL_NAME}\n       members: 3\n     - clusterName: ${MDB_CLUSTER_2_FULL_NAME}\n       members: 2\n     - clusterName: ${MDB_CLUSTER_3_FULL_NAME}\n       members: 3\n\nThe |k8s-op-short| copies the ConfigMap with the |certauth| created in\nthe central cluster to each member cluster, generates a concatenated\n|pem| secret, and distributes it to the member clusters."
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_1_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_2_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_3_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context=$MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace mongodb \\\n  get mdbmc multi-replica-set -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n--namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-cert \\\n--cert=<resource-tls-cert> \\\n--key=<resource-tls-key> \\\n--dry-run=client \\\n-o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key> \\\n  --dry-run=client \\\n  -o yaml | kubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context $MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace=<metadata.namespace> \\\ncreate secret tls <prefix>-<metadata.name>-clusterfile \\\n  --cert=<resource-clusterfile-tls-cert> \\\n   --key=<resource-clusterfile-tls-key> \\\n   --dry-run=client \\\n   -o yaml | kubectl apply -f -"
                }
            ],
            "preview": "This guide instructs you on how to configure:",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/deploy-om-container",
            "title": "Deploy an Ops Manager Resource",
            "headings": [
                "Considerations",
                "Encrypting Connections",
                "Deploying on the Central Cluster in a Multi-Kubernetes-Cluster Deployment",
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Create secrets for your certificates.",
                "Add additional certificates to custom CA (Certificate Authority) certificates.",
                "Copy one of the following Ops Manager Kubernetes object examples.",
                "Open your preferred text editor and paste the object specification into a new text file.",
                "Configure the settings specific to your deployment.",
                "Optional: Configure Backup settings.",
                "Optional: Configure any additional settings for an Ops Manager backup.",
                "Optional: Configure any additional settings for an Ops Manager deployment.",
                "Save this file with a .yaml file extension.",
                "Create your Ops Manager instance.",
                "Track the status of your Ops Manager instance.",
                "Access the Ops Manager application.",
                "Create credentials for the Kubernetes Operator.",
                "Create a project using a ConfigMap.",
                "Deploy MongoDB database resources to complete the backup configuration.",
                "Confirm that the Ops Manager resource is running.",
                "Configure kubectl to default to your namespace.",
                "Copy one of the following Ops Manager Kubernetes object examples.",
                "Open your preferred text editor and paste the object specification into a new text file.",
                "Configure the settings included in the previous example.",
                "Optional: Configure backup settings.",
                "Optional: Configure any additional settings for an Ops Manager backup.",
                "Optional: Configure any additional settings for an Ops Manager deployment.",
                "Save this file with a .yaml file extension.",
                "Create your Ops Manager instance.",
                "Track the status of your Ops Manager instance.",
                "Access the Ops Manager application.",
                "Optional: Create credentials for the Kubernetes Operator.",
                "Optional: Create a project using a ConfigMap.",
                "Optional: Deploy MongoDB database resources to complete the backup configuration.",
                "Optional: Confirm that the Ops Manager resource is running."
            ],
            "paragraphs": "You can deploy  Ops Manager  as a resource in a  Kubernetes  container using the  Kubernetes Operator . The following considerations apply: When you configure your  Ops Manager  deployment, you must choose whether to run connections over  HTTPS (Hypertext Transfer Protocol Secure)  or  HTTP (Hypertext Transfer Protocol) . The following  HTTPS (Hypertext Transfer Protocol Secure)  procedure: The following  HTTP (Hypertext Transfer Protocol)  procedure: When running over  HTTPS (Hypertext Transfer Protocol Secure) ,  Ops Manager  runs on port  8443  by\ndefault. Select the appropriate tab based on whether you want to encrypt\nyour  Ops Manager  and application database connections with  TLS (Transport Layer Security) . Establishes  TLS (Transport Layer Security) -encrypted connections to/from the  Ops Manager \napplication. Establishes  TLS (Transport Layer Security) -encrypted connections between the application\ndatabase's replica set members. Requires valid certificates for  TLS (Transport Layer Security)  encryption. Doesn't encrypt connections to or from the  Ops Manager  application. Doesn't encrypt connections between the\napplication database's replica set members. Has fewer setup requirements. To deploy an  Ops Manager  instance in the central cluster and connect to it,\nuse the following procedures: These procedures are the same as the procedures for single clusters\ndeployed with the  Kubernetes Operator  with the following exceptions: Review the Ops Manager resource architecture Review the Ops Manager resource considerations and prerequisites Deploy an Ops Manager instance on the central cluster with TLS encryption Set the context and the namespace. If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Configure external connectivity for Ops Manager. To connect member clusters to the  Ops Manager  resource's deployment in the\ncentral cluster in a  multi-Kubernetes-cluster deployment , use one of the following methods: Set the  spec.externalConnectivity  to  true  and specify\nthe  Ops Manager  port in it. Use the  ops-manager-external.yaml \nexample script, modify it to your needs, and apply the configuration.\nFor example, run: Add the central cluster and all member clusters to the same service mesh.\nThe service mesh establishes communication from the the central and all\nmember clusters to the  Ops Manager  instance. To learn more, see the\n Multi-Kubernetes-Cluster Quick Start \nprocedures and see the step that references the  istio-injection=enabled \nlabel for Istio. Also, see  Automatic sidecar injection \nin the Istio documentation. Deploy Ops Manager and the Application Database on the central cluster. You can choose to deploy  Ops Manager  and the Application Database only on the central cluster,\nusing the same procedure as for single  Kubernetes  clusters. To learn more,\nsee  Deploy an Ops Manager instance on the central cluster with TLS encryption . Deploy Ops Manager on the central cluster and the Application Database on selected member clusters. You can choose to deploy  Ops Manager  on the central cluster and the Application\nDatabase on a subset of selected member clusters, to increase the\nApplication Database's resilience and availability in  Ops Manager . Configure\nthe following settings in the  Ops Manager  CRD: To learn more, see  Deploy Ops Manager ,\nreview the  multi-Kubernetes-cluster deployment  example and specify  MultiCluster  for\n topology . Use  topology  to specify the  MultiCluster  value. Specify the  clusterSpecList  and\ninclude in it the  clusterName \nof each selected  Kubernetes  member cluster on which you want to deploy the Application Database, and the\nnumber of  members \n(MongoDB nodes) in each  Kubernetes  member cluster. If you deploy the Application Database on selected member clusters in\nyour  multi-Kubernetes-cluster deployment , you must include the central cluster and\nmember clusters in the same service mesh configuration. This enables\nbi-directional communication from  Ops Manager  to the Application Database. Complete the  Prerequisites . Read the  Considerations . Create one  TLS (Transport Layer Security)  certificate for the Application\nDatabase's  replica set . This  TLS (Transport Layer Security)  certificate requires the following attributes: DNS Names Ensure that you add  SAN (Subject Alternative Name) s or Subject Names\nfor each  Pod  that hosts a member of the\nApplication Database replica set. The  SAN (Subject Alternative Name)  for each pod\nmust use the following format: Key Usages Ensure that the  TLS (Transport Layer Security)  certificates include the following\nkey-usages ( 5280 ): \"server auth\" \"client auth\" The  Kubernetes Operator  uses  kubernetes.io/tls  secrets\nto store  TLS (Transport Layer Security)  certificates and private keys for  Ops Manager  and MongoDB\nresources. Starting in  Kubernetes Operator  version 1.17.0, the\n Kubernetes Operator  doesn't support concatenated  PEM (Privacy-Enhanced Mail)  files stored as\n Opaque secrets . Before you deploy an  Ops Manager  resource, make sure you  plan for\nyour Ops Manager resource : Complete the  Prerequisites Read the  Considerations . Follow these steps to deploy the  Ops Manager  resource to run over\n HTTPS (Hypertext Transfer Protocol Secure)  and secure the application database using  TLS (Transport Layer Security) . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . If you're using  HashiCorp Vault  as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. To learn about your options for secret\nstorage, see  Configure Secret Storage . Once you have your  TLS (Transport Layer Security)  certificates and private keys, run the\nfollowing command to create a  secret  that stores  Ops Manager 's\n TLS (Transport Layer Security)  certificate: Run the following command to create a new  secret  that stores\nthe application database's  TLS (Transport Layer Security)  certificate: If your  Ops Manager   TLS (Transport Layer Security)  certificate is signed by a custom  CA (Certificate Authority) ,\nthe  CA (Certificate Authority)  certificate must also contain\nadditional certificates that allow  Ops Manager  Backup Daemon to download\nMongoDB binaries from the internet. To create the  TLS (Transport Layer Security) \ncertificate(s), create a  ConfigMap  to hold the  CA (Certificate Authority) \ncertificate: The  Kubernetes Operator  requires that your  Ops Manager  certificate is named\n mms-ca.crt  in the ConfigMap. Obtain the entire  TLS (Transport Layer Security)  certificate chain for  Ops Manager  from\n downloads.mongodb.com . The following  openssl  command\noutputs the certificate in the chain to your current working\ndirectory, in  .crt  format: Concatenate your  CA (Certificate Authority) 's certificate file for  Ops Manager \nwith the entire  TLS (Transport Layer Security)  certificate chain from\n downloads.mongodb.com  that\nyou obtained in the previous step: Create the  ConfigMap  for  Ops Manager : Change the settings to match your  Ops Manager  and application database configuration. Key Type Description Example metadata.name string Name for this  Kubernetes   Ops Manager   object . Resource names must be 44 characters or less. metadata.name Kubernetes  documentation on  names . om spec.replicas number Number of  Ops Manager  instances to run in parallel. The minimum valid value is  1 . For high availability, set this value to more than  1 .\n Multiple Ops Manager instances \ncan read from the same Application Database, ensuring failover if\none instance is unavailable and enabling you to update the\n Ops Manager  resource without downtime. 1 spec.version string Version of  Ops Manager  to be installed. The format should be  X.Y.Z .\nTo view available  Ops Manager  versions, view the\n container registry . 6.0.0 spec.adminCredentials string Name of the  secret  you  created \nfor the  Ops Manager  admin user. Configure the secret to use the same  namespace  as the\n Ops Manager  resource. om-admin-secret string Required . Text to prefix to the name of the secret that contains\n Ops Manager s  TLS (Transport Layer Security)  certificates. om-prod string Name of the  ConfigMap  you created to verify your\n Ops Manager   TLS (Transport Layer Security) \ncertificates signed using a custom  CA (Certificate Authority) . This field is required if you signed your\n Ops Manager   TLS (Transport Layer Security) \ncertificates using a custom  CA (Certificate Authority) . om-http-cert-ca string The  Kubernetes  service  ServiceType \nthat exposes  Ops Manager  outside of  Kubernetes . Exclude the\n spec. externalConnectivity  setting\nand its children if you don't want the  Kubernetes Operator  to\ncreate a  Kubernetes  service to route external traffic to the\n Ops Manager  application. LoadBalancer integer Number of members of the  Ops Manager Application Database \nreplica set. 3 string Required . Version of MongoDB that the  Ops Manager Application Database \nshould run. The format should be\n X.Y.Z-ubi8  for the  Enterprise edition  and  X.Y.Z  for the Community edition.\nDo not add the  -ubi8  tag suffix to the Community edition image because the  Kubernetes Operator  adds the tag suffix automatically. To learn more about MongoDB versioning, see\n MongoDB Versioning  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. For best results, use the\n latest available enterprise MongoDB version  that is  compatible  with your\n Ops Manager  version. string Optional . The type of the  Kubernetes  deployment for the Application Database.\nIf omitted, the default is  SingleCluster . If you specify  MultiCluster , the  Kubernetes Operator  ignores values\nthat you set for the  spec.applicationDatabase.members  field, if specified.\nInstead, you must specify the  clusterSpecList \nand include in it the  clusterName \nof each selected  Kubernetes  member cluster on which you want to deploy the\nApplication Database, and the number of  members \n(MongoDB nodes) in each  Kubernetes  cluster. See also the  example of the resource specification . You can't convert a single cluster  Ops Manager  instance to a\n multi-Kubernetes-cluster deployment  instance by modifying the\n topology  and\nthe  clusterSpecList \nsettings in the CRD. MultiCluster string Required . Text to prefix to the name of the secret that contains\nthe application database's  TLS (Transport Layer Security)  certificates. appdb-prod string Name of the  ConfigMap  you created to verify your\napplication database  TLS (Transport Layer Security) \ncertificates signed using a custom  CA (Certificate Authority) . This field is required if you signed your\napplication database  TLS (Transport Layer Security) \ncertificates using a custom  CA (Certificate Authority) . ca The  Kubernetes Operator  mounts the  CA (Certificate Authority)  you add using the\n spec.applicationDatabase.security.tls.ca  setting to\nboth the  Ops Manager  and the Application Database Pods. To configure backup, you must enable it, and then: You must also configure an  S3 snapshot store \nor a  blockstore . To configure a snapshot store, configure the following settings: To configure a blockstore, configure the following settings: Choose to configure an  S3 snapshot store \nor a  blockstore . If you deploy\nboth an  S3 (Simple Storage Service)   snapshot store  and a\n blockstore ,  Ops Manager \nrandomly choses one to use for backup. Choose to configure an oplog store or an  S3 (Simple Storage Service)  oplog store.\nIf you deploy both an oplog store and an  S3 (Simple Storage Service)  oplog store,  Ops Manager \nrandomly choses one of them to use for the oplog backup. Key Type Description Example boolean Flag that indicates that Backup is enabled. You must\nspecify  spec.backup.enabled: true  to configure settings\nfor the head database, oplog store, and snapshot store. true collection A collection of configuration settings for the\n head database . For descriptions of the individual\nsettings in the collection, see\n spec.backup.headDB . string Name of the oplog store. oplog1 string Name of the  S3 (Simple Storage Service)  oplog store. my-s3-oplog-store string Name of the  MongoDB  resource  or  MongoDBMultiCluster  resource  for the oplog store.\nThe resource's  metadata.name  must match this name. my-oplog-db string Name of the  MongoDB  resource  or  MongoDBMultiCluster  resource  for the  S3 (Simple Storage Service)  oplog store.\nThe resource's  metadata.name  must match this name. my-s3-oplog-db If you deploy both an  S3 snapshot store \nand a  blockstore ,  Ops Manager \nrandomly choses one to use for Backup. Key Type Description Example string Name of the  S3 (Simple Storage Service)  snapshot store. s3store1 string Name of the  secret  that contains the  accessKey  and\n secretKey  fields. The  Backup Daemon Service  uses the\nvalues of these fields as credentials to access the  S3 (Simple Storage Service)  or\n S3 (Simple Storage Service) -compatible bucket. my-s3-credentials string URL (Uniform Resource Locator)  of the  S3 (Simple Storage Service)  or  S3 (Simple Storage Service) -compatible bucket that\n stores  the\ndatabase Backup snapshots. s3.us-east-1.amazonaws.com string Name of the  S3 (Simple Storage Service)  or  S3 (Simple Storage Service) -compatible bucket that stores the\ndatabase Backup snapshots. my-bucket Key Type Description Example string Name of the blockstore. blockStore1 string Name of the  MongoDB  resource  that you create for the blockstore. You must\ndeploy this database resource in the same namespace as the  Ops Manager  resource. my-mongodb-blockstore Add any  optional settings  for backups\nthat you want to apply to your deployment to the  object  specification\nfile. For example, for each type of backup store, and for  Ops Manager  backup\ndaemon processes, you can assign labels to associate particular backup\nstores or backup daemon processes with specific projects.\nUse  spec.backup.[*].assignmentLabels  elements of the OpsManager\nresources. Add any  optional settings  that you\nwant to apply to your deployment to the  object  specification file. Run the following  kubectl  command on the filename of the\n Ops Manager  resource definition: If you are deploying an  Ops Manager  resource on a  multi-Kubernetes-cluster deployment , run: To check the status of your  Ops Manager  resource, invoke the following\ncommand: The command returns the output similar to the following under the  status \nfield while the resource deploys: The  Kubernetes Operator  reconciles the resources in the following order: The  Kubernetes Operator  doesn't reconcile a resource until the preceding\none enters the  Running  phase. After the  Ops Manager  resource completes the  Pending  phase, the\ncommand returns output similar to the following under the  status \nfield if you enabled Backup: Backup remains in a  Pending  state until you configure the Backup\ndatabases. After the resource completes the  Pending  phase, the command\nreturns output similar to the following under the  status  field: Backup remains in a  Pending  state until you configure the Backup\ndatabases. Application Database. Ops Manager . Backup. The  status.opsManager.url  field states the resource's\nconnection  URL (Uniform Resource Locator) . Using this  URL (Uniform Resource Locator) , you can reach  Ops Manager  from\ninside the  Kubernetes  cluster or  create a project using a\nConfigMap . The  status.opsManager.url  field states the resource's\nconnection  URL (Uniform Resource Locator) . Using this  URL (Uniform Resource Locator) , you can reach  Ops Manager  from\ninside the  Kubernetes  cluster or  create a project using a\nConfigMap . The steps you take differ based on how you are routing traffic to the\n Ops Manager  application in  Kubernetes . If you configured the  Kubernetes Operator  to\ncreate a  Kubernetes  service for you, or you created a  Kubernetes  service\nmanually, use one of the following methods to access the  Ops Manager \napplication: To learn how to access the  Ops Manager  application using a third-party\nservice, refer to the documentation for your solution. Query your cloud provider to get the  FQDN (fully qualified domain name)  of the load\nbalancer service. See your cloud provider's documentation\nfor details. Open a browser window and navigate to the  Ops Manager \napplication using the  FQDN (fully qualified domain name)  and port number of your load\nbalancer service. Log in to  Ops Manager  using the  admin user credentials . Set your firewall rules to allow access from the Internet to\nthe  spec.externalConnectivity. port \non the host on which your  Kubernetes  cluster is running. Open a browser window and navigate to the  Ops Manager \napplication using the  FQDN (fully qualified domain name)  and the\n spec.externalConnectivity. port . Log in to  Ops Manager  using the  admin user credentials . To configure credentials, you must create an  Ops Manager  organization,\ngenerate programmatic API keys, and create a  secret . These\nactivities follow the prerequisites and procedure on the\n Create Credentials for the  Kubernetes Operator  page. To create a project, follow the prerequisites and procedure on the\n Create One Project using a ConfigMap  page. Set the following fields in your project ConfigMap: Set  data.baseUrl  in the ConfigMap to the  Ops Manager Application 's  URL (Uniform Resource Locator) .\nTo find this  URL (Uniform Resource Locator) , invoke the following command: The command returns the URL of the  Ops Manager Application  in the\n status.opsManager.url  field, similar to the following example: If you deploy  Ops Manager  with the  Kubernetes Operator  and  Ops Manager  will\nmanage MongoDB database resources deployed  outside  of the  Kubernetes \ncluster it's deployed to, you must set  data.baseUrl  to the same\nvalue of the\n spec.configuration.mms.centralUrl \nsetting in the  Ops Manager  resource specification. Managing External MongoDB Deployments Set  data.sslMMSCAConfigMap  to the name of your\n ConfigMap  containing the root\n CA (Certificate Authority)  certificate used to sign the\n Ops Manager  host's certificate. The  Kubernetes Operator  requires that you name\nthis  Ops Manager  resource's certificate  mms-ca.crt  in the ConfigMap. By default,  Ops Manager  enables  Backup .\nCreate a MongoDB database resource for the oplog and snapshot stores\nto complete the configuration. Deploy a  MongoDB database resource  for the oplog store in the same\nnamespace as the  Ops Manager  resource. Match the  metadata.name  of the resource with the\n spec.backup.opLogStores.mongodbResourceRef.name \nthat you specified in your  Ops Manager  resource definition. Create this database as a three-member  replica set . Deploy a  MongoDB database resource  for the  S3 (Simple Storage Service)  snapshot store in the\nsame namespace as the  Ops Manager  resource. Match the  metadata.name  of the resource to the\n spec.backup.s3Stores.mongodbResourceRef.name \nthat you specified in your  Ops Manager  resource definition. Create the  S3 (Simple Storage Service)  snapshot store as a replica set. To check the status of your  Ops Manager  resource, invoke the following\ncommand: When  Ops Manager  is running, the command returns the output similar to the\nfollowing, under the  status  field: See  Troubleshoot the  Kubernetes Operator  for information about the\nresource deployment statuses. Follow these steps to deploy the  Ops Manager  resource to run over\n HTTP (Hypertext Transfer Protocol) : If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Change the settings to match your  Ops Manager  configuration. Key Type Description Example metadata.name string Name for this  Kubernetes   Ops Manager   object . Resource names must be 44 characters or less. metadata.name Kubernetes  documentation on  names . om spec.replicas number Number of  Ops Manager  instances to run in parallel. The minimum valid value is  1 . For high availability, set this value to more than  1 .\n Multiple Ops Manager instances \ncan read from the same Application Database, ensuring failover if\none instance is unavailable and enabling you to update the\n Ops Manager  resource without downtime. 1 spec.version string Version of  Ops Manager  to be installed. The format should be  X.Y.Z .\nFor the list of available  Ops Manager  versions, view the\n container registry . 6.0.0 spec.adminCredentials string Name of the secret you  created \nfor the  Ops Manager  admin user. Configure the secret to use the same  namespace  as the\n Ops Manager  resource. om-admin-secret string Optional . The  Kubernetes  service  ServiceType \nthat exposes  Ops Manager  outside of  Kubernetes . Exclude the\n spec. externalConnectivity  setting\nand its children if you don't want the  Kubernetes Operator  to\ncreate a  Kubernetes  service to route external traffic to the\n Ops Manager  application. LoadBalancer integer Number of members of the  Ops Manager Application Database \nreplica set. 3 string Required . Version of MongoDB that the  Ops Manager Application Database \nshould run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ubi8  for the  Enterprise edition . To learn more about MongoDB versioning, see\n MongoDB Versioning  in the MongoDB Manual. Ensure that you choose a  compatible MongoDB Server version . Compatible versions differ depending on the base image that the\nMongoDB database resource uses. For best results, use the\n latest available enterprise MongoDB version  that is  compatible  with your\n Ops Manager  version. string Optional . The type of the  Kubernetes  deployment for the Application Database.\nIf omitted, the default is  SingleCluster . If you specify  MultiCluster , the  Kubernetes Operator  ignores\nvalues that you set for the\n spec.applicationDatabase.members \nfield, if specified. Instead, you must specify the  clusterSpecList \nand include in it the  clusterName \nof each selected  Kubernetes  member cluster on which you want to deploy the Application Database, and\nthe number of  members \n(MongoDB nodes) in each  Kubernetes  cluster. See also the  example of the resource specification . You can't convert a single cluster  Ops Manager  instance to a\n multi-Kubernetes-cluster deployment  instance by modifying the\n topology  and\nthe  clusterSpecList \nsettings in the CRD. MultiCluster To configure backup, you must enable it, and then: You must also configure an  S3 snapshot store \nor a  blockstore . To configure an  S3 (Simple Storage Service)  snapshot store, configure the following settings: To configure a blockstore, configure the following settings: Choose to configure an  S3 snapshot store \nor a  blockstore . If you deploy\nboth an  S3 (Simple Storage Service)   snapshot store  and a\n blockstore ,  Ops Manager \nrandomly choses one to use for backup. Choose to configure an oplog store or an  S3 (Simple Storage Service)  oplog store.\nIf you deploy both an oplog store and an  S3 (Simple Storage Service)  oplog store,  Ops Manager \nrandomly choses one of them to use for the oplog backup. Key Type Description Example boolean Flag that indicates that backup is enabled. You must specify\n spec.backup.enabled: true  to configure settings\nfor the head database, oplog store,  S3 (Simple Storage Service)  oplog store, and snapshot store. true collection A collection of configuration settings for the\n head database . For descriptions of the individual\nsettings in the collection, see\n spec.backup.headDB . string Name of the oplog store. oplog1 string Name of the  S3 (Simple Storage Service)  oplog store. my-s3-oplog-store string Name of the  MongoDB  resource  or  MongoDBMultiCluster  resource  for the oplog store.\nThe resource's  metadata.name  must match this name. my-oplog-db string Name of the MongoDB database resource for the  S3 (Simple Storage Service)  oplog store. my-s3-oplog-db If you deploy both an  S3 (Simple Storage Service)   snapshot store \nand a  blockstore ,  Ops Manager \nrandomly choses one to use for backup. Key Type Description Example string Name of the  S3 (Simple Storage Service)  snapshot store. s3store1 string Name of the secret that contains the  accessKey  and\n secretKey  fields. The  Backup Daemon Service  uses the\nvalues of these fields as credentials to access the  S3 (Simple Storage Service)  or\n S3 (Simple Storage Service) -compatible bucket. my-s3-credentials string URL (Uniform Resource Locator)  of the  S3 (Simple Storage Service)  or  S3 (Simple Storage Service) -compatible bucket that\n stores  the\ndatabase backup snapshots. s3.us-east-1.amazonaws.com string Name of the  S3 (Simple Storage Service)  or  S3 (Simple Storage Service) -compatible bucket that stores the\ndatabase backup snapshots. my-bucket string Region where your  S3 (Simple Storage Service) -compatible bucket resides. Use this\nfield only if your  S3 (Simple Storage Service)  store's\n s3BucketEndpoint \ndoesn't include a region in its  URL (Uniform Resource Locator) . Don't use this field with  AWS (Amazon Web Services)   S3 (Simple Storage Service)  buckets. us-east-1 Key Type Description Example string Name of the blockstore. blockStore1 string Name of the  MongoDB  resource   that you create for the blockstore.\nYou must deploy this database resource in the same namespace as\nthe  Ops Manager  resource. The resource's  metadata.name  must match this name. my-mongodb-blockstore Add any  optional settings  for backups\nthat you want to apply to your deployment to the  object  specification\nfile. For example, for each type of backup store, and for  Ops Manager  backup\ndaemon processes, you can assign labels to associate particular backup\nbackup stores or backup daemon processes with specific projects.\nUse  spec.backup.[*].assignmentLabels  elements of the OpsManager\nresources. Add any  optional settings  that you\nwant to apply to your deployment to the  object  specification file. Run the following  kubectl  command on the filename of the  Ops Manager  resource definition: If you are deploying an  Ops Manager  resource on a  multi-Kubernetes-cluster deployment , run: To check the status of your  Ops Manager  resource, invoke the following command: The command returns output similar to the following, under the  status  field\nwhile the resource deploys: The  Kubernetes Operator  reconciles the resources in the following order: The  Kubernetes Operator  doesn't reconcile a resource until the preceding\none enters the  Running  phase. After the  Ops Manager  resource completes the  Pending  phase, the\ncommand returns the output similar to the following under the  status \nfield if you enabled backup: Backup remains in a  Pending  state until you configure the backup\ndatabases. Application Database. Ops Manager . Backup. The  status.opsManager.url  field states the resource's\nconnection  URL (Uniform Resource Locator) . Using this  URL (Uniform Resource Locator) , you can reach  Ops Manager  from\ninside the  Kubernetes  cluster or  create a project using a\nConfigMap . The steps you take differ based on how you are routing traffic to the\n Ops Manager  application in  Kubernetes . If you configured the  Kubernetes Operator  to\ncreate a  Kubernetes  service for you, or you created a  Kubernetes  service\nmanually, use one of the following methods to access the  Ops Manager \napplication: To learn how to access the  Ops Manager  application using a third-party\nservice, refer to the documentation for your solution. Query your cloud provider to get the  FQDN (fully qualified domain name)  of the load\nbalancer service. See your cloud provider's documentation\nfor details. Open a browser window and navigate to the  Ops Manager \napplication using the  FQDN (fully qualified domain name)  and port number of your load\nbalancer service. Log in to  Ops Manager  using the  admin user credentials . Set your firewall rules to allow access from the Internet to\nthe  spec.externalConnectivity. port \non the host on which your  Kubernetes  cluster is running. Open a browser window and navigate to the  Ops Manager \napplication using the  FQDN (fully qualified domain name)  and the\n spec.externalConnectivity. port . Log in to  Ops Manager  using the  admin user credentials . If you enabled backup, you must create an  Ops Manager  organization,\ngenerate programmatic API keys, and create a secret in your  secret-storage-tool .\nThese activities follow the prerequisites and procedure on the\n Create Credentials for the  Kubernetes Operator  page. If you enabled backup, create a project by following the prerequisites\nand procedure on the  Create One Project using a ConfigMap  page. You must set  data.baseUrl  in the ConfigMap to the  Ops Manager Application 's  URL (Uniform Resource Locator) . To find this  URL (Uniform Resource Locator) , invoke the following command: The command returns the URL of the  Ops Manager Application  in the\n status.opsManager.url  field, similar to the following example: If you deploy  Ops Manager  with the  Kubernetes Operator  and  Ops Manager  will\nmanage MongoDB database resources deployed  outside  of the  Kubernetes \ncluster it's deployed to, you must set  data.baseUrl  to the same\nvalue of the\n spec.configuration.mms.centralUrl \nsetting in the  Ops Manager  resource specification. Managing External MongoDB Deployments If you enabled  Backup ,\ncreate a MongoDB database resource for the oplog and snapshot stores\nto complete the configuration. Deploy a  MongoDB database resource  for the oplog store in the same\nnamespace as the  Ops Manager  resource. Match the  metadata.name  of the resource with the\n spec.backup.opLogStores.mongodbResourceRef.name \nthat you specified in your  Ops Manager  resource definition. Create this database as a  replica set . Choose one of the following: Deploy a  MongoDB database resource  for the blockstore in the\nsame namespace as the  Ops Manager  resource. Match the  metadata.name  of the resource to the\n spec.backup.blockStores.mongodbResourceRef.name \nthat you specified in your  Ops Manager  resource definition. Configure an  S3 (Simple Storage Service)  bucket to use as the  S3 (Simple Storage Service)  snapshot store. Ensure that you can access the  S3 (Simple Storage Service)  bucket using the details\nthat you specified in your  Ops Manager  resource definition. If you enabled backup, check the status of your  Ops Manager  resource by\ninvoking the following command: When  Ops Manager  is running, the command returns the following\noutput under the  status  field: See  Troubleshoot the  Kubernetes Operator  for information about the\nresource deployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply \\\n --context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" \\\n --namespace \"mongodb\" \\\n  -f https://raw.githubusercontent.com/mongodb/mongodb-enterprise-kubernetes/master/samples/ops-manager/ops-manager-external.yaml"
                },
                {
                    "lang": "sh",
                    "value": "<opsmgr-metadata.name>-db-<index>.<opsmgr-metadata.name>-db-svc.<namespace>.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-cert \\\n  --cert=<om-tls-cert> \\\n  --key=<om-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-db-cert \\\n  --cert=<appdb-tls-cert> \\\n  --key=<appdb-tls-key>"
                },
                {
                    "lang": "sh",
                    "value": "openssl s_client -showcerts -verify 2 \\\n-connect downloads.mongodb.com:443 -servername downloads.mongodb.com < /dev/null \\\n| awk '/BEGIN/,/END/{ if(/BEGIN/){a++}; out=\"cert\"a\".crt\"; print >out}'"
                },
                {
                    "lang": "sh",
                    "value": "cat cert1.crt cert2.crt cert3.crt cert4.crt  >> mms-ca.crt"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap om-http-cert-ca --from-file=\"mms-ca.crt\""
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: <myopsmanager>\nspec:\n  replicas: 1\n  version: <opsmanagerversion>\n  adminCredentials: <adminusercredentials> # Should match metadata.name\n                                           # in the Kubernetes secret\n                                           # for the admin user\n\n  externalConnectivity:\n    type: LoadBalancer\n  security:\n      certsSecretPrefix: <prefix> # Required. Text to prefix \n                                  # the name of the secret that contains\n                                  # Ops Manager's TLS certificate.\n      tls:\n        ca: \"om-http-cert-ca\"  # Optional. Name of the ConfigMap file\n                               # containing the certificate authority that\n                               # signs the certificates used by the Ops\n                               # Manager custom resource.\n\n  applicationDatabase:\n    topology: SingleCluster\n    members: 3\n    version: \"4.4.0-ubi8\"\n    security:\n      certsSecretPrefix: <prefix> # Required. Text to prefix to the \n                                  # name of the secret that contains the Application\n                                  # Database's TLS certificate. Name the secret \n                                  # <prefix>-<metadata.name>-db-cert.\n      tls:\n        ca: \"appdb-ca\" # Optional. Name of the ConfigMap file\n                       # containing the certicate authority that\n                       # signs the certificates used by the\n                       # application database.\n\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: <myopsmanager>\nspec:\n  replicas: 1\n  version: <opsmanagerversion>\n  adminCredentials: <adminusercredentials> # Should match metadata.name\n                                           # in the Kubernetes secret\n                                           # for the admin user\n\n  externalConnectivity:\n    type: LoadBalancer\n  security:\n      certsSecretPrefix: <prefix> # Required. Text to prefix \n                                  # the name of the secret that contains\n                                  # Ops Manager's TLS certificate.\n      tls:\n        ca: \"om-http-cert-ca\"  # Optional. Name of the ConfigMap file\n                               # containing the certificate authority that\n                               # signs the certificates used by the Ops\n                               # Manager custom resource.\n\n  applicationDatabase:\n    topology: MultiCluster\n    clusterSpecList:\n    - clusterName: cluster1.example.com\n      members: 4\n    - clusterName: cluster2.example.com\n      members: 3\n    - clusterName: cluster3.example.com\n      members: 2\n    version: \"4.4.0-ubi8\"\n    security:\n      certsSecretPrefix: <prefix> # Required. Text to prefix to the \n                                  # name of the secret that contains the Application\n                                  # Database's TLS certificate. Name the secret \n                                  # <prefix>-<metadata.name>-db-cert.\n      tls:\n        ca: \"appdb-ca\" # Optional. Name of the ConfigMap file\n                       # containing the certicate authority that\n                       # signs the certificates used by the\n                       # application database.\n\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply \\\n  --context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" \\\n  --namespace \"mongodb\"\n   -f https://raw.githubusercontent.com/mongodb/mongodb-enterprise-kubernetes/master/samples/ops-manager/ops-manager-external.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n applicationDatabase:\n  lastTransition: \"2022-04-01T09:49:22Z\"\n  message: AppDB Statefulset is not ready yet\n  phase: Pending\n  type: \"\"\n  version: \"\"\n backup:\n  phase: \"\"\n opsManager:\n  phase: \"\""
                },
                {
                    "lang": "yaml",
                    "value": "status:\n applicationDatabase:\n     lastTransition: \"2022-04-01T09:50:20Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: \"6.0.5-ubi8\"\n  backup:\n   lastTransition: \"2022-04-01T09:57:42Z\"\n   message: The MongoDB object <namespace>/<oplogresourcename>\n     doesn't exist\n   phase: Pending\n   opsManager:\n     lastTransition: \"2023-04-01T09:57:40Z\"\n     phase: Running\n     replicas: 1\n     url: https://om-svc.cloudqa.svc.cluster.local:8443\n     version: \"6.0.17\""
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n     lastTransition: \"2022-12-06T18:23:22Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: \"6.0.5-ubi8\"\n   opsManager:\n     lastTransition: \"2022-12-06T18:23:26Z\"\n     message: The MongoDB object namespace/oplogdbname doesn't exist\n     phase: Pending\n     url: https://om-svc.dev.svc.cluster.local:8443\n     version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "https://ops.example.com:8443"
                },
                {
                    "lang": "sh",
                    "value": "https://ops.example.com:30036"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n   applicationDatabase:\n     lastTransition: \"2022-12-06T18:23:22Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: \"6.0.5-ubi8\"\n   opsManager:\n     lastTransition: \"2022-12-06T18:23:26Z\"\n     message: The MongoDB object namespace/oplogdbname doesn't exist\n     phase: Pending\n     url: https://om-svc.dev.svc.cluster.local:8443\n     version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2022-12-06T17:46:15Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: \"6.0.5-ubi8\"\n  opsManager:\n    lastTransition: \"2022-12-06T17:46:32Z\"\n    phase: Running\n    replicas: 1\n    url: https://om-backup-svc.dev.svc.cluster.local:8443\n    version: \"6.0.17\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: <myopsmanager>\nspec:\n  replicas: 1\n  version: <opsmanagerversion>\n  adminCredentials: <adminusercredentials> # Should match metadata.name\n                                           # in the secret\n                                           # for the admin user\n  externalConnectivity:\n    type: LoadBalancer\n\n  applicationDatabase:\n    topology: SingleCluster\n    members: 3\n    version: <mongodbversion>\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: <myopsmanager>\nspec:\n  replicas: 1\n  version: <opsmanagerversion>\n  adminCredentials: <adminusercredentials> # Should match metadata.name\n                                           # in the Kubernetes secret\n                                           # for the admin user\n\n  externalConnectivity:\n    type: LoadBalancer\n\n  applicationDatabase:\n    topology: MultiCluster\n    clusterSpecList:\n    - clusterName: cluster1.example.com\n      members: 4\n    - clusterName: cluster2.example.com\n      members: 3\n    - clusterName: cluster3.example.com\n      members: 2\n    version: \"6.0.5-ubi8\"\n\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply \\\n  --context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" \\\n  --namespace \"mongodb\"\n   -f https://raw.githubusercontent.com/mongodb/mongodb-enterprise-kubernetes/master/samples/ops-manager/ops-manager-external.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n applicationDatabase:\n  lastTransition: \"2023-04-01T09:49:22Z\"\n  message: AppDB Statefulset is not ready yet\n  phase: Pending\n  type: \"\"\n  version: \"\"\n backup:\n  phase: \"\"\n opsManager:\n  phase: \"\""
                },
                {
                    "lang": "yaml",
                    "value": " status:\n   applicationDatabase:\n     lastTransition: \"2023-04-01T09:50:20Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: \"6.0.5-ubi8\"\n  backup:\n   lastTransition: \"2022-04-01T09:57:42Z\"\n   message: The MongoDB object <namespace>/<oplogresourcename>\n     doesn't exist\n   phase: Pending\n   opsManager:\n     lastTransition: \"2022-04-01T09:57:40Z\"\n     phase: Running\n     replicas: 1\n     url: http://om-svc.cloudqa.svc.cluster.local:8080\n     version: \"6.0.17\""
                },
                {
                    "lang": "sh",
                    "value": "http://ops.example.com:8080"
                },
                {
                    "lang": "sh",
                    "value": "http://ops.example.com:30036"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": " status:\n   applicationDatabase:\n     lastTransition: \"2022-04-01T10:00:32Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: \"6.0.5-ubi8\"\n  backup:\n   lastTransition: \"2022-04-01T09:57:42Z\"\n   message: The MongoDB object <namespace>/<oplogresourcename>\n     doesn't exist\n   phase: Pending\n   opsManager:\n     lastTransition: \"2022-04-01T09:57:40Z\"\n     phase: Running\n     replicas: 1\n     url: http://om-svc.cloudqa.svc.cluster.local:8080\n     version: \"6.0.17\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2022-04-01T10:00:32Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: \"6.0.5-ubi8\"\n  backup:\n    lastTransition: \"2022-04-01T10:00:53Z\"\n    phase: Running\n    version: \"6.0.5-ubi8\"\n  opsManager:\n    lastTransition: \"2022-04-01T10:00:34Z\"\n    phase: Running\n    replicas: 1\n    url: http://om-svc.cloudqa.svc.cluster.local:8080\n    version: \"6.0.17\""
                }
            ],
            "preview": "You can deploy Ops Manager as a resource in a Kubernetes container using the Kubernetes Operator.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/upgrade-k8s-operator",
            "title": "Upgrade the MongoDB Enterprise Kubernetes Operator",
            "headings": [
                "Upgrade using Kubernetes",
                "Upgrade the CustomResourceDefinitions for MongoDB deployments.",
                "Optional: Customize the Kubernetes Operator YAML (Yet Another Markup Language) before upgrading it.",
                "If you are upgrading the Operator to version 1.13.0 or later, specify the spec.opsManager.configMapRef.name or the spec.cloudManager.configMapRef.name settings:",
                "Upgrade to the new version of the Kubernetes Operator.",
                "Update to the latest version of the MongoDB Helm Charts for Kubernetes.",
                "If you are upgrading the Operator to version 1.13.0 or later, specify the spec.opsManager.configMapRef.name or the spec.cloudManager.configMapRef.name settings:",
                "Optional: Customize your Helm Chart before upgrading it.",
                "Upgrade the Kubernetes Operator.",
                "If you are upgrading the Operator to version 1.13.0 or later, specify the spec.opsManager.configMapRef.name or the spec.cloudManager.configMapRef.name settings:",
                "Optional: Customize your Helm Chart before upgrading it.",
                "Upgrade to the latest version of the Kubernetes Operator.",
                "Upgrade using OpenShift",
                "Upgrade the CustomResourceDefinitions for MongoDB deployments.",
                "Optional: Customize the Kubernetes Operator YAML (Yet Another Markup Language) before upgrading it.",
                "If you are upgrading the Operator to version 1.13.0 or later, specify the spec.opsManager.configMapRef.name or the spec.cloudManager.configMapRef.name settings:",
                "Upgrade to the new version of the Kubernetes Operator.",
                "Update to the latest version of the MongoDB Helm Charts for Kubernetes.",
                "If you are upgrading the Operator to version 1.13.0 or later, specify the spec.opsManager.configMapRef.name or the spec.cloudManager.configMapRef.name settings:",
                "Optional: Customize your Helm Chart before upgrading it.",
                "Upgrade the Kubernetes Operator.",
                "If you are upgrading the Operator to version 1.13.0 or later, specify the spec.opsManager.configMapRef.name or the spec.cloudManager.configMapRef.name settings:",
                "Optional: Customize your Helm Chart before upgrading it.",
                "Upgrade to the latest version of the Kubernetes Operator."
            ],
            "paragraphs": "The following procedure outlines how to upgrade the  Kubernetes Operator \nto its latest version. In  Kubernetes Operator  1.20, the  container registry  changed for the  application database  image and the images use a new tag suffix. When you  upgrade the Kubernetes Operator , the  Kubernetes Operator  automatically updates the earlier suffix,  -ent , for all images that reference the new container registry to  -ubi8  or the suffix set in  MDB_IMAGE_TYPE  or  mongodb.imageType . For example, the  Kubernetes Operator  changes  quay.io/mongodb/mongodb-enterprise-server:4.4.5-ent  to  quay.io/mongodb/mongodb-enterprise-server:4.4.5-ubi8 . To stop the  Kubernetes Operator  from automatically updating the suffix, set  MDB_APPDB_ASSUME_OLD_FORMAT  or  mongodb.appdbAssumeOldFormat  to  true . For example, you might want to stop the automatic suffix change if you're mirroring this image from your own repository. The following steps depend on how your environment is configured: Invoke the following  kubectl  command: To learn about optional  Kubernetes Operator  installation settings,\nsee  Operator kubectl and oc Installation Settings . To learn more, see  Changes to the MongoDB Resource . Open the  MongoDB Database Resource Specification  in the editor of your choice. Add the value to the  spec.opsManager.configMapRef.name \nsetting or the\n spec.cloudManager.configMapRef.name  setting and save\nthe specification. Invoke the following  helm  command: To learn more, see  Changes to the MongoDB Resource . Open the  MongoDB Database Resource Specification  in the editor of your choice. Add the value to the  spec.opsManager.configMapRef.name \nsetting or the\n spec.cloudManager.configMapRef.name  setting and save\nthe specification. To learn about optional  Kubernetes Operator  installation settings, see\n Operator Helm Installation Settings . Invoke the following  helm  command: To upgrade the  Kubernetes Operator  on a host not connected to the\nInternet: To learn more, see  Changes to the MongoDB Resource . Open the  MongoDB Database Resource Specification  in the editor of your choice. Add the value to the  spec.opsManager.configMapRef.name \nsetting or the\n spec.cloudManager.configMapRef.name  setting and save\nthe specification. To learn about optional  Kubernetes Operator  installation settings, see\n Operator Helm Installation Settings . Invoke the following  helm upgrade  command.\nUse the  registry.pullPolicy=IfNotPresent  setting. To learn\nabout optional  Kubernetes Operator  installation settings, see\n Operator Helm Installation Settings . The following steps depend on how your environment is configured: Invoke the following  oc  command: To learn about optional  Kubernetes Operator  installation settings,\nsee  Operator kubectl and oc Installation Settings . To learn more, see  Changes to the MongoDB Resource . Open the  MongoDB Database Resource Specification  in the editor of your choice. Add the value to the  spec.opsManager.configMapRef.name \nsetting or the\n spec.cloudManager.configMapRef.name  setting and save\nthe specification. Invoke the following  oc  command: To learn more, see  Changes to the MongoDB Resource . Open the  MongoDB Database Resource Specification  in the editor of your choice. Add the value to the  spec.opsManager.configMapRef.name \nsetting or the\n spec.cloudManager.configMapRef.name  setting and save\nthe specification. To learn about optional  Kubernetes Operator  installation settings, see\n Operator Helm Installation Settings . Invoke the following  helm upgrade  command.\nUse  values-openshift.yaml  settings. To learn\nabout optional  Kubernetes Operator  installation settings, see\n Operator Helm Installation Settings . To upgrade the  Kubernetes Operator  on a host not connected to the\nInternet: To learn more, see  Changes to the MongoDB Resource . Open the  MongoDB Database Resource Specification  in the editor of your choice. Add the value to the  spec.opsManager.configMapRef.name \nsetting or the\n spec.cloudManager.configMapRef.name  setting and save\nthe specification. To learn about optional  Kubernetes Operator  installation settings, see\n Operator Helm Installation Settings . Invoke the following  helm upgrade  command: Use the  values-openshift.yaml \nsettings,  registry.pullPolicy=IfNotPresent , and\n registry.imagePullSecrets=<openshift-pull-secret> . To learn\nabout optional  Kubernetes Operator  installation settings, see\n Operator Helm Installation Settings . To troubleshoot your  Kubernetes Operator , see  Review Logs from the  Kubernetes Operator \nand other  troubleshooting topics . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl replace -f crds.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade enterprise-operator mongodb/enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "helm repo update mongodb https://mongodb.github.io/helm-charts"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade enterprise-operator mongodb/enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade enterprise-operator mongodb/enterprise-operator \\\n  --set registry.pullPolicy='IfNotPresent'"
                },
                {
                    "lang": "sh",
                    "value": "oc replace -f crds.yaml"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f mongodb-enterprise-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm repo update mongodb https://mongodb.github.io/helm-charts"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade enterprise-operator mongodb/enterprise-operator \\\n  --values https://raw.githubusercontent.com/mongodb/helm-charts/main/charts/enterprise-operator/values-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm upgrade enterprise-operator mongodb/enterprise-operator \\\n    --set registry.pullPolicy='IfNotPresent' \\\n    --set registry.imagePullSecrets='<openshift-pull-secret>' \\\n    --values https://raw.githubusercontent.com/mongodb/helm-charts/main/charts/enterprise-operator/values-openshift.yaml"
                }
            ],
            "preview": "The following procedure outlines how to upgrade the Kubernetes Operator\nto its latest version.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/configure-om-queryable-backups",
            "title": "Configure Queryable Backups for Ops Manager Resources",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Create the PEM file for backups.",
                "Create a secret containing the PEM file.",
                "Configure Ops Manager custom resource to use the secret.",
                "Save your Ops Manager config file.",
                "Apply changes to your Ops Manager deployment.",
                "Track the status of the mounted volumes and Secrets."
            ],
            "paragraphs": "You can configure  queryable backups \nfor  Ops Manager  resources that you deploy in the  Kubernetes Operator . Queryable backups allow you to  run queries \non specific backup snapsnots from your  Ops Manager  resources. Querying  Ops Manager \nbackups helps you compare data from different snapshots and identify the\nbest snapshot to use for  restoring data . In the following procedure you: Once the  Kubernetes Operator  deploys the updated configuration for its custom\nresource,  Ops Manager  can read the secret from the  spec.backup.queryableBackupSecretRef.name \nparameter. You can now access the backup snapshots and run queries on them. In the  Ops Manager  documentation, queryable backups are also\nreferred to as queryable snapshots, or queryable restores. Create the  queryable.pem \nfile that holds the certificates for accessing the backup snapshots that you intend to query. Create the secret containing the  queryable.pem  file. Configure an  Ops Manager  custom resource to use the secret for queryable backups. Save the  Ops Manager  custom resource configuration and apply it. Before you configure queryable backups, complete the following tasks: Install the Kubernetes Operator . Deploy the Ops Manager application . Configure Backup Settings for the Ops Manager Resource .\nIn the linked procedures, see the steps for configuring backups. After you configure queryable backups, you can  query them \nto select the best backup snapshot to use for  restoring data . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Create the  Ops Manager queryable.pem \nfile that you will use for accessing and querying backups based on\nyour deployment's  TLS (Transport Layer Security)  requirements. The PEM file contains a public\nkey certificate and its associated private key that are needed to\naccess and run queries on backup snapshots in  Ops Manager . To learn more about the PEM file's requirements, see\n Authorization and Authentication Requirements in Ops Manager . Run the following command to create a secret with the\n queryable.pem \nfile that you created in the previous step: If you're using  HashiCorp Vault  as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. To learn about your options for secret\nstorage, see  Configure Secret Storage . Configure  spec.backup.queryableBackupSecretRef.name  to\nreference the  queryable.pem \nsecret. Invoke the following  kubectl  command on the filename of the\n Ops Manager  resource definition: When you apply the changes to your  Ops Manager  resource\ndefinition,  Kubernetes  updates the  Ops Manager  StatefulSet,\ncreates the volumes, and mounts the Secrets. Obtain the list of persistent volume claims: Obtain the Secrets: Check the status of your  Ops Manager  resources: The  -w  flag means \"watch\". With the \"watch\" flag set, the\noutput refreshes immediately when the configuration changes until\nthe status phase achieves the  Running  state. To learn more about the resource deployment statuses, see\n Troubleshoot the  Kubernetes Operator .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic queryable-pem --from-file=./queryable.pem"
                },
                {
                    "lang": "yaml",
                    "value": "  apiVersion: mongodb.com/v1\n  kind: MongoDBOpsManager\n  metadata:\n    name: ops-manager\n  spec:\n    replicas: 1\n    version: 6.0.0\n    adminCredentials: ops-manager-admin-secret\n    backup:\n      enabled: true\n      queryableBackupSecretRef:\n        name: om-queryable-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pvc"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get secrets"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om <resource-name> -o yaml -w"
                }
            ],
            "preview": "You can configure queryable backups\nfor Ops Manager resources that you deploy in the Kubernetes Operator.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/create-vault-secret",
            "title": "Create Secrets in HashiCorp Vault",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Obtain the Ops Manager public and private Keys.",
                "Create the secret in Vault.",
                "Verify the Vault secret creation was successful."
            ],
            "paragraphs": "After you  set your secret storage tool  to\n HashiCorp Vault , you must also create secrets in  Vault . This applies\nwhen you're manually migrating your existing  Kubernetes   secrets \nor you're creating secrets for the first time. For a list of secrets that you must manually migrate to  Vault , see the\n Vault  section of  Configure Secret Storage . The following tutorial stores your  Programmatic API Key  in  Vault . You can adapt the commands in this\nprocedure to add other secrets to  Vault  by changing the base path, the\nnamespace, and the secret name. To learn more about  secret storage tools , see  Configure Secret Storage . To create credentials for the  Kubernetes Operator  in  Vault , you must: Have or create an  Ops Manager \n Organization . Have or generate a\n Programmatic API Key . Grant this new  Programmatic API Key  the  Project Owner  role. Add the  IP (Internet Protocol)  or  CIDR (Classless Inter-Domain Routing)  block of any hosts that serve the\n Kubernetes Operator  to the\n API Access List . Set up a  Vault  instance and  enable Vault . Ensure that  Vault  is  not  running in  dev mode \nand that your  Vault  installation follows any applicable\n configuration recommendations . To create your secret in  Vault : Make sure you have the public and private keys for your desired\n Ops Manager   Programmatic API Key . Invoke the following  Vault  command to create your secret, replacing\nthe variables with the values in the table: Placeholder Description {Namespace} Label that identifies the namespace where you deployed  Kubernetes Operator . {SecretName} Human-readable label that identifies the secret you're creating in  Vault . {PublicKey} The public key for your desired  Ops Manager   Programmatic API Key . {PrivateKey} The private key for your desired  Ops Manager   Programmatic API Key . Invoke the following  Vault  command to verify your secret, replacing\nthe variables with the values in the following table: This command returns a secret description in the shell: Placeholder Description {Namespace} Label that identifies the namespace where you deployed  Kubernetes Operator . {SecretName} Human-readable label that identifies the secret you're creating in  Vault .",
            "code": [
                {
                    "lang": "sh",
                    "value": "  vault kv put secret/data/mongodbenterprise/operator/{Namespace}/{SecretName} publicKey={PublicKey} privateKey={PrivateKey}\n\nThe path in this command is the default path. You can replace ``mongodbenterprise/operator`` with\nyour base path if you customized your |k8s-op-short| configuration."
                },
                {
                    "lang": "sh",
                    "value": "vault kv get secret/data/mongodbenterprise/operator/{Namespace}/{SecretName}"
                },
                {
                    "lang": "sh",
                    "value": "====== Metadata ======\nKey              Value\n---              -----\ncreated_time     2021-12-15T17:20:22.985303Z\ndeletion_time    n/a\ndestroyed        false\nversion          1\n\n======= Data =======\nKey          Value\n---          -----\npublicKey    {PublicKey}\nprivateKey   {PrivateKey}"
                }
            ],
            "preview": "After you set your secret storage tool to\nHashiCorp Vault, you must also create secrets in Vault. This applies\nwhen you're manually migrating your existing Kubernetes secrets\nor you're creating secrets for the first time.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/configure-kmip-backup-encryption",
            "title": "Configure KMIP Backup Encryption for Ops Manager",
            "headings": [
                "Limitations",
                "Procedure",
                "Create the ConfigMap of the CA (Certificate Authority).",
                "Configure the Ops Manager custom resource to use KMIP (Key Management Interoperability) backup encryption.",
                "Save your Ops Manager config file.",
                "Apply changes to your Ops Manager deployment.",
                "Check the status of your Ops Manager resources.",
                "Create the secret of the client certificate and private key.",
                "Configure your MongoDB database deployment.",
                "Save your MongoDB database deployment config file.",
                "Apply changes to your MongoDB database deployment.",
                "Check the status of your MongoDB database deployment."
            ],
            "paragraphs": "Ops Manager  can encrypt backup jobs. You can use the  Kubernetes Operator  to\nconfigure  KMIP (Key Management Interoperability)  backup encryption for  Ops Manager . To learn more, see\n Encrypted Backup Snapshots . For deployments where the same  Kubernetes Operator  instance is not managing both the\n MongoDBOpsManager  and\n MongoDB  custom resources,\nyou must manually configure  KMIP (Key Management Interoperability) \nbackup encryption client settings in the\n MongoDBOpsManager  custom resource.\nThis requirement involves including client certificates for each MongoDB database,\nwhich you can achieve by overriding the  Ops Manager  Pod's StatefulSet to mount\nthe certificates. To learn more, see  Manually Configure KMIP Backup Encryption . Run the following command: Configure the  spec.backup.encryption.kmip  settings. Invoke the following  kubectl  command on the filename of the\n Ops Manager  resource definition: Run the following command: Run the following command: The client certificate  secret  name has the following naming\nconvention inferred from the  MongoDB   CustomResourceDefinition : To learn more, see  kubernetes.io/tls . clientCertificatePrefix Human-readable label specified in the\n spec.backup.encryption.kmip.client.clientCertificatePrefix  field of the  MongoDB   CustomResourceDefinition . objectMeta.name Human-readable label specified in the  metadata.name \nfield of the  MongoDB   CustomResourceDefinition . client-kmip Fixed suffix that the  Kubernetes Operator  assumes. Configure the  spec.backup.encryption.kmip  settings. To learn more, see  deploy a replica set \nor  deploy a sharded cluster . Invoke the following  kubectl  command on the filename of the\n Ops Manager  resource definition: Run the following command:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create configmap mongodb-kmip-certificate-authority-pem --from-file=ca-pem"
                },
                {
                    "lang": "yaml",
                    "value": "  apiVersion: mongodb.com/v1\n  kind: MongoDBOpsManager\n  metadata:\n    name: om-backup-kmip\n  spec:\n    replicas: 1\n    version: 6.0.0\n    adminCredentials: ops-manager-admin-secret\n    backup:\n      encryption:\n        kmip:\n          server:\n            url: kmip.corp.mongodb.com:5696\n            ca: mongodb-kmip-certificate-authority-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls mongodb-kmip-client-pem-my-replica-set-client-kmip \\\n--cert=<path-to-cert-file> \\\n--key=<path-to-key-file>"
                },
                {
                    "lang": "sh",
                    "value": "<clientCertificatePrefix>-<objectMeta.name>-client-kmip"
                },
                {
                    "lang": "yaml",
                    "value": "  apiVersion: mongodb.com/v1\n  kind: MongoDB\n  metadata:\n    name: my-replica-set\n  spec:\n    members: 3\n    version: 4.0.20\n    type: ReplicaSet\n    backup:\n      encryption:\n        kmip:\n          client:\n            clientCertificatePrefix: mongodb-kmip-client-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <mdb-database-deployment>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "Ops Manager can encrypt backup jobs. You can use the Kubernetes Operator to\nconfigure KMIP (Key Management Interoperability) backup encryption for Ops Manager. To learn more, see\nEncrypted Backup Snapshots.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/multi-cluster-secure-ldap-auth",
            "title": "Secure Client Authentication with LDAP",
            "headings": [
                "Considerations",
                "Prerequisites",
                "Configure LDAP Client Authentication for a Multi-Kubernetes-Cluster Replica Set",
                "Update your MongoDBMultiCluster resource to enable LDAP (Lightweight Directory Access Protocol) authentication.",
                "Verify that the MongoDBMultiCluster resources are running."
            ],
            "paragraphs": "You can use the  Kubernetes Operator  to configure  LDAP (Lightweight Directory Access Protocol)  to authenticate your\nclient applications that connect to your  multi-Kubernetes-cluster deployments . This guide\ndescribes how to configure  LDAP (Lightweight Directory Access Protocol)  authentication from client applications\nto your  multi-Kubernetes-cluster deployments . MongoDB Enterprise \nsupports: To learn more, see the  LDAP Proxy Authentication \nand  LDAP Authorization  sections\nin the MongoDB Server documentation. Proxying authentication requests to a Lightweight Directory Access\nProtocol (LDAP) service. Simple and SASL binding to LDAP servers. MongoDB Enterprise can bind\nto an LDAP server via  saslauthd  or through the operating system\nlibraries. To configure  LDAP (Lightweight Directory Access Protocol)  in  CustomResourceDefinitions , use the parameters under the\n spec.security.authentication.ldap  and other\n security LDAP settings  specific to the\nMongoDB Agent, from the  Kubernetes Operator  MongoDB resource specification.\nThe procedures in this section describe the required settings and\nprovide examples of  LDAP (Lightweight Directory Access Protocol)  configuration. To improve security, consider deploying a\n TLS-encrypted multi-cluster .\nEncryption with  TLS (Transport Layer Security)  is optional. By default,  LDAP (Lightweight Directory Access Protocol)  traffic is sent\nas plain text. This means that username and password are exposed to\nnetwork threats. Many modern directory services, such as Microsoft\nActive Directory, require encrypted connections. Consider using\n LDAP (Lightweight Directory Access Protocol)  over  TLS (Transport Layer Security) / SSL (Secure Sockets Layer)  to encrypt authentication requests in your\n Kubernetes Operator  MongoDB deployments. Before you secure your  multi-Kubernetes-cluster deployment  using  TLS (Transport Layer Security) \nencryption, complete the following tasks: Follow the steps in the  Quick Start Prerequisites . Deploy a multi-cluster using a  Multi-Kubernetes-Cluster Quick Start . Update your MongoDBMultiCluster custom resource \nwith security settings from the  Kubernetes Operator \n MongoDBMultiCluster resource specification . To enable  LDAP (Lightweight Directory Access Protocol)  in your deployment, configure the following\nsettings in your  Kubernetes  object: The resulting configuration may look similar to the following\nexample: For a full list of LDAP settings, see security settings in the  Kubernetes Operator \n MongoDBMultiCluster resource specification .\nAlso see the  spec.security.authentication.agents.automationUserName \nsetting for the MongoDB Agent user in your LDAP-enabled  Kubernetes Operator \ndeployment. Key Type and necessity Description Example Set to  true  to enable LDAP authentication. true Specify the LDAP Distinguished Name to which MongoDB binds when\nconnecting to the LDAP server. cn=admin,dc=example,dc=org Specify the name of the  secret  that contains the\nLDAP Bind Distinguished Name's password with which MongoDB binds\nwhen connecting to an LDAP server. <secret-name> Add the  ConfigMap 's name that stores the custom  CA (Certificate Authority) \nthat you used to sign your deployment's  TLS (Transport Layer Security)  certificates. <configmap-name> Add the field name that stores the  CA (Certificate Authority)  which validates the\nLDAP server's  TLS (Transport Layer Security)  certificate. <configmap-key> Specify the list of  hostname:port  combinations of one or more\nLDAP servers. For each server, use a separate line. <example.com:636> Set to  tls  to use LDAPS (LDAP over  TLS (Transport Layer Security) ). Leave blank if\nyour LDAP server doesn't accept TLS. You must enable TLS when you\ndeploy the database resource to use this setting. tls Specify the mapping that maps the username provided to\n mongod  or  mongos  for authentication\nto an LDAP Distinguished Name (DN). To learn more, see  security.ldap.userToDNMapping \nand  LDAP Query Templates  in the\nMongoDB Server documentation. <match: \"(.+)\",substitution: \"uid={0},ou=groups,dc=example,dc=org\"> Set to  LDAP  to enable authentication through LDAP. LDAP For member clusters, run the following commands to verify that\nthe MongoDB Pods are in the running state: In the central cluster, run the following command to verify that\nthe  MongoDBMultiCluster  resource  is in the running state:",
            "code": [
                {
                    "lang": "yaml",
                    "value": "security:\n authentication:\n   enabled: true\n   # Enabled LDAP Authentication Mode\n   modes:\n     - \"LDAP\"\n     - \"SCRAM\"\n     # LDAP related configuration\n   ldap:\n   # Specify the hostname:port combination of one or\n   # more LDAP servers\n     servers:\n       - \"ldap1.example.com:636\"\n       - \"ldap2.example.com:636\"\n\n   # Set to \"tls\" to use LDAP over TLS. Leave blank if\n   # the LDAP server doesn't accept TLS. You must enable TLS when\n   # you deploy the multi-cluster resource to use this setting.\n   transportSecurity: \"tls\"\n\n   # If TLS is enabled, add a reference to a ConfigMap that\n   # contains a CA certificate that validates the LDAP server's\n   # TLS certificate.\n   caConfigMapRef:\n     name: \"<configmap-name>\"\n     key: \"<configmap-entry-key>\"\n\n   # Specify the LDAP Distinguished Name to which\n   # MongoDB binds when connecting to the LDAP server\n   bindQueryUser: \"cn=admin,dc=example,dc=org\"\n\n   # Specify the password with which MongoDB binds\n   # when connecting to an LDAP server. This is a\n   # reference to a Secret Kubernetes Object containing\n   # one \"password\" key.\n   bindQueryPasswordSecretRef:\n     name: \"<secret-name>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_1_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_2_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods \\\n --context=$MDB_CLUSTER_3_FULL_NAME \\\n --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl --context=$MDB_CENTRAL_CLUSTER_FULL_NAME \\\n  --namespace mongodb \\\n  get mdbmc multi-replica-set -o yaml -w"
                }
            ],
            "preview": "You can use the Kubernetes Operator to configure LDAP (Lightweight Directory Access Protocol) to authenticate your\nclient applications that connect to your multi-Kubernetes-cluster deployments. This guide\ndescribes how to configure LDAP (Lightweight Directory Access Protocol) authentication from client applications\nto your multi-Kubernetes-cluster deployments.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/secure-x509-auth",
            "title": "Secure Client Authentication with X.509",
            "headings": [
                "General Prerequisites",
                "Configure X.509 Client Authentication for a Replica Set",
                "Prerequisites",
                "Enable X.509 Client Authentication",
                "Copy the sample replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the general X.509 settings for your replica set resource.",
                "Save your replica set config file.",
                "Apply your changes to your replica set deployment.",
                "Track the status of your deployment.",
                "Renew X.509 Certificates for a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Renew the secret for your TLS certificates.",
                "Renew the secret for your agents' X.509 certificates.",
                "Configure X.509 Client Authentication for a Sharded Cluster",
                "Prerequisites",
                "Enable X.509 Client Authentication",
                "Copy the sample sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the general X.509 settings for your sharded cluster resource.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Track the status of your deployment.",
                "Renew X.509 Certificates for a Sharded Cluster",
                "Configure kubectl to default to your namespace.",
                "Renew the secret for your Shards' TLS certificates.",
                "Renew the secret for your config server's TLS certificates.",
                "Renew the secret for your mongos server's TLS certificates.",
                "Renew the secret for your agents' X.509 certificates."
            ],
            "paragraphs": "The  MongoDB Enterprise Kubernetes Operator  can use X.509 certificates to authenticate your\nclient applications to your MongoDB deployments. This guide instructs you on how to configure X.509 authentication from\nclients to your MongoDB instances. Before you secure your MongoDB deployment using  TLS (Transport Layer Security)  encryption,\ncomplete the following: Install the Kubernetes Operator Create Credentials for the Kubernetes Operator Enabling X.509 authentication at the project level configures all\nagents to use X.509 client authentication when communicating with\nMongoDB deployments. X.509 client authentication requires one of the following: Cloud Manager Ops Manager  4.1.7 or later Ops Manager  4.0.11 or later Before you secure your replica set using X.509,\n deploy a TLS-encrypted replica set . Change the settings of this  YAML (Yet Another Markup Language)  file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the  object  specification\nat the end of your resource file in the  spec  section. To enable  TLS (Transport Layer Security)  and X.509 in your deployment, configure the following\nsettings in your  Kubernetes  object: Key Type Necessity Description Example boolean Required Set this value to  true  to enable authentication on the\nMongoDB deployment. true array Conditional Set this value to  [\"X509\"] . [\"X509\"] Invoke the following  Kubernetes  command to update your\n replica set : To check the status of your  MongoDB  resource , use the following\ncommand: With the  -w  (watch) flag set, when the configuration changes, the output\nrefreshes immediately until the status phase achieves the  Running  state.\nTo learn more about resource deployment statuses, see  Troubleshoot the  Kubernetes Operator . If you have already created certificates, we recommend that you renew\nthem periodically using the following procedure. To automate certificate renewal for  Ops Manager  deployments, consider setting up the  cert-manager integration . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Run this  kubectl  command to renew an existing  secret  that\nstores the replica set's certificates: Run this  kubectl  command to renew an existing  secret  that\nstores the agents' X.509 certificates: Before you secure your sharded cluster using X.509,\n deploy a TLS-encrypted sharded cluster . Change the settings of this  YAML (Yet Another Markup Language)  file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the  object  specification\nat the end of your resource file in the  spec  section. To enable  TLS (Transport Layer Security)  and X.509 in your deployment, configure the following\nsettings in your  Kubernetes  object: Key Type Necessity Description Example boolean Required Set this value to  true  to enable authentication on the\nMongoDB deployment. true array Conditional Set this value to  [\"X509\"] . [\"X509\"] In any directory, invoke the following  Kubernetes  command to update and\nrestart your  sharded cluster : To check the status of your  MongoDB  resource , use the following\ncommand: With the  -w  (watch) flag set, when the configuration changes, the output\nrefreshes immediately until the status phase achieves the  Running  state.\nTo learn more about resource deployment statuses, see  Troubleshoot the  Kubernetes Operator . If you have already created certificates, we recommend that you renew\nthem periodically using the following procedure. To automate certificate renewal for  Ops Manager  deployments, consider setting up the  cert-manager integration . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Run this  kubectl  command to renew an existing  secret  that\nstores the sharded cluster shards' certificates: Run this  kubectl  command to renew an existing  secret  that\nstores the sharded cluster config server's certificates: Run this  kubectl  command to renew an existing  secret  that\nstores the sharded cluster  mongos  certificates: Run this  kubectl  command to renew an existing  secret  that\nstores the agents' X.509 certificates:",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n            # Must match metadata.name in ConfigMap file\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      ca: <custom-ca>\n    certsSecretPrefix: <prefix>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-cert \\\n  --cert=<replica-set-tls-cert> \\\n  --key=<replica-set-tls-key> \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key> \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: \"4.2.2-ent\"\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      ca: <custom-ca>\n    certsSecretPrefix: <prefix>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-0-cert \\\n  --cert=<shard-0-tls-cert> \\\n  --key=<shard-0-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -\n\nkubectl -n mongodb create secret tls <prefix>-<metadata.name>-1-cert \\\n  --cert=<shard-1-tls-cert> \\\n  --key=<shard-1-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-config-cert \\\n  --cert=<config-tls-cert> \\\n  --key=<config-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret tls <prefix>-<metadata.name>-mongos-cert \\\n  --cert=<mongos-tls-cert> \\\n  --key=<mongos-tls-key> \\\n  --dry-run=client \\\n  -o yaml |\nkubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret tls <prefix>-<metadata.name>-agent-certs \\\n  --cert=<agent-tls-cert> \\\n  --key=<agent-tls-key> \\\n  --dry-run=client \\\n   -o yaml |\nkubectl apply -f -"
                }
            ],
            "preview": "The MongoDB Enterprise Kubernetes Operator can use X.509 certificates to authenticate your\nclient applications to your MongoDB deployments.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/set-scope-k8s-operator",
            "title": "Set Scope for MongoDB Enterprise Kubernetes Operator Deployment",
            "headings": [
                "Kubernetes Operator Deployment Scopes",
                "Operator Uses the Same Single Namespace as Resources",
                "Operator Uses a Subset of Namespaces",
                "Operator Uses Cluster-Wide Scope",
                "Next Steps"
            ],
            "paragraphs": "Before you install the  Kubernetes Operator , you can set the scope of the\n Kubernetes Operator  deployment. The scopes depend on the namespaces in\nwhich you choose to deploy  Ops Manager  and  MongoDB  resources . You can set one of these scopes: Operator Uses the Same Single Namespace as Resources   (Default) Operator Uses a Subset of Namespaces Operator Uses Cluster-Wide Scope You can set the scope for the  Kubernetes Operator  to use the same  namespace  as\nresources. In this case, the  Kubernetes Operator  watches  Ops Manager  and\n MongoDB  resources  in that same  namespace . When you  install  the  Kubernetes Operator , it\nuses the default namespace. You can set the scope for the  Kubernetes Operator  to use one or more  namespaces \nthat differ from the namespace used by the  Kubernetes Operator  resources.\nIn this case, the  Kubernetes Operator  watches  Ops Manager  and  MongoDB  resources \nin a subset of  namespaces  that you specify. To install the  Kubernetes Operator  instances with this\nscope, use  helm  with the  operator.watchNamespace  parameter. Follow the relevant  installation instructions  for  helm , but specify one or more namespaces\nin the  operator.watchNamespace  parameter for the  Kubernetes Operator  to\nwatch: When installing the  Kubernetes Operator  to watch resources in one or more\nnamespaces other than the namespace in which the  Kubernetes Operator  is\ndeployed: The following example illustrates how the  ClusterRole  and  ClusterRoleBinding  work\ntogether in the cluster. Suppose you create a ServiceAccount in the  mongodb  namespace, and\nthen install the  Kubernetes Operator  in this namespace. The  Kubernetes Operator \nuses this ServiceAccount. To set the  Kubernetes Operator  scope to watch namespaces  ns1  and  ns2 : See also  operator.watchNamespace . Watching a subset of namespaces is useful in deployments where a single\n Kubernetes Operator  instance watches a different cluster resource type.\nFor example, you can configure the  Kubernetes Operator  to watch  MongoDB  resources \nin one subset of namespaces, and to watch  MongoDBMultiCluster  resources  in another\nsubset of namespaces. To avoid race conditions during resource reconciliation,\nfor each custom resource type that you want the  Kubernetes Operator  to watch,\nensure that you set scope to a distinct subset of namespaces. Create the following resources: A  ClusterRole  with access to multiple resources. For the full resource\ndefinition, see the\n operator-roles.yaml \nexample. This is a cluster-scoped resource. Create a  ClusterRoleBinding  to link  ClusterRole  with ServiceAccount. This\n clusterRoleBinding  will bind the  clusterRole  that you\ncreated  with the ServiceAccount that the  Kubernetes Operator  is using\non the namespace where you install it. Include the  ClusterRole  and  ClusterRoleBinding \nin the default configuration files that you apply during the\ninstallation. Obtain  cluster-admin privileges . Using these privileges, create a cluster-wide, non-namespaced  ClusterRole . Create a  ClusterRoleBinding  in three namespaces:  mongodb ,  ns1 \nand  ns2 . This  ClusterRoleBinding  will bind the\n ClusterRole  to the ServiceAccount in the  mongodb  namespace.\nThe  clusterRoleBinding  will allow the  Kubernetes Operator  deployed in\nthe  mongodb  namespace to access the resources described in the\n clusterRole  of the target namespace, that is, in  mongodb ,\n ns1  and  ns2 . You can set the scope for the  Kubernetes Operator  to the  Kubernetes  cluster.\nIn this case, the  Kubernetes Operator  watches  Ops Manager  and  MongoDB  resources \nin all  namespaces  in the  Kubernetes  cluster. To set a cluster-wide scope for the  Kubernetes Operator , follow the\ninstructions for your preferred installation method. You can deploy only one instance of the  Kubernetes Operator  with a\ncluster-wide scope per  Kubernetes  cluster. Use the  mongodb-enterprise.yaml \nsample  YAML (Yet Another Markup Language)  file from the  MongoDB Enterprise Kubernetes Operator GitHub repository . Set the  spec.template.spec.containers.name.env.name:WATCH_NAMESPACE \nin  mongodb-enterprise.yaml \nto  \"*\" . You must include the double quotation marks\n( \" ) around the asterisk ( * ) in the  YAML (Yet Another Markup Language)  file. In  mongodb-enterprise.yaml ,\nchange: to: Add the following code to the  ClusterRole  that you\nhave just modified: In  mongodb-enterprise.yaml ,\nchange: to: In the  mongodb-enterprise.yaml  file, change the\n <namespace>  value to the namespace where you want\nthe  Kubernetes Operator  to deploy resources and apply the\n YAML (Yet Another Markup Language)  fle. Create local  Kubernetes   service accounts : For each namespace, create some or all of the following\nlocal  Kubernetes   service accounts : Copy and paste the applicable examples and replace the  <namespace> \nvalue with the label that identifies the namespace. If you want to deploy a MongoDB instance in the\nnamespace, use  mongodb-enterprise-database-pods . If you want to deploy  Ops Manager  in the namespace, use\n mongodb-enterprise-appdb  and  mongodb-enterprise-ops-manager . Before you deploy the  Kubernetes Operator , configure the following\nitems: Configure the  Kubernetes Operator  to watch all namespaces: Create local  Kubernetes   service accounts : For each namespace, create some or all of the following\nlocal  Kubernetes   service accounts : Copy and paste the applicable examples and replace the  <namespace> \nvalue with the label that identifies the namespace. If you want to deploy a MongoDB instance in the\nnamespace, use  mongodb-enterprise-database-pods . If you want to deploy  Ops Manager  in the namespace, use\n mongodb-enterprise-appdb  and  mongodb-enterprise-ops-manager . Before you deploy the  Kubernetes Operator , configure the following\nitems: Use the  mongodb-enterprise-openshift.yaml \nsample  YAML (Yet Another Markup Language)  file from the  MongoDB Enterprise Kubernetes Operator GitHub repository . Set the  spec.template.spec.containers.name.env.name:WATCH_NAMESPACE  in\n mongodb-enterprise-openshift.yaml \nto  \"*\" . You must include the double quotation marks\n( \" ) around the asterisk ( * ) in the  YAML (Yet Another Markup Language)  file. Create the corresponding roles for these accounts. In\n mongodb-enterprise-openshift.yaml ,\nchange: to: Add the following code to the  ClusterRole  that you\nhave just modified: In  mongodb-enterprise-openshift.yaml ,\nchange: to: Create the secret only in the namespace where you will\ndeploy the  Kubernetes Operator .\nIf you deploy MongoDB resources in  multiple namespaces  or with a  cluster-wide\nscope , the  Kubernetes Operator \nsynchronizes the secret across all watched namespaces.\nTo learn more, see the  registry.imagePullSecrets \nsetting in the  Helm installation settings . In the  mongodb-enterprise.yaml  file, replace\n <namespace>  with the namespace in which you want to\ninstall the  Kubernetes Operator . Use  oc  or the OpenShift\nContainer Platform UI to apply the resulting  YAML (Yet Another Markup Language)  file. Create local  Kubernetes   service accounts : For each namespace, create some or all of the following\nlocal  Kubernetes   service accounts : Copy and paste the applicable examples and replace the  <namespace> \nvalue with the label that identifies the namespace. If you want to deploy a MongoDB instance in the\nnamespace, use  mongodb-enterprise-database-pods . If you want to deploy  Ops Manager  in the namespace, use\n mongodb-enterprise-appdb  and  mongodb-enterprise-ops-manager . Before you deploy the  Kubernetes Operator , configure the following\nitems: Configure the  Kubernetes Operator  to watch all namespaces: Create the secret only in the namespace where you will\ndeploy the  Kubernetes Operator .\nIf you deploy MongoDB resources in  multiple namespaces  or with a  cluster-wide\nscope , the  Kubernetes Operator \nsynchronizes the secret across all watched namespaces.\nTo learn more, see the  registry.imagePullSecrets \nsetting in the  Helm installation settings . In the  mongodb-enterprise.yaml  file, replace  <namespace> \nwith the namespace in which you want to install the\n Kubernetes Operator . Use  oc  or the OpenShift Container\nPlatform UI to apply the resulting  YAML (Yet Another Markup Language)  file. Create local  Kubernetes   service accounts : For each namespace, create some or all of the following\nlocal  Kubernetes   service accounts : Copy and paste the applicable examples and replace the  <namespace> \nvalue with the label that identifies the namespace. If you want to deploy a MongoDB instance in the\nnamespace, use  mongodb-enterprise-database-pods . If you want to deploy  Ops Manager  in the namespace, use\n mongodb-enterprise-appdb  and  mongodb-enterprise-ops-manager . After setting up the scope for the  MongoDB Enterprise Kubernetes Operator , you can: Read the  Considerations . Complete the  Prerequisites . Install the Kubernetes Operator .",
            "code": [
                {
                    "lang": "yaml",
                    "value": "# Watch one namespace\nhelm install enterprise-operator mongodb/enterprise-operator \\\n  --set operator.watchNamespace='namespace-to-watch' <...>"
                },
                {
                    "lang": "yaml",
                    "value": "# Watch both namespace-a and namespace-b\nhelm install enterprise-operator mongodb/enterprise-operator \\\n  --set operator.watchNamespace=\"namespace-a\\,namespace-b\""
                },
                {
                    "lang": "yaml",
                    "value": "# Operator with name `mongodb-enterprise-operator-qa-envs` will\n# watch ns-dev, ns-qa and ns-uat namespaces\n\nhelm install mongodb-enterprise-operator-qa-envs mongodb/enterprise-operator \\\n  --set operator.watchNamespace=\"ns-dev\\,ns-qa\\,ns-uat\""
                },
                {
                    "lang": "yaml",
                    "value": "# Operator with name `mongodb-enterprise-operator-staging` will\n# watch ns-staging and ns-pre-prod\nhelm install mongodb-operator helm-chart --set operator.watchNamespace=\"ns-staging\\,ns-pre-prod\" mongodb-enterprise-operator-staging"
                },
                {
                    "lang": "sh",
                    "value": "WATCH_NAMESPACE: \"*\""
                },
                {
                    "lang": "sh",
                    "value": "kind:  Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: mongodb-enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "kind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: mongodb-enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "- apiGroups:\n  - \"\"\n  resources:\n  - namespaces\n  verbs:\n  - list\n  - watch"
                },
                {
                    "lang": "sh",
                    "value": "kind:  RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: mongodb-enterprise-operator\n namespace: mongodb\nroleRef:\n apiGroup: rbac.authorization.k8s.io\n kind: Role\n name: mongodb-enterprise-operator\nsubjects:\n - kind: ServiceAccount\n name: mongodb-enterprise-operator\n namespace: mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kind:  ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: mongodb-enterprise-operator\n namespace: mongodb\nroleRef:\n apiGroup: rbac.authorization.k8s.io\n kind: ClusterRole\n name: mongodb-enterprise-operator\nsubjects:\n - kind: ServiceAccount\n name: mongodb-enterprise-operator\n namespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-database-pods\n  namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-ops-manager\n  namespace: <namespace>\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - secrets\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - patch\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: mongodb-enterprise-appdb\nsubjects:\n  - kind: ServiceAccount\n    name: mongodb-enterprise-appdb\n    namespace: <namespace>\n...\n\n"
                },
                {
                    "lang": "sh",
                    "value": "---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n name: mongodb-enterprise-database-pods\n namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n name: mongodb-enterprise-appdb\n namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n name: mongodb-enterprise-ops-manager\n namespace: <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator \\\n  --set operator.watchNamespace=\"*\""
                },
                {
                    "lang": "sh",
                    "value": "helm template mongodb/enterprise-operator \\\n  --set operator.namespace=<metadata.namespace> \\\n  --show-only templates/database-roles.yaml | kubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "WATCH_NAMESPACE: \"*\""
                },
                {
                    "lang": "sh",
                    "value": "kind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "kind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: enterprise-operator"
                },
                {
                    "lang": "sh",
                    "value": "- apiGroups:\n  - \"\"\n  resources:\n  - namespaces\n  verbs:\n  - list\n  - watch"
                },
                {
                    "lang": "sh",
                    "value": "kind:  RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: enterprise-operator\n namespace: mongodb\nroleRef:\n apiGroup: rbac.authorization.k8s.io\n kind: Role\n name: enterprise-operator\nsubjects:\n - kind: ServiceAccount\n name: enterprise-operator\n namespace: mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kind:  ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: enterprise-operator\n namespace: mongodb\nroleRef:\n apiGroup: rbac.authorization.k8s.io\n kind: ClusterRole\n name: enterprise-operator\nsubjects:\n - kind: ServiceAccount\n name: enterprise-operator\n namespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-database-pods\n  namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-ops-manager\n  namespace: <namespace>\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - secrets\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - patch\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: mongodb-enterprise-appdb\nsubjects:\n  - kind: ServiceAccount\n    name: mongodb-enterprise-appdb\n    namespace: <namespace>\n...\n\n"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator \\\n  --set operator.watchNamespace=\"*\" \\"
                },
                {
                    "lang": "yaml",
                    "value": "---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-database-pods\n  namespace: <namespace>\n---\nkind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: mongodb-enterprise-ops-manager\n  namespace: <namespace>\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - secrets\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - patch\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: mongodb-enterprise-appdb\nsubjects:\n  - kind: ServiceAccount\n    name: mongodb-enterprise-appdb\n    namespace: <namespace>\n...\n\n"
                },
                {
                    "lang": "sh",
                    "value": "helm template mongodb/enterprise-operator \\\n  --set operator.namespace=<metadata.namespace> \\\n  --show-only templates/database-roles.yaml | oc apply -f -"
                }
            ],
            "preview": "Before you install the Kubernetes Operator, you can set the scope of the\nKubernetes Operator deployment. The scopes depend on the namespaces in\nwhich you choose to deploy Ops Manager and MongoDB resources.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/plan-om-resource",
            "title": "Plan Your Ops Manager Resource",
            "headings": [
                "Architecture",
                "Considerations",
                "Encryption Key",
                "Application Database",
                "Topology",
                "Monitoring",
                "Authentication",
                "Offline Deployments",
                "Streamlined Configuration",
                "Backup",
                "Oplog Store",
                "S3 Oplog Store",
                "Blockstore",
                "S3 Snapshot Store",
                "Disable Backup",
                "Manually Configure KMIP Backup Encryption",
                "Prerequisites",
                "Procedure",
                "Configure Ops Manager to Run over HTTPS",
                "Ops Manager Application Access",
                "Deploying Ops Manager in Remote or Local Mode",
                "Managing External MongoDB Deployments",
                "Using Ops Manager  with Multi-Kubernetes-Cluster Deployments",
                "Prerequisites"
            ],
            "paragraphs": "MongoDB  Ops Manager  is an enterprise application that manages, backs\nup, and monitors MongoDB deployments. With  Ops Manager , you can scale and\nupgrade MongoDB, optimize queries, perform point-in-time restores, receive\nperformance alerts, and monitor your deployments. To manage and maintain\n Ops Manager  and its underlying database, you can use the  MongoDB Enterprise Kubernetes Operator  to run\n Ops Manager  as a resource deployed in a container on  Kubernetes . Before you deploy an  Ops Manager  resource, make sure you read the\n considerations  and complete\nthe  prerequisites . For  Ops Manager  resource architecture details, see  Ops Manager  Architecture in  Kubernetes . The  Kubernetes Operator  generates an encryption key to protect sensitive\ninformation in the  Ops Manager Application Database . The  Kubernetes Operator \nsaves this key in a  secret  in the same namespace as the  Ops Manager \nresource. The  Kubernetes Operator  names the secret\n <om-resource-name>-gen-key . If you remove the  Ops Manager  resource, the key remains stored in the\nsecret on the  Kubernetes  cluster. If you stored the Application Database in\na  Persistent Volume  and you create another  Ops Manager  resource with the same name,\nthe  Kubernetes Operator  reuses the secret. If you create an  Ops Manager \nresource with a different name, then  Kubernetes Operator  creates a new\nsecret and Application Database, and the old secret isn't reused. To avoid storing secrets in  Kubernetes , you can migrate all  secrets \nto a  secret storage tool . When you create an instance of  Ops Manager  through the  Kubernetes Operator  in\na single  Kubernetes  cluster deployment of MongoDB, the  Ops Manager Application Database \nis deployed as a  replica set . You can't configure the Application Database\nas a  standalone  database or  sharded cluster . If you have\nconcerns about performance or size requirements for the Application Database,\ncontact  MongoDB Support . When you create an instance of  Ops Manager  through\nthe  Kubernetes Operator  in a  multi-Kubernetes-cluster deployment , the  Kubernetes Operator  can configure\nthe  Ops Manager Application Database  on multiple member clusters. The  Kubernetes Operator  automatically configures  Ops Manager  to monitor the\nApplication Database that backs the  Ops Manager Application . The  Kubernetes Operator \ncreates a project named  <ops-manager-deployment-name>-db  for you to\nmonitor the Application Database deployment. Ops Manager  monitors the Application Database deployment, but  Ops Manager  doesn't\nmanage it. You can't change the Application Database's configuration in\nthe  Ops Manager Application . The  Ops Manager  UI might display warnings in the\n <ops-manager-deployment-name>-db  project stating that the\nagents for the Application Database are out of date. You can safely\nignore these warnings. The  Kubernetes Operator  enforces  SCRAM-SHA-256 \n authentication  on\nthe Application Database. The  Kubernetes Operator  creates the database user which  Ops Manager  uses to\nconnect to the Application Database. This database user has the\nfollowing attributes: You can't modify the  Ops Manager  database user's name and roles. You\n create a secret  to set the database user's\npassword. You edit the secret to update the password. If you don't\ncreate a secret or delete an existing secret, the  Kubernetes Operator \ngenerates a password and stores it. To learn about other options for secret\nstorage, see  Configure Secret Storage . Username mongodb-ops-manager Authentication Database admin Roles readWriteAnyDatabase dbAdminAnyDatabase clusterMonitor The  Kubernetes Operator  requires that you specify the MongoDB Enterprise version\nfor the  Application Database  image to enable any\ndeployments of  Ops Manager  resources, including offline deployments. After you deploy  Ops Manager , you need to configure it. The regular\nprocedure involves setting up  Ops Manager  through the\n configuration wizard . If you\nset some essential settings in your object specification before you\ndeploy, you can bypass the configuration wizard. In the  spec.configuration  block of your  Ops Manager  object\nspecification, you need to: Add  mms.ignoreInitialUiSetup  and set to\n true . Add the  minimum configuration settings  to\nallow the  Ops Manager  instance to start without errors. To disable the  Ops Manager  configuration wizard, configure the\nfollowing settings in your  spec.configuration  block: Replace the example values with the values you want your  Ops Manager  to\nuse. Kubernetes Operator  enables  Backup  by\ndefault. The  Kubernetes Operator  deploys a  StatefulSet  comprised of\none Pod to host the  Backup Daemon Service  and then creates a  Persistent Volume Claim \nand  Persistent Volume  for the Backup Daemon's  head database . The\n Kubernetes Operator  uses the  Ops Manager API  to\nenable the Backup Daemon and configure the head database. To configure Backup, you must create the  MongoDB  resources  or  MongoDBMultiCluster  resources \nfor the  oplog store  and for one of the\nfollowing: The  Ops Manager  resource remains in a  Pending  state until you configure these Backup resources. You can also  encrypt backup jobs , but\n limitations  apply to deployments where the same\n Kubernetes Operator  instance is not managing both the\n MongoDBOpsManager  and  MongoDB \ncustom resources. oplog store  or  S3 (Simple Storage Service)  oplog store.\nIf you deploy both the oplog store and the  S3 (Simple Storage Service)   oplog store,  Ops Manager \nchooses one to use for Backup at random. S3 (Simple Storage Service)   snapshot store  or  blockstore .\nIf you deploy both an  S3 (Simple Storage Service)   snapshot store  and a\n blockstore ,  Ops Manager  chooses one\nto use for Backup at random. You must deploy a three-member replica set to store your\n oplog slices . The Oplog database only supports the  SCRAM  authentication mechanism.\nYou cannot enable other authentication mechanisms. If you enable  SCRAM  authentication on the oplog database, you\nmust: Create a MongoDB user resource to connect  Ops Manager  to the oplog\ndatabase. Specify the  name \nof the user in the  Ops Manager  resource definition. To configure an  S3 (Simple Storage Service)  oplog store, you must create an  AWS (Amazon Web Services)   S3 (Simple Storage Service)  or\n S3 (Simple Storage Service) -compatible bucket to store your database Backup Oplog. You can configure the oplog store for both  MongoDB  resource  and  MongoDBMultiCluster  resource ,\nusing the  spec.backup.s3OpLogStores.mongodbResourceRef.name  setting\nin the  Ops Manager  resource definition. To configure a  blockstore , you\nmust deploy a replica set to store snapshots. To configure an  S3 (Simple Storage Service)   snapshot store , you\nmust create an  AWS (Amazon Web Services)   S3 (Simple Storage Service)  or  S3 (Simple Storage Service) -compatible bucket to store your\ndatabase Backup  snapshots . The default configuration stores snapshot metadata in the Application\nDatabase. You can also deploy a replica set to store snapshot metadata,\nthen configure it using the\n spec.backup.s3Stores.mongodbResourceRef.name  settings in\nthe  Ops Manager  resource definition. You can configure the  S3 (Simple Storage Service)  snapshot store for both  MongoDB  resource  and  MongoDBMultiCluster  resource . You can update any additional  S3 (Simple Storage Service) \n configuration settings \nthat  Kubernetes Operator  doesn't manage through the  Ops Manager Application . To disable backup after you enabled it: Set the  Ops Manager   Kubernetes   object   spec.backup.enabled \nsetting to  false . Disable backups  in the\n Ops Manager Application . Delete the  Backup Daemon Service   StatefulSet : The  Persistent Volume Claim  and  Persistent Volume  for the Backup Daemon's  head database  are not deleted when you delete the  Backup Daemon Service \n StatefulSet . You can retrieve stored data before you delete\nthese  Kubernetes  resources. To learn about reclaiming  Persistent Volumes , see the\n Kubernetes documentation . For deployments where the same  Kubernetes Operator  instance is  not  managing both the\n MongoDBOpsManager  and\n MongoDB  custom resources,\nyou must manually configure  KMIP (Key Management Interoperability)  backup encryption client settings in  Ops Manager \nusing the following procedure. If the  Kubernetes Operator   is  managing both resources,\nsee  Configure KMIP Backup Encryption for Ops Manager  instead. A running  KMIP (Key Management Interoperability)  server. A running  Ops Manager  instance,  configured to use KMIP . A  TLS (Transport Layer Security)  secret that  concatenates the private key and the KMIP client certificate in PEM format . Mount the  TLS (Transport Layer Security)  secret to the  MongoDBOpsManager  custom resource. For example: Configure the  KMIP (Key Management Interoperability)  settings for your project in  Ops Manager  following the procedure\nin  Configure Your Project to Use KMIP . You can configure your  Ops Manager  instance created through the  Kubernetes Operator \nto run over  HTTPS (Hypertext Transfer Protocol Secure)  instead of  HTTP (Hypertext Transfer Protocol) . To configure your  Ops Manager  instance to run over  HTTPS (Hypertext Transfer Protocol Secure) : For detailed instructions, see  Deploy an  Ops Manager  Resource . Create a secret that contains the  TLS (Transport Layer Security)  certificate and private key. Add this secret to the  Ops Manager  configuration object. If you have existing deployments, you must restart them manually\nafter enabling  HTTPS (Hypertext Transfer Protocol Secure) . To avoid restarting your deployments,\nconfigure  HTTPS (Hypertext Transfer Protocol Secure)  before deploying your managed resources. To learn more, see  HTTPS Enabled After Deployment . By default, the  Kubernetes Operator  doesn't create a  Kubernetes  service to route\ntraffic originating from outside of the  Kubernetes  cluster to the  Ops Manager \napplication. To access the  Ops Manager  application, you can: The simplest method is configuring the  Kubernetes Operator  to create a  Kubernetes \nservice that routes external traffic to the  Ops Manager  application. The\n Ops Manager  deployment procedure instructs you to add the following\nsettings to the  object  specification that configures the\n Kubernetes Operator  to create a service: Configure the  Kubernetes Operator  to create a  Kubernetes  service. Create a  Kubernetes  service manually. MongoDB recommends using a\n LoadBalancer   Kubernetes  service if your cloud provider supports it. If you're using OpenShift, use\n routes . Use a third-party service, such as Istio. spec. externalConnectivity spec.externalConnectivity. type You can use the  Kubernetes Operator  to configure  Ops Manager  to operate in\n Local  or  Remote  mode if your environment prevents granting hosts\nin your  Kubernetes  cluster access to the Internet. In these modes, the Backup\nDaemons and managed MongoDB resources download installation archives\nfrom  Ops Manager  instead of from the Internet: Configure an  Ops Manager  Resource to use Remote Mode :  Ops Manager  reads the\ninstallation archives from HTTP endpoints on a web server\nor S3-compatible file store deployed to your  Kubernetes  cluster. Configure an  Ops Manager  Resource to use Local Mode :  Ops Manager  reads the instalation\narchives from a  Persistent Volume  that you create for the  Ops Manager  StatefulSet. When you deploy  Ops Manager  with the  Kubernetes Operator ,  Ops Manager  can manage\nMongoDB database resources deployed: If  Ops Manager  manages MongoDB database resources deployed to different\n Kubernetes  clusters than  Ops Manager  or outside of  Kubernetes  clusters, you must: To the same  Kubernetes  cluster as  Ops Manager . Outside of  Kubernetes  clusters. Add the  mms.centralUrl  setting to  spec.configuration  in the\n Ops Manager  resource specification. Set the value to the URL by which  Ops Manager  is exposed outside of the\n Kubernetes  cluster: Update the ConfigMaps  referenced by\nall MongoDB database resources inside the  Kubernetes  cluster that you\ndeployed with the  Kubernetes Operator . Set  data.baseUrl  to the same value of the\n spec.configuration.mms.centralUrl \nsetting in the  Ops Manager  resource specification. This includes the ConfigMaps that the  MongoDB database resources\nfor the oplog and snapshot stores reference . To deploy an  Ops Manager  instance in the central cluster and connect to it,\nuse the following procedures: These procedures are the same as the procedures for single clusters\ndeployed with the  Kubernetes Operator  with the following exceptions: Review the Ops Manager resource architecture Review the Ops Manager resource considerations and prerequisites Deploy an Ops Manager instance on the central cluster with TLS encryption Set the context and the namespace. If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Configure external connectivity for Ops Manager. To connect member clusters to the  Ops Manager  resource's deployment in the\ncentral cluster in a  multi-Kubernetes-cluster deployment , use one of the following methods: Set the  spec.externalConnectivity  to  true  and specify\nthe  Ops Manager  port in it. Use the  ops-manager-external.yaml \nexample script, modify it to your needs, and apply the configuration.\nFor example, run: Add the central cluster and all member clusters to the same service mesh.\nThe service mesh establishes communication from the the central and all\nmember clusters to the  Ops Manager  instance. To learn more, see the\n Multi-Kubernetes-Cluster Quick Start \nprocedures and see the step that references the  istio-injection=enabled \nlabel for Istio. Also, see  Automatic sidecar injection \nin the Istio documentation. Deploy Ops Manager and the Application Database on the central cluster. You can choose to deploy  Ops Manager  and the Application Database only on the central cluster,\nusing the same procedure as for single  Kubernetes  clusters. To learn more,\nsee  Deploy an Ops Manager instance on the central cluster with TLS encryption . Deploy Ops Manager on the central cluster and the Application Database on selected member clusters. You can choose to deploy  Ops Manager  on the central cluster and the Application\nDatabase on a subset of selected member clusters, to increase the\nApplication Database's resilience and availability in  Ops Manager . Configure\nthe following settings in the  Ops Manager  CRD: To learn more, see  Deploy Ops Manager ,\nreview the  multi-Kubernetes-cluster deployment  example and specify  MultiCluster  for\n topology . Use  topology  to specify the  MultiCluster  value. Specify the  clusterSpecList  and\ninclude in it the  clusterName \nof each selected  Kubernetes  member cluster on which you want to deploy the Application Database, and the\nnumber of  members \n(MongoDB nodes) in each  Kubernetes  member cluster. If you deploy the Application Database on selected member clusters in\nyour  multi-Kubernetes-cluster deployment , you must include the central cluster and\nmember clusters in the same service mesh configuration. This enables\nbi-directional communication from  Ops Manager  to the Application Database. If you have not already, run the following command to run all\n kubectl  commands in the namespace you  created : If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . Install  the  MongoDB Enterprise Kubernetes Operator . Ensure that the host on which you want to deploy  Ops Manager  has a\nminimum of five gigabytes of memory. Create a  Kubernetes   secret  for an admin user in the same  namespace \nas the  Ops Manager  resource. If you are deploying  Ops Manager  in a  multi-Kubernetes-cluster deployment ,\nuse the same  namespace  that you set for your  multi-Kubernetes-cluster deployment   scope . When you deploy the  Ops Manager  resource,  Ops Manager  creates a user with\nthese credentials and grants it the  Global Owner  role.\nUse these credentials to log in to  Ops Manager  for the first time. Once\nyou deploy  Ops Manager , change the password or remove this secret. If you're using  HashiCorp Vault  as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. To learn about your options for secret\nstorage, see  Configure Secret Storage . The admin user's password must adhere to the  Ops Manager \n password complexity requirements . ( Optional ) To set the password for the  Ops Manager  database user,\ncreate a  secret  in the same  namespace  as the  Ops Manager  resource. The  Kubernetes Operator  creates the database user that  Ops Manager  uses to\nconnect to the  Ops Manager Application Database . You can set the\npassword for this database user by invoking the following command to\ncreate a secret: If you don't create a secret, then the  Kubernetes Operator  automatically\ngenerates a password and stores it internally. To learn more,\nsee  Authentication . If you're using  HashiCorp Vault  as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. If you choose to create a secret for the  Ops Manager  database user,\nyou must specify the secret's\n name \nin the  Ops Manager  resource definition. By default, the\n Kubernetes Operator  looks for the password value in the  password \nkey. If you stored the password value in a different key, you\nmust also specify that\n key \nname in the  Ops Manager  resource definition. ( Optional ). To configure Backup to an  S3 (Simple Storage Service)  snapshot store, create\na  secret  in the same namespace as the  Ops Manager  resource. This secret stores your  S3 (Simple Storage Service)  credentials so that the  Kubernetes Operator \ncan connect  Ops Manager  to your  AWS (Amazon Web Services)   S3 (Simple Storage Service)  or  S3 (Simple Storage Service) -compatible bucket.\nThe secret must contain the following key-value pairs: To create the secret, invoke the following command: To learn more about managing  S3 (Simple Storage Service)  snapshot storage, see the\n Prerequisites . If you're using  HashiCorp Vault  as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. Key Value accessKey Unique identifer of the  AWS (Amazon Web Services)  user who owns the  S3 (Simple Storage Service)  or\n S3 (Simple Storage Service) -compatible bucket. secretKey Secret key of the  AWS (Amazon Web Services)  user who owns the  S3 (Simple Storage Service)  or\n S3 (Simple Storage Service) -compatible bucket.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "spec:\n  configuration:\n    mms.ignoreInitialUiSetup: \"true\"\n    automation.versions.source: \"remote\"\n    mms.adminEmailAddr: cloud-manager-support@mongodb.com\n    mms.fromEmailAddr: cloud-manager-support@mongodb.com\n    mms.mail.hostname: email-smtp.us-east-1.amazonaws.com\n    mms.mail.port: \"465\"\n    mms.mail.ssl: \"true\"\n    mms.mail.transport: smtp\n    mms.minimumTLSVersion: TLSv1.2\n    mms.replyToEmailAddr: cloud-manager-support@mongodb.com\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete statefulset <metadata.name> -backup-daemon \\\n -n <metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: ops-manager-pod-spec\nspec:\n  < ... omitted ... >\n  statefulSet:\n    spec:\n      template:\n        spec:\n          volumes:\n          - name: kmip-client-test-prefix-mdb-latest-kmip-client\n            secretName: test-prefix-mdb-latest-kmip-client\n          containers:\n            - name: mongodb-ops-manager\n              volumeMounts:\n              - mountPath: /mongodb-ops-manager/kmip/client/test-prefix-mdb-latest-kmip-client\n                name: kmip-client-test-prefix-mdb-latest-kmip-client\n                readOnly: true\n  ..."
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  configuration:\n    mms.centralUrl: https://a9a8f8566e0094380b5c257746627b82-1037623671.us-east-1.elb.example.com:8080/"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply \\\n --context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" \\\n --namespace \"mongodb\" \\\n  -f https://raw.githubusercontent.com/mongodb/mongodb-enterprise-kubernetes/master/samples/ops-manager/ops-manager-external.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) \\\n  -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <adminusercredentials> \\\n  --from-literal=Username=\"<username>\" \\\n  --from-literal=Password=\"<password>\" \\\n  --from-literal=FirstName=\"<firstname>\" \\\n  --from-literal=LastName=\"<lastname>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <om-db-user-secret-name> \\\n  --from-literal=password=\"<om-db-user-password>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <my-aws-s3-credentials> \\\n  --from-literal=accessKey=\"<AKIAIOSFODNN7EXAMPLE>\" \\\n  --from-literal=secretKey=\"<wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY>\""
                }
            ],
            "preview": "MongoDB Ops Manager is an enterprise application that manages, backs\nup, and monitors MongoDB deployments. With Ops Manager, you can scale and\nupgrade MongoDB, optimize queries, perform point-in-time restores, receive\nperformance alerts, and monitor your deployments. To manage and maintain\nOps Manager and its underlying database, you can use the MongoDB Enterprise Kubernetes Operator to run\nOps Manager as a resource deployed in a container on Kubernetes.",
            "tags": null,
            "facets": {
                "genre": [
                    "reference"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/connect-from-inside-k8s",
            "title": "Connect to a MongoDB Database Resource from Inside Kubernetes",
            "headings": [
                "Considerations",
                "Procedure",
                "Open the Topology view for your deployment.",
                "Click  for the deployment to which you want to connect.",
                "Click Connect to this instance.",
                "Copy the connection command displayed in the Connect to your Deployment dialog.",
                "Run the connection command in a terminal to connect to the deployment.",
                "Run the command to view the Kubernetes secret file.",
                "Copy the connectionString.standard value displayed in the Kubernetes secret file.",
                "Run the connection command.",
                "(Optional) Mount the Kubernetes secret in your pod."
            ],
            "paragraphs": "The following procedure describes how to connect to a MongoDB resource\ndeployed by  Kubernetes  from inside of the  Kubernetes  cluster. You must be able to connect to the host and port where you deployed your\n Kubernetes  resource. To learn more about connecting to your deployment, see\n Connect to a MongoDB Process . Retrieve and run the connection command for your deployment. You can retrieve\nthe connection command from the  Ops Manager  or\n Cloud Manager \napplication, depending on where your clusters are hosted. You can also retrieve the connection command from\nthe  Kubernetes   secret  that the  Kubernetes Operator  creates automatically when you\n add a MongoDB user with SCRAM authentication  or X509. The procedure for connecting to a MongoDB Database resource varies based\non how you want to retrieve your connection string: Perform the following steps in the  Ops Manager  or  Cloud Manager \napplication, depending on where your clusters are hosted: When connecting to a resource from inside of  Kubernetes , the\nhostname to which you connect has the following form: Click  Deployment  in the left navigation. To connect to a sharded cluster resource named\n shardedcluster , you might use the following connection\nstring: Perform the following steps to view the credentials and\nuse the connection string to connect to MongoDB: When you create a new MongoDB database user,  Kubernetes Operator  automatically\ncreates a new  Kubernetes   secret . The  Kubernetes   secret \ncontains the following information about the new database user: username : Username for the database user password : Password for the database user connectionString.standard :  Standard connection string \nthat can connect you to the database as this database user. connectionString.standardSrv :  DNS seed list connection string  that can\nconnect you to the database as this database user. Alternatively, you can specify an optional\n spec.connectionStringSecretName  field in the\n MongoDB User Resource Specification  to specify\nthe name of the connection string secret that the\n Kubernetes Operator  creates. Run the following command in a terminal to view the  secret , replacing\nthe variables with the values in the table: If this command returns an error, you can verify the name of the  secret  by\nrunning the following command and retrieving the correct name: Placeholder Description {MongoDB-Resource-Name} Human-readable label that identifies the MongoDB resource. {User-Name} Human-readable label that identifies the MongoDB user. Use the  connectionString.standard  value within a  connection string \nto connect to the deployment. You can  mount the secret in your pod \nto ensure that your applications can access the credentials.",
            "code": [
                {
                    "lang": "sh",
                    "value": "<k8s-pod-name>.<k8s-internal-service-name>.<k8s-namespace>.<cluster-name>"
                },
                {
                    "lang": "none",
                    "value": "mongosh --host shardedcluster-mongos-0.shardedcluster-svc.mongodb.svc.cluster.local \\\n  --port 27017"
                },
                {
                    "lang": "none",
                    "value": "kubectl get secret {MongoDB-Resource-Name}-{User-Name}-admin -o jsonpath='{.data}'"
                },
                {
                    "lang": "none",
                    "value": "kubectl get secrets"
                },
                {
                    "lang": "none",
                    "value": "mongosh {connectionString.standard}"
                }
            ],
            "preview": "The following procedure describes how to connect to a MongoDB resource\ndeployed by Kubernetes from inside of the Kubernetes cluster.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/manage-database-users-scram",
            "title": "Manage Database Users Using SCRAM Authentication",
            "headings": [
                "Considerations",
                "Supported SCRAM Implementations",
                "Supported Authentication Mechanisms",
                "Prerequisites",
                "Add a Database User",
                "Create User Secret",
                "Configure kubectl to default to your namespace.",
                "Copy the following example secret.",
                "Create a new User Secret YAML file.",
                "Change the highlighted lines.",
                "Save the User Secret file with a .yaml extension.",
                "Create MongoDBUser",
                "Copy the following example MongoDBUser.",
                "Create a new MongoDBUser file.",
                "Change the highlighted lines.",
                "Add any additional roles for the user to the MongoDBUser.",
                "Save the MongoDBUser file with a .yaml extension.",
                "Create the user.",
                "View the newly created user in Cloud Manager or Ops Manager.",
                "Delete a Database User",
                "Change Authentication Mechanism"
            ],
            "paragraphs": "The  Kubernetes Operator  supports managing database users using SCRAM\nauthentication on MongoDB deployments. When you specify  SCRAM  as the authentication mechanism, the\nimplementation of SCRAM used depends upon: The version of MongoDB and If the database is the Application Database or another database. MongoDB Version Database SCRAM Implementation 3.6 or earlier Any except Application Database SCRAM-SHA-1 4.0 or later Any except Application Database SCRAM-SHA-256 Any Application Database SCRAM-SHA-1 After enabling SCRAM authentication, you can add SCRAM users using the\n Ops Manager  interface or by configuring the users in the  CustomResourceDefinition  based on the  MongoDB User Resource Specification . The  Kubernetes Operator  supports SCRAM, LDAP, and X.509 authentication\nmechanisms in deployments it creates. In an  Kubernetes Operator -created\ndeployment, you cannot use  Ops Manager  to: Configure other authentication mechanisms for deployments. Manage users  not  using SCRAM, LDAP, or X.509 authentication. Before managing database users, you must deploy a\n standalone ,\n replica set , or\n sharded cluster . For  multi-Kubernetes-cluster deployments , you must deploy replica sets.\nSee  Deploy Multiple Clusters . You cannot assign the same database user\nto more than one MongoDB\n standalone ,\n replica set , or\n sharded cluster .\nThis includes database users with\n admin  roles. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created . If you are deploying an  Ops Manager  resource in a  multi-Kubernetes-cluster deployment : Set the  context  to the name of the central cluster, such as:\n kubectl config set context \"$MDB_CENTRAL_CLUSTER_FULL_NAME\" . Set the  --namespace  to the same  scope \nthat you used for your  multi-Kubernetes-cluster deployment , such as:  kubectl config --namespace \"mongodb\" . You can choose to use a cleartext password: or you can choose to use a Base64-encoded password: Make sure to copy the desired password configuration. Plaintext\npasswords use  stringData.password  and Base64-encoded\npasswords use  data.password Open your preferred text editor. Paste this User Secret into a new text file. If you're using  HashiCorp Vault  as your  secret storage tool ,\nyou can  Create a Vault Secret  instead. To learn about your options for secret\nstorage, see  Configure Secret Storage . Use the following table to guide you through changing the highlighted\nlines in the Secret: Key Type Description Example metadata.name string Name of the database password secret. Resource names must be 44 characters or less. mms-scram-user-1-password stringData.password string Plaintext password for the desired user. Use this option and value  or   data.password . You\ncan't use both. <my-plain-text-password> data.password string Base64-encoded password for the desired user. Use this option and value  or   stringData.password .\nYou can't use both. You must encode your password into Base64 yourself then\npaste the resulting value with this option. There are\ntools for most every platform and multiple web-based\ntools as well. <my-base64-encoded-password> Open your preferred text editor. Paste this MongoDBUser into a new YAML file. Use the following table to guide you through changing the highlighted\nlines in the  MongoDB User Resource Specification : Key Type Description Example metadata.name string Name of the database user resource. Resource names must be 44 characters or less. mms-scram-user-1 spec.username string Name of the database user. mms-scram-user-1 spec.passwordSecretKeyRef.name string metadata.name  value of the secret that stores the\nuser's password. my-resource spec.mongodbResourceRef.name string Name of the  MongoDB resource \nthis user is associated with. my-resource spec.roles.db string Database on which the  role  can act. admin spec.roles.name string Name of the  role  to grant the database\nuser. The role name can be any\n built-in MongoDB role  or\n custom role  that\nexists in  Cloud Manager or Ops Manager . readWriteAnyDatabase You may grant additional roles to this user. Invoke the following  Kubernetes  command to create your database user: You can use these credentials to\n Connect to a MongoDB Database Resource from Inside Kubernetes . When you create a new MongoDB database user,  Kubernetes Operator  automatically\ncreates a new  Kubernetes   secret . The  Kubernetes   secret \ncontains the following information about the new database user: username : Username for the database user password : Password for the database user connectionString.standard :  Standard connection string \nthat can connect you to the database as this database user. connectionString.standardSrv :  DNS seed list connection string  that can\nconnect you to the database as this database user. Alternatively, you can specify an optional\n spec.connectionStringSecretName  field in the\n MongoDB User Resource Specification  to specify\nthe name of the connection string secret that the\n Kubernetes Operator  creates. You can view the newly-created user in  Cloud Manager or Ops Manager : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. To delete a database user, pass the  metadata.name  from the user\nMongoDBUser to the following command: To change your user authenication mechanism to SCRAM: Disable authentication. Under  spec.security.authentication , change  enabled  to\n false . Reapply the user's resource definition. Wait for the MongoDBResource to reach the  running  state. Enable SCRAM authentication. Under  spec.security.authentication , change  enabled  to\n true  and set  spec.security.authentication.modes  to ``\n[\"SCRAM\"]``. Reapply the MongoDBUser resource. Wait for the MongoDBResource to reach the  running  state.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<metadata.namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: <mms-user-1-password>\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\nstringData:\n  password: <my-plain-text-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key\ndata:\n  password: <base-64-encoded-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: <mms-user-1-password>\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\nstringData:\n  password: <my-plain-text-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key\ndata:\n  password: <base-64-encoded-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: <mms-scram-user-1>\nspec:\n  passwordSecretKeyRef:\n    name: <mms-user-1-password>\n    # Match to metadata.name of the User Secret\n    key: password\n  username: \"<mms-scram-user-1>\"\n  db: \"admin\" #\n  mongodbResourceRef:\n    name: \"<my-replica-set>\"\n    # Match to MongoDB resource using authenticaiton\n  roles:\n    - db: \"admin\"\n      name: \"clusterAdmin\"\n    - db: \"admin\"\n      name: \"userAdminAnyDatabase\"\n    - db: \"admin\"\n      name: \"readWrite\"\n    - db: \"admin\"\n      name: \"userAdminAnyDatabase\"\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mdbu <metadata.name>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  security:\n    authentication:\n      enabled : false"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  security:\n    authentication:\n      enabled : true\n      modes: [\"SCRAM\"]"
                }
            ],
            "preview": "The Kubernetes Operator supports managing database users using SCRAM\nauthentication on MongoDB deployments.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/plan-k8s-op-architecture",
            "title": "Kubernetes Operator Architecture",
            "headings": [],
            "paragraphs": "The  MongoDB Enterprise Kubernetes Operator  provides a container image for the MongoDB Agent in  Ops Manager . This\nallows you to manage and deploy MongoDB database clusters with full monitoring,\nbackups, and automation provided by  Ops Manager . The  Kubernetes  container serves as a host on which  Ops Manager  orchestrates the\ninstallation of  mongod  processes and deploys the cluster configuration. As part of deployment, the  Kubernetes Operator  creates  Persistent Volumes  for\nthe  Ops Manager  StatefulSets. The  Kubernetes  container uses  Persistent Volumes  to maintain the cluster state\nbetween restarts. The  Kubernetes Operator  architecture consists of: This section is for single  Kubernetes  cluster deployments only. For\n multi-Kubernetes-cluster deployments , see  Architecture, Capabilities, and Limitations . An Ops Manager custom resource . Through this resource, the  Kubernetes Operator \ndeploys  Ops Manager  components: the application database, the  Ops Manager \napplication, and the Backup Daemon in the  Kubernetes  containers. After the deployment\nis operational, the  Ops Manager  components reconcile updates that you make to\nthe MongoDB cluster configuration. To learn more, see  Ops Manager  Architecture in  Kubernetes . MongoDB database custom resources . The  Kubernetes Operator  deploys the  MongoDB \ndatabase and the  MongoDB User Resource Specification . After the deployment is\noperational, these resources reconcile updates that you make to the\nuser or the MongoDB cluster configuration. To learn more, see  MongoDB Database Architecture in  Kubernetes .",
            "code": [],
            "preview": "The MongoDB Enterprise Kubernetes Operator provides a container image for the MongoDB Agent in Ops Manager. This\nallows you to manage and deploy MongoDB database clusters with full monitoring,\nbackups, and automation provided by Ops Manager.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        },
        {
            "slug": "tutorial/deploy-prometheus",
            "title": "Deploy a Resource to Use with Prometheus",
            "headings": [
                "Quick Start",
                "Prerequisites",
                "Install the Prometheus Operator",
                "Install the MongoDB Enterprise Kubernetes Operator",
                "Create a MongoDB Resource",
                "Optional: Enable TLS on the Prometheus Endpoint",
                "Install Cert-Manager",
                "Enable TLS on the MongoDB CRD",
                "Update ServiceMonitor",
                "mongodb-prometheus-sample.yaml",
                "Examples",
                "MongoDB Resource with Prometheus",
                "ServiceMonitor",
                "Endpoint Credentials"
            ],
            "paragraphs": "You can use the  mongodb-prometheus-sample.yaml  file to deploy a MongoDB resource in your\n Kubernetes  cluster, with a  ServiceMonitor \nto indicate to Prometheus how to consume metrics data from\nit. The sample specifies a simple MongoDB resource with one user,\nand the  spec.prometheus  attribute with basic HTTP\nauthentication and no  TLS (Transport Layer Security) . The sample lets you test\nthe metrics that MongoDB sends to Prometheus. You can't use Prometheus with a  multi-Kubernetes-cluster deployment . We tested this setup with version 0.54 of the\n Prometheus Operator . Kubernetes 1.16+ Helm 3+ You can install the Prometheus Operator using Helm. To learn\nmore, see the  installation instructions . To install the Prometheus Operator using Helm, run the\nfollowing commands: Run the following command to install the  Kubernetes Operator  and create a\nnamespace to contain the  Kubernetes Operator  and resources: To learn more, see  Install the  MongoDB Enterprise Kubernetes Operator . You can use the  mongodb-prometheus-sample.yaml  file to deploy a MongoDB resource in your\n Kubernetes  cluster, with a  ServiceMonitor \nto indicate to Prometheus how to consume metrics data from\nit. You can apply the sample directly with the following command: This command creates two  secrets  that contain authentication\nfor a new MongoDB user and basic HTTP authentication for the\nPrometheus endpoint. The command creates both  secrets  in the\n mongodb  namespace. This command also creates a  ServiceMonitor  that\nconfigures Prometheus to consume this resource's metrics. This command\ncreates the  ServiceMonitor  in the  prometheus-system \nnamespace. Specify the full path to the  mongodb-prometheus-sample.yaml  file. Ensure you specify\n spec.credentials  and\n spec.cloudManager.configMapRef.name . To install  cert-manager  using Helm,\nsee the  cert-manager installation documentation . To create a cert-manager  Issuer , see the\n cert-manager configuration documentation To create a certificate, see the  cert-manager usage documentation . To enable  TLS (Transport Layer Security) , you must add a new entry to the\n spec.prometheus  section of the MongoDB custom resource. Run\nthe following  patch \noperation to add the needed entry. The following response appears: After a few minutes, the MongoDB resource should return to the\nRunning phase. Now you must configure the Prometheus\n ServiceMonitor \nto point to the HTTPS endpoint. Do  NOT  use this configuration in Production\nenvironments! A security expert should advise you about how to\nconfigure  TLS (Transport Layer Security) . tlsSecretKeyRef.name  points at a  secret  of type\n kubernetes.io/tls  that holds a  Server certificate . To update the  ServiceMonitor , run\nthe following command to patch the resource again: The following reponse appears: With these changes, the new  ServiceMonitor \npoints to the HTTPS endpoint (defined in\n /spec/endpoints/0/scheme ). You also set\n spec/endpoints/0/tlsConfig/insecureSkipVerify  to  true ,\nso that Prometheus doesn't verify the  TLS (Transport Layer Security)  certificates on\nMongoDB's end. Prometheus should now be able to scrape the MongoDB target\nusing HTTPS. Create the following  mongodb-prometheus-sample.yaml  file to deploy\na MongoDB resource in your  Kubernetes  cluster, with a\n ServiceMonitor \nto indicate to Prometheus how to consume metrics data from\nit. This sample file specifies a simple MongoDB resource with one user,\nand the  spec.prometheus  attribute with basic HTTP\nauthentication and no  TLS (Transport Layer Security) . The sample lets you test\nthe metrics that MongoDB sends to Prometheus. To learn more, see  Prometheus Settings . The following examples show the resource definitions required to use\nPrometheus with your MongoDB resource. To learn more, see  Prometheus Settings .",
            "code": [
                {
                    "lang": "sh",
                    "value": "helm repo add prometheus-community https://prometheus-community.github.io/helm-charts"
                },
                {
                    "lang": "sh",
                    "value": "helm repo update"
                },
                {
                    "lang": "sh",
                    "value": "helm install prometheus prometheus-community/kube-prometheus-stack \\\n  --namespace <prometheus-system> \\\n  --create-namespace"
                },
                {
                    "lang": "sh",
                    "value": "helm install enterprise-operator mongodb/enterprise-operator \\\n  --namespace <mongodb> --create-namespace"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <mongodb-prometheus-sample.yaml>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl patch mdbc mongodb --type='json' \\\n  -p='[{\"op\": \"add\", \"path\": \"/spec/prometheus/tlsSecretKeyRef\", \"value\":{\"name\": \"prometheus-target-cert\"}}]' \\\n  --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "mongodbenterprise.mongodbenterprise.mongodb.com/mongodb patched"
                },
                {
                    "lang": "sh",
                    "value": "kubectl patch servicemonitors mongodb-sm --type='json' \\\n    -p='\n[\n    {\"op\": \"replace\", \"path\": \"/spec/endpoints/0/scheme\", \"value\": \"https\"},\n    {\"op\": \"add\",     \"path\": \"/spec/endpoints/0/tlsConfig\", \"value\": {\"insecureSkipVerify\": true}}\n]\n' \\\n    --namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "servicemonitor.monitoring.coreos.com/mongodb-sm patched"
                },
                {
                    "lang": "sh",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: 5.0.6-ent\n\n  cloudManager:\n    configMapRef:\n      name: <project-configmap>\n\n  credentials: <credentials-secret>\n  type: ReplicaSet\n\n  persistent: true\n\n  prometheus:\n    passwordSecretRef:\n      # SecretRef to a Secret with a 'password' entry on it.\n      name: metrics-endpoint-password\n\n    # change this value to your Prometheus username\n    username: prometheus-username\n\n    # Enables HTTPS on the prometheus scrapping endpoint\n    # This should be a reference to a Secret type kuberentes.io/tls\n    # tlsSecretKeyRef:\n    #   name: <prometheus-tls-cert-secret>\n\n    # Port for Prometheus, default is 9216\n    # port: 9216\n    #\n    # Metrics path for Prometheus, default is /metrics\n    # metricsPath: '/metrics'\n\n---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n\n  # This needs to match `spec.ServiceMonitorSelector.matchLabels` from your\n  # `prometheuses.monitoring.coreos.com` resouce.\n  labels:\n    release: prometheus\n\n  name: mongodb-sm\n\n  # Make sure this namespace is the same as in `spec.namespaceSelector`.\n  namespace: mongodb\nspec:\n  endpoints:\n\n  # Configuring a Prometheus Endpoint with basic Auth.\n  # `prom-secret` is a Secret containing a `username` and `password` entries.\n  - basicAuth:\n      password:\n        key: password\n        name: metrics-endpoint-creds\n      username:\n        key: username\n        name: metrics-endpoint-creds\n\n    # This port matches what we created in our MongoDB Service.\n    port: prometheus\n\n    # If using HTTPS enabled endpoint, change scheme to https\n    scheme: http\n\n    # Configure different TLS related settings. For more information, see:\n    # https://github.com/prometheus-operator/prometheus-operator/blob/main/pkg/apis/monitoring/v1/types.go#L909\n    # tlsConfig:\n    #    insecureSkipVerify: true\n\n  # What namespace to watch\n  namespaceSelector:\n    matchNames:\n    # Change this to the namespace the MongoDB resource was deployed.\n    - mongodb\n\n  # Service labels to match\n  selector:\n    matchLabels:\n      app: my-replica-set-svc\n\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: metrics-endpoint-creds\n  namespace: mongodb\ntype: Opaque\nstringData:\n  password: 'Not-So-Secure!'\n  username: prometheus-username\n\n..."
                },
                {
                    "lang": "sh",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: 5.0.6-ent\n  cloudManager:\n    configMapRef:\n      name: <project-configmap>\n  credentials: <credentials-secret>\n  type: ReplicaSet\n  persistent: true\n  prometheus:\n    passwordSecretRef:\n      name: metrics-endpoint-password\n    username: prometheus-username\n\n..."
                },
                {
                    "lang": "sh",
                    "value": "---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  labels:\n    release: prometheus\n  name: mongodb-sm\n  namespace: mongodb\nspec:\n  endpoints:\n  - basicAuth:\n      password:\n        key: password\n        name: metrics-endpoint-creds\n      username:\n        key: username\n        name: metrics-endpoint-creds\n    port: prometheus\n    scheme: http\n  namespaceSelector:\n    matchNames:\n    - mongodb\n  selector:\n    matchLabels:\n      app: my-replica-set-svc\n\n..."
                },
                {
                    "lang": "sh",
                    "value": "---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: metrics-endpoint-creds\n  namespace: mongodb\ntype: Opaque\nstringData:\n  password: 'Not-So-Secure!'\n  username: prometheus-username\n\n..."
                }
            ],
            "preview": "You can use the mongodb-prometheus-sample.yaml file to deploy a MongoDB resource in your\nKubernetes cluster, with a ServiceMonitor\nto indicate to Prometheus how to consume metrics data from\nit.",
            "tags": null,
            "facets": {
                "genre": [
                    "tutorial"
                ],
                "target_product": [
                    "enterprise-kubernetes-operator"
                ]
            }
        }
    ]
}