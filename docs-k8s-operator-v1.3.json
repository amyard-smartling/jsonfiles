{
    "url": "http://mongodb.com/docs/kubernetes-operator/v1.3",
    "includeInGlobalSearch": false,
    "documents": [
        {
            "slug": "reference",
            "title": "Reference",
            "headings": [],
            "paragraphs": "",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "secure",
            "title": "Secure  Deployments",
            "headings": [],
            "paragraphs": "",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "deploy",
            "title": "Deploy Resources",
            "headings": [],
            "paragraphs": "",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "upgrade",
            "title": "Upgrade the  from Prior Versions",
            "headings": [],
            "paragraphs": "",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "connect",
            "title": "Deployment Access",
            "headings": [],
            "paragraphs": "The following pages describe how to connect to your MongoDB\nresource deployed by  :",
            "code": [],
            "preview": "The following pages describe how to connect to your MongoDB\nresource deployed by :",
            "tags": null,
            "facets": null
        },
        {
            "slug": "",
            "title": "MongoDB Enterprise Kubernetes Operator",
            "headings": [],
            "paragraphs": "The   translates the human knowledge of creating a MongoDB\ninstance into a scalable, repeatable, and standardized method.\nKubernetes needs help creating and managing stateful applications like\ndatabases. It needs to configure the network, persist storage, and\ndedicate computing capacity without additional human effort on each\ncontainer. The   manages the typical lifecycle events for a MongoDB\ncluster: provisioning storage and computing power, configuring network\nconnections, setting up users, and changing these settings as needed.\nIt accomplishes this using the Kubernetes API and tools. You provide the Operator with the specifications for your MongoDB\ncluster. The Operator uses this information to tell Kubernetes how to\nconfigure that cluster including provisioning storage, setting up the\nnetwork connections, and configuring other resources. The   works together with MongoDB  , which further\nconfigures to MongoDB clusters. When MongoDB is deployed and running in\nKubernetes, you can manage MongoDB tasks using  . You can then deploy MongoDB databases as you deploy them now after the\ncluster is created. You can use the   console to run MongoDB at\noptimal performance.",
            "code": [],
            "preview": "The  translates the human knowledge of creating a MongoDB\ninstance into a scalable, repeatable, and standardized method.\nKubernetes needs help creating and managing stateful applications like\ndatabases. It needs to configure the network, persist storage, and\ndedicate computing capacity without additional human effort on each\ncontainer.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "installation",
            "title": "Install and Configure the ",
            "headings": [],
            "paragraphs": "",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "configure-k8s-operator-for-mdb-resources",
            "title": "Configure the  for MongoDB Resources",
            "headings": [],
            "paragraphs": "",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "third-party-licenses",
            "title": "Third-Party Licenses",
            "headings": [
                "Apache License 2.0"
            ],
            "paragraphs": "MongoDB   uses third-party libraries or other resources\nthat may be distributed under licenses different than the MongoDB\nsoftware.  depends upon the following third-party packages. These\npackages are licensed as shown in the following list. Should we\naccidentally fail to list a required license, please\n contact the MongoDB Legal Department . Package k8s.io/api k8s.io/apiextensions-apiserver k8s.io/apimachinery k8s.io/client-go k8s.io/kube-openapi",
            "code": [],
            "preview": "MongoDB  uses third-party libraries or other resources\nthat may be distributed under licenses different than the MongoDB\nsoftware.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "release-notes",
            "title": "Release Notes for ",
            "headings": [
                " 1.3.1",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes (Alpha Release)",
                " 1.3.0",
                "Specification Schema Changes",
                "Ops Manager Resource Changes (Alpha Release)",
                "Bug Fixes",
                " 1.2.5",
                "CVE Description",
                "Common Weakness Enumeration",
                "Affected Versions",
                "Fixed Versions",
                " 1.2.4",
                " 1.2.3",
                " 1.2.2",
                " 1.2.1",
                " 1.2",
                "GA Release",
                "Alpha Release",
                " 1.1",
                " 1.0",
                " 0.12",
                " 0.11",
                " 0.10",
                " 0.9",
                " 0.8",
                " 0.7",
                " 0.6",
                " 0.5",
                " 0.4",
                " 0.3",
                " 0.2",
                " 0.1"
            ],
            "paragraphs": "Released 2019-11-08 This release introduces significant changes that may not be\ncompatible with previous deployments or resource configurations. Read\n Migrate to One Resource per Project (Required for Version 1.3.0)  before\nupgrading the  . Requires one MongoDB resource per   project. If you\nhave more than one MongoDB resource in a project, all resources will\nchange to a  Pending  status and the   won\u2019t perform\nany changes on them. The existing MongoDB databases will still be\naccessible. You must  migrate to one resource per project . Supports  SCRAM-SHA  authentication mode. See \nfor examples. Requires that the project ( ConfigMap ) and\ncredentials ( secret )\nreferenced from a MongoDB resource be in the same namespace. Adds OpenShift installation files (  file and Helm chart\nconfiguration). Supports highly available  Ops Manager resources  by introducing the  spec.replicas \nsetting. Runs   as a non-root user. Released 2019-10-25 This release introduces significant changes that may not be\ncompatible with previous deployments or resource configurations. Read\n Migrate to One Resource per Project (Required for Version 1.3.0)  before installing or\nupgrading the  . Moves to a\n one cluster per project configuration .\nThis follows the warnings introduced in a\n previous version of the operator .\nThe operator now requires each cluster to be contained within a new\nproject. Authentication settings are now contained within the\n security section  of the MongoDB resource\nspecification rather than the project ConfigMap. Replaces the  project  field with the\n spec.opsManager.configMapRef.name  or\n spec.cloudManager.configMapRef.name  fields. User resources  now refer to MongoDB resources\nrather than project ConfigMaps. No longer requires  data.projectName  in the project ConfigMap. The\nname of the project defaults to the name of the MongoDB resource in\n . This release introduces signficant changes to the   resource's\narchitecture. The   application database is now managed by\nthe  , not by  . Stops unnecessary recreation of NodePorts. Fixes logging so it's always in JSON format. Sets  USER  in the   Docker image. Fixes CVE-2020-7922:   Operator generates potentially insecure certificates. X.509 certificates generated by the   may allow an attacker with\naccess to the   cluster improper access to MongoDB instances.\nCustomers who do not use X.509 authentication, and those who do not use\nthe   to generate their X.509 certificates are unaffected. CWE-295: Improper Certificate Validation\nCVSS score: 6.4\nCVSS:3.1/AV:A/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:N 1.0, 1.1 1.2.0 - 1.2.4 1.3.0 - 1.3.1 1.4.0 - 1.4.4 1.2.5 1.4.5 and above Released 2019-10-02 Increases stability of Sharded Cluster deployments. Improves internal testing infrastructure. Released 2019-09-13 Update:  The   will remove support for multiple\nclusters per project in a future release. If a project contains more\nthan one cluster, a warning will be added to the status of the\nMongoDB Resources. Additionally, any new cluster being added to a\nnon-empty project will result in a  Failed  state, and won't\nbe processed. Fix:  The overall stability of the operator has been improved. The\noperator is now more conservative in resource updates both on\n  and  . Released 2019-08-30 Security Fix:  Clusters configured by   versions\n1.0 through 1.2.1 used an insufficiently strong keyfile for internal\ncluster authentication between  mongod  processes. This only affects\nclusters which are using X.509 for user authentication, but are not\nusing X.509 for internal cluster authentication. Users are advised to\nupgrade to version 1.2.2, which will replace all managed keyfiles. Security Fix:  Clusters configured by   versions 1.0\nthrough 1.2.1 used an insufficiently strong password to authenticate\nthe MongoDB Agent. This only affects clusters which have been manually\nconfigured to enable  SCRAM-SHA-1 , which is not a supported\nconfiguration. Users are advised to upgrade to version 1.2.2, which\nwill reset these passwords. Released 2019-08-23 Fix:  The   no longer recreates   when X.509\nauthentication is enabled and the approved   have been deleted. Fix:  If the  OPERATOR_ENV  environment variable is set to\nsomething unrecognized by the  , it will no longer result\nin a  CrashLoopBackOff  of the pod. A default value of  prod  is\nused. The   now supports more than 100 agents in a given\nproject. Released 2019-08-13 Adds a\n readinessprobe \nto the MongoDB Pods to improve the reliability of rolling upgrades. This feature is an alpha release. It is not ready for production use. Can use the   to manage   4.2. To  deploy an\n|onprem| instance , you use a new\n resource :  MongoDBOpsManager . Released 2019-07-19 Fix:  Sample yaml files, in particular, the attribute related to\n featureCompatibilityVersion . Fix:    can be disabled in a deployment. Improvement:  Added\n script  in the\n support  directory that can gather information of\nyour MongoDB resources in Kubernetes. Improvement:  In a   environment, the   can use a\ncustom Certificate Authority. All the certificates must be passed as\nKubernetes Secret objects. Released 2019-06-18 Supports Kubernetes v1.11 or later. Provisions any kind of MongoDB deployment in the Kubernetes Cluster\nof your Organization: Standalone Replica Set Sharded Cluster Configures   on the MongoDB deployments and encrypt all traffic.\nHosts and clients can verify each other\u2019s identities. Manages MongoDB users. Supports X.509 authentication to your MongoDB databases. If you have any questions regarding this release, use the\n #enterprise-kubernetes \nSlack channel. Released 2019-06-07 Rolling upgrades of MongoDB resources ensure that  rs.stepDown() \nis called for the primary member. Requires MongoDB patch version 4.0.8 and\nlater or MongoDB patch version 4.1.10 and later. During a MongoDB major version upgrade, the\n featureCompatibilityVersion  field can be set. Fixed a bug where replica sets with more than seven members could\nnot be created. X.509 Authentication can be enabled at the\n Project level . Requires  ,  \npatch version 4.0.11 and later, or   patch version 4.1.7 and later. Internal cluster authentication based on X.509 can be enabled at the\n deployment  level. MongoDB users with X.509 authentication can be created, using the\nnew  MongoDBUser  custom resource. Released 2019-04-29 NodePort  service creation can be disabled.  can be enabled for internal authentication between MongoDB in\nreplica sets and sharded clusters. The   certificates are created\nautomatically by the  . Please refer to the sample\n .yaml  files in the\n GitHub repository \nfor examples. Wide or asterisk roles have been replaced with strict listing of\nverbs in  roles.yaml . Printing  mdb  objects with  kubectl  will provide more\ninformation about the MongoDB object: type, state, and MongoDB server\nversion. Released 2019-04-02 The   and database images are now based on ubuntu:16.04. The   now uses a single   named  MongoDB \ninstead of the  MongoDbReplicaSet ,  MongoDbShardedCluster , and\n MongoDbStandalone  CRDs. Follow the  upgrade procedure  to\ntransfer existing  MongoDbReplicaSet ,  MongoDbShardedCluster ,\nand  MongoDbStandalone  resources to the new format. For a list of the packages installed and any security vulnerabilities\ndetected in our build process, see: MongoDB Enterprise Operator MongoDB Enterprise Database Released 2019-03-19 The Operator and Database images are now based on\n debian:stretch-slim  which is the latest and up-to-date Docker\nimage for Debian 9. Released 2019-02-26 Perform   clean-up on deletion of MongoDB resource without the\nuse of finalisers. Bug fix:  Race conditions when communicating with  . Bug fix:   ImagePullSecrets  being incorrectly initialized in\nOpenShift. Bug fix:  Unintended fetching of closed projects. Bug fix:  Creation of duplicate organizations. Bug fix:  Reconciliation could fail for the MongoDB resource if\nsome other resources in   were in error state. Released 2019-02-01 Improved detailed status field for MongoDB resources. The   watches changes to configuration parameters in a\nproject configMap and the credentials secret then performs a rolling\nupgrade for relevant Kubernetes resources. Added   structured logging for Automation Agent pods. Support   records for MongoDB access. Bug fix: Avoiding unnecessary reconciliation. Bug fix: Improved Ops Manager/Cloud Manager state management for\ndeleted resources. Released 2018-12-17 Refactored code to use the  controller-runtime  library to fix issues\nwhere Operator could leave resources in inconsistent state. This also\nintroduced a proper reconciliation process. Added new  status  field for all MongoDB Kubernetes resources. Can configure Operator to watch any single namespace or all\nnamespaces in a cluster (requires cluster role). Improved database logging by adding a new configuration property\n logLevel . This property is set to  INFO  by default.\nAutomation Agent and MongoDB logs are merged in to a single log\nstream. Added new configuration Operator timeout. It defines waiting time\nfor database pods start while updating  . Fix:  Fixed failure detection for  mongos . Released 2018-11-14 Image for database no longer includes the binary for the Automation\nAgent. The container downloads the Automation Agent binary from\n  when it starts. Fix:  Communication with   failed if the project with the same\nname existed in different organization. Released 2018-10-04 If a backup was enabled in   for a Replica Set or Sharded\nCluster that the   created, then the  \ndisables the backup before removing a resource. Improved persistence support: The data, journal and log directories are mounted to three\nmountpoints in one or three volumes depending upon the\n podSpec.persistence  setting. Prior to this release, only the data directory was mounted to\npersistent storage. Setting Mount Directories to podSpec.persistence.single One volume podSpec.persistence.multiple Three volumes A new parameter,  labelSelector , allows you to specify the\nselector for volumes that   should consider mounting. If   is not specified in the  persistence \nconfiguration, then the default  StorageClass  for the cluster is\nused. In most of public cloud providers, this results in dynamic\nvolume provisioning. Released 2018-08-07 The Operator no longer creates the CustomResourceDefinition objects.\nThe user needs to create them manually. Download and apply\n this new yaml file \n( crd.yaml ) to create/configure these objects. ClusterRoles are no longer required. How the Operator watches\nresources has changed. Until the last release, the Operator would\nwatch for any resource on any  . With 0.3, the Operator\nwatches for resources in the same namespace in which it was created.\nTo support multiple namespaces, multiple Operators can be installed.\nThis allows isolation of MongoDB deployments. Permissions changes were made to how PersistentVolumes are mounted. Added configuration to Operator to not create\n SecurityContexts \nfor  . This solves an issue with OpenShift which does not\nallow this setting when  SecurityContextContraints  are used. If you are using Helm, set  managedSecurityContext  to  true .\nThis tells the Operator to not create  SecurityContext  for\n , satisfying the OpenShift requirement. The combination of  projectName  and  orgId  replaces\n projectId  alone to configure the connection to  .\nThe project is created if it doesn't exist. Released 2018-08-03 Calculates WiredTiger memory cache. Released 2018-06-27 Initial Release Can deploy standalone instances, replica sets, sharded clusters\nusing   configuration files.",
            "code": [],
            "preview": "Released 2019-11-08",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/k8s-resource-specs",
            "title": " Resource Definitions",
            "headings": [],
            "paragraphs": "You can use the   to deploy MongoDB   and database\nresources. To deploy these resources, review the specification for each\ntype.",
            "code": [],
            "preview": "You can use the  to deploy MongoDB  and database\nresources. To deploy these resources, review the specification for each\ntype.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/k8s-operator-om-specification",
            "title": "Ops Manager Object Specification",
            "headings": [
                "Example",
                "Required  Resource Settings",
                "Optional  Resource Settings"
            ],
            "paragraphs": "The \ncreates a containerized   deployment from specification files\nthat you write. After you create or update an   resource\nspecification, you direct   to apply this specification to\nyour   environment.   creates the services and custom\n  resources that   requires, then deploys   and its\nbacking application database in containers in your   environment. Each   resource uses an   specification in\n YAML (Yet Another Markup Language)  to define the characteristics\nand settings of the deployment. Don't use the   resource in\nproduction environments. The following example shows a resource specification for an  \ndeployment: This section describes settings that you must use for all  \nresources. Type : string Required . Version of the MongoDB   resource schema. Type : string Required . Kind of MongoDB Kubernetes resource to create. Set this\nto  MongoDBOpsManager . Type : string Required . Name of the MongoDB   resource you are creating. Type : number Required . Number of   instances to run in parallel. The minimum accepted value is  1 . For high availability, set this value to more than  1 . Multiple\n  instances can read from the same Application\nDatabase, ensuring failover if one instance is unavailable and\nenabling you to update the   resource without downtime. Type : number Required . Version of   that you want to install\non this MongoDB   resource. Type : string Required . Name of the     you created for\nthe   admin user. When you deploy the   resource,\n  creates a user with these credentials. The admin user is granted the\n Global Owner \nrole. Type : collection Required .    Application Database \nresource definition. The following settings from the\n replica set  resource specification are\nrequired: The following settings from the\n replica set  resource specification are\noptional: Do not use the following settings from the\n replica set  resource specification: spec.applicationDatabase. spec.members spec.applicationDatabase. spec.version spec.applicationDatabase. spec.persistent spec.applicationDatabase. spec.logLevel spec.applicationDatabase. spec.featureCompatibilityVersion spec.applicationDatabase.podSpec. spec.podSpec.cpu spec.applicationDatabase.podSpec. spec.podSpec.cpuRequests spec.applicationDatabase.podSpec. spec.podSpec.memory spec.applicationDatabase.podSpec. spec.podSpec.memoryRequests spec.applicationDatabase.podSpec. spec.podSpec.persistence.single spec.applicationDatabase.podSpec. spec.podSpec.persistence.multiple.data spec.applicationDatabase.podSpec. spec.podSpec.persistence.multiple.journal spec.applicationDatabase.podSpec. spec.podSpec.persistence.multiple.logs spec.applicationDatabase.podSpec. spec.podSpec.podAffinity spec.applicationDatabase.podSpec. spec.podSpec.podAntiAffinityTopologyKey spec.applicationDatabase.podSpec. spec.podSpec.nodeAffinity spec.additionalMongodConfig.net.ssl.mode spec.security.authentication.internalCluster spec.security.tls.enabled spec.opsManager.configMapRef.name spec.credentials  resources can use the following settings: Type : string  assigns each   a  . The   calculates\nthe   for each   using a provided  clusterName .  \ndoes not provide an   to query these hostnames. Type : collection  configuration properties.\nSee  Ops Manager Configuration Settings  for property names and descriptions.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: om\nspec:\n replicas: 1\n version: 4.2.0\n adminCredentials: ops-manager-admin\n configuration:\n   mms.fromEmailAddr: \"admin@example.com\"\n   mms.security.allowCORS: \"false\"   \n\n applicationDatabase:\n   members: 3\n   version: 4.0.7\n   persistent: true\n   podSpec:\n     cpu: '0.25'\n"
                }
            ],
            "preview": "Type: string",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/k8s-op-exclusive-settings",
            "title": "MongoDB  Exclusive Settings",
            "headings": [
                "Kubernetes Operator Overrides Some Ops Manager Settings"
            ],
            "paragraphs": "You can use   to deploy MongoDB instances with\n  version 4.0 or later and Cloud Manager. At any place in\nthis guide that says  Ops Manager , you can substitute\n Cloud Manager . Some settings that you configure using   cannot be set or\noverridden in the  . Settings that the   does\nnot manage are accepted. The following list of settings are exclusive to  . This list may\nchange at a later date. These settings can be found on the\n Automation Configuration \npage: In addition to the list of Automation settings, the   uses attributes\noutside of the deployment from the Monitoring and Backup Agent configurations. auth.autoAuthMechanisms auth.authoritativeSet auth.autoPwd auth.autoUser auth.deploymentAuthMechanisms auth.disabled auth.key auth.keyfile auth.keyfileWindows auth.usersWanted auth.usersWanted[n].mechanisms auth.usersWanted[n].roles auth.usersWanted[n].roles[m].role auth.usersWanted[n].roles[m].db auth.usersWanted[n].user auth.usersWanted[n].authenticationRestrictions processes.args2_6.net.port processes.args2_6.net.ssl.clusterAuthFile processes.args2_6.replication.replSetName processes.args2_6.security.clusterAuthMode processes.args2_6.storage.dbPath processes.args2_6.systemLog.path processes.authSchemaVersion processes.cluster  (mongos processes) processes.featureCompatibilityVersion processes.hostname processes.name processes.version replicaSets._id replicaSets.members._id replicaSets.members.host replicaSets.members replicaSets.version sharding.clusterRole  (config server) sharding.configServerReplica sharding.name sharding.shards._id sharding.shards.rs ssl.CAFilePath ssl.autoPEMKeyFilePath ssl.clientCertificateMode backupAgentTemplate.username backupAgentTemplate.sslPEMKeyFile monitoringAgentTemplate.username monitoringAgentTemplate.sslPEMKeyFile  creates a replica set of 3 members. You changed  storage.wiredTiger.engineConfig.cacheSizeGB \nto  40 . This setting is not in the   exclusive settings\nlist. You then use the   to scale the replica set to\n5 members. The  storage.wiredTiger.engineConfig.cacheSizeGB  on the\nnew members should still be  40 .",
            "code": [],
            "preview": "Some settings that you configure using  cannot be set or\noverridden in the . Settings that the  does\nnot manage are accepted.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/known-issues",
            "title": "Known Issues in the ",
            "headings": [
                "Deleting a Resource with Backups Removes All Snapshots",
                "Configure Persistent Storage Correctly",
                "Remove Resources before Removing ",
                "Create Separate Namespaces for  and MongoDB Resources",
                "Difficulties with Updates",
                "Machine Memory vs. Container Memory",
                "Changes to Avoid"
            ],
            "paragraphs": "When you delete a resource that has backup configured, the\n  terminates all backups. Deleting a resource removes all\nexisting snapshots without warning. If there are no\n persistent volumes \navailable when you create a resource, the resulting   stays in\ntransient state and the Operator fails  (after 20 retries) with the\nfollowing error: To prevent this error, either: For testing only, you may also set  persistent : false . This\n must not be used in production , as data is not preserved between\nrestarts. Provide   or Set  persistent : false  for the resource Sometimes   can diverge from  . This mostly occurs when\n  resources are removed manually.   can keep displaying an\nAutomation Agent which has been shut down. If you want to remove deployments of MongoDB on  , use the\nresource specification to delete resources first so no dead Automation\nAgents remain. The best strategy is to create   and its resources in\ndifferent namespaces so that the following operations would work\ncorrectly: or If the   and resources sit in the same  mongodb \n , then operator would also be removed in the same operation.\nThis would mean that it could not clean the configurations, which\nwould have to be done in the  . In some cases, the   can stop receiving change events. As\nthis problem is hard to reproduce, the recommended workaround is to\ndelete the operator pod.   starts the new  \nautomatically and starts working correctly: Kubernetes Operator installation MongoDB versions older than 3.6.13, 4.0.9, and 4.1.9 report host system RAM, not\ncontainer RAM. The   will not be able to apply the following change on a MongoDB Deployment simultaneously: If both operations are applied at the same time, the MongoDB Resource could go into a unrecoverable state. The TLS configuration is disabled ( security.tls.enabled: false ) The number of members in a Replica Set is increased",
            "code": [
                {
                    "lang": "sh",
                    "value": "Failed to update Ops Manager automation config: Some agents failed to register"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete pods --all"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods;\nkubectl delete pod mongodb-enterprise-operator-<podId>`"
                }
            ],
            "preview": "When you delete a resource that has backup configured, the\n terminates all backups. Deleting a resource removes all\nexisting snapshots without warning.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/troubleshooting",
            "title": "Troubleshooting the ",
            "headings": [
                "Get Status of a Deployed Resource",
                "Review the Logs",
                "Review Logs from the ",
                "Find a Specific Pod",
                "Review Logs from Specific Pod",
                "View All  Specifications",
                "Restore StatefulSet that Failed to Deploy",
                "Remove a ",
                "Remove the ",
                "Remove the ",
                "Remove the "
            ],
            "paragraphs": "To find the status of a resource deployed with the  ,\ninvoke one of the following commands: The following key-value pairs describe the resource deployment statuses: For   resource deployments: The  status.applicationDatabase.phase  field displays the\nApplication Database resource deployment status. The\n status.opsManager.phase  field displays the   resource\ndeployment status. For MongoDB resource deployments: The  status.phase  field displays the MongoDB resource deployment\nstatus. Key Value message Message explaining why the resource is in a  Pending \nor  Failed  state. phase Status Meaning Pending The   is unable to reconcile the resource\ndeployment state. This happens when a reconciliation times\nout or if the   requires you to take action\nfor the resource to enter a running state. If a resource is pending because a reconciliation timed\nout, the   attempts to reconcile the resource\nstate in 10 seconds. Reconciling The   is reconciling the resource state. Resources enter this state after you create or update them\nor if the   is attempting to reconcile a\nresource previously in a  Pending  or  Failed  state. The   attempts to reconcile the resource\nstate in 10 seconds. Running The resource is running properly. Failed The resource is not running properly. The  message \nfield provides additional details. The   attempts to reconcile the resource\nstate in 10 seconds. lastTransition  when the last reconciliation happened. link Deployment   in  . Resource specific fields For descriptions of these fields, see\n MongoDB   Object Specification . If you want to see what the status of a replica set named\n my-replica-set  in the  developer  namespace, run: If  my-replica-set  is running, you should see: If  my-replica-set  is not running, you should see: To review the   logs, invoke this command: You could check the  Ops Manager Logs  as\nwell to see if any issues were reported to  . To find which pods are available, invoke this command first: If you want to narrow your review to a specific  , you can\ninvoke this command: If your  replica set  is labeled  myrs , the   log\ncommand is invoked as: This returns the  Automation Agent Log  for this\nreplica set. To view all   specifications in the provided\n : To read details about the  dublin  standalone resource, invoke\nthis command: This returns the following response: A StatefulSet   may hang with a status of  Pending  if it\nencounters an error during deployment. Pending    do not automatically terminate, even if you\nmake  and apply  configuration changes to resolve the error. To return the StatefulSet to a healthy state, apply the configuration\nchanges to the MongoDB resource in the  Pending  state, then delete\nthose pods. A host system has a number of running  : my-replica-set-2  is stuck in the  Pending  stage. To gather\nmore data on the error, run the following: The output indicates an error in memory allocation. Updating the memory allocations in the MongoDB resource is\ninsufficient, as the pod does not terminate automatically after\napplying configuration updates. To remedy this issue, update the configuration, apply the\nconfiguration, then delete the hung pod: Once this hung pod is deleted, the other pods restart with your new\nconfiguration as part of rolling upgrade of the Statefulset. To learn more about this issue, see\n Kubernetes Issue 67250 . To remove any instance that   deployed, you must use  . You can only use the   to remove  -deployed\ninstances. If you use   to remove the instance,   throws an\nerror. To remove a single MongoDB instance you created using  : To remove all MongoDB instances you created using  : To remove the  : Remove all Kubernetes resources : Remove the  : To remove the  : Remove all Kubernetes resources : Remove the  : To remove the  : Remove all Kubernetes resources : Remove the  :",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl get <resource-name> -n <namespace> -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb my-replica-set -n developer -o yaml"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n    lastTransition: \"2019-01-30T10:51:40Z\"\n    link: http://ec2-3-84-128-187.compute-1.amazonaws.com:9080/v2/5c503a8a1b90141cbdc60a77\n    members: 1\n    phase: Running\n    version: 4.0.0"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  lastTransition: 2019-02-01T13:00:24Z\n  link: http://ec2-34-204-36-217.compute-1.amazonaws.com:9080/v2/5c51c040d6853d1f50a51678\n  members: 1\n  message: 'Failed to create/update replica set in Ops Manager: Status: 400 (Bad Request),\n    Detail: Something went wrong validating your Automation Config. Sorry!'\n  phase: Failed\n  version: 4.0.0"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs -f deployment/mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs <podName> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs myrs-0 -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl get mdb -n <namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl get mdb dublin -n <namespace> -o yaml"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"mongodb.com/v1\",\"kind\":\"MongoDB\",\"metadata\":{\"annotations\":{},\"name\":\"dublin\",\"namespace\":\"mongodb\"},\"spec\":{\"credentials\":\"alis-credentials\",\"persistent\":false,\"podSpec\":{\"memory\":\"1G\"},\"project\":\"my-om-config\",\"type\":\"Standalone\",\"version\":\"4.0.0-ent\"}}\n  clusterName: \"\"\n  creationTimestamp: 2018-09-12T17:15:32Z\n  generation: 1\n  name: dublin\n  namespace: mongodb\n  resourceVersion: \"337269\"\n  selfLink: /apis/mongodb.com/v1/namespaces/mongodb/mongodbstandalones/dublin\n  uid: 7442095b-b6af-11e8-87df-0800271b001d\nspec:\n  credentials: my-credentials\n  type: Standalone\n  persistent: false\n  podSpec:\n    memory: 1G\n  project: my-om-config\n  version: 4.0.0-ent"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods\n\nmy-replica-set-0     1/1 Running 2 2h\nmy-replica-set-1     1/1 Running 2 2h\nmy-replica-set-2     0/1 Pending 0 2h"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe pod my-replica-set-2\n\n<describe output omitted>\n\nWarning FailedScheduling 15s (x3691 over 3h) default-scheduler 0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient memory."
                },
                {
                    "lang": "sh",
                    "value": "vi <my-replica-set>.yaml\n\nkubectl apply -f <my-replica-set>.yaml\n\nkubectl delete pod my-replica-set-2"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb <name> -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete deployment mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete namespace <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete crd --all"
                }
            ],
            "preview": "To find the status of a resource deployed with the ,\ninvoke one of the following commands:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/edit-deployment",
            "title": "Edit a Deployment's Configuration",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Considerations"
            ],
            "paragraphs": "You can modify a deployment's configuration and topology, including its\nMongoDB versions, storage engines, and numbers of hosts or shards. You\ncan make modifications at all levels of a deployment's topology from a\ntop-level  sharded cluster  or\n replica set  to lower levels, such as a replica set within a\nsharded cluster, or an individual process within a replica set. You can\nalso modify  standalone  processes. To update a MongoDB  , you need to have completed the following procedures: Install and Configure the  Create Credentials for the  Create a Project using a ConfigMap Deploy a database Edit the   resource specification file. Modify or add any settings you need added or changed. Save your specification file. Invoke the following   command to update your resource. Changes cannot be made to individual members of a replica set or\nsharded cluster, only to the whole set or cluster. If a setting is not available for a MongoDB Kubernetes resource,\nthen the change must be made in the   or\n Cloud Manager  application. MongoDB custom resources do not support all\n mongod  command line options. If you use an\nunsupported option in your object specification file, the backing\n MongoDB Agent \noverrides the unsupported options. For a complete list of options\nsupported by MongoDB custom resources, see  MongoDB   Object Specification . Certain settings can only be configured using  . To\nreview the list of settings,\nsee  MongoDB   Exclusive Settings .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <standalone-conf>.yaml"
                }
            ],
            "preview": "You can modify a deployment's configuration and topology, including its\nMongoDB versions, storage engines, and numbers of hosts or shards. You\ncan make modifications at all levels of a deployment's topology from a\ntop-level sharded cluster or\nreplica set to lower levels, such as a replica set within a\nsharded cluster, or an individual process within a replica set. You can\nalso modify standalone processes.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/k8s-operator-specification",
            "title": "MongoDB  Object Specification",
            "headings": [
                "Common Resource Settings",
                "Required",
                "Conditional",
                "Optional",
                "Deployment-Specific Resource Settings",
                "Standalone Settings",
                "Replica Set Settings",
                "Sharded Cluster Settings",
                "Security Settings",
                "Examples"
            ],
            "paragraphs": "The  MongoDB Enterprise Kubernetes Operator \ncreates     from specification files that you\nwrote. MongoDB resources are created in Kubernetes as  custom resources .\nAfter you create or update a MongoDB resource specification, you direct\n  to apply this specification to your   environment.\n  creates the defined  , services and\nother Kubernetes resources. After the Operator finishes creating those\nobjects, it updates Ops Manager deployment configuration to reflect\nchanges. Each   uses an   specification in\n YAML (Yet Another Markup Language)  to define the characteristics and settings of the MongoDB object: standalone,  replica set ,\nand  sharded cluster . You can use   to deploy MongoDB instances with\n  version 4.0 or later and Cloud Manager. At any place in\nthis guide that says  Ops Manager , you can substitute\n Cloud Manager . Deployment Type StatefulSets Size of StatefulSet Standalone 1 1 per member Replica Set 1 1 per member Sharded Cluster 1 1 per shard or config server member Every resource type must use the following settings: Every resource must use  one  of the following settings: Every resource type may use the following settings: Other settings you can and must use in a   specification\ndepend upon which MongoDB deployment item you want to create: Standalone Settings Replica Set Settings Sharded Cluster Settings The following settings only apply to replica set resource types: All of the  Standalone Settings  also apply to replica set\nresources. The following settings only apply to sharded cluster resource types: The following   settings only apply to replica set and sharded\ncluster resource types: The following example shows a resource specification for a\nstandlone deployment with every setting provided: The following example shows a resource specification for a\n replica set  with every setting provided: The following example shows a resource specification for a\n sharded cluster  with every setting provided:",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone\n  namespace: mongodb\nspec:\n  version: 4.0.0\n  service: my-service\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  persistent: true\n  type: Standalone\n  podSpec:\n    cpu: '0.25'\n    memory: 512M\n    persistence:\n      single:\n        storage: 12G\n        storageClass: standard\n        labelSelector:\n          matchExpressions:\n          - {key: environment, operator: In, values: [dev]}\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/e2e-az-name\n            operator: In\n            values:\n            - e2e-az1\n            - e2e-az2\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\n  namespace: mongodb\nspec:\n  members: 3\n  version: 4.0.0\n  service: my-service\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  persistent: true\n  type: ReplicaSet\n  podSpec:\n    cpu: '0.25'\n    memory: 512M\n    persistence:\n      multiple:\n        data:\n          storage: 10Gi\n        journal:\n          storage: 1Gi\n          labelSelector:\n            matchLabels:\n              app: \"my-app\"\n        logs:\n          storage: 500M\n          storageClass: standard\n    podAntiAffinityTopologyKey: nodeId\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/e2e-az-name\n            operator: In\n            values:\n            - e2e-az1\n            - e2e-az2\n  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n  additionalMongodConfig:\n    net:\n      ssl:\n        mode: requireSSL\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster\n  namespace: mongodb\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.0.0\n  service: my-service\n  type: ShardedCluster\n\n  ## Please Note: The default Kubernetes cluster name is\n  ## `cluster.local`.\n  ## If your cluster has been configured with another name, you can\n  ## specify it with the `clusterName` attribute.\n\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n\n  persistent: true\n  configSrvPodSpec:\n    cpu: '0.5'\n    memory: 512M\n\n    # if \"persistence\" element is omitted then Operator uses the\n    # default size (5G) for mounting single Persistent Volume\n\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n  mongosPodSpec:\n    cpu: '0.8'\n    memory: 1G\n    podAntiAffinityTopologyKey: rackId\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n  shardPodSpec:\n    cpu: '0.6'\n    memory: 3G\n    persistence:\n      multiple:\n        # if the child of \"multiple\" is omitted then the default size will be used.\n        # 16G for \"data\", 1G for \"journal\", 3Gb for \"logs\"\n        data:\n          storage: 20G\n        logs:\n          storage: 4G\n          storageClass: standard\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n  additionalMongodConfig:\n    net:\n      ssl:\n        mode: requireSSL\n...\n"
                }
            ],
            "preview": "The MongoDB Enterprise Kubernetes Operator\ncreates   from specification files that you\nwrote.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/migrate-to-single-resource",
            "title": "Migrate to One Resource per Project (Required for Version 1.3.0)",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Create a new ConfigMap for each MongoDB resource in the project.",
                "Update MongoDB resource objects.",
                "Update MongoDB user objects.",
                "Delete the original project ConfigMap.",
                "(Optional) Remove Orphaned Clusters from ."
            ],
            "paragraphs": "Starting in   version 1.3.0, you can only have one MongoDB\nresource per project. This document explains how to migrate existing\nprojects which have multiple MongoDB resources into configurations with\na single resource per project. Before completing this procedure, ensure that you have upgraded your\n  to version 1.3.0. For upgrade instructions, see\n Upgrade the Operator . Complete the following steps for each project that contains multiple\nMongoDB resources: To associate each MongoDB resource with a single project, each\nresource must have a distinct  . The new ConfigMaps: All other fields can remain the same as the original project\nConfigMap. Invoke the following command for each ConfigMap to apply\nthem to  : To learn more about creating a project using a ConfigMap, see\n Create a Project using a ConfigMap . Must have unique  projectName  fields. Cannot contain the  credentials  or  authenticationMode \nfields. For each MongoDB resource in the project: If  X.509 authentication  is enabled, add the\nfollowing fields to the    : Field Type Description spec.security.authentication object Contains authentication specifications for the\ndeployment. spec.security.authentication.enabled boolean Specifies whether authentication is enabled for the\ndeployment. Set this value to  true . spec.security.authentication.modes array Specifies supported authentication mechanisms for the\ndeployment. Set this value to  [\"X509\"] If internal cluster authentication is enabled, set\n spec.security.authentication.internalCluster  to  X509 . Add the  spec.opsManager.configMapRef.name  field to the\n    and set the value to the  metadata.name  value\nof the corresponding ConfigMap you created in step 1. Remove the  spec.project  field from the resource object. Invoke the following command for each resource object to apply\nthe updated configuration(s). When you apply a new configuration,\nthe Operator creates a new project in   containing the\ndeployment from the corresponding MongoDB resource. All\ndata on the resource database remains the same after the migration. For each  MongoDB user  resource: Remove the  spec.project   field. Add the  spec.mongodbResourceRef.name  field and set the value\nto the name of the relevant MongoDB resource in the same namespace. This may require duplicating your  MongoDBUser  resource if\nyou wish to have the same user in multiple clusters. Invoke the following command to delete the original project ConfigMap\nfrom your   namespace: After reconfiguring your deployments to exist in dedicated clusters,\nyou may have clusters remaining in the original project which are no\nlonger managed by the  . You can remove these clusters if\nyou wish. Removing clusters will delete their historical backups and\nmonitoring data.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <myconfigmap.yaml>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <configuration>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete -f <configMap>.yaml"
                }
            ],
            "preview": "Before completing this procedure, ensure that you have upgraded your\n to version 1.3.0. For upgrade instructions, see\nUpgrade the Operator.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/production-notes",
            "title": " Production Notes",
            "headings": [
                "Ensure Proper Persistence Configuration",
                "Name Your MongoDB Service with its Purpose",
                "Specify Resource Requirements",
                "Use Multiple Availability Zones",
                "Co-locate mongos Pods with Your Applications",
                "Manage Multitenancy with Labels",
                "Enable TLS",
                "Enable Authentication",
                "Example Deployment CRD",
                "Example User CRD"
            ],
            "paragraphs": "This page details system configuration recommendations for the\n  when running in production. You use a   to ensure stateful configurations of  \ndeployments. The storage of your   deployment must persist, so\nverify that the   are configured to meet your storage needs. The  : Supports mounting storage devices to one or more directories\ncalled mount points. Creates one   per MongoDB mount point. Sets the default path in each container to  /data . spec.persistent spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data If using your own MongoDB Service, set the  spec.service  parameter\nto something that helps you identify this deployment's purpose. For the replica sets, sharded clusters, and config servers you create\nusing the  , set the resource utilization bounds for both\ncompute and memory.   refers to the lower bound of a resource as a\n request  and the upper bound as a  limit . Set  CPU requests and limits \nto guarantee the CPU allocation and resource reporting. Set  Memory requests and limits \nto guarantee the requested memory allocation for the WiredTiger\ncache and resource reporting. Monitoring tools report the size of the   rather than the\nactual size of the container. Set the   and   to distribute all members\nof one replica set to different   to ensure high\navailability. The lightweight  mongos  instance can be run in the same  \nas your apps using MongoDB. The   supports standard  \n node-affinity and node anti-affinity \nfeatures. Using these features, you can force install the  mongos \non the same pod as your application. The  podAffinity  key determines if an application should be\ninstalled on the same pod, node, or data center as another\napplication. You add a label and value in the  spec.template.metadata.labels \n  collection to tag the deployment. In this example, the  web-store  value for the  app  key labels\nthe pod on which the  web-server  is installed. The\n labelSelector  key declares that the  mongos  app must be\ninstalled on the same pod that has its  app  label set to be\n In  values that include  web-store . If you need to physically separate different MongoDB resources (such\nas  test  and  staging  environments) or want to place  \non some specific nodes (such as   support) use the\n pod affinity    feature. The   supports   encryption. Use   with your\nMongoDB deployment to encrypt your data over the network. The   supports X.509 user authentication. You must create\nan additional   for your MongoDB users and the MongoDB Agents.\nThe Operator generates and distributes the certificate.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.0.0\n  service: my-service\n  featureCompatibilityVersion: \"3.6\"\n  project: my-project\n  credentials: my-credentials\n  type: ShardedCluster\n  persistent: true\n  configSrvPodSpec:\n    cpu: '0.5'\n    memory: 512M\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n  mongosPodSpec:\n    cpu: '0.8'\n    memory: 1G\n    podAntiAffinityTopologyKey: rackId\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n  shardPodSpec:\n    cpu: '0.6'\n    memory: 3G\n    persistence:\n      multiple:\n        data:\n          storage: 20G\n        logs:\n          storage: 4G\n          storageClass: standard\n    podAntiAffinityTopologyKey: kubernetes.io/hostname"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: 4.0.0\n  service: drilling-pumps-geosensors\n  featureCompatibilityVersion: \"3.6\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.0.0\n  service: my-service\n  featureCompatibilityVersion: \"3.6\"\n  project: my-project\n  credentials: my-credentials\n  type: ShardedCluster\n  persistent: true\n  configSrvPodSpec:\n    cpu: '0.5'\n    memory: 512M\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n  mongosPodSpec:\n    cpu: '0.8'\n    memory: 1G\n    podAntiAffinityTopologyKey: rackId\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n  shardPodSpec:\n    cpu: '0.6'\n    memory: 3G\n    persistence:\n      multiple:\n        data:\n          storage: 20G\n        logs:\n          storage: 4G\n          storageClass: standard\n    podAntiAffinityTopologyKey: kubernetes.io/hostname"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.0.0\n  service: my-service\n  featureCompatibilityVersion: \"3.6\"\n  project: my-project\n  credentials: my-credentials\n  type: ShardedCluster\n  persistent: true\n  configSrvPodSpec:\n    cpu: '0.5'\n    memory: 512M\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n  mongosPodSpec:\n    cpu: '0.8'\n    memory: 1G\n    podAntiAffinityTopologyKey: rackId\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n  shardPodSpec:\n    cpu: '0.6'\n    memory: 3G\n    persistence:\n      multiple:\n        data:\n          storage: 20G\n        logs:\n          storage: 4G\n          storageClass: standard\n    podAntiAffinityTopologyKey: kubernetes.io/hostname"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-server\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web-store\n\nmongosPodSpec:\n  podAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n        matchExpressions:\n        - key: app\n          operator: In\n          values:\n          - web-store"
                },
                {
                    "lang": "yaml",
                    "value": "mongosPodSpec:\n  podAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n        matchExpressions:\n        - key: app\n          operator: In\n          values:\n          - web-store"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-tls-enabled-rs\nspec:\n  type: ReplicaSet\n  members: 3\n  version: 4.0.4\n  project: my-project\n  credentials: my-credentials\n  security:\n    tls:\n      enabled: true\n  additionalMongodConfig:\n    net:\n      ssl:\n        mode: \"preferSSL\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-tls-enabled-rs\nspec:\n  type: ReplicaSet\n  members: 3\n  version: 4.0.4\n  project: my-project\n  credentials: my-credentials\n  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: user-with-roles\nspec:\n  username: \"CN=mms-user-1,OU=cloud,O=MongoDB,L=New York,ST=New York,C=US\"\n  db: \"$external\"\n  project: my-project\n  roles:\n    - db: \"admin\"\n      name: \"clusterAdmin\""
                }
            ],
            "preview": "This page details system configuration recommendations for the\n when running in production.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/create-project-using-configmap",
            "title": "Create a Project using a ConfigMap",
            "headings": [
                "Prerequisites",
                "Kubernetes",
                "Procedure",
                "Copy the following example ConfigMap.",
                "Open your preferred text editor and paste the example  into a new text file.",
                "Update the values in the lines highlighted in the example ConfigMap.",
                "Save this file with a .yaml file extension.",
                "Invoke the  command to create your .",
                "Invoke the  command to verify your .",
                "Considerations",
                "Next Steps"
            ],
            "paragraphs": "The   uses a     to create or link your\n   Project . To create a\n  ConfigMap, you need to edit a few lines of the\n example ConfigMap    file and apply\nthe ConfigMap. Kubernetes version 1.11 or later or Openshift version\n3.11 or later.  version 0.11 or later\n installed . Key Type Description Example metadata.name string Label for a    . metadata.name  documentation on  names .\nThis name must follow  RFC1123  naming\nconventions, using only lowercase alphanumeric\ncharacters, '-' or '.', and must start and end with an\nalphanumeric character. myconfigmap metadata.namespace string Scope of object names. Used to limit what can be managed to\na subset of all  . The default value is  mongodb . The  ,  , and  s\n must  be created in the same  . metadata.namespace  documentation on  mongodb data.projectName string Label for your  \n Project . If you need or want to use an existing Project, you can find\nthe  projectName  by clicking the  All Clusters \nlink at the top left of the screen, then either search by\nname in the  Search  box or scroll to find the\nname in the list. Each card in this list represents the\ncombination of one  Organization  and  Project . The   creates the   Project if it does\nnot exist. It is  strongly recommended  to use the\nOperator to create a new Project for   to manage. The\nOperator adds additional internal information to Projects\nthat it creates. If you omit the  projectName , the   creates\na project with the same name as your   resource. Development data.orgId string 24 character hex string that uniquely identifies your\nMongoDB  Organization .\nYou can find the  orgId  in your    : Click the  Context  menu. Select your Organization. View the current   in your browser and copy the value\ndisplayed in the  <orgId>  placeholder below: This field is  optional . If you omit the  orgId ,\n  creates an Organization called  projectName \nthat contains a Project also called  projectName . You must have the  Organization Project Creator \nrole to create a new project\n within an existing organization . If you set this value, it can be for a  \norganization only. If you try to use an Atlas\norganization, the   may not work as\nintended. data.baseUrl string  to your   including the   and port\nnumber. You may use   for the  data.baseUrl  value. https://ops.example.com:8443 All subsequent  kubectl  commands you invoke must add the\n -n  option with the  metadata.namespace  you\nspecified in your  . This command returns a ConfigMap description in the shell:  defaults to an empty namespace if you do not specify\nthe  -n  option, resulting in deployment failures. You must\nspecify the value of the  <metadata.namespace>  field.\nThe  ,  , and  s should\nrun in the same unique namespace. Starting in   version 1.3.0, you can only have one MongoDB\nresource per project. To learn how to deploy a MongoDB resource\nin your project, see  Deploy Resources . Now that you created your ConfigMap,  Create Credentials for the   before\nyou start  deploying MongoDB resources .",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: <myconfigmap>\n  namespace: <myNamespace>\ndata:\n  projectName: <myOpsManagerProjectName> # Optional\n  orgId: <orgId> # Optional\n  baseUrl: https://<myOpsManagerURL>\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <myconfigmap.yaml>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe configmaps <myconfigmap> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Name:           <myconfigmap>\nNamespace:      <metadata.namespace>\nLabels:         <none>\nAnnotations:    <none>"
                }
            ],
            "preview": "The  uses a   to create or link your\n Project. To create a\n ConfigMap, you need to edit a few lines of the\nexample ConfigMap  file and apply\nthe ConfigMap.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/manage-database-users-x509",
            "title": "Manage Database Users for X.509 Deployments",
            "headings": [
                "Prerequisites",
                "Add a Database User",
                "Copy the following example .",
                "Open your preferred text editor and paste the example ConfigMap into a new text file.",
                "Change the five highlighted lines.",
                "Add any additional roles for the user to the ConfigMap.",
                "Create the user.",
                "View the newly created user in .",
                "Delete a Database User"
            ],
            "paragraphs": "The   supports managing database users for deployments\nrunning with   and X.509 internal cluster authentication enabled. The   does not support other authentication mechanisms\nin deployments it creates. In an Operator-created deployment, you\ncannot use   to: After enabling X.509 authentication, you can add X.509 users using the   interface or the  . Add other authentication methods to users. Manage users  not  using X.509 authentication. Before managing database users, you must deploy a\n replica set  or\n sharded cluster  with   and X.509\nenabled. If you need to generate X.509 certificates for your MongoDB users,\nsee  Generate X.509 Client Certificates . Use the following table to guide you through changing the highlighted\nlines in the ConfigMap: Key Type Description Example metadata.name string The name of the database user resource. mms-user-1 spec.username string The subject line of the x509 client certificate signed\nby the     (Kube CA). To get the subject line of the X.509 certificate, run the\nfollowing command: The username must comply with the\n RFC 2253 \nLDAPv3 Distinguished Name standard. CN=mms-user,U=My Organizational Unit,O=My Org,L=New York,ST=New York,C=US spec.opsManager.configMapRef.name string The name of the project containing the MongoDB database\nwhere user will be added. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. my-project spec.roles.db string The database the  role  can act on. admin spec.mongodbResourceRef.name string The name of the  MongoDB resource  to\nwhich this user is associated. my-resource spec.roles.name string The name of the  role  to grant the database\nuser. The role name can be any\n built-in MongoDB role  or\n custom role  that exists\nin  . readWriteAnyDatabase You may grant additional roles to this user using the format defined\nin the following example: Invoke the following   command to create your database user: You can view the newly-created user in  : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. To delete a database user, pass the  metadata.name  from the user\n  to the following command:",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: <resource-name>\nspec:\n  username: <rfc2253-subject>\n  db: \"$external\"\n  mongodbResourceRef:\n    name: '<MongoDB-Resource-name>'\n  roles:\n    - db: <database-name>\n      name: <role-name>\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "openssl x509 -noout \\\n  -subject -in <my-cert.pem> \\\n  -nameopt RFC2253"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: mms-user-1\nspec:\n  username: CN=mms-user,U=My Organizational Unit,O=My Org,L=New York,ST=New York,C=US\n  project: my-project\n  db: \"$external\"\n  roles:\n    - db: admin\n      name: backup\n    - db: admin\n      name: restore\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mdbu <metadata.name>"
                }
            ],
            "preview": "The  supports managing database users for deployments\nrunning with  and X.509 internal cluster authentication enabled.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-replica-set",
            "title": "Deploy a Replica Set",
            "headings": [
                "Prerequisites",
                "Considerations",
                "Procedure",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example to create a new replica set resource.",
                "Change the highlighted settings to your preferred values.",
                "Add any additional accepted settings for a replica set deployment.",
                "Save this replica set config file with a .yaml file extension.",
                "Start your replica set deployment.",
                "Track the status of your replica set deployment."
            ],
            "paragraphs": "A  replica set  is a group of MongoDB deployments that maintain\nthe same data set. Replica sets provide redundancy and high\navailability and are the basis for all production deployments. To learn more about replica sets, see the\n Replication Introduction  in\nthe MongoDB manual. Use this procedure to deploy a new replica set that   manages.\nAfter deployment, use   to manage the replica set, including such\noperations as adding, removing, and reconfiguring members. You can use   to deploy MongoDB instances with\n  version 4.0 or later and Cloud Manager. At any place in\nthis guide that says  Ops Manager , you can substitute\n Cloud Manager . To deploy a  replica set  using an  , you need to complete the\nfollowing procedures: Install Kubernetes Operator Create a Project using a ConfigMap Create Credentials for the  Starting in   version 1.3.0, you can only have one\nMongoDB resource per project. To learn how to migrate your project to\na single-cluster configuration, see  Migrate to One Resource per Project (Required for Version 1.3.0) . Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\ninto a new text file. Key Type Description Example metadata.name string Label for this    replica set   . metadata.name Kubernetes documentation on\n names . myproject metadata.namespace string Scope of object names.     where this\n  and other   are created. Using two different namespaces allows you to delete your\nstandalone or all of the resources in the namespace without\naffecting your  . metadata.namespace  documentation on  mongodb spec.members integer Number of members of the  replica set . 3 spec.version string Version of MongoDB that this  replica set  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. 3.6.7 string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value  must  match the value you provided for\n metadata.name  in your  \n project ConfigMap . If this   is in a different   than the\n project ConfigMap , you should\nset this value to the namespace  and  name of the\nConfigMap in this format:\n <metadata.namespace>/<metadata.name> The   tracks any changes to the ConfigMap and\nreconciles the state of the  . <myconfigmap> spec.credentials string Name of the     you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . This value  must  match the value you provided for\n namespace  and  name  for your    \n Secret . The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. ReplicaSet spec.persistent string Optional. Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. If this value is  true , then\n spec.podSpec.persistence.single  is set to its\ndefault value of  16G . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: If you want one   for each  , configure the\n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs Your containers must have permissions to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the deployment item does not fix\nissues with your  , contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n  specification file for a  replica set  deployment: spec.clusterName spec.featureCompatibilityVersion spec.logLevel spec.podSpec.cpu spec.podSpec.cpuRequests spec.podSpec.memory spec.podSpec.memoryRequests spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs spec.podSpec.podAffinity spec.podSpec.podAntiAffinityTopologyKey spec.podSpec.nodeAffinity You must set  spec.clusterName  if your   cluster has a\n default domain \ndifferent from default  cluster.local . If you neither use the\ndefault nor set this option, the   may not function as\nexpected. Invoke the following   command to create your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml -w"
                }
            ],
            "preview": "A replica set is a group of MongoDB deployments that maintain\nthe same data set. Replica sets provide redundancy and high\navailability and are the basis for all production deployments.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/create-operator-credentials",
            "title": "Create Credentials for the ",
            "headings": [
                "Prerequisites",
                "Procedure"
            ],
            "paragraphs": "For the   to create or update   in your  \nProject, you need to store your\n Programmatic API Key  as a  \n . Creating a secret stores authentication credentials so\nonly   can access them. Multiple secrets can exist in the same namespace. Each user should\nhave their own secret. To create credentials for the  , you must: Have or create an  \n Organization . Unlike earlier   versions, use the Operator to\ncreate your   project. The Operator adds additional metadata\nto Projects that it creates to help manage the deployments. Have or generate a\n Programmatic API Key . Grant this new   the  Project Owner  role. Add the   or   block of any hosts that serve the\n  to the\n API Whitelist . To create your   secret: Make sure you have the Public and Private Keys for your desired\n   . Invoke the following   command to create your secret: The  -n  flag limits the   to which this secret\napplies. All MongoDB   resources must be in the same\nnamespace with the   and  . The\n  does not use either the secrets or ConfigMaps. Invoke the following   command to verify your secret: This command returns a secret description in the shell:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl -n <metadata.namespace> \\\n  create secret generic <myCredentials> \\\n  --from-literal=\"user=<publicKey>\" \\\n  --from-literal=\"publicApiKey=<privateKey>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe secrets/<myCredentials> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Name:         <myCredentials>\nNamespace:    <metadata.namespace>\nLabels:       <none>\nAnnotations:  <none>\n\nType:  Opaque\n\nData\n====\npublicApiKey:  31 bytes\nuser:          22 bytes"
                }
            ],
            "preview": "For the  to create or update  in your \nProject, you need to store your\nProgrammatic API Key as a \n. Creating a secret stores authentication credentials so\nonly  can access them.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/create-x509-client-certs",
            "title": "Generate X.509 Client Certificates",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Generate a Private Key and Certificate Signing Request",
                "Create a new directory to complete this tutorial.",
                "Enter your newly created directory.",
                "Copy and save the following example JSON.",
                "Generate a key file.",
                "Generate the Certificate Signing Request.",
                "Submit the New CSR to the Kubernetes ",
                "Create a  in Kubernetes.",
                "View your CSRs.",
                "Approve the CSR.",
                "Verify that your certificate has been approved",
                "Obtain the Newly Issued Certificate from the Kubernetes CA",
                "Generate the X.509 certificate from the CSR (Certificate Signing Request).",
                "Concatenate the user private key and Kubernetes certificate.",
                "Connect to the X.509-Enabled MongoDB Deployment",
                "Copy and save the following example .",
                "Create the X.509 MongoDB user.",
                "Verify your newly created user",
                "Use your X.509 user to connect to the MongoDB deployment"
            ],
            "paragraphs": "The   can deploy MongoDB instances with\n X.509 authentication \nenabled. If X.509 authentication has been enabled for the deployment,\nyou must generate and use an X.509 certificate to connect to the\ndeployment. This new client certificate must be signed by the\nKubernetes   to be accepted by the MongoDB deployment. Use the procedure outlined in this document to: Generate an X.509 certificate. Get that certificate signed by the Kubernetes\n CA (Certificate Authority) . Use the certificate to connect to your X.509-enabled MongoDB\ndeployment. A full description of Transport Layer Security (TLS), Public Key Infrastructure (PKI)\ncertificates, and Certificate Authorities is beyond the scope of this\ndocument. This page assumes prior knowledge of   and\nX.509 authentication. To complete this tutorial, you must have the  \ninstalled. For instructions on installing the  ,\nsee  Install the  . This tutorial assumes you have a MongoDB deployment which\nrequires X.509 authentication. For instructions on deploying\nMongoDB resources, see  Deploy Resources . This tutorial uses  CFSSL  to generate X.509 certificates.  CFSSL \nis a certificate generation tool built by\n Cloudflare . For instructions on\ninstalling  CFSSL , refer to the\n CFSSL GitHub page . The user configuration files used in this tutorial are\nstrictly examples. You may need to adjust the values in the\nexamples to suit your deployment's needs. For more\ninformation on formatting user ConfigMaps,\nsee  Manage Database Users for X.509 Deployments . Run the following command to create a new directory for\nthe configuration files used in this tutorial: In the  client-x509-certs-tutorial  directory, save the following\nJSON as   x509_user.json : Run the following command to pass the JSON from the previous step\nto  CFSSL  and generate a key file: You should see output similar to the following: You now have a file called  x509_user_key.json  containing\na new private key. Run the following command to use your  x509_user_key.json  key\nfile to generate a certificate signing request (CSR): This command generates two files: x509_user-key.pem , the private key for the user x509_user.csr , the CSR that represents the user Kubernetes' own certificate authority provides the trusted  \nfor the Kubernetes cluster. You need the  .csr  and  .pem  files\ngenerated in the previous section to request a new certificate from\nKubernetes. Run the following command to create a CSR\nin Kubernetes: Run the following command to view a list of CSRs: You should see an output similar to the following: The CSR remains in  Pending  condition\nuntil Kubernetes approves it. Run the following command to\napprove the certificate: You should see an output similar to the following: Run the following command to verify that the Kubernetes   has\napproved your certificate: You should see an output similar to the following: You can use the new certificate.\nThe  status.certificate  attribute of the CSR\ngenerated in the  previous section \ncontains the certificate. Run the following command to generate the certificate from the\nCSR object to a file called  client.crt : A   client can use the  client.crt \ncertificate to connect to the X.509-enabled MongoDB deployment. You need both the  x509_user-key.pem  and  client.crt  files\nto connect to the deployment. Run the following command to\nconcatenate the two files into the a new  .pem  file: With the client certificate created, you can create a MongoDB user\nand connect to the X.509-enabled deployment. Save the following ConfigMap as  x509-mongodb-user.yaml : This ConfigMap  .yaml  file describes a  MongoDBUser  custom object. You\ncan use these custom objects to create MongoDB users. In this example, the ConfigMap describes the user as an X.509\nuser that the client can use to connect to MongoDB with the\ncorresponding X.509 certificate. Run the following command to apply the ConfigMap and create the\nX.509 MongoDB user: You should see an output similar to the following: Run the following command to check the state of the  new-x509-user : You should see an output similar to the following: Once you have created your X.509 user, try to connect to the\ndeployment using the mongo Shell: On Kubernetes Pods, the   file is saved in\n /var/run/secrets/kubernetes.io/serviceaccount/ca.crt , which\nis the file location used for the  --sslCAFile  connection\noption.",
            "code": [
                {
                    "lang": "sh",
                    "value": "mkdir client-x509-certs-tutorial"
                },
                {
                    "lang": "sh",
                    "value": "cd client-x509-certs-tutorial"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"names\": [\n    {\"O\": \"organization\"},\n    {\"OU\": \"organizationalunit\"}\n  ],\n  \"CN\": \"my-x509-authenticated-user\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 4096\n  }\n}"
                },
                {
                    "lang": "sh",
                    "value": "cfssl genkey x509_user.json > x509_user_key.json"
                },
                {
                    "lang": "sh",
                    "value": "2019/06/04 18:12:38 [INFO] generate received request\n2019/06/04 18:12:38 [INFO] received CSR\n2019/06/04 18:12:38 [INFO] generating key: rsa-4096\n2019/06/04 18:12:40 [INFO] encoded CSR"
                },
                {
                    "lang": "sh",
                    "value": "cfssljson -f x509_user_key.json -bare x509_user"
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | kubectl apply -f -\napiVersion: certificates.k8s.io/v1beta1\nkind: CertificateSigningRequest\nmetadata:\n  name: x509-user.some-namespace\nspec:\n  groups:\n  - system:authenticated\n  request: $(cat x509_user.csr | base64 | tr -d '\\n')\n  usages:\n  - digital signature\n  - key encipherment\n  - client auth\nEOF"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                         AGE    REQUESTOR                                 CONDITION\nx509-user.some-namespace     1m     system:serviceaccount:some-namespace      Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve x509-user.some-namespace"
                },
                {
                    "lang": "sh",
                    "value": "certificatesigningrequest.certificates.k8s.io/x509-user.some-namespace approved"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                       AGE   REQUESTOR                              CONDITION\nx509-user.some-namespace   45m   system:serviceaccount:some-namespace   Approved,Issued"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr x509-user.some-namespace -o jsonpath='{.status.certificate}' | base64 --decode > client.crt"
                },
                {
                    "lang": "sh",
                    "value": "cat x509_user-key.pem client.crt > x509-full.pem"
                },
                {
                    "lang": "none",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: new-x509-user\nspec:\n  username: \"CN=my-x509-authenticated-user, OU=organizationalunit, O=organization\"\n  db: \"$external\"\n  mongodbResourceRef:\n    name: '<name of the MongoDB resource>'\n  roles:\n    - db: \"admin\"\n      name: \"clusterAdmin\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n {namespace} apply -f x509-mongodb-user.yaml"
                },
                {
                    "lang": "sh",
                    "value": "mongodbuser.mongodb.com/new-x509-user created"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n {namespace} get mdbu/new-x509-user -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "NAME            CREATED AT\nnew-x509-user   8m"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host {host} --tls --tlsCAFile /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --tlsCertificateKeyFile x509-full.pem --authenticationMechanism MONGODB-X509 --authenticationDatabase '$external'"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host {host} --ssl --sslCAFile /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --sslPEMKeyFile x509-full.pem --authenticationMechanism MONGODB-X509 --authenticationDatabase '$external'"
                }
            ],
            "preview": "The  can deploy MongoDB instances with\nX.509 authentication\nenabled. If X.509 authentication has been enabled for the deployment,\nyou must generate and use an X.509 certificate to connect to the\ndeployment. This new client certificate must be signed by the\nKubernetes  to be accepted by the MongoDB deployment.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/connect-from-outside-k8s",
            "title": "Connect to a MongoDB Resource from Outside Kubernetes",
            "headings": [
                "Procedure",
                "Example Deployment Configurations"
            ],
            "paragraphs": "The following procedure describes how to connect to a MongoDB resource\ndeployed by   from outside of the   cluster. You can only connect to a  standalone  or a  sharded cluster  resource from outside of the\n  cluster. To connect to a MongoDB resource deployed by   from outside of\nthe   cluster, you must set the resource's\n spec.exposedExternally  flag to  true . When this flag is set to  true , the   creates a\n NodePort service .\nThe NodePort service exposes the deployment as a network service,\nthereby enabling access from outside of the   cluster. The following example standalone configuration object exposes\nthe deployment outside of the   cluster by setting\n spec.exposedExternally  to  true : Once your standalone instance is deployed in  , run the\nfollowing command to get information on the NodePort service\ncreated by the Operator: The list output by this command should contain an entry\nsimilar to the following: In this case, the  mongod  is exposed on port\n 27017  in the   container, and the NodePort service\nexposes the  mongod  via port  30994 . When you connect to your deployment, you must specify the external\n DNS (Domain Name System)  of a node in the   cluster as\nthe  --host  option in your  mongo  command. If a node in the\n  cluster has an external DNS of\n ec2-54-212-23-143.us-west-2.compute.amazonaws.com , you\ncan connect to this standalone instance from outside of  \nusing the following command: To obtain the external  DNS (Domain Name System)  of your\nKubernetes cluster, you can run the following command: This command displays the external DNS in the\n Addresses.ExternalDNS  section of the output. Alternatively, you can output the external DNS directly by running: The following example sharded cluster configuration object exposes\nthe deployment outside of the   cluster by setting\n spec.exposedExternally  to  true : Once your sharded cluster is deployed in  , run the\nfollowing command to get information on the NodePort service\ncreated by the Operator: The list output by this command should contain an entry\nsimilar to the following: In this case, the  mongos  is exposed on port\n 27017  in the   container, and the NodePort service\nexposes the  mongos  via port  30078 . When you connect to your deployment, you must specify the external\n DNS (Domain Name System)  of a node in the   cluster as\nthe  --host  option in your  mongo  command. If a node in the\n  cluster has an external DNS of\n ec2-54-212-23-143.us-west-2.compute.amazonaws.com , you\ncan connect to this standalone instance from outside of  \nusing the following command: To obtain the external  DNS (Domain Name System)  of your\nKubernetes cluster, you can run the following command: This command displays the external DNS in the\n Addresses.ExternalDNS  section of the output. Alternatively, you can output the external DNS directly by running:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl get services -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\n\nstandalone-svc-external   NodePort    10.102.27.116   <none>        27017:30994/TCP   8m30s"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host ec2-54-212-23-143.us-west-2.compute.amazonaws.com --port 30994"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\n  namespace: <metadata.namespace>       # Should match\n                                        # metadata.namespace in\n                                        # your configmap file.\nspec:\n  version: 4.2.1\n  opsManager:                           # Alias of cloudManager\n    configMapRef:\n      name: <configMap.metadata.name>   # Should match metadata.name\n                                        # in your configmap file.\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true\n  exposedExternally: true\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe nodes"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get nodes -o jsonpath='{ $.items[*].status.addresses[?(@.type==\"ExternalDNS\")].address }'"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get services -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "NAME                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\n\nshardedcluster-svc-external   NodePort    10.106.44.30    <none>        27017:30078/TCP   10s"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host ec2-54-212-23-143.us-west-2.compute.amazonaws.com --port 30078"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\n  namespace: <metadata.namespace>       # Should match\n                                        # metadata.namespace in\n                                        # your configmap file.\nspec:\n  version: 4.2.1\n  opsManager:                           # Alias of cloudManager\n    configMapRef:\n      name: <configMap.metadata.name>   # Should match metadata.name\n                                        # in your configmap file.\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  credentials: my-secret\n  type: ShardedCluster\n  persistent: true\n  exposedExternally: true\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe nodes"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get nodes -o jsonpath='{ $.items[*].status.addresses[?(@.type==\"ExternalDNS\")].address }'"
                }
            ],
            "preview": "The following procedure describes how to connect to a MongoDB resource\ndeployed by  from outside of the  cluster.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/scale-resources",
            "title": "Scale a Deployment",
            "headings": [
                "Considerations",
                "Examples"
            ],
            "paragraphs": "You can scale your  replica set  and  sharded cluster \ndeployments up or down to match your desired configuration. Scaling up\nincreases the number of members and/or shards in the deployment,\nthereby improving your deployment's redundancy and availability. The\nscale of your deployment is configured in its corresponding\n . To scale your replica set deployment, set the  spec.members \nsetting to the desired number of replica set members. To learn more\nabout replication, see  Replication  in the\nMongoDB manual. To scale your sharded cluster deployment, set the following settings\nas desired: To learn more about sharded cluster configurations, see\n Sharded Cluster Components  in the MongoDB manual. Setting Description spec.shardCount Number of  shards  in the sharded cluster. spec.mongodsPerShardCount Number of members per shard. spec.mongosCount Number of Shard Routers. spec.configServerCount Number of members in the Config Server. The   does not support modifying deployment types.\nFor example, you cannot convert a standalone deployment to a\nreplica set. To modify the type of a deployment,\nwe recommend the following procedure: Create the new deployment with the desired configuration. Back up the data  from\nyour current deployment. Restore the data  from your current\ndeployment to the new deployment. Test your application connections to the new deployment as needed. Once you have verified that the new deployment contains the\nrequired data and can be reached by your application(s), bring\ndown the old deployment. Select the desired tab based on the deployment configuration you\nwant to scale: Consider a replica set resource with the following\n : To scale up this replica set and add more members: Adjust the  spec.members  setting to the desired\nnumber of members: Reapply the configuration to  : Consider a sharded cluster resource with the following\n : To scale up this sharded cluster: Adjust the following settings to the desired values: spec.shardCount spec.mongodsPerShardCount spec.mongosCount spec.configServerCount Reapply the configuration to  :",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-secure-replica-set>\n  namespace: <configMap.metadata.namespace>\nspec:\n  members: 4\n  version: 4.2.1\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <repl-set-config>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-secure-sharded-cluster>\n  namespace: <configMap.metadata.namespace>\nspec:\n  shardCount: 3\n  mongodsPerShardCount: 3\n  mongosCount: 3\n  configServerCount: 4\n  version: 4.2.1\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-config>.yaml"
                }
            ],
            "preview": "You can scale your replica set and sharded cluster\ndeployments up or down to match your desired configuration. Scaling up\nincreases the number of members and/or shards in the deployment,\nthereby improving your deployment's redundancy and availability. The\nscale of your deployment is configured in its corresponding\n.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/upgrade-mdb-version",
            "title": "Upgrade MongoDB Version and FCV",
            "headings": [
                "Example"
            ],
            "paragraphs": "You can upgrade the major, minor, and/or feature compatibility versions\nof your MongoDB resource. These settings are configured in your\nresource's  . To upgrade your resource's major and/or minor versions, set the\n spec.version  setting to the desired MongoDB version. To modify your resource's\n feature compatibility version ,\nset the  spec.featureCompatibilityVersion  setting to the\ndesired version. If you update  spec.version  to a later version, consider setting\n spec.featureCompatibilityVersion  to the current working\nMongoDB version to give yourself the option to downgrade if\nnecessary. To learn more about feature compatibility, see\n setFeatureCompatibilityVersion  in the MongoDB Manual. Consider the following   for a standalone resource: This resource has a MongoDB version of  4.0.6 . The following steps\nupgrade the deployment's MongoDB version to  4.2.0 :  automatically reconfigures your deployment with the new\nspecifications. You can see these changes reflected in the\n  or  Cloud Manager \napplication. Perform the following modifications to the resource's\nConfigMap: Set  spec.version  to the desired MongoDB version. Set  spec.featureCompatibilityVersion  to the current\nworking MongoDB version: Setting  featureCompatibilityVersion  to  4.2  disables\n 4.2 features incompatible with MongoDB 4.0 . Reapply the configuration to  :",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone-downgrade\nspec:\n  version: 4.0.6\n  type: Standalone\n  project: my-project\n  credentials: my-credentials\n  persistent: false\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone-downgrade\nspec:\n  version: 4.2.0\n  featureCompatibilityVersion: 3.6\n  type: Standalone\n  project: my-project\n  credentials: my-credentials\n  persistent: false\n..."
                },
                {
                    "lang": "none",
                    "value": "kubectl apply -f <standalone-config>.yaml"
                }
            ],
            "preview": "You can upgrade the major, minor, and/or feature compatibility versions\nof your MongoDB resource. These settings are configured in your\nresource's .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-tls",
            "title": "Secure Deployments using TLS",
            "headings": [
                "Prerequisites",
                "Configure TLS for a Replica Set",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the TLS settings for your replica set resource.",
                "Save your replica set config file.",
                "Update and restart your replica set deployment.",
                "Check the status of your deployment.",
                "Retrieve the CSRs for each host and agent in your deployment.",
                "Approve the CSR for each host in your deployment.",
                "Track the status of your deployment.",
                "Configure TLS for a Sharded Cluster",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the TLS settings for your sharded cluster resource.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Check the status of your deployment.",
                "Retrieve the CSRs for each host and agent in your deployment.",
                "Approve the CSR for each host in your deployment.",
                "Track the status of your deployment."
            ],
            "paragraphs": "The   can use   certificates to encrypt connections\nbetween: This guide instructs you on how to configure the   to use\n  for its MongoDB instances. MongoDB hosts in a replica set or sharded cluster Client applications and MongoDB deployments Before you secure your MongoDB deployment using   encryption,\ncomplete the following: Install the Kubernetes Operator Create Credentials for the Kubernetes Operator Deploy the Database Resource  that you want to secure: Replica Set Sharded Cluster Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true Invoke the following   command to update and restart your\n replica set : The   creates the MongoDB resources and requests the\n    to approve the database host's certificates. Run the\nfollowing command to verify that the certificates are pending\napproval: The  status  field of the output should resemble the following: If you do not see the  status.message  above, see\n Troubleshooting the   to help diagnose the issue. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the replica set\nexample: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true Invoke the following   command to update and restart your\n sharded cluster : The   creates the MongoDB resources and requests the\n    to approve the database host's certificates. Run the\nfollowing command to verify that the certificates are pending\napproval: The  status  field of the output should resemble the following: If you do not see the  status.message  above, see\n Troubleshooting the   to help diagnose the issue. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the sharded cluster\nexample: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "status:\n  lastTransition: 2019-05-01T15:36:59Z\n  message: Not all certificates have been approved by Kubernetes CA\n  phase: Failed\n  type: \"\"\n  version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmy-secure-rs-0.mongodb                      33s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-1.mongodb                      31s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-2.mongodb                      24s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-rs-0.mongodb\nkubectl certificate approve my-secure-rs-1.mongodb\nkubectl certificate approve my-secure-rs-2.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "status:\n  lastTransition: 2019-05-01T15:36:59Z\n  message: Not all certificates have been approved by Kubernetes CA\n  phase: Failed\n  type: \"\"\n  version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmy-secure-sc-0-0.mongodb                    30s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-1.mongodb                    28s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-2.mongodb                    27s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-0.mongodb                    22s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-1.mongodb                    13s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-2.mongodb                    6s        system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-0.mongodb               36s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-1.mongodb               34s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-2.mongodb               32s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-0.mongodb               49s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-1.mongodb               42s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-sc-0-0.mongodb\nkubectl certificate approve my-secure-sc-0-1.mongodb\nkubectl certificate approve my-secure-sc-0-2.mongodb\nkubectl certificate approve my-secure-sc-1-0.mongodb\nkubectl certificate approve my-secure-sc-1-1.mongodb\nkubectl certificate approve my-secure-sc-1-2.mongodb\nkubectl certificate approve my-secure-sc-config-0.mongodb\nkubectl certificate approve my-secure-sc-config-1.mongodb\nkubectl certificate approve my-secure-sc-config-2.mongodb\nkubectl certificate approve my-secure-sc-mongos-0.mongodb\nkubectl certificate approve my-secure-sc-mongos-1.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml -w"
                }
            ],
            "preview": "The  can use  certificates to encrypt connections\nbetween:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-internal-auth",
            "title": "Secure Internal Authentication with X.509 and TLS",
            "headings": [
                "Prerequisites",
                "Configure X.509 Internal Authentication for a Replica Set",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the TLS settings for your replica set resource.",
                "Configure the general X.509 settings for your replica set resource.",
                "Configure the internal X.509 settings for your replica set resource.",
                "Save your replica set config file.",
                "Update and restart your replica set deployment.",
                "Check the status of your deployment.",
                "Retrieve the CSRs for each host and agent in your deployment.",
                "Approve the CSR for each host in your deployment.",
                "Retrieve the cluster file CSR for each host in your deployment.",
                "Approve the cluster file CSR for each host in your deployment.",
                "Approve the CSR for each agent in your deployment.",
                "Track the status of your deployment.",
                "Configure X.509 Internal Authentication for a Sharded Cluster",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the TLS settings for your sharded cluster resource.",
                "Configure the general X.509 settings for your sharded cluster resource.",
                "Configure the internal X.509 settings for your sharded cluster resource.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Check the status of your deployment.",
                "Retrieve the CSRs for each host in your deployment.",
                "Approve the CSR for each host in your deployment.",
                "Retrieve the cluster file CSR for each host in your deployment.",
                "Approve the cluster file CSR for each host in your deployment.",
                "Approve the CSR for each agent in your deployment.",
                "Track the status of your deployment."
            ],
            "paragraphs": "This guide instructs you on how to configure: X.509 internal authentication between MongoDB nodes in a cluster. X.509 authentication from clients to your MongoDB instances.  to encrypt connections between MongoDB hosts in a replica set\nor sharded cluster.  to encrypt connections client applications and MongoDB\ndeployments. Before you secure your MongoDB deployment using X.509 internal\nauthentication, complete the following: Install the Kubernetes Operator Create Credentials for the Kubernetes Operator Deploy the Database Resource  that you want to secure: Replica Set Sharded Cluster Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true , authentication is enabled on the\nMongoDB deployment. true array Conditional If you enabled authentication, you must set an authentication\nmechanism. Accepted values are  X509 . X509 To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example string Conditional If you enabled authentication, you can enable\n X.509 internal cluster authentication .\nAccepted values are  X509 . Once internal cluster authentication is enabled, it can not\nbe disabled. X509 Invoke the following   command to update and restart your\n replica set : The   creates the MongoDB resources and requests the\n    to approve the database host's certificates. Run the\nfollowing command to verify that the certificates are pending\napproval: The  status  field of the output should resemble the following: If you do not see the  status.message  above, see\n Troubleshooting the   to help diagnose the issue. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the replica set\nexample: When  spec.security.clusterAuthenticationMode  is set to\n x509 ,   generates an additional   per host for\nthe clusterfile. After the first batch of certificates are approved, run the\ncommand to retrieve the   again: The clusterfile   are now present in the output: The   generates   for the \u200bs when\nMongoDB resources are present. Approve the clusterfile   using the same command: The following commands approve the clusterfile certificates: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the replica set\nexample: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true , authentication is enabled on the\nMongoDB deployment. true array Conditional If you enabled authentication, you must set an authentication\nmechanism. Accepted values are  X509 . X509 To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example string Conditional If you enabled authentication, you can enable\n X.509 internal cluster authentication .\nAccepted values are  X509 . Once internal cluster authentication is enabled, it can not\nbe disabled. X509 Invoke the following   command to update and restart your\n sharded cluster : The   creates the MongoDB resources and requests the\n    to approve the database host's certificates. Run the\nfollowing command to verify that the certificates are pending\napproval: The  status  field of the output should resemble the following: If you do not see the  status.message  above, see\n Troubleshooting the   to help diagnose the issue. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the sharded cluster\nexample: When  spec.security.clusterAuthenticationMode  is set to\n x509 ,   generates an additional   per host for\nthe clusterfile. After the first batch of certificates are approved, run the\ncommand to retrieve the   again: The clusterfile   are now present in the output: The   generates   for the \u200bs when\nMongoDB resources are present. Approve the clusterfile   using the same command: The following commands approve the clusterfile certificates: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the sharded cluster\nexample: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "status:\n  lastTransition: 2019-05-01T15:36:59Z\n  message: Not all certificates have been approved by Kubernetes CA\n  phase: Failed\n  type: \"\"\n  version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmms-automation-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-backup-agent.mongodb                    15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-monitoring-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-rs-0.mongodb                      33s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-1.mongodb                      31s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-2.mongodb                      24s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-rs-0.mongodb\nkubectl certificate approve my-secure-rs-1.mongodb\nkubectl certificate approve my-secure-rs-2.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmms-automation-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-backup-agent.mongodb                    15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-monitoring-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-rs-0-clusterfile.mongodb          13s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-0.mongodb                      105s      system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-rs-1-clusterfile.mongodb          7s        system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-1.mongodb                      103s      system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-rs-2-clusterfile.mongodb          3s        system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-2.mongodb                      100s      system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-rs-0-clusterfile.mongodb\nkubectl certificate approve my-secure-rs-1-clusterfile.mongodb\nkubectl certificate approve my-secure-rs-2-clusterfile.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve mms-automation-agent.mongodb\nkubectl certificate approve mms-backup-agent.mongodb\nkubectl certificate approve mms-monitoring-agent.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "status:\n  lastTransition: 2019-05-01T15:36:59Z\n  message: Not all certificates have been approved by Kubernetes CA\n  phase: Failed\n  type: \"\"\n  version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmms-automation-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-backup-agent.mongodb                    15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-monitoring-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-0-0.mongodb                    30s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-1.mongodb                    28s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-2.mongodb                    27s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-0.mongodb                    22s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-1.mongodb                    13s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-2.mongodb                    6s        system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-0.mongodb               36s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-1.mongodb               34s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-2.mongodb               32s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-0.mongodb               49s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-1.mongodb               42s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-sc-0-0.mongodb\nkubectl certificate approve my-secure-sc-0-1.mongodb\nkubectl certificate approve my-secure-sc-0-2.mongodb\nkubectl certificate approve my-secure-sc-1-0.mongodb\nkubectl certificate approve my-secure-sc-1-1.mongodb\nkubectl certificate approve my-secure-sc-1-2.mongodb\nkubectl certificate approve my-secure-sc-config-0.mongodb\nkubectl certificate approve my-secure-sc-config-1.mongodb\nkubectl certificate approve my-secure-sc-config-2.mongodb\nkubectl certificate approve my-secure-sc-mongos-0.mongodb\nkubectl certificate approve my-secure-sc-mongos-1.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmms-automation-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-backup-agent.mongodb                    15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-monitoring-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-0-0-clusterfile.mongodb        40s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-0.mongodb                    2m22s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-0-1-clusterfile.mongodb        36s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-1.mongodb                    2m20s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-0-2-clusterfile.mongodb        32s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-2.mongodb                    2m19s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-1-0-clusterfile.mongodb        27s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-0.mongodb                    2m14s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-1-1-clusterfile.mongodb        23s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-1.mongodb                    2m5s      system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-1-2-clusterfile.mongodb        20s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-2.mongodb                    118s      system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-config-0-clusterfile.mongodb   10s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-0.mongodb               2m28s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-config-1-clusterfile.mongodb   5s        system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-1.mongodb               2m26s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-config-2-clusterfile.mongodb   2s        system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-2.mongodb               2m24s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-mongos-0-clusterfile.mongodb   18s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-0.mongodb               2m41s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-mongos-1-clusterfile.mongodb   12s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-1.mongodb               2m34s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-sc-0-0-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-0-1-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-0-2-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-1-0-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-1-1-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-1-2-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-config-0-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-config-1-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-config-2-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-mongos-0-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-mongos-1-clusterfile.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve mms-automation-agent.mongodb\nkubectl certificate approve mms-backup-agent.mongodb\nkubectl certificate approve mms-monitoring-agent.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml -w"
                }
            ],
            "preview": "This guide instructs you on how to configure:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-sharded-cluster",
            "title": "Deploy a Sharded Cluster",
            "headings": [
                "Prerequisites",
                "Considerations",
                "Do Not Deploy Monitoring Agents inside and outside ",
                "Procedure",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example to create a new sharded cluster resource.",
                "Configure the settings highlighted in the preceding step as follows.",
                "Add any additional accepted settings for a sharded cluster  deployment.",
                "Save this file with a .yaml file extension.",
                "Start your sharded cluster deployment.",
                "Track the status of your sharded cluster deployment."
            ],
            "paragraphs": "Sharded clusters  provide horizontal scaling\nfor large data sets and enable high throughput operations by\ndistributing the data set across a group of servers. To learn more about sharding, see\n Sharding Introduction  in the\nMongoDB manual. Use this procedure to deploy a new sharded cluster that   manages.\nLater, you can use   to add shards and perform other maintenance\noperations on the cluster. You can use   to deploy MongoDB instances with\n  version 4.0 or later and Cloud Manager. At any place in\nthis guide that says  Ops Manager , you can substitute\n Cloud Manager . To deploy a  sharded cluster  using an  , you need to complete the\nfollowing procedures: Install Kubernetes Operator Create a Project using a ConfigMap Create Credentials for the  Starting in   version 1.3.0, you can only have one\nMongoDB resource per project. To learn how to migrate your project to\na single-cluster configuration, see  Migrate to One Resource per Project (Required for Version 1.3.0) . Do not mix MongoDB deployments outside   with ones insider  \nin the same Project. Due to   network translation, a Monitoring Agent outside  \ncannot monitor MongoDB instances inside  . For this reason, k8s\nand non-k8s deployments in the same Project is not supported. Use\nseparate projects. The procedure for deploying a sharded cluster depends on whether you\nrequire the deployment to run with   enabled for intra-cluster\ncommunication and clients connecting to the database: Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. This is a   file that you can modify to meet your desired\nconfiguration. Change the highlighted settings to match your desired\n sharded cluster  configuration. Open your preferred text editor and paste the   specification\ninto a new text file. Key Type Description Example metadata.name string Label for this    sharded cluster   . metadata.name  documentation on  names . myproject metadata.namespace string Scope of object names.     where this\n  and other   are created. Using two different namespaces allows you to delete your\n sharded cluster  or all of the resources in the\nnamespace without affecting your  . metadata.namespace  documentation on  mongodb spec.shardCount integer Number of shards to deploy. 2 spec.mongodsPerShardCount integer Number of shard members per shard. 3 spec.mongosCount integer Number of shard routers to deploy. 2 spec.configServerCount integer Number of members of the config server replica set. 3 spec.version string Version of MongoDB that this  sharded cluster  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. 3.6.7 spec.opsManager.configMapRef.name string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value  must  match the value you provided for\n metadata.name  in your  \n project ConfigMap . The   tracks any changes to the ConfigMap and\nreconciles the state of the  . <myproject> spec.credentials string Name of the     you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . This value  must  match the value you provided for\n namespace  and  name  for your    \n Secret . The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. ShardedCluster spec.persistent string Optional. Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. If this value is  true , then the following values are set\nto their default value of  16G : To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: spec.shardPodSpec.persistence.single spec.configSrvPodSpec.persistence.single If you want one   for each  , configure the\n spec.shardPodSpec.persistence.single  and\n spec.configSrvPodSpec.persistence.single \ncollections. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: In the  spec.configSrvPodSpec.persistence.multiple \ncollection:\n-  .data \n-  .journal \n-  .logs In the  spec.configSrvPodSpec.persistence.multiple  collection:\n-  .data \n-  .journal \n-  .logs Your containers must have permissions to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the deployment item does not fix\nissues with your  , contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n  specification file for a  sharded cluster \ndeployment: For config server For shard routers For shard members spec.clusterName spec.exposedExternally spec.logLevel spec.featureCompatibilityVersion You must set  spec.clusterName  if your   cluster has a\n default domain \ndifferent from default  cluster.local . If you neither use the\ndefault nor set this option, the   may not function as\nexpected. spec.configSrvPodSpec.cpu spec.configSrvPodSpec.cpuRequests spec.configSrvPodSpec.memory spec.configSrvPodSpec.memoryRequests spec.configSrvPodSpec.persistence.single spec.configSrvPodSpec.persistence.multiple.data spec.configSrvPodSpec.persistence.multiple.journal spec.configSrvPodSpec.persistence.multiple.logs spec.configSrvPodSpec.nodeAffinity spec.configSrvPodSpec.podAffinity spec.configSrvPodSpec.podAntiAffinityTopologyKey spec.mongosPodSpec.cpu spec.mongosPodSpec.cpuRequests spec.mongosPodSpec.memory spec.mongosPodSpec.memoryRequests spec.mongosPodSpec.nodeAffinity spec.mongosPodSpec.podAffinity spec.mongosPodSpec.podAntiAffinityTopologyKey spec.shardPodSpec.cpu spec.shardPodSpec.cpuRequests spec.shardPodSpec.memory spec.shardPodSpec.memoryRequests spec.shardPodSpec.nodeAffinity spec.shardPodSpec.persistence.single spec.shardPodSpec.persistence.multiple.data spec.shardPodSpec.persistence.multiple.journal spec.shardPodSpec.persistence.multiple.logs spec.shardPodSpec.podAffinity spec.shardPodSpec.podAntiAffinityTopologyKey Invoke the following   command to create your\n sharded cluster : Check the log  after running this\ncommand. If the creation was successful, you should see a message\nsimilar to the following: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "2018-06-26T10:30:30.346Z INFO operator/shardedclusterkube.go:52 Created! {\"sharded cluster\": \"my-sharded-cluster\"}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml -w"
                }
            ],
            "preview": "Sharded clusters provide horizontal scaling\nfor large data sets and enable high throughput operations by\ndistributing the data set across a group of servers.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-standalone",
            "title": "Deploy a Standalone MongoDB Instance",
            "headings": [
                "Prerequisites",
                "Considerations",
                "Procedure",
                "Copy the following example standalone  .",
                "Open your preferred text editor and paste the  specification into a new text file.",
                "Configure the settings highlighted in the preceeding step as follows.",
                "Add any additional accepted settings for a Standalone deployment.",
                "Save this file with a .yaml file extension.",
                "Start your Standalone deployment.",
                "Track the status of your standalone deployment."
            ],
            "paragraphs": "You can deploy a  standalone  MongoDB instance for   to\nmanage. Use standalone instances for testing and development.\n Do not  use these deployments for production systems as they lack\nreplication and high availability. For all production deployments\nuse replica sets. To learn about replica sets, see\n Deploy a Replica Set . You can use   to deploy MongoDB instances with\n  version 4.0 or later and Cloud Manager. At any place in\nthis guide that says  Ops Manager , you can substitute\n Cloud Manager . To deploy a  standalone  using an  , you need to complete the\nfollowing procedures: Install Kubernetes Operator Create a Project using a ConfigMap Create Credentials for the  Starting in   version 1.3.0, you can only have one\nMongoDB resource per project. To learn how to migrate your project to\na single-cluster configuration, see  Migrate to One Resource per Project (Required for Version 1.3.0) . To troubleshoot your sharded cluster, see: This is a   file that you can modify to meet your desired\nconfiguration. Change the highlighted settings to match your desired\nstandalone configuration. Key Type Description Example metadata.name string Label for this   standalone  . metadata.name  documentation on  names . my-project metadata.namespace string Scope of object names.     where this\n  and other   are created. Using two different namespaces allows you to delete your\nstandalone or all of the resources in the namespace without\naffecting your  . metadata.namespace  documentation on  mongodb spec.version string Version of MongoDB that is installed on this\nstandalone. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. 3.6.7 spec.opsManager.configMapRef.name string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value  must  match the value you provided for\n metadata.name  in your  \n project ConfigMap . The   tracks any changes to the ConfigMap and\nreconciles the state of the  . <myproject> spec.credentials string Name of the     you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . This value  must  match the value you provided for\n namespace  and  name  for your    \n Secret . The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. Standalone spec.persistent string Optional. If this value is  true , then\n spec.podSpec.persistence.single  is set to its\ndefault value of  16G . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: If you want one   for each  , configure the\n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs Your containers must have permissions to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the deployment item does not fix\nissues with your  , contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n  specification file for a Standalone deployment: spec.exposedExternally spec.logLevel spec.featureCompatibilityVersion spec.podSpec.cpu spec.podSpec.cpuRequests spec.podSpec.memory spec.podSpec.memoryRequests spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs spec.podSpec.podAffinity spec.podSpec.nodeAffinity Invoke the following   command to create your standalone: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. Find a Specific Pod Review Logs from Specific Pod",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\n  namespace: <metadata.namespace>       # Should match\n                                        # metadata.namespace in\n                                        # your configmap file.\nspec:\n  version: 4.2.1\n  opsManager:                           # Alias of cloudManager\n    configMapRef:\n      name: <configMap.metadata.name>   # Should match metadata.name\n                                        # in your configmap file.\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <standalone-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml -w"
                }
            ],
            "preview": "You can deploy a standalone MongoDB instance for  to\nmanage. Use standalone instances for testing and development.\nDo not use these deployments for production systems as they lack\nreplication and high availability. For all production deployments\nuse replica sets. To learn about replica sets, see\nDeploy a Replica Set.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/upgrade-k8s-operator-v9-and-earlier",
            "title": "Upgrade from Operator Version 0.9 and Earlier",
            "headings": [
                "Prerequisites",
                "Upgrade the ",
                "Recreate MongoDB Resources and Delete the Version 0.9 CRDs",
                "Troubleshooting"
            ],
            "paragraphs": "Version 0.10 of the   consolidated the\n MongoDbStandalone ,  MongoDbShardedCluster , and\n MongoDbReplicaSet    into a\nsingle  CustomResourceDefinition  called  MongoDB . Version 0.10 of the   included breaking changes and\nrequires some additional preparation before upgrading. The\nfollowing procedure outlines the upgrade process for  \nversions 0.9 and earlier. If you are already running version 0.10\nor later, see  Upgrade the Operator  for upgrade instructions. The following upgrade procedure allows you to keep data stored in\npersistent volumes from previous deployments that the  \nmanaged. If you do not wish to retain data from previous\ndeployments and plan on deploying new resources, skip to the\n Upgrade  section. Verify you have the  .yaml  configuration file for each MongoDB\nresource you have deployed. If you have standalone resources but do not have the  .yaml \nconfiguration file for them, run the following command to generate\nthe configuration file: If you have replica set resources but do not have the  .yaml \nconfiguration file for them, run the following command to generate\nthe configuration file: If you have sharded cluster resources but do not have the  .yaml \nconfiguration file for them, run the following command to generate\nthe configuration file: Edit each  .yaml  configuration file match the new  : After you edit each  .yaml  file, it should look like the following\nexample: Change the  kind  to  MongoDB Add the  spec.type  field and set it to  Standalone ,\n ReplicaSet , or  ShardedCluster  depending on your resource. The   does not support changing the type of an existing\nconfiguration even though it will accept a valid configuration for a\ndifferent type. For example, if your MongoDB resource is a\nstandalone, you cannot set the value of  spec.type  to\n ReplicaSet  and set  spec.members . If you do, the\n  throws an error and requires you to revert to the\npreviously working configuration. If you change the  metadata.name  field you will lose your\nresource's data. To upgrade to the latest version of the   from version v0.9\nor earlier: Change to the directory in which you cloned the  \nrepository. The following steps depend on how your environment is\nconfigured: To troubleshoot your  , see\n Review Logs from the  . Upgrade the   for MongoDB deployments using the\nfollowing   command: If you use  OpenShift  as\nyour   orchestrator, you need to allow OpenShift to\nmanage the Security Context for the  . Change the  MANAGED_SECURITY_CONTEXT  value as described\nin the next step. You can edit the Operator   file to further customize\nyour Operator before upgrading it. Open your  mongodb-enterprise.yaml  in your preferred\ntext editor. You may need to add one or more of the following\noptions: Environment Variable When to Use OPERATOR_ENV Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . You can set the following pair of values: If  OPERATOR_ENV  is Log Level is set to Log Format is set to dev debug text prod info json WATCH_NAMESPACE Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . You can set the following pair of values: If you need to watch more than one namespace, set the value\nof  WATCH_NAMESPACE  to  *  (all). This environment\nvariable can watch one namespace or all namespaces. MANAGED_SECURITY_CONTEXT If you use  OpenShift  as your\n  orchestrator, set this to  'true'  to allow OpenShift to\nmanage the Security Context for the  . Accepted values are:  'true' ,  'false' . Default value is:  'false' . You can set the following pair of values: OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the image for an  Ops\nManager resource  is downloaded. Default value is:\n quay.io/mongodb/mongodb-enterprise-ops-manager OPS_MANAGER_IMAGE_PULL_POLICY Pull policy  for the\nimage deployed to an  Ops Manager resource . Accepted values are:  Always ,  IfNotPresent ,  Never Default value is:  Always Any values enclosed in single or double quotes\n require  those quotes. Include the quotes when\nsetting these values. Upgrade the   using the following\n  command: If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . To troubleshoot your  , see\n Review Logs from the  . Upgrade the latest version of the   using the\nfollowing  helm  command: You can customize your Chart before installing it by using\nthe  --set  option. For this Chart, you may need to add\none or more of the following options: --set  option When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . managedSecurityContext If you use  OpenShift  as your\n  orchestrator, set this to  true  to allow OpenShift to\nmanage the Security Context for the  . Accepted values are:  true ,  false . Default value is:  false . operator.podWaitSeconds Time in seconds that the Operator waits for  \nto start when   are being created or updated before\nretrying. Default values depend upon  operator.env : If  operator.env  is operator.podWaitSeconds  is set to dev 3 prod 5 operator.podSetWaitRetries Maximum number of retries that the Operator attempts when\nwaiting for   to start after   are\ncreated or updated. Default values depend upon  operator.env : If  operator.env  is operator.podSetWaitRetries  is set to dev 60 prod 180 If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . To upgrade the   on a host not connected to the\nInternet, you have two options, you can download the\n  files from either: To troubleshoot your  , see\n Review Logs from the  . Upgrade the latest version of the  \nwith modified pull policy values using the\nfollowing  helm  command: You can further customize your Chart before\ninstalling it by using the  --set  option. For\nthis Chart, you may need to add one or more of the\nfollowing options: --set  option When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . managedSecurityContext If you use  OpenShift  as your\n  orchestrator, set this to  true  to allow OpenShift to\nmanage the Security Context for the  . Accepted values are:  true ,  false . Default value is:  false . operator.podWaitSeconds Time in seconds that the Operator waits for  \nto start when   are being created or updated before\nretrying. Default values depend upon  operator.env : If  operator.env  is operator.podWaitSeconds  is set to dev 3 prod 5 operator.podSetWaitRetries Maximum number of retries that the Operator attempts when\nwaiting for   to start after   are\ncreated or updated. Default values depend upon  operator.env : If  operator.env  is operator.podSetWaitRetries  is set to dev 60 prod 180 If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . Upgrade the latest version of the  \nwith modified pull policy values using the\nfollowing  helm  command: You can further customize your Chart before\ninstalling it by using the  --set  option. For\nthis Chart, you may need to add one or more of the\nfollowing options: --set  option When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . managedSecurityContext If you use  OpenShift  as your\n  orchestrator, set this to  true  to allow OpenShift to\nmanage the Security Context for the  . Accepted values are:  true ,  false . Default value is:  false . operator.podWaitSeconds Time in seconds that the Operator waits for  \nto start when   are being created or updated before\nretrying. Default values depend upon  operator.env : If  operator.env  is operator.podWaitSeconds  is set to dev 3 prod 5 operator.podSetWaitRetries Maximum number of retries that the Operator attempts when\nwaiting for   to start after   are\ncreated or updated. Default values depend upon  operator.env : If  operator.env  is operator.podSetWaitRetries  is set to dev 60 prod 180 Once the version 0.9   are deleted, the   upgrade\nis complete. After you upgrade the  , verify you have four CRDs by\nrunning the following command: The following output contains the new  mongodb.mongodb.com  CRD and\nthe version 0.9 CRDs: Remove the old resources from Kubernetes. Run each of the following commands to remove all MongoDB resources: Removing MongoDB resources will remove the database server pods\nand drop any client connections to the database. Connections are\nreestablished when the new MongoDB resources are created in\nKubernetes. MongoDB resources that have  persistent: true  set in their\n .yaml  configuration file will not lose data as it is stored in\npersistent volumes. The previous command only deletes pods\ncontaining MongoDB and not the persistent volumes containing the\ndata. Persistent volume claims referencing persistent volumes stay\nalive and are reused by the new MongoDB resources. Create the MongoDB resources again. Use the  .yaml  resource configuration file to recreate each\nresource: Run the following command to check the status of each resource and\nverify that the  phase  reaches the  Running  status: For an example of this command's output, see\n Get Status of a Deployed Resource . If the old resources had  persistent: true  set and the\n metadata.name  haven't changed, the new MongoDB pods will\nreuse the data from the old pods. Delete the old CRDs. Once all the resources are up and running, delete all of the v0.9\nCRDs as the   no longer watches them: Run the following command to verify the old CRDs were removed: The output of the command above should look similar to the following: To troubleshoot your  , see  Review Logs from the  . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl mst <standalone-name> -n <namespace> -o yaml > <standalone-conf-name>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mrs <replicaset-name> -n <namespace> -o yaml > <replicaset-conf-name>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get msc <shardedcluster-name> -n <namespace> -o yaml > <shardedcluster-conf-name>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\n  namespace: <metadata.namespace>       # Should match\n                                        # metadata.namespace in\n                                        # your configmap file.\nspec:\n  version: 4.2.1\n  opsManager:                           # Alias of cloudManager\n    configMapRef:\n      name: <configMap.metadata.name>   # Should match metadata.name\n                                        # in your configmap file.\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: OPERATOR_ENV\nspec.template.spec.containers.name.env.value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPERATOR_ENV\n          value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: WATCH_NAMESPACE\nspec.template.spec.containers.name.env.value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: WATCH_NAMESPACE\n          value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: MANAGED_SECURITY_CONTEXT\nspec.template.spec.containers.name.env.value: 'true'"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MANAGED_SECURITY_CONTEXT\n          value: 'true'"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f mongodb-enterprise.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set namespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set operator.env=dev \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set operator.watchNamespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set managedSecurityContext=false \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  --set operator.podSetWaitRetries=20 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set namespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.watchNamespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set managedSecurityContext=false \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  --set operator.podSetWaitRetries=20 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set namespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.watchNamespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set managedSecurityContext=false \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  --set operator.podSetWaitRetries=20 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get crds"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                 CREATED AT\nmongodb.mongodb.com                  2019-03-27T19:30:09Z\nmongodbreplicasets.mongodb.com       2018-12-07T18:25:42Z\nmongodbshardedclusters.mongodb.com   2018-12-07T18:25:42Z\nmongodbstandalones.mongodb.com       2018-12-07T18:25:42Z"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mst --all"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mrs --all"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete msc --all"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <resource-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete crd mongodbreplicasets.mongodb.com"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete crd mongodbshardedclusters.mongodb.com"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete crd mongodbstandalones.mongodb.com"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get crds"
                },
                {
                    "lang": "sh",
                    "value": "NAME                  CREATED AT\nmongodb.mongodb.com   2019-03-27T19:30:09Z"
                }
            ],
            "preview": "Version 0.10 of the  consolidated the\nMongoDbStandalone, MongoDbShardedCluster, and\nMongoDbReplicaSet  into a\nsingle CustomResourceDefinition called MongoDB.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-om-container",
            "title": "Deploy an  Instance",
            "headings": [
                "Prerequisites",
                "Considerations",
                "Encryption Key",
                "Application Database Replica Set",
                "Procedure",
                "Copy the following example   .",
                "Open your preferred text editor and paste the  specification into a new text file.",
                "Configure the settings highlighted in the prior example.",
                "(Optional) Configure any additional settings for an  deployment.",
                "Save this file with a .yaml file extension.",
                "Create your  instance.",
                "Track the status of your  instance.",
                "Access your  instance from a browser."
            ],
            "paragraphs": "You can deploy   in a container with the  . Don't use the   resource in\nproduction environments. To deploy an   resource you must: Install  the   1.3.0\nor newer. Ensure that the host on which you want to deploy   has a\nminimum of five gigabytes of memory. Create a     for an admin user in the same   as\nthe   resource. When you deploy the   resource,   creates a user with\nthese credentials and grants it the  Global Owner \nrole. Use these credentials to log in to  \nfor the first time. Once   is deployed, you should change the\npassword or remove this secret. The   generates an encryption key to protect\nsensitive information in the  mms-application-database . The\n  saves this key in a   in the same namespace as\nthe   resource. The   names the secret\n <om-resource-name>-gen-key . If you remove the   resource, the key remains stored in the\nsecret on   cluster. If you stored the Application Database in a\n  and you create another   resource with the same name,\nthe   reuses the secret. If you create an   resource\nwith a different name, then   creates a new secret and\nApplication Database, and the old secret isn't reused. When you create an instance of   through the  , the\n mms-application-database  is deployed as a  replica set .\nYou can't configure the Application Database as a  standalone \ndatabase or  sharded cluster . If you\nhave concerns about performance or size requirements for the Application\nDatabase, contact  MongoDB Support . Change the highlighted settings to match your desired\n  configuration. Key Type Description Example metadata.name string Name for this      . metadata.name  documentation on  names . om spec.replicas number Number of   instances to run in parallel. The minimum valid value is  1 . For high availability, set this value to more than  1 . Multiple\n  instances can read from the same Application\nDatabase, ensuring failover if one instance is unavailable and\nenabling you to update the   resource without downtime. 1 spec.version string Version of   to be installed. The format should be  X.Y.Z .\nTo view available   versions, view the\n container registry . 4.2.0 spec.adminCredentials string Name of the   you  created \nfor the   admin user. Configure the secret to use the same   as the  \nresource. om-admin-secret integer Number of members of the  mms-application-database \nreplica set. 3 string Version of MongoDB that the  mms-application-database \nshould run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the  Enterprise edition . To learn more about MongoDB versioning, see see\n release-version-numbers  in the MongoDB Manual. 4.0.7 boolean Optional. Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. If this value is  true , then\n spec.applicationDatabase.podSpec.persistence. \n spec.podSpec.persistence.single \nis set to its default value of  16G . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: If you want one   for each  , configure the\n spec.applicationDatabase. \n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: Grant your containers permission to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the resource does not fix\nissues with your  , contact MongoDB support. true You can add any of the following optional settings to the\n  specification file for an   deployment: spec.clusterName spec.configuration spec.applicationDatabase. spec.logLevel spec.applicationDatabase. spec.featureCompatibilityVersion spec.applicationDatabase.podSpec. spec.podSpec.cpu spec.applicationDatabase.podSpec. spec.podSpec.cpuRequests spec.applicationDatabase.podSpec. spec.podSpec.memory spec.applicationDatabase.podSpec. spec.podSpec.memoryRequests spec.applicationDatabase.podSpec.persistence. spec.podSpec.persistence.single spec.applicationDatabase.podSpec.persistence.multiple. spec.podSpec.persistence.multiple.data spec.applicationDatabase.podSpec.persistence.multiple. spec.podSpec.persistence.multiple.journal spec.applicationDatabase.podSpec.persistence.multiple. spec.podSpec.persistence.multiple.logs spec.applicationDatabase.podSpec. spec.podSpec.podAffinity spec.applicationDatabase.podSpec. spec.podSpec.podAntiAffinityTopologyKey spec.applicationDatabase.podSpec. spec.podSpec.nodeAffinity Invoke the following  kubectl  command on the filename of the\n  resource definition: To check the status of your   resource, invoke the following\ncommand: The command returns the following output under the  status  field\nwhile the resource deploys: After the resource completes the  Reconciling  phase, the command\nreturns the following output under the  status  field: The  status.opsManager.url  is the connection URL of the resource,\nwhich can be used to reach   from inside the  \ncluster. If the deployment fails, see  Troubleshooting the  . After the resource deploys successfully, find the external port to\nyour   instance. Invoke the following  kubectl  command on  <metadata.name>-svc-external :\n metadata.name  : The command returns the external port in the  PORT(S)  column. In\nthe following example output, the external port is  30036 : Set your firewall rules to allow access from the Internet to the\nexternal port on the host. Open a browser window and navigate to the   application\nusing the   and port number. Log in to   using the  admin user credentials .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <adminusercredentials>\n  --from-literal=Username=\"<username>\"\n  --from-literal=Password=\"<password>\"\n  --from-literal=FirstName=\"<firstname>\"\n  --from-literal=LastName=\"<lastname>\"\n  -n <namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: <myopsmanager>\nspec:\n  replicas: 1\n  version: <opsmanagerversion>\n  adminCredentials: <adminusercredentials> # Should match metadata.name\n                                           # in the Kubernetes secret\n                                           # for the admin user\n  applicationDatabase:\n    members: 3\n    version: <mongodbversion>\n    persistent: true\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -n <namespace> -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n applicationDatabase:\n  lastTransition: \"2019-11-15T19:48:01Z\"\n  message: AppDB Statefulset is not ready yet\n  phase: Reconciling\n  type: \"\"\n  version: \"\"\n opsManager:\n  lastTransition: \"2019-11-15T19:48:01Z\"\n  message: Ops Manager is still waiting to start\n  phase: Reconciling\n  version: \"\""
                },
                {
                    "lang": "yaml",
                    "value": "status:\n applicationDatabase:\n  lastTransition: \"2019-11-05T17:26:42Z\"\n  phase: Running\n  type: \"\"\n  version: 4.0.7\n opsManager:\n  lastTransition: \"2019-11-05T17:26:34Z\"\n  phase: Running\n  replicas: 1\n  url: http://om-test-svc.dev.svc.cluster.local:8080\n  version: 4.2.0"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get svc <metadata.name>-svc-external -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "NAME                            TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE\n<metadata.name>-svc-external    NodePort   100.66.92.110    <none>        8080:30036/TCP    1d"
                },
                {
                    "lang": "sh",
                    "value": "http://ops.example.com:30036"
                }
            ],
            "preview": "You can deploy  in a container with the .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/upgrade-k8s-operator",
            "title": "Upgrade the Operator",
            "headings": [
                "Procedure"
            ],
            "paragraphs": "Starting in   version 1.3.0, you can only have one\nMongoDB resource per project. To learn how to migrate your project to\na single-cluster configuration, see  Migrate to One Resource per Project (Required for Version 1.3.0) . Change to the directory in which you cloned the  \nrepository. The following steps depend on how your environment is\nconfigured: To troubleshoot your  , see\n Review Logs from the  . Upgrade the   for MongoDB deployments using the\nfollowing   command: If you use  OpenShift  as\nyour   orchestrator, you need to allow OpenShift to\nmanage the Security Context for the  . Change the  MANAGED_SECURITY_CONTEXT  value as described\nin the next step. You can edit the Operator   file to further customize\nyour Operator before upgrading it. Open your  mongodb-enterprise.yaml  in your preferred\ntext editor. You may need to add one or more of the following\noptions: Environment Variable When to Use OPERATOR_ENV Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . You can set the following pair of values: If  OPERATOR_ENV  is Log Level is set to Log Format is set to dev debug text prod info json WATCH_NAMESPACE Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . You can set the following pair of values: If you need to watch more than one namespace, set the value\nof  WATCH_NAMESPACE  to  *  (all). This environment\nvariable can watch one namespace or all namespaces. MANAGED_SECURITY_CONTEXT If you use  OpenShift  as your\n  orchestrator, set this to  'true'  to allow OpenShift to\nmanage the Security Context for the  . Accepted values are:  'true' ,  'false' . Default value is:  'false' . You can set the following pair of values: OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the image for an  Ops\nManager resource  is downloaded. Default value is:\n quay.io/mongodb/mongodb-enterprise-ops-manager OPS_MANAGER_IMAGE_PULL_POLICY Pull policy  for the\nimage deployed to an  Ops Manager resource . Accepted values are:  Always ,  IfNotPresent ,  Never Default value is:  Always Any values enclosed in single or double quotes\n require  those quotes. Include the quotes when\nsetting these values. Upgrade the   using the following\n  command: If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . To troubleshoot your  , see\n Review Logs from the  . Upgrade the latest version of the   using the\nfollowing  helm  command: You can customize your Chart before installing it by using\nthe  --set  option. For this Chart, you may need to add\none or more of the following options: --set  option When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . managedSecurityContext If you use  OpenShift  as your\n  orchestrator, set this to  true  to allow OpenShift to\nmanage the Security Context for the  . Accepted values are:  true ,  false . Default value is:  false . operator.podWaitSeconds Time in seconds that the Operator waits for  \nto start when   are being created or updated before\nretrying. Default values depend upon  operator.env : If  operator.env  is operator.podWaitSeconds  is set to dev 3 prod 5 operator.podSetWaitRetries Maximum number of retries that the Operator attempts when\nwaiting for   to start after   are\ncreated or updated. Default values depend upon  operator.env : If  operator.env  is operator.podSetWaitRetries  is set to dev 60 prod 180 If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . To upgrade the   on a host not connected to the\nInternet, you have two options, you can download the\n  files from either: To troubleshoot your  , see\n Review Logs from the  . Upgrade the latest version of the  \nwith modified pull policy values using the\nfollowing  helm  command: You can further customize your Chart before\ninstalling it by using the  --set  option. For\nthis Chart, you may need to add one or more of the\nfollowing options: --set  option When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . managedSecurityContext If you use  OpenShift  as your\n  orchestrator, set this to  true  to allow OpenShift to\nmanage the Security Context for the  . Accepted values are:  true ,  false . Default value is:  false . operator.podWaitSeconds Time in seconds that the Operator waits for  \nto start when   are being created or updated before\nretrying. Default values depend upon  operator.env : If  operator.env  is operator.podWaitSeconds  is set to dev 3 prod 5 operator.podSetWaitRetries Maximum number of retries that the Operator attempts when\nwaiting for   to start after   are\ncreated or updated. Default values depend upon  operator.env : If  operator.env  is operator.podSetWaitRetries  is set to dev 60 prod 180 If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . Upgrade the latest version of the  \nwith modified pull policy values using the\nfollowing  helm  command: You can further customize your Chart before\ninstalling it by using the  --set  option. For\nthis Chart, you may need to add one or more of the\nfollowing options: --set  option When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . managedSecurityContext If you use  OpenShift  as your\n  orchestrator, set this to  true  to allow OpenShift to\nmanage the Security Context for the  . Accepted values are:  true ,  false . Default value is:  false . operator.podWaitSeconds Time in seconds that the Operator waits for  \nto start when   are being created or updated before\nretrying. Default values depend upon  operator.env : If  operator.env  is operator.podWaitSeconds  is set to dev 3 prod 5 operator.podSetWaitRetries Maximum number of retries that the Operator attempts when\nwaiting for   to start after   are\ncreated or updated. Default values depend upon  operator.env : If  operator.env  is operator.podSetWaitRetries  is set to dev 60 prod 180",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: OPERATOR_ENV\nspec.template.spec.containers.name.env.value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPERATOR_ENV\n          value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: WATCH_NAMESPACE\nspec.template.spec.containers.name.env.value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: WATCH_NAMESPACE\n          value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: MANAGED_SECURITY_CONTEXT\nspec.template.spec.containers.name.env.value: 'true'"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MANAGED_SECURITY_CONTEXT\n          value: 'true'"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f mongodb-enterprise.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set namespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set operator.env=dev \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set operator.watchNamespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set managedSecurityContext=false \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  --set operator.podSetWaitRetries=20 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set namespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.watchNamespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set managedSecurityContext=false \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  --set operator.podSetWaitRetries=20 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set namespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.watchNamespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set managedSecurityContext=false \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  --set operator.podSetWaitRetries=20 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                }
            ],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/connect-from-inside-k8s",
            "title": "Connect to a MongoDB Resource from Inside Kubernetes",
            "headings": [
                "Considerations",
                "Procedure",
                "Open the Topology view for your deployment.",
                "Click  for the deployment to which you want to connect.",
                "Click Connect to this instance.",
                "Copy the connection command displayed in the Connect to your Deployment dialog.",
                "Run the connection command in a terminal to connect to the deployment."
            ],
            "paragraphs": "The following procedure describes how to connect to a MongoDB resource\ndeployed by   from inside of the   cluster. You must be able to connect to the host and port where you deployed your\n  resource. To learn more about connecting to your deployment, see\n Connect to a MongoDB Process . Perform the following steps in the   or\n Cloud Manager \napplication, depending on where your clusters are hosted: When connecting to a resource from inside of  , the\nhostname to which you connect has the following form: Click  Deployment  in the left navigation. To connect to a sharded cluster resource named\n shardedcluster , you might use the following connection\nstring:",
            "code": [
                {
                    "lang": "sh",
                    "value": "<k8s-pod-name>.<k8s-internal-service-name>.<k8s-namespace>.<cluster-name>"
                },
                {
                    "lang": "none",
                    "value": "mongo --host shardedcluster-mongos-0.shardedcluster-svc.mongodb.svc.cluster.local --port 27017"
                }
            ],
            "preview": "The following procedure describes how to connect to a MongoDB resource\ndeployed by  from inside of the  cluster.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-x509-auth",
            "title": "Secure Client Authentication with X.509",
            "headings": [
                "Prerequisites",
                "Configure X.509 Client Authentication for a Replica Set",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the TLS settings for your replica set resource.",
                "Configure the general X.509 settings for your replica set resource.",
                "Save your replica set config file.",
                "Update and restart your replica set deployment.",
                "Check the status of your deployment.",
                "Retrieve the CSRs for each host and agent in your deployment.",
                "Approve the CSR for each host in your deployment.",
                "Approve the CSR for each agent in your deployment.",
                "Track the status of your deployment.",
                "Configure X.509 Client Authentication for a Sharded Cluster",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the TLS settings for your sharded cluster resource.",
                "Configure the general X.509 settings for your sharded cluster resource.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Check the status of your deployment.",
                "Retrieve the CSRs for each host and agent in your deployment.",
                "Approve the CSR for each host in your deployment.",
                "Approve the CSR for each agent in your deployment.",
                "Track the status of your deployment."
            ],
            "paragraphs": "The   can use X.509 certificates to authenticate your\nclient applications to your MongoDB deployments. This guide instructs you on how to configure: X.509 authentication from clients to your MongoDB instances.  to encrypt connections between MongoDB hosts in a replica set\nor sharded cluster.  to encrypt connections client applications and MongoDB\ndeployments. Before you secure your MongoDB deployment using X.509 client\nauthentication, complete the following: Install the Kubernetes Operator Create Credentials for the Kubernetes Operator Deploy the Database Resource  that you want to secure: Replica Set Sharded Cluster Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true , authentication is enabled on the\nMongoDB deployment. true array Conditional If you enabled authentication, you must set an authentication\nmechanism. Accepted values are  X509 . X509 Invoke the following   command to update and restart your\n replica set : The   creates the MongoDB resources and requests the\n    to approve the database host's certificates. Run the\nfollowing command to verify that the certificates are pending\napproval: The  status  field of the output should resemble the following: If you do not see the  status.message  above, see\n Troubleshooting the   to help diagnose the issue. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the replica set\nexample: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the replica set\nexample: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true , authentication is enabled on the\nMongoDB deployment. true array Conditional If you enabled authentication, you must set an authentication\nmechanism. Accepted values are  X509 . X509 Invoke the following   command to update and restart your\n sharded cluster : The   creates the MongoDB resources and requests the\n    to approve the database host's certificates. Run the\nfollowing command to verify that the certificates are pending\napproval: The  status  field of the output should resemble the following: If you do not see the  status.message  above, see\n Troubleshooting the   to help diagnose the issue. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the sharded cluster\nexample: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the sharded cluster\nexample: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "status:\n  lastTransition: 2019-05-01T15:36:59Z\n  message: Not all certificates have been approved by Kubernetes CA\n  phase: Failed\n  type: \"\"\n  version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmms-automation-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-backup-agent.mongodb                    15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-monitoring-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-rs-0.mongodb                      33s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-1.mongodb                      31s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-2.mongodb                      24s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-rs-0.mongodb\nkubectl certificate approve my-secure-rs-1.mongodb\nkubectl certificate approve my-secure-rs-2.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve mms-automation-agent.mongodb\nkubectl certificate approve mms-backup-agent.mongodb\nkubectl certificate approve mms-monitoring-agent.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "status:\n  lastTransition: 2019-05-01T15:36:59Z\n  message: Not all certificates have been approved by Kubernetes CA\n  phase: Failed\n  type: \"\"\n  version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmy-secure-sc-0-0.mongodb                    30s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-1.mongodb                    28s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-2.mongodb                    27s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-0.mongodb                    22s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-1.mongodb                    13s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-2.mongodb                    6s        system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-0.mongodb               36s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-1.mongodb               34s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-2.mongodb               32s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-0.mongodb               49s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-1.mongodb               42s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-sc-0-0.mongodb\nkubectl certificate approve my-secure-sc-0-1.mongodb\nkubectl certificate approve my-secure-sc-0-2.mongodb\nkubectl certificate approve my-secure-sc-1-0.mongodb\nkubectl certificate approve my-secure-sc-1-1.mongodb\nkubectl certificate approve my-secure-sc-1-2.mongodb\nkubectl certificate approve my-secure-sc-config-0.mongodb\nkubectl certificate approve my-secure-sc-config-1.mongodb\nkubectl certificate approve my-secure-sc-config-2.mongodb\nkubectl certificate approve my-secure-sc-mongos-0.mongodb\nkubectl certificate approve my-secure-sc-mongos-1.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve mms-automation-agent.mongodb\nkubectl certificate approve mms-backup-agent.mongodb\nkubectl certificate approve mms-monitoring-agent.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml -w"
                }
            ],
            "preview": "The  can use X.509 certificates to authenticate your\nclient applications to your MongoDB deployments.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/install-k8s-operator",
            "title": "Install the ",
            "headings": [
                "Prerequisites",
                "Considerations",
                " Compatibility",
                "Docker Container Details",
                "Only One Operator per Namespace",
                "Install the ",
                "Verify the Installation",
                "Next Steps"
            ],
            "paragraphs": " allows you to deploy MongoDB deployment items with  \nand  . This Operator uses   and     methods to\ndeploy standalone, replica set, and sharded cluster deployments that\n  manages. You can use   to deploy MongoDB instances with\n  version 4.0 or later and Cloud Manager. At any place in\nthis guide that says  Ops Manager , you can substitute\n Cloud Manager . This tutorial presumes some knowledge of  , but does link to\nrelevant   documentation where possible. If you are unfamiliar\nwith  , please review that documentation first. To install the MongoDB  , you must: Have a   solution available to use. If you need a   solution, see the  \n documentation on picking the right solution . Clone the  MongoDB Enterprise Kubernetes Operator repository . You can use  Helm  to install the\n . To learn how to install Helm, see its\n documentation on GitHub Create a   for your   deployment. By default, The\n  uses the  mongodb  namespace. To simplify your\ninstallation, consider creating a namespace labeled  mongodb \nusing the following   command: If you do not want to use the  mongodb  namespace, you can label\nyour namespace anything you like:  is compatible with   v1.13 or later. MongoDB builds the container images from the latest builds of the\nfollowing operating systems: MongoDB, Inc. updates all packages on these images before releasing\nthem every three weeks. If you get your   from... ...the Container uses quay.io \nor  Ubuntu 16.04 OpenShift Red Hat Enterprise Linux 7 The   can only exist in one  . Your deployment can\nhave: Do not try to deploy more than one Kubernetes Operator in the same\nnamespace as another Operator. Multiple Operators cannot coordinate\nwith one another within the same namespace. One cluster-wide   or Multiple Kubernetes Operators in their own namespaces To troubleshoot your  , see\n Review Logs from the  . The following examples assume that you created a  \nusing the default   namespace of  mongodb .\nIf you specified a different label for your namespace when\nyou  created it , change all\nvalues for  metadata.namespace  to that namespace. To change the label for the namespace for the following\ndeployment to  production , edit all values for\n metadata.namespace  in  mongodb-enterprise.yaml : Change to the directory in which you cloned the repository. Install the   for MongoDB deployments using the\nfollowing   command: If you use  OpenShift  as\nyour   orchestrator, you need to allow OpenShift to\nmanage the Security Context for the  . Change the  MANAGED_SECURITY_CONTEXT  value as described\nin the next step. You can edit the Operator   file to further customize\nyour Operator before installing it. Open your  mongodb-enterprise.yaml  in your preferred\ntext editor. You may need to add one or more of the following\noptions: Environment Variable When to Use OPERATOR_ENV Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . You can set the following pair of values: If  OPERATOR_ENV  is Log Level is set to Log Format is set to dev debug text prod info json WATCH_NAMESPACE Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . You can set the following pair of values: If you need to watch more than one namespace, set the value\nof  WATCH_NAMESPACE  to  *  (all). This environment\nvariable can watch one namespace or all namespaces. MANAGED_SECURITY_CONTEXT If you use  OpenShift  as your\n  orchestrator, set this to  'true'  to allow OpenShift to\nmanage the Security Context for the  . Accepted values are:  'true' ,  'false' . Default value is:  'false' . You can set the following pair of values: OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the image for an  Ops\nManager resource  is downloaded. Default value is:\n quay.io/mongodb/mongodb-enterprise-ops-manager OPS_MANAGER_IMAGE_PULL_POLICY Pull policy  for the\nimage deployed to an  Ops Manager resource . Accepted values are:  Always ,  IfNotPresent ,  Never Default value is:  Always Install the   using the following\n  command: If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . If you have not already installed Helm, follow the\ninstructions on  GitHub \nto install it. To troubleshoot your  , see\n Review Logs from the  . Change to the directory in which you cloned the repository. Install the   using the following\n helm  command: You can customize your Chart before installing it by using\nthe  --set  option. For this Chart, you may need to add\none or more of the following options: --set  option When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . managedSecurityContext If you use  OpenShift  as your\n  orchestrator, set this to  true  to allow OpenShift to\nmanage the Security Context for the  . Accepted values are:  true ,  false . Default value is:  false . operator.podWaitSeconds Time in seconds that the Operator waits for  \nto start when   are being created or updated before\nretrying. Default values depend upon  operator.env : If  operator.env  is operator.podWaitSeconds  is set to dev 3 prod 5 operator.podSetWaitRetries Maximum number of retries that the Operator attempts when\nwaiting for   to start after   are\ncreated or updated. Default values depend upon  operator.env : If  operator.env  is operator.podSetWaitRetries  is set to dev 60 prod 180 If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . Before continuing, install Helm following the instructions on\n GitHub To install the   on a host not connected to the\nInternet, you have two options, you can download the\n  files from either: To troubleshoot your  , see\n Review Logs from the  . Connect to the Internet. Use  docker  to request the files. Disconnect from the Internet. Install the   with modified pull\npolicy values using the following  helm \ncommand: You can further customize your Chart before\ninstalling it by using the  --set  option. For\nthis Chart, you may need to add one or more of the\nfollowing options: --set  option When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . managedSecurityContext If you use  OpenShift  as your\n  orchestrator, set this to  true  to allow OpenShift to\nmanage the Security Context for the  . Accepted values are:  true ,  false . Default value is:  false . operator.podWaitSeconds Time in seconds that the Operator waits for  \nto start when   are being created or updated before\nretrying. Default values depend upon  operator.env : If  operator.env  is operator.podWaitSeconds  is set to dev 3 prod 5 operator.podSetWaitRetries Maximum number of retries that the Operator attempts when\nwaiting for   to start after   are\ncreated or updated. Default values depend upon  operator.env : If  operator.env  is operator.podSetWaitRetries  is set to dev 60 prod 180 If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . To troubleshoot your  , see\n Review Logs from the  . Use  docker  to request the files on a host\nconnected to the Internet. Save the Operator files to transferrable files. Copy these  .tar  files to the host running the\n   docker  daemon. Import the  .tar  files into  docker . Install the   with modified pull\npolicy values using the following  helm \ncommand: You can further customize your Chart before\ninstalling it by using the  --set  option. For\nthis Chart, you may need to add one or more of the\nfollowing options: --set  option When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . managedSecurityContext If you use  OpenShift  as your\n  orchestrator, set this to  true  to allow OpenShift to\nmanage the Security Context for the  . Accepted values are:  true ,  false . Default value is:  false . operator.podWaitSeconds Time in seconds that the Operator waits for  \nto start when   are being created or updated before\nretrying. Default values depend upon  operator.env : If  operator.env  is operator.podWaitSeconds  is set to dev 3 prod 5 operator.podSetWaitRetries Maximum number of retries that the Operator attempts when\nwaiting for   to start after   are\ncreated or updated. Default values depend upon  operator.env : If  operator.env  is operator.podSetWaitRetries  is set to dev 60 prod 180 If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . To verify that the   installed correctly, run the\nfollowing command and verify the output: By default, deployments exist in the  mongodb  namespace. If the\nfollowing error message appears, ensure you use the correct\nnamespace: After installing the  , you can: Create an instance of Ops Manager Configure the Kubernetes Operator to deploy MongoDB resources",
            "code": [
                {
                    "lang": "sh",
                    "value": "git clone https://github.com/mongodb/mongodb-enterprise-kubernetes.git"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace <namespaceName>"
                },
                {
                    "lang": "yaml",
                    "value": "##---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-operator\n  namespace: production\n##---\n# Source: mongodb-enterprise-operator/templates/operator.yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-enterprise-operator\n  namespace: production\n\n---\n# Example truncated\n---\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: OPERATOR_ENV\nspec.template.spec.containers.name.env.value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPERATOR_ENV\n          value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: WATCH_NAMESPACE\nspec.template.spec.containers.name.env.value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: WATCH_NAMESPACE\n          value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: MANAGED_SECURITY_CONTEXT\nspec.template.spec.containers.name.env.value: 'true'"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MANAGED_SECURITY_CONTEXT\n          value: 'true'"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f mongodb-enterprise.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set namespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set operator.env=dev \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set operator.watchNamespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set managedSecurityContext=false \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  --set operator.podSetWaitRetries=20 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "docker pull quay.io/mongodb/mongodb-enterprise-operator:0.1; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-database:0.1"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set namespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.watchNamespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set managedSecurityContext=false \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  --set operator.podSetWaitRetries=20 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "docker pull quay.io/mongodb/mongodb-enterprise-operator:0.1; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-database:0.1"
                },
                {
                    "lang": "sh",
                    "value": "docker save quay.io/mongodb/mongodb-enterprise-operator:0.1 -o mongodb-enterprise-operator.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-database:0.1 -o mongodb-enterprise-database.tar"
                },
                {
                    "lang": "sh",
                    "value": "docker import mongodb-enterprise-operator.tar quay.io/mongodb/mongodb-enterprise-operator:0.1; \\\ndocker import mongodb-enterprise-database.tar quay.io/mongodb/mongodb-enterprise-database:0.1"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set namespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.watchNamespace=<testNamespace> \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set managedSecurityContext=false \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template \\\n  --set registry.pullPolicy=IfNotPresent \\\n  --set operator.env=dev \\\n  --set operator.podWaitSeconds=10 \\\n  --set operator.podSetWaitRetries=20 \\\n  helm_chart > operator.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe deployments mongodb-enterprise-operator -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Error from server (NotFound): deployments.apps \"mongodb-enterprise-operator\" not found"
                }
            ],
            "preview": " allows you to deploy MongoDB deployment items with \nand . This Operator uses  and   methods to\ndeploy standalone, replica set, and sharded cluster deployments that\n manages.",
            "tags": null,
            "facets": null
        }
    ]
}