{
    "url": "http://mongodb.com/docs/kubernetes-operator/v1.4",
    "includeInGlobalSearch": false,
    "documents": [
        {
            "slug": "manage-users",
            "title": "Manage Database Users",
            "headings": [],
            "paragraphs": "Manage database users using SCRAM authentication on MongoDB\ndeployments. Manage database users for deployments running with TLS and X.509\ninternal cluster authentication enabled.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference",
            "title": "Reference",
            "headings": [],
            "paragraphs": "Review the     object specification. Review the MongoDB   object specifications. Review settings that only the   can set. Find solutions to   issues. Open Source licenses that the   uses.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "secure",
            "title": "Secure a Database Resource",
            "headings": [],
            "paragraphs": "Configure   for   deployments. Configure X.509 for client authentication. Configure   and X.509 for internal authentication.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "deploy",
            "title": "Deploy a MongoDB Database Resource",
            "headings": [],
            "paragraphs": "Use   to deploy a new standalone MongoDB instance. Use   to deploy a replica set. Use   to deploy a sharded cluster.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "upgrade",
            "title": "Upgrade the  from Prior Versions",
            "headings": [],
            "paragraphs": "Follow this upgrade procedure if you are running version 0.10 or\nlater. Follow this upgrade procedure if you are running version 0.9 or\nearlier. Starting in   version 1.3.0, you can only have one MongoDB\nresource per project. This document explains how to migrate existing\nprojects which have multiple MongoDB resources into configurations with\na single resource per project.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "mdb-resources",
            "title": "Deploy and Configure MongoDB Database Resources",
            "headings": [],
            "paragraphs": "You can use the   to deploy and manage MongoDB clusters\nfrom the    , without having to configure them in\n  or  . Configure the   to deploy MongoDB database resources. Deploy a standalone, replica set, or sharded cluster resource. Modify the configuration of a MongoDB database resource. Configure authentication for client applications and encrypt\nconnections to your MongoDB resources. Configure authentication for MongoDB database users. Access database resources from inside or outside  .",
            "code": [],
            "preview": "You can use the  to deploy and manage MongoDB clusters\nfrom the  , without having to configure them in\n or .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "",
            "title": "MongoDB Enterprise Kubernetes Operator",
            "headings": [],
            "paragraphs": "The   translates the human knowledge of creating a MongoDB\ninstance into a scalable, repeatable, and standardized method.\nKubernetes needs help creating and managing stateful applications like\ndatabases. It needs to configure the network, persist storage, and\ndedicate computing capacity without additional human effort on each\ncontainer. The   manages the typical lifecycle events for a MongoDB\ncluster: provisioning storage and computing power, configuring network\nconnections, setting up users, and changing these settings as needed.\nIt accomplishes this using the Kubernetes API and tools. You provide the Operator with the specifications for your MongoDB\ncluster. The Operator uses this information to tell Kubernetes how to\nconfigure that cluster including provisioning storage, setting up the\nnetwork connections, and configuring other resources. The   works together with MongoDB  , which further\nconfigures to MongoDB clusters. When MongoDB is deployed and running in\nKubernetes, you can manage MongoDB tasks using  . You can then deploy MongoDB databases as you deploy them now after the\ncluster is created. You can use the   console to run MongoDB at\noptimal performance.",
            "code": [],
            "preview": "The  translates the human knowledge of creating a MongoDB\ninstance into a scalable, repeatable, and standardized method.\nKubernetes needs help creating and managing stateful applications like\ndatabases. It needs to configure the network, persist storage, and\ndedicate computing capacity without additional human effort on each\ncontainer.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "installation",
            "title": "Install and Configure the ",
            "headings": [],
            "paragraphs": "Review   deployment scopes, considerations, and\nprerequisites. Install the  . Upgrade from earlier versions of  .",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "configure-k8s-operator-for-mdb-resources",
            "title": "Configure the  for MongoDB Database Resources",
            "headings": [],
            "paragraphs": "Create a   so the   can create and update\n  in your   Project. Create a   to link the   to your  \nProject. Create an X.509 certificate to connect to an X.509-enabled\nMongoDB deployment.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "connect",
            "title": "Access Database Resources",
            "headings": [],
            "paragraphs": "The following pages describe how to connect to a MongoDB\ndatabase resource that is deployed to  : Connect to a MongoDB database resource from inside\nof the   cluster. Connect to a MongoDB database resource from outside\nof the   cluster.",
            "code": [],
            "preview": "The following pages describe how to connect to a MongoDB\ndatabase resource that is deployed to :",
            "tags": null,
            "facets": null
        },
        {
            "slug": "om-resources",
            "title": "Deploy and Configure Ops Manager Resources",
            "headings": [],
            "paragraphs": "Review the   resource architecture, considerations, and\nprerequisites. Use the   to deploy an   instance.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "third-party-licenses",
            "title": "Third-Party Licenses",
            "headings": [
                "Apache License 2.0"
            ],
            "paragraphs": "MongoDB   uses third-party libraries or other resources\nthat may be distributed under licenses different than the MongoDB\nsoftware.  depends upon the following third-party packages. These\npackages are licensed as shown in the following list. Should we\naccidentally fail to list a required license, please\n contact the MongoDB Legal Department . Package k8s.io/api k8s.io/apiextensions-apiserver k8s.io/apimachinery k8s.io/client-go k8s.io/kube-openapi",
            "code": [],
            "preview": "MongoDB  uses third-party libraries or other resources\nthat may be distributed under licenses different than the MongoDB\nsoftware.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "release-notes",
            "title": "Release Notes for ",
            "headings": [
                " 1.4.5",
                "CVE Description",
                "Common Weakness Enumeration",
                "Affected Versions",
                "Fixed Versions",
                " 1.4.4",
                "MongoDB Resource Changes",
                " Resource Changes (Beta Release)",
                "Bug Fixes",
                " 1.4.3",
                " Changes",
                "MongoDB Resource Changes",
                " Resource Changes (Beta Release)",
                "Bug Fixes",
                " 1.4.2",
                "MongoDB Resource Changes",
                " Resource Changes (Beta Release)",
                "Bug Fixes",
                " 1.4.1",
                " 1.4.0",
                "MongoDB Resource Changes",
                " Resource Changes (Beta Release)",
                "Bug Fixes",
                " 1.3.1",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes (Alpha Release)",
                " 1.3.0",
                "Specification Schema Changes",
                "Ops Manager Resource Changes (Alpha Release)",
                "Bug Fixes",
                " 1.2.5",
                "CVE Description",
                "Common Weakness Enumeration",
                "Affected Versions",
                "Fixed Versions",
                " 1.2.4",
                " 1.2.3",
                " 1.2.2",
                " 1.2.1",
                " 1.2",
                "GA Release",
                "Alpha Release",
                " 1.1",
                " 1.0",
                " 0.12",
                " 0.11",
                " 0.10",
                " 0.9",
                " 0.8",
                " 0.7",
                " 0.6",
                " 0.5",
                " 0.4",
                " 0.3",
                " 0.2",
                " 0.1"
            ],
            "paragraphs": "Fixes CVE-2020-7922:   Operator generates potentially insecure certificates. X.509 certificates generated by the   may allow an attacker with\naccess to the   cluster improper access to MongoDB instances.\nCustomers who do not use X.509 authentication, and those who do not use\nthe   to generate their X.509 certificates are unaffected. CWE-295: Improper Certificate Validation\nCVSS score: 6.4\nCVSS:3.1/AV:A/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:N 1.0, 1.1 1.2.0 - 1.2.4 1.3.0 - 1.3.1 1.4.0 - 1.4.4 1.2.5 1.4.5 and above Supports changes in the  Cloud Manager API . Properly terminates resources with a termination hook. Implements stricter validations. MongoDB resources: Fixes an issue when working with   with custom  \ncertificates. Released 2020-02-24 Adds a  webhook  to validate\na   configuration. Adds support for sidecars for   pods using the\n spec.podSpec.podTemplate  setting. Allows users to change the  PodSecurityContext  to allow privileged\nsidecar containers. Adds the  spec.podSpec  configuration settings for\n , the Backup Daemon, and the Application Database. See\n Ops Manager Resource Specification .  image for version 4.2.8 is available. See the  for new\nfeature usage examples. MongoDB resources: Fixes potential race conditions when deleting  .  resources: Supports the  spec.clusterDomain  setting for  \nand Application Database resources. No longer starts monitoring and backup processes for the Application\nDatabase. Released 2020-01-24 Runs MongoDB database   pods under a dedicated   service\naccount:  mongodb-enterprise-database-pods . Adds the  spec.podSpec.podTemplate  setting, which allows\nyou to apply templates to   pods that the  \ngenerates for each database  . Renames the  spec.clusterName  setting to\n spec.clusterDomain . Adds  offline mode support  for the Application\nDatabase. Bundles MongoDB Enterprise version 4.2.2 with the\nApplication Database image. Internet access is not required to\ninstall the application database if\n spec.applicationDatabase.version  is set to\n 4.2.2-ent  or omitted. Renames the  spec.clusterName  setting to\n spec.clusterDomain .  images for versions 4.2.6 and 4.2.7 are available. See the  for new\nfeature usage examples. MongoDB resources: Fixes the order of sharded cluster component creation. Allows   to be enabled on Amazon EKS.  resources: Enables the   to use the  spec.clusterDomain  setting. Released 2019-12-13 Includes  CVE fixes  and\n RHSA security fixes . Fixes an issue that prevented backup from starting on MongoDB 4.0. Released 2019-12-09 Adds split horizon DNS support for MongoDB replica sets, which allows\nclients to connect to a replica set from outside of the  \ncluster. Supports requests for  -generated certificates for\nadditional certificate domains, which makes them valid for the\nspecified subdomains. For more information on how to enable new features, see the sample YAML\nfiles in the . Promotes the  MongoDBOpsManager   resource  to Beta.   version\n4.2.4 is available. Supports Backup and restore in  -deployed  \ninstances. This is a semi-automated process that deploys everything\nyou need to enable backups in  . You can enable Backup by\nsetting the  spec.backup.enabled  setting in the  \ncustom resource. You can configure the Head Database, Oplog Store, and\nS3 Snapshot Store by using the  MongoDBOpsManager   resource\nspecification . Supports access to   from outside the  \ncluster through the  spec.externalConnectivity  setting. Enables SCRAM-SHA-1 authentication  on  's\nApplication Database by default. Adds support for OpenShift (Red Hat UBI Images). Improves overall stability of X.509 user management. Released 2019-11-08 This release introduces significant changes that may not be\ncompatible with previous deployments or resource configurations. Read\n Migrate to One Resource per Project (Required for Version 1.3.0)  before\nupgrading the  . Requires one MongoDB resource per   project. If you\nhave more than one MongoDB resource in a project, all resources will\nchange to a  Pending  status and the   won\u2019t perform\nany changes on them. The existing MongoDB databases will still be\naccessible. You must  migrate to one resource per project . Supports  SCRAM-SHA  authentication mode. See \nfor examples. Requires that the project ( ConfigMap ) and\ncredentials ( secret )\nreferenced from a MongoDB resource be in the same namespace. Adds OpenShift installation files (  file and Helm chart\nconfiguration). Supports highly available  Ops Manager resources  by introducing the  spec.replicas \nsetting. Runs   as a non-root user. Released 2019-10-25 This release introduces significant changes that may not be\ncompatible with previous deployments or resource configurations. Read\n Migrate to One Resource per Project (Required for Version 1.3.0)  before installing or\nupgrading the  . Moves to a\n one cluster per project configuration .\nThis follows the warnings introduced in a\n previous version of the operator .\nThe operator now requires each cluster to be contained within a new\nproject. Authentication settings are now contained within the\n security section  of the MongoDB resource\nspecification rather than the project ConfigMap. Replaces the  project  field with the\n spec.opsManager.configMapRef.name  or\n spec.cloudManager.configMapRef.name  fields. User resources  now refer to MongoDB resources\nrather than project ConfigMaps. No longer requires  data.projectName  in the project ConfigMap. The\nname of the project defaults to the name of the MongoDB resource in\n . This release introduces signficant changes to the   resource's\narchitecture. The   application database is now managed by\nthe  , not by  . Stops unnecessary recreation of NodePorts. Fixes logging so it's always in JSON format. Sets  USER  in the   Docker image. Fixes CVE-2020-7922:   Operator generates potentially insecure certificates. X.509 certificates generated by the   may allow an attacker with\naccess to the   cluster improper access to MongoDB instances.\nCustomers who do not use X.509 authentication, and those who do not use\nthe   to generate their X.509 certificates are unaffected. CWE-295: Improper Certificate Validation\nCVSS score: 6.4\nCVSS:3.1/AV:A/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:N 1.0, 1.1 1.2.0 - 1.2.4 1.3.0 - 1.3.1 1.4.0 - 1.4.4 1.2.5 1.4.5 and above Released 2019-10-02 Increases stability of Sharded Cluster deployments. Improves internal testing infrastructure. Released 2019-09-13 Update:  The   will remove support for multiple\nclusters per project in a future release. If a project contains more\nthan one cluster, a warning will be added to the status of the\nMongoDB Resources. Additionally, any new cluster being added to a\nnon-empty project will result in a  Failed  state, and won't\nbe processed. Fix:  The overall stability of the operator has been improved. The\noperator is now more conservative in resource updates both on\n  and  . Released 2019-08-30 Security Fix:  Clusters configured by   versions\n1.0 through 1.2.1 used an insufficiently strong keyfile for internal\ncluster authentication between  mongod  processes. This only affects\nclusters which are using X.509 for user authentication, but are not\nusing X.509 for internal cluster authentication. Users are advised to\nupgrade to version 1.2.2, which will replace all managed keyfiles. Security Fix:  Clusters configured by   versions 1.0\nthrough 1.2.1 used an insufficiently strong password to authenticate\nthe MongoDB Agent. This only affects clusters which have been manually\nconfigured to enable  SCRAM-SHA-1 , which is not a supported\nconfiguration. Users are advised to upgrade to version 1.2.2, which\nwill reset these passwords. Released 2019-08-23 Fix:  The   no longer recreates   when X.509\nauthentication is enabled and the approved   have been deleted. Fix:  If the  OPERATOR_ENV  environment variable is set to\nsomething unrecognized by the  , it will no longer result\nin a  CrashLoopBackOff  of the pod. A default value of  prod  is\nused. The   now supports more than 100 agents in a given\nproject. Released 2019-08-13 Adds a\n readinessprobe \nto the MongoDB Pods to improve the reliability of rolling upgrades. This feature is an alpha release. It is not ready for production use. Can use the   to manage   4.2. To  deploy an\n|onprem| instance , you use a new\n resource :  MongoDBOpsManager . Released 2019-07-19 Fix:  Sample yaml files, in particular, the attribute related to\n featureCompatibilityVersion . Fix:    can be disabled in a deployment. Improvement:  Added\n script  in the\n support  directory that can gather information of\nyour MongoDB resources in Kubernetes. Improvement:  In a   environment, the   can use a\ncustom Certificate Authority. All the certificates must be passed as\nKubernetes Secret objects. Released 2019-06-18 Supports Kubernetes v1.11 or later. Provisions any kind of MongoDB deployment in the Kubernetes Cluster\nof your Organization: Standalone Replica Set Sharded Cluster Configures   on the MongoDB deployments and encrypt all traffic.\nHosts and clients can verify each other\u2019s identities. Manages MongoDB users. Supports X.509 authentication to your MongoDB databases. If you have any questions regarding this release, use the\n #enterprise-kubernetes \nSlack channel. Released 2019-06-07 Rolling upgrades of MongoDB resources ensure that  rs.stepDown() \nis called for the primary member. Requires MongoDB patch version 4.0.8 and\nlater or MongoDB patch version 4.1.10 and later. During a MongoDB major version upgrade, the\n featureCompatibilityVersion  field can be set. Fixed a bug where replica sets with more than seven members could\nnot be created. X.509 Authentication can be enabled at the\n Project level . Requires  ,  \npatch version 4.0.11 and later, or   patch version 4.1.7 and later. Internal cluster authentication based on X.509 can be enabled at the\n deployment  level. MongoDB users with X.509 authentication can be created, using the\nnew  MongoDBUser  custom resource. Released 2019-04-29 NodePort  service creation can be disabled.  can be enabled for internal authentication between MongoDB in\nreplica sets and sharded clusters. The   certificates are created\nautomatically by the  . Please refer to the sample\n .yaml  files in the\n GitHub repository \nfor examples. Wide or asterisk roles have been replaced with strict listing of\nverbs in  roles.yaml . Printing  mdb  objects with  kubectl  will provide more\ninformation about the MongoDB object: type, state, and MongoDB server\nversion. Released 2019-04-02 The   and database images are now based on ubuntu:16.04. The   now uses a single   named  MongoDB \ninstead of the  MongoDbReplicaSet ,  MongoDbShardedCluster , and\n MongoDbStandalone  CRDs. Follow the  upgrade procedure  to\ntransfer existing  MongoDbReplicaSet ,  MongoDbShardedCluster ,\nand  MongoDbStandalone  resources to the new format. For a list of the packages installed and any security vulnerabilities\ndetected in our build process, see: MongoDB Enterprise Operator MongoDB Enterprise Database Released 2019-03-19 The Operator and Database images are now based on\n debian:stretch-slim  which is the latest and up-to-date Docker\nimage for Debian 9. Released 2019-02-26 Perform   clean-up on deletion of MongoDB resource without the\nuse of finalisers. Bug fix:  Race conditions when communicating with  . Bug fix:   ImagePullSecrets  being incorrectly initialized in\nOpenShift. Bug fix:  Unintended fetching of closed projects. Bug fix:  Creation of duplicate organizations. Bug fix:  Reconciliation could fail for the MongoDB resource if\nsome other resources in   were in error state. Released 2019-02-01 Improved detailed status field for MongoDB resources. The   watches changes to configuration parameters in a\nproject configMap and the credentials secret then performs a rolling\nupgrade for relevant Kubernetes resources. Added   structured logging for Automation Agent pods. Support   records for MongoDB access. Bug fix: Avoiding unnecessary reconciliation. Bug fix: Improved Ops Manager/Cloud Manager state management for\ndeleted resources. Released 2018-12-17 Refactored code to use the  controller-runtime  library to fix issues\nwhere Operator could leave resources in inconsistent state. This also\nintroduced a proper reconciliation process. Added new  status  field for all MongoDB Kubernetes resources. Can configure Operator to watch any single namespace or all\nnamespaces in a cluster (requires cluster role). Improved database logging by adding a new configuration property\n logLevel . This property is set to  INFO  by default.\nAutomation Agent and MongoDB logs are merged in to a single log\nstream. Added new configuration Operator timeout. It defines waiting time\nfor database pods start while updating  . Fix:  Fixed failure detection for  mongos . Released 2018-11-14 Image for database no longer includes the binary for the Automation\nAgent. The container downloads the Automation Agent binary from\n  when it starts. Fix:  Communication with   failed if the project with the same\nname existed in different organization. Released 2018-10-04 If a backup was enabled in   for a Replica Set or Sharded\nCluster that the   created, then the  \ndisables the backup before removing a resource. Improved persistence support: The data, journal and log directories are mounted to three\nmountpoints in one or three volumes depending upon the\n podSpec.persistence  setting. Prior to this release, only the data directory was mounted to\npersistent storage. Setting Mount Directories to podSpec.persistence.single One volume podSpec.persistence.multiple Three volumes A new parameter,  labelSelector , allows you to specify the\nselector for volumes that   should consider mounting. If   is not specified in the  persistence \nconfiguration, then the default  StorageClass  for the cluster is\nused. In most of public cloud providers, this results in dynamic\nvolume provisioning. Released 2018-08-07 The Operator no longer creates the CustomResourceDefinition objects.\nThe user needs to create them manually. Download and apply\n this new yaml file \n( crd.yaml ) to create/configure these objects. ClusterRoles are no longer required. How the Operator watches\nresources has changed. Until the last release, the Operator would\nwatch for any resource on any  . With 0.3, the Operator\nwatches for resources in the same namespace in which it was created.\nTo support multiple namespaces, multiple Operators can be installed.\nThis allows isolation of MongoDB deployments. Permissions changes were made to how PersistentVolumes are mounted. Added configuration to Operator to not create\n SecurityContexts \nfor  . This solves an issue with OpenShift which does not\nallow this setting when  SecurityContextContraints  are used. If you are using Helm, set  managedSecurityContext  to  true .\nThis tells the Operator to not create  SecurityContext  for\n , satisfying the OpenShift requirement. The combination of  projectName  and  orgId  replaces\n projectId  alone to configure the connection to  .\nThe project is created if it doesn't exist. Released 2018-08-03 Calculates WiredTiger memory cache. Released 2018-06-27 Initial Release Can deploy standalone instances, replica sets, sharded clusters\nusing   configuration files.",
            "code": [],
            "preview": "Fixes CVE-2020-7922:  Operator generates potentially insecure certificates.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/k8s-op-exclusive-settings",
            "title": "MongoDB  Exclusive Settings",
            "headings": [
                "Kubernetes Operator Overrides Some Ops Manager Settings"
            ],
            "paragraphs": "You can use the   to deploy MongoDB resources with\n  version 4.0.11 or later and Cloud Manager. At any place in\nthis guide that says  Ops Manager , you can substitute\n Cloud Manager . Some settings that you configure using   cannot be set or\noverridden in the  . Settings that the   does\nnot manage are accepted. The following list of settings are exclusive to  . This list may\nchange at a later date. These settings can be found on the\n Automation Configuration \npage: In addition to the list of Automation settings, the   uses attributes\noutside of the deployment from the Monitoring and Backup Agent configurations. auth.autoAuthMechanisms auth.authoritativeSet auth.autoPwd auth.autoUser auth.deploymentAuthMechanisms auth.disabled auth.key auth.keyfile auth.keyfileWindows auth.usersWanted auth.usersWanted[n].mechanisms auth.usersWanted[n].roles auth.usersWanted[n].roles[m].role auth.usersWanted[n].roles[m].db auth.usersWanted[n].user auth.usersWanted[n].authenticationRestrictions processes.args2_6.net.port processes.args2_6.net.ssl.clusterAuthFile processes.args2_6.replication.replSetName processes.args2_6.security.clusterAuthMode processes.args2_6.storage.dbPath processes.args2_6.systemLog.path processes.authSchemaVersion processes.cluster  (mongos processes) processes.featureCompatibilityVersion processes.hostname processes.name processes.version replicaSets._id replicaSets.members._id replicaSets.members.host replicaSets.members replicaSets.version sharding.clusterRole  (config server) sharding.configServerReplica sharding.name sharding.shards._id sharding.shards.rs ssl.CAFilePath ssl.autoPEMKeyFilePath ssl.clientCertificateMode backupAgentTemplate.username backupAgentTemplate.sslPEMKeyFile monitoringAgentTemplate.username monitoringAgentTemplate.sslPEMKeyFile  creates a replica set of 3 members. You changed  storage.wiredTiger.engineConfig.cacheSizeGB \nto  40 . This setting is not in the   exclusive settings\nlist. You then use the   to scale the replica set to\n5 members. The  storage.wiredTiger.engineConfig.cacheSizeGB  on the\nnew members should still be  40 .",
            "code": [],
            "preview": "Some settings that you configure using  cannot be set or\noverridden in the . Settings that the  does\nnot manage are accepted.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/known-issues",
            "title": "Known Issues in the ",
            "headings": [
                "Deleting a Resource with Backups Removes All Snapshots",
                "Configure Persistent Storage Correctly",
                "Remove Resources before Removing ",
                "Create Separate Namespaces for  and MongoDB Resources",
                "Difficulties with Updates",
                "Machine Memory vs. Container Memory",
                "Changes to Avoid"
            ],
            "paragraphs": "When you delete a resource that has backup configured, the\n  terminates all backups. Deleting a resource removes all\nexisting snapshots without warning. If there are no\n persistent volumes \navailable when you create a resource, the resulting   stays in\ntransient state and the Operator fails  (after 20 retries) with the\nfollowing error: To prevent this error, either: For testing only, you may also set  persistent : false . This\n must not be used in production , as data is not preserved between\nrestarts. Provide   or Set  persistent : false  for the resource Sometimes   can diverge from  . This mostly occurs when\n  resources are removed manually.   can keep displaying an\nAutomation Agent which has been shut down. If you want to remove deployments of MongoDB on  , use the\nresource specification to delete resources first so no dead Automation\nAgents remain. The best strategy is to create   and its resources in\ndifferent namespaces so that the following operations would work\ncorrectly: or If the   and resources sit in the same  mongodb \n , then operator would also be removed in the same operation.\nThis would mean that it could not clean the configurations, which\nwould have to be done in the  . In some cases, the   can stop receiving change events. As\nthis problem is hard to reproduce, the recommended workaround is to\ndelete the operator pod.   starts the new  \nautomatically and starts working correctly: Kubernetes Operator installation MongoDB versions older than 3.6.13, 4.0.9, and 4.1.9 report host system RAM, not\ncontainer RAM. The   will not be able to apply the following change on a MongoDB Deployment simultaneously: If both operations are applied at the same time, the MongoDB Resource could go into a unrecoverable state. The TLS configuration is disabled ( security.tls.enabled: false ) The number of members in a Replica Set is increased",
            "code": [
                {
                    "lang": "sh",
                    "value": "Failed to update Ops Manager automation config: Some agents failed to register"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete pods --all"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods;\nkubectl delete pod mongodb-enterprise-operator-<podId>`"
                }
            ],
            "preview": "When you delete a resource that has backup configured, the\n terminates all backups. Deleting a resource removes all\nexisting snapshots without warning.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/production-notes",
            "title": " Production Notes",
            "headings": [
                "Ensure Proper Persistence Configuration",
                "Name Your MongoDB Service with its Purpose",
                "Specify Resource Requirements",
                "Use Multiple Availability Zones",
                "Co-locate mongos Pods with Your Applications",
                "Manage Multitenancy with Labels",
                "Enable TLS",
                "Enable Authentication",
                "Example Deployment CRD",
                "Example User CRD"
            ],
            "paragraphs": "This page details system configuration recommendations for the\n  when running in production. You use a   to ensure stateful configurations of  \ndeployments. The storage of your   deployment must persist, so\nverify that the   are configured to meet your storage needs. The  : Supports mounting storage devices to one or more directories\ncalled mount points. Creates one   per MongoDB mount point. Sets the default path in each container to  /data . spec.persistent spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data If using your own MongoDB Service, set the  spec.service  parameter\nto something that helps you identify this deployment's purpose. For the replica sets, sharded clusters, and config servers you create\nusing the  , set the resource utilization bounds for both\ncompute and memory.   refers to the lower bound of a resource as a\n request  and the upper bound as a  limit . Set  CPU requests and limits \nto guarantee the CPU allocation and resource reporting. Set  Memory requests and limits \nto guarantee the requested memory allocation for the WiredTiger\ncache and resource reporting. Monitoring tools report the size of the   rather than the\nactual size of the container. Set the   and   to distribute all members\nof one replica set to different   to ensure high\navailability. The lightweight  mongos  instance can be run in the same  \nas your apps using MongoDB. The   supports standard  \n node-affinity and node anti-affinity \nfeatures. Using these features, you can force install the  mongos \non the same pod as your application. The  podAffinity  key determines if an application should be\ninstalled on the same pod, node, or data center as another\napplication. You add a label and value in the  spec.template.metadata.labels \n  collection to tag the deployment. In this example, the  web-store  value for the  app  key labels\nthe pod on which the  web-server  is installed. The\n labelSelector  key declares that the  mongos  app must be\ninstalled on the same pod that has its  app  label set to be\n In  values that include  web-store . If you need to physically separate different MongoDB resources (such\nas  test  and  staging  environments) or want to place  \non some specific nodes (such as   support) use the\n pod affinity    feature. The   supports   encryption. Use   with your\nMongoDB deployment to encrypt your data over the network. The   supports X.509 user authentication. You must create\nan additional   for your MongoDB users and the MongoDB Agents.\nThe Operator generates and distributes the certificate.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.0.14\n  service: my-service\n  featureCompatibilityVersion: \"3.6\"\n  project: my-project\n  credentials: my-credentials\n  type: ShardedCluster\n  persistent: true\n  configSrvPodSpec:\n    cpu: '0.5'\n    memory: 512M\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n  mongosPodSpec:\n    cpu: '0.8'\n    memory: 1Gi\n    podAntiAffinityTopologyKey: rackId\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n  shardPodSpec:\n    cpu: '0.6'\n    memory: 3Gi\n    persistence:\n      multiple:\n        data:\n          storage: 20Gi\n        logs:\n          storage: 4Gi\n          storageClass: standard\n    podAntiAffinityTopologyKey: kubernetes.io/hostname"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: 4.0.14\n  service: drilling-pumps-geosensors\n  featureCompatibilityVersion: \"3.6\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.0.14\n  service: my-service\n  featureCompatibilityVersion: \"3.6\"\n  project: my-project\n  credentials: my-credentials\n  type: ShardedCluster\n  persistent: true\n  configSrvPodSpec:\n    cpu: '0.5'\n    memory: 512M\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n  mongosPodSpec:\n    cpu: '0.8'\n    memory: 1Gi\n    podAntiAffinityTopologyKey: rackId\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n  shardPodSpec:\n    cpu: '0.6'\n    memory: 3Gi\n    persistence:\n      multiple:\n        data:\n          storage: 20Gi\n        logs:\n          storage: 4Gi\n          storageClass: standard\n    podAntiAffinityTopologyKey: kubernetes.io/hostname"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.0.14\n  service: my-service\n  featureCompatibilityVersion: \"3.6\"\n  project: my-project\n  credentials: my-credentials\n  type: ShardedCluster\n  persistent: true\n  configSrvPodSpec:\n    cpu: '0.5'\n    memory: 512M\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n  mongosPodSpec:\n    cpu: '0.8'\n    memory: 1Gi\n    podAntiAffinityTopologyKey: rackId\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n  shardPodSpec:\n    cpu: '0.6'\n    memory: 3Gi\n    persistence:\n      multiple:\n        data:\n          storage: 20Gi\n        logs:\n          storage: 4Gi\n          storageClass: standard\n    podAntiAffinityTopologyKey: kubernetes.io/hostname"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-server\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web-store\n\nmongosPodSpec:\n  podAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n        matchExpressions:\n        - key: app\n          operator: In\n          values:\n          - web-store"
                },
                {
                    "lang": "yaml",
                    "value": "mongosPodSpec:\n  podAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n        matchExpressions:\n        - key: app\n          operator: In\n          values:\n          - web-store"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-tls-enabled-rs\nspec:\n  type: ReplicaSet\n  members: 3\n  version: 4.0.14\n  project: my-project\n  credentials: my-credentials\n  security:\n    tls:\n      enabled: true\n  additionalMongodConfig:\n    net:\n      ssl:\n        mode: \"preferSSL\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-tls-enabled-rs\nspec:\n  type: ReplicaSet\n  members: 3\n  version: 4.0.14\n  project: my-project\n  credentials: my-credentials\n  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: user-with-roles\nspec:\n  username: \"CN=mms-user-1,OU=cloud,O=MongoDB,L=New York,ST=New York,C=US\"\n  db: \"$external\"\n  project: my-project\n  roles:\n    - db: \"admin\"\n      name: \"clusterAdmin\""
                }
            ],
            "preview": "This page details system configuration recommendations for the\n when running in production.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/edit-deployment",
            "title": "Edit a Database Resource",
            "headings": [
                "Considerations",
                "Prerequisites",
                "Procedure"
            ],
            "paragraphs": "You can modify a deployment's configuration and topology, including its\nMongoDB versions, storage engines, and numbers of hosts or shards. You\ncan make modifications at all levels of a deployment's topology from a\ntop-level  sharded cluster  or\n replica set  to lower levels, such as a replica set within a\nsharded cluster, or an individual process within a replica set. You can\nalso modify  standalone  processes. Changes cannot be made to individual members of a replica set or\nsharded cluster, only to the whole set or cluster. If a setting isn't available for a MongoDB Kubernetes resource,\nthen the change must be made in the  Ops Manager  or\n Cloud Manager  application. MongoDB custom resources do not support all\n mongod  command line options. If you use an\nunsupported option in your object specification file, the backing\n MongoDB Agent \noverrides the unsupported options. For a complete list of options\nsupported by MongoDB custom resources, see  MongoDB Database Resource Specification . Certain settings can only be configured using  . To\nreview the list of settings, see\n MongoDB   Exclusive Settings . To update a MongoDB  , you need to have completed the following procedures: Install and Configure the  Create Credentials for the  Create One Project using a ConfigMap Deploy a database Edit the   resource specification file. Modify or add any settings you need added or changed. Save your specification file. Invoke the following   command to update your resource.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <standalone-conf>.yaml"
                }
            ],
            "preview": "You can modify a deployment's configuration and topology, including its\nMongoDB versions, storage engines, and numbers of hosts or shards. You\ncan make modifications at all levels of a deployment's topology from a\ntop-level sharded cluster or\nreplica set to lower levels, such as a replica set within a\nsharded cluster, or an individual process within a replica set. You can\nalso modify standalone processes.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/troubleshooting",
            "title": "Troubleshooting the ",
            "headings": [
                "Get Status of a Deployed Resource",
                "Review the Logs",
                "Review Logs from the ",
                "Find a Specific Pod",
                "Review Logs from Specific Pod",
                "View All  Specifications",
                "Restore StatefulSet that Failed to Deploy",
                "Remove a ",
                "Remove the ",
                "Remove the ",
                "Remove the ",
                "Disable  Feature Controls"
            ],
            "paragraphs": "To find the status of a resource deployed with the  ,\ninvoke one of the following commands: The following key-value pairs describe the resource deployment statuses: For   resource deployments: The  status.applicationDatabase.phase  field displays the\nApplication Database resource deployment status. The\n status.opsManager.phase  field displays the   resource\ndeployment status. For MongoDB resource deployments: The  status.phase  field displays the MongoDB resource deployment\nstatus. Key Value message Message explaining why the resource is in a  Pending  or\n Failed  state. phase Status Meaning Pending The   is unable to reconcile the resource\ndeployment state. This happens when a reconciliation\ntimes out or if the   requires you to take\naction for the resource to enter a running state. If a resource is pending because a reconciliation timed\nout, the   attempts to reconcile the\nresource state in 10 seconds. Reconciling The   is reconciling the resource state. Resources enter this state after you create or update\nthem or if the   is attempting to reconcile\na resource previously in a  Pending  or  Failed \nstate. The   attempts to reconcile the resource\nstate in 10 seconds. Running The resource is running properly. Failed The resource is not running properly. The  message \nfield provides additional details. The   attempts to reconcile the resource\nstate in 10 seconds. lastTransition  when the last reconciliation happened. link Deployment   in  . Resource specific fields For descriptions of these fields, see\n MongoDB Database Resource Specification . If you want to see what the status of a replica set named\n my-replica-set  in the  developer  namespace, run: If  my-replica-set  is running, you should see: If  my-replica-set  is not running, you should see: To review the   logs, invoke this command: You could check the  Ops Manager Logs  as\nwell to see if any issues were reported to  . To find which pods are available, invoke this command first: If you want to narrow your review to a specific  , you can\ninvoke this command: If your  replica set  is labeled  myrs , the   log\ncommand is invoked as: This returns the  Automation Agent Log  for this\nreplica set. To view all   specifications in the provided\n : To read details about the  dublin  standalone resource, invoke\nthis command: This returns the following response: A StatefulSet   may hang with a status of  Pending  if it\nencounters an error during deployment. Pending    do not automatically terminate, even if you\nmake  and apply  configuration changes to resolve the error. To return the StatefulSet to a healthy state, apply the configuration\nchanges to the MongoDB resource in the  Pending  state, then delete\nthose pods. A host system has a number of running  : my-replica-set-2  is stuck in the  Pending  stage. To gather\nmore data on the error, run the following: The output indicates an error in memory allocation. Updating the memory allocations in the MongoDB resource is\ninsufficient, as the pod does not terminate automatically after\napplying configuration updates. To remedy this issue, update the configuration, apply the\nconfiguration, then delete the hung pod: Once this hung pod is deleted, the other pods restart with your new\nconfiguration as part of rolling upgrade of the Statefulset. To learn more about this issue, see\n Kubernetes Issue 67250 . To remove any instance that   deployed, you must use  . You can only use the   to remove  -deployed\ninstances. If you use   to remove the instance,   throws an\nerror. To remove a single MongoDB instance you created using  : To remove all MongoDB instances you created using  : To remove the  : Remove all Kubernetes resources : Remove the  : To remove the  : Remove all Kubernetes resources : Remove the  : To remove the  : Remove all Kubernetes resources : Remove the  : When you manage an   project through the  , the\n  places the  EXTERNALLY_MANAGED_LOCK   feature\ncontrol policy \non the project. This policy disables certain features in the  \napplication that might compromise your   configuration. If\nyou need to use these blocked features, you can remove the policy\nthrough the  feature controls API ,\nmake changes in the   application, and then restore the original\npolicy through the  API . The following procedure enables you to use features in the  \napplication that are otherwise blocked by the  . Retrieve the feature control policies \nfor your   project. Your response should be similar to: Save the response that the API returns. After you make changes in\nthe   application, you must add these policies back to\nthe project. Update \nthe  policies  array with an empty list: The previously blocked features are now available in the\n  application. Make your changes in the   application. Update \nthe  policies  array with the original feature control policies: The features are now blocked again, preventing you from making\nfurther changes through the   application. However, the\n  retains any changes you made in the  \napplication while features were available.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl get <resource-name> -n <namespace> -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb my-replica-set -n developer -o yaml"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n    lastTransition: \"2019-01-30T10:51:40Z\"\n    link: http://ec2-3-84-128-187.compute-1.amazonaws.com:9080/v2/5c503a8a1b90141cbdc60a77\n    members: 1\n    phase: Running\n    version: 4.2.2-ent"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  lastTransition: 2019-02-01T13:00:24Z\n  link: http://ec2-34-204-36-217.compute-1.amazonaws.com:9080/v2/5c51c040d6853d1f50a51678\n  members: 1\n  message: 'Failed to create/update replica set in Ops Manager: Status: 400 (Bad Request),\n    Detail: Something went wrong validating your Automation Config. Sorry!'\n  phase: Failed\n  version: 4.2.2-ent"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs -f deployment/mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs <podName> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs myrs-0 -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl get mdb -n <namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl get mdb dublin -n <namespace> -o yaml"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"mongodb.com/v1\",\"kind\":\"MongoDB\",\"metadata\":{\"annotations\":{},\"name\":\"dublin\",\"namespace\":\"mongodb\"},\"spec\":{\"credentials\":\"credentials\",\"persistent\":false,\"podSpec\":{\"memory\":\"1G\"},\"project\":\"my-om-config\",\"type\":\"Standalone\",\"version\":\"4.0.0-ent\"}}\n  clusterDomain: \"\"\n  creationTimestamp: 2018-09-12T17:15:32Z\n  generation: 1\n  name: dublin\n  namespace: mongodb\n  resourceVersion: \"337269\"\n  selfLink: /apis/mongodb.com/v1/namespaces/mongodb/mongodbstandalones/dublin\n  uid: 7442095b-b6af-11e8-87df-0800271b001d\nspec:\n  credentials: my-credentials\n  type: Standalone\n  persistent: false\n  podSpec:\n    memory: 1Gi\n  project: my-om-config\n  version: 4.2.2-ent"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods\n\nmy-replica-set-0     1/1 Running 2 2h\nmy-replica-set-1     1/1 Running 2 2h\nmy-replica-set-2     0/1 Pending 0 2h"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe pod my-replica-set-2\n\n<describe output omitted>\n\nWarning FailedScheduling 15s (x3691 over 3h) default-scheduler 0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient memory."
                },
                {
                    "lang": "sh",
                    "value": "vi <my-replica-set>.yaml\n\nkubectl apply -f <my-replica-set>.yaml\n\nkubectl delete pod my-replica-set-2"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb <name> -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete deployment mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete namespace <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete crd --all"
                },
                {
                    "lang": "sh",
                    "value": "curl --user \"{USERNAME}:{APIKEY}\" --digest \\\n     --header \"Accept: application/json\" \\\n     --header \"Content-Type: application/json\" \\\n     --include \\\n     --request GET \"https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true\""
                },
                {
                    "lang": "json",
                    "value": " {\n  \"created\": \"2020-02-25T04:09:42Z\",\n  \"externalManagementSystem\": {\n    \"name\": \"mongodb-enterprise-operator\",\n    \"systemId\": \"6d6c139ae5528707b6e8e3b2\",\n    \"version\": \"1.4.2\"\n  },\n  \"policies\": [\n    {\n      \"disabledParams\": [],\n      \"policy\": \"EXTERNALLY_MANAGED_LOCK\"\n    },\n    {\n      \"disabledParams\": [],\n      \"policy\": \"DISABLE_AUTHENTICATION_MECHANISMS\"\n    }\n  ],\n  \"updated\": \"2020-02-25T04:10:12Z\"\n}"
                },
                {
                    "lang": "sh",
                    "value": "curl --user \"{USERNAME}:{APIKEY}\" --digest \\\n     --header \"Accept: application/json\" \\\n     --header \"Content-Type: application/json\" \\\n     --include \\\n     --request PUT \"https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true\" \\\n     --data\n       '{\n         \"externalManagementSystem\": {\n           \"name\": \"mongodb-enterprise-operator\",\n           \"systemId\": \"6d6c139ae5528707b6e8e3b2\",\n           \"version\": \"1.4.2\"\n         },\n         \"policies\": []\n       }'"
                },
                {
                    "lang": "sh",
                    "value": "curl --user \"{USERNAME}:{APIKEY}\" --digest \\\n     --header \"Accept: application/json\" \\\n     --header \"Content-Type: application/json\" \\\n     --include \\\n     --request PUT \"https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true\" \\\n     --data\n       '{\n         \"externalManagementSystem\": {\n           \"name\": \"mongodb-enterprise-operator\",\n           \"systemId\": \"6d6c139ae5528707b6e8e3b2\",\n           \"version\": \"1.4.2\"\n         },\n         \"policies\": [\n           {\n             \"disabledParams\": [],\n             \"policy\": \"EXTERNALLY_MANAGED_LOCK\"\n           },\n           {\n             \"disabledParams\": [],\n             \"policy\": \"DISABLE_AUTHENTICATION_MECHANISMS\"\n           }\n         ]\n       }'"
                }
            ],
            "preview": "To find the status of a resource deployed with the ,\ninvoke one of the following commands:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/k8s-operator-specification",
            "title": "MongoDB Database Resource Specification",
            "headings": [
                "Common Resource Settings",
                "Required",
                "Conditional",
                "Optional",
                "Deployment-Specific Resource Settings",
                "Standalone Settings",
                "Replica Set Settings",
                "Sharded Cluster Settings",
                "Security Settings",
                "Examples"
            ],
            "paragraphs": "The \ncreates     from specification files that you\nwrote. MongoDB resources are created in Kubernetes as\n custom resources .\nAfter you create or update a   specification, you direct\n  to apply this specification to your   environment.\n  creates the defined  , services and\nother Kubernetes resources. After the Operator finishes creating those\nobjects, it updates the   deployment configuration to\nreflect changes. Each   uses an object specification in   to define the\ncharacteristics and settings of the MongoDB object: standalone,\n replica set , and  sharded cluster . You can use the   to deploy MongoDB resources with\n  version 4.0.11 or later and Cloud Manager. At any place in\nthis guide that says  Ops Manager , you can substitute\n Cloud Manager . Deployment Type StatefulSets Size of StatefulSet Standalone 1 1  Replica Set 1 1   per member Sharded Cluster <numberOfShards> + 2 1   per  , shard, or config server member Every resource type must use the following settings: Every resource must use  one  of the following settings: Every resource type may use the following settings: Other settings you can and must use in a   specification\ndepend upon which MongoDB deployment item you want to create: Standalone Settings Replica Set Settings Sharded Cluster Settings The following settings only apply to replica set resource types: All of the  Standalone Settings  also apply to replica set\nresources. The following settings only apply to sharded cluster resource types: The following   settings only apply to replica set and sharded\ncluster resource types: The following example shows a resource specification for a\nstandlone deployment with every setting provided: The following example shows a resource specification for a\n replica set  with every setting provided: The following example shows a resource specification for a\n sharded cluster  with every setting provided:",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone\nspec:\n  version: 4.2.2-ent\n  service: my-service\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  persistent: true\n  type: Standalone\n  podSpec:\n    cpu: '0.25'\n    memory: 512M\n    persistence:\n      single:\n        storage: 12Gi\n        storageClass: standard\n        labelSelector:\n          matchExpressions:\n          - {key: environment, operator: In, values: [dev]}\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/e2e-az-name\n            operator: In\n            values:\n            - e2e-az1\n            - e2e-az2\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: 4.2.2-ent\n  service: my-service\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  persistent: true\n  type: ReplicaSet\n  podSpec:\n    cpu: '0.25'\n    memory: 512M\n    persistence:\n      multiple:\n        data:\n          storage: 10Gi\n        journal:\n          storage: 1Gi\n          labelSelector:\n            matchLabels:\n              app: \"my-app\"\n        logs:\n          storage: 500M\n          storageClass: standard\n    podAntiAffinityTopologyKey: nodeId\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/e2e-az-name\n            operator: In\n            values:\n            - e2e-az1\n            - e2e-az2\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n  additionalMongodConfig:\n    net:\n      ssl:\n        mode: requireSSL\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  service: my-service\n  type: ShardedCluster\n\n  ## Please Note: The default Kubernetes cluster name is\n  ## `cluster.local`.\n  ## If your cluster has been configured with another name, you can\n  ## specify it with the `clusterDomain` attribute.\n\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n\n  persistent: true\n  configSrvPodSpec:\n    cpu: '0.5'\n    memory: 512M\n\n    # if \"persistence\" element is omitted then Operator uses the\n    # default size (5Gi) for mounting single Persistent Volume\n\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n  mongosPodSpec:\n    cpu: '0.8'\n    memory: 1Gi\n    podAntiAffinityTopologyKey: rackId\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n  shardPodSpec:\n    cpu: '0.6'\n    memory: 3Gi\n    persistence:\n      multiple:\n        # if the child of \"multiple\" is omitted then the default size will be used.\n        # 16GB for \"data\", 1GB for \"journal\", 3GB for \"logs\"\n        data:\n          storage: 20Gi\n        logs:\n          storage: 4Gi\n          storageClass: standard\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n  additionalMongodConfig:\n    net:\n      ssl:\n        mode: requireSSL\n...\n"
                }
            ],
            "preview": "The \ncreates   from specification files that you\nwrote.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/create-project-using-configmap",
            "title": "Create One Project using a ConfigMap",
            "headings": [
                "Considerations",
                "Prerequisites",
                "Kubernetes",
                "Create One Project Using a ConfigMap",
                "Configure kubectl to default to your namespace.",
                "Invoke the following command to create a ConfigMap.",
                "Invoke the following  command to verify your .",
                "Connect to HTTPS-enabled Ops Manager Using a Custom CA",
                "Create a ConfigMap for the Certificate Authority certificate.",
                "Copy the highlighted section of the following example ConfigMap.",
                "Add the highlighted section to your project's ConfigMap.",
                "Specify the TLS settings",
                "Save your updated ConfigMap.",
                "Invoke the  command to verify your .",
                "Next Steps"
            ],
            "paragraphs": "The   uses a     to create or link your\n   Project . To create a\n  ConfigMap, you need to edit a few lines of the\n example ConfigMap    file and apply\nthe ConfigMap. Starting in   version 1.3.0, you can only have one MongoDB\nresource per project. To learn how to deploy a MongoDB resource\nin your project, see  Deploy a MongoDB Database Resource . Kubernetes version 1.11 or later or Openshift version\n3.11 or later.  version 0.11 or later\n installed . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Key Type Description Example <configmap-name> string Name of the    . Resource names must be 44 characters or less. metadata.name  documentation on  names .\nThis name must follow  RFC1123  naming\nconventions, using only lowercase alphanumeric\ncharacters, '-' or '.', and must start and end with an\nalphanumeric character. myconfigmap baseUrl string  to your   including the   and port\nnumber. If you deploy   with the   and   will\nmanage MongoDB database resources deployed  outside  of the  \ncluster it's deployed to, you must set  data.baseUrl  to the same\nvalue of the\n spec.configuration.mms.centralUrl \nsetting in the   resource specification. If you're using  , set the  data.baseUrl  value\nto  https://cloud.mongodb.com . https://ops.example.com:8443 projectName string Label for your  \n Project . The   creates the   project if it does\nnot exist. If you omit the  projectName , the  \ncreates a project with the same name as your   resource. If you need or want to use an existing project, you can find\nthe  projectName  by clicking the  All Clusters \nlink at the top left of the screen, then either search by\nname in the  Search  box or scroll to find the\nname in the list. Each card in this list represents the\ncombination of one  Organization  and  Project . Development orgId string 24 character hex string that uniquely identifies your\nMongoDB  Organization .\nYou can find the  orgId  in your    : Click the  Context  menu. Select your Organization. View the current   in your browser and copy the value\ndisplayed in the  <orgId>  placeholder below: This field is  optional . If you omit the  orgId ,\n  creates an Organization called  projectName \nthat contains a project also called  projectName . You must have the  Organization project Creator \nrole to create a new project\n within an existing organization . If you set this value, it can be for a  \norganization only. If you try to use an Atlas\norganization, the   may not work as\nintended. This command returns a ConfigMap description in the shell: You might have chosen to use your own   certificate to enable\n  for your   instance. If you used a custom certificate,\nyou need to add the CA that signed that custom certificate to the\n . To add your custom CA, complete the following: The   requires the root   certificate of the\nCertificate Authority that issued the   host's certificate. Run\nthe following command to create a   containing the root\nCA certificate in the same namespace of your database pods: The   requires that the certificate is named\n mms-ca.crt  in the ConfigMap. Invoke the following command to edit your project's ConfigMap in\nthe default configured editor: Paste the highlighted section in the example   at\nthe end of the project ConfigMap. Change the following   keys: Key Type Description Example sslMMSCAConfigMap string Name of the   created in the first step\ncontaining the root   certificate used to sign the\n  host's certificate. This mounts the CA certificate\nto the   and database resources. my-root-ca sslRequireValidMMSServerCertificates boolean Forces the Operator to require a valid   certificate\nfrom  . The value must be enclosed in single quotes or the\noperator will throw an error. 'true' This command returns a ConfigMap description in the shell:  defaults to an empty namespace if you do not specify the\n -n  option, resulting in deployment failures. The\n ,  , and  s should run in the\nsame unique namespace. Now that you created your ConfigMap,  Create Credentials for the   before\nyou start  deploying MongoDB resources .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap <configmap-name> \\\n  --from-literal=\"baseUrl=<myOpsManagerURL>\" \\\n  --from-literal=\"projectName=<myOpsManagerProjectName>\" \\ #Optional\n  --from-literal=\"orgId=<orgID>\" #Optional"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe configmaps <configmap-name>"
                },
                {
                    "lang": "sh",
                    "value": "Name:           <configmap-name>\nNamespace:      <namespace>\nLabels:         <none>\nAnnotations:    <none>\n\nData\n====\nbaseUrl:\n----\n<myOpsManagerURL>\nEvents:  <none>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n <namespace> create configmap <root-ca-configmap-name> \\\n  --from-file=mms-ca.crt"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: <my-configmap>\n  namespace: <my-namespace>\ndata:\n  projectName: <my-ops-manager-project-name>\n  orgId: <org-id> # Optional\n  baseUrl: https://<my-ops-manager-URL>\n\n"
                },
                {
                    "lang": "yaml",
                    "value": "  sslMMSCAConfigMap: <root-ca-configmap-name>\n  sslRequireValidMMSServerCertificates: \u2018true\u2019\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl edit configmaps <my-configmap> -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe configmaps <my-configmap> -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Name:           <my-configmap>\nNamespace:      <namespace>\nLabels:         <none>\nAnnotations:    <none>\n\nData\n====\nsslMMSCAConfigMap:\n----\n<root-ca-configmap-name>\nsslRequireValidMMSServerCertificates:\n----\ntrue\nEvents:  <none>"
                }
            ],
            "preview": "The  uses a   to create or link your\n Project. To create a\n ConfigMap, you need to edit a few lines of the\nexample ConfigMap  file and apply\nthe ConfigMap.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/resize-pv-storage",
            "title": "Resize Storage for One Database Resource",
            "headings": [
                "Prerequisites",
                "Storage Class Must Support Resizing",
                "Procedure",
                "Create or identify a persistent .",
                "Insert data to the database that the resource serves.",
                "Patch each persistence volume.",
                "Remove the StatefulSets.",
                "Update the database resource with a new storage value.",
                "Update the pods in a rolling fashion.",
                "Validate data exists on the updated ."
            ],
            "paragraphs": "Make sure the   and volume plugin provider that the  \nuse supports resize: If you don't have a StorageClass that supports resizing, ask your  \nadministrator to help. Use an existing database resource or create a new one with persistent\nstorage. Wait until the peristent volume gets to the  Running \nstate. A database resource with persistent storage would include: Start   in the   cluster. Insert data into the  test  database. Invoke the following commands for the entire replica set: Wait until each   gets to the following condition: Delete a   resource. This step removes the   only. The pods remain\nunchanged and running. Update the disk size. Open your preferred text editor and make\nchanges similar to this example: To update the disk size of the replica set to 2 GB, change the\n storage  value in database resource specification: Recreate a   resource with the new volume size. Wait until this StatefulSet achieves the  Running  state. Invoke the following command: The new pods mount the resized volume. If the   were reused, the data that you inserted in  Step\n2  can be found on the databases stored in  :",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl patch storageclass/<my-storageclass> --type='json' \\\n        -p='[{\"op\": \"add\", \"path\": \"/allowVolumeExpansion\", \"value\": true }]'"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.0.4\n  project: my-project\n  credentials: my-credentials\n  type: ReplicaSet\n  podSpec:\n    memory: 300M\n    persistence:\n      single:\n        storage: 1G"
                },
                {
                    "lang": "sh",
                    "value": "$kubectl exec -it <my-replica-set>-0 \\\n         /var/lib/mongodb-mms-automation/mongodb-linux-x86_64-4.0.4/bin/mongo"
                },
                {
                    "lang": "javascript",
                    "value": "<my-replica-set>:PRIMARY> use test\n\nswitched to db test\n\n<my-replica-set>:PRIMARY> db.tmp.insert({\"foo\":\"bar\"})\n\nWriteResult({ \"nInserted\" : 1 })"
                },
                {
                    "lang": "sh",
                    "value": "kubectl patch pvc/\"data-<my-replica-set>-0\" -p='{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"2Gi\"}}}}'\nkubectl patch pvc/\"data-<my-replica-set>-1\" -p='{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"2Gi\"}}}}'\nkubectl patch pvc/\"data-<my-replica-set>-2\" -p='{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"2Gi\"}}}}'"
                },
                {
                    "lang": "yaml",
                    "value": "- lastProbeTime: null\n  lastTransitionTime: \"2019-08-01T12:11:39Z\"\n  message: Waiting for user to (re-)start a pod to finish file\n           system resize of volume on node.\n  status: \"True\"\n  type: FileSystemResizePending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete sts --cascade=false <my-replica-set>"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.0.4\n  project: my-project\n  credentials: my-credentials\n  type: ReplicaSet\n  podSpec:\n    memory: 300M\n    persistence:\n      single:\n        storage: 2G"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f my-replica-set-vol.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl rollout restart sts <my-replica-set>"
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl exec -it <my-replica-set>-1 \\\n          /var/lib/mongodb-mms-automation/mongodb-linux-x86_64-4.0.4/bin/mongo"
                },
                {
                    "lang": "javascript",
                    "value": "<my-replica-set>:PRIMARY> use test\nswitched to db test\n\n<my-replica-set>:PRIMARY> db.tmp.count()\n1"
                }
            ],
            "preview": "Make sure the  and volume plugin provider that the \nuse supports resize:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/migrate-to-single-resource",
            "title": "Migrate to One Resource per Project (Required for Version 1.3.0)",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Create a new ConfigMap for each MongoDB resource in the project.",
                "Update MongoDB resource objects.",
                "Update MongoDB user objects.",
                "Delete the original project ConfigMap.",
                "(Optional) Remove Orphaned Clusters from ."
            ],
            "paragraphs": "Starting in   version 1.3.0, you can only have one MongoDB\nresource per project. This document explains how to migrate existing\nprojects which have multiple MongoDB resources into configurations with\na single resource per project. Before completing this procedure, ensure that you have upgraded your\n  to version 1.3.0. For upgrade instructions, see\n Upgrade from Operator Version 0.10 or Later . Complete the following steps for each project that contains multiple\nMongoDB resources: To associate each MongoDB resource with a single project, each\nresource must have a distinct  . The new ConfigMaps: All other fields can remain the same as the original project\nConfigMap. Invoke the following command for each ConfigMap to apply\nthem to  : To learn more about creating a project using a ConfigMap, see\n Create One Project using a ConfigMap . Must have unique  projectName  fields. Cannot contain the  credentials  or  authenticationMode \nfields. For each MongoDB resource in the project: If  X.509 authentication  is enabled, add the\nfollowing fields to the    : Field Type Description spec.security.authentication object Contains authentication specifications for the\ndeployment. spec.security.authentication.enabled boolean Specifies whether authentication is enabled for the\ndeployment. Set this value to  true . spec.security.authentication.modes array Specifies supported authentication mechanisms for the\ndeployment. Set this value to  [\"X509\"] If internal cluster authentication is enabled, set\n spec.security.authentication.internalCluster  to  X509 . Add the  spec.opsManager.configMapRef.name  field to the\n    and set the value to the  metadata.name  value\nof the corresponding ConfigMap you created in step 1. Remove the  spec.project  field from the resource object. Invoke the following command for each resource object to apply\nthe updated configuration(s). When you apply a new configuration,\nthe Operator creates a new project in   containing the\ndeployment from the corresponding MongoDB resource. All\ndata on the resource database remains the same after the migration. For each  MongoDB user  resource: Remove the  spec.project   field. Add the  spec.mongodbResourceRef.name  field and set the value\nto the name of the relevant MongoDB resource in the same namespace. This may require duplicating your  MongoDBUser  resource if\nyou wish to have the same user in multiple clusters. Invoke the following command to delete the original project ConfigMap\nfrom your   namespace: After reconfiguring your deployments to exist in dedicated clusters,\nyou may have clusters remaining in the original project which are no\nlonger managed by the  . You can remove these clusters if\nyou wish. Removing clusters will delete their historical backups and\nmonitoring data.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <myconfigmap.yaml>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <configuration>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete -f <configMap>.yaml"
                }
            ],
            "preview": "Before completing this procedure, ensure that you have upgraded your\n to version 1.3.0. For upgrade instructions, see\nUpgrade from Operator Version 0.10 or Later.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/manage-database-users-x509",
            "title": "Manage Database Users Using X.509 Authentication",
            "headings": [
                "Prerequisites",
                "Add a Database User",
                "Configure kubectl to default to your namespace.",
                "Copy the following example .",
                "Open your preferred text editor and paste the example ConfigMap into a new text file.",
                "Change the five highlighted lines.",
                "Add any additional roles for the user to the ConfigMap.",
                "Create the user.",
                "View the newly created user in .",
                "Delete a Database User"
            ],
            "paragraphs": "The   supports managing database users for deployments\nrunning with   and X.509 internal cluster authentication enabled. The   does not support other authentication mechanisms\nin deployments it creates. In an Operator-created deployment, you\ncannot use   to: After enabling X.509 authentication, you can add X.509 users using the   interface or the  . Add other authentication methods to users. Manage users  not  using X.509 authentication. Before managing database users, you must deploy a\n replica set  or\n sharded cluster  with   and X.509\nenabled. If you need to generate X.509 certificates for your MongoDB users,\nsee  Generate X.509 Client Certificates . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Use the following table to guide you through changing the highlighted\nlines in the ConfigMap: Key Type Description Example metadata.name string The name of the database user resource. Resource names must be 44 characters or less. mms-user-1 spec.username string The subject line of the x509 client certificate signed\nby the     (Kube CA). To get the subject line of the X.509 certificate, run the\nfollowing command: The username must comply with the\n RFC 2253 \nLDAPv3 Distinguished Name standard. CN=mms-user,U=My Organizational Unit,O=My Org,L=New York,ST=New York,C=US spec.opsManager.configMapRef.name string The name of the project containing the MongoDB database\nwhere user will be added. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. my-project spec.roles.db string The database the  role  can act on. admin spec.mongodbResourceRef.name string The name of the  MongoDB resource  to\nwhich this user is associated. my-resource spec.roles.name string The name of the  role  to grant the database\nuser. The role name can be any\n built-in MongoDB role  or\n custom role  that exists\nin  . readWriteAnyDatabase You may grant additional roles to this user using the format defined\nin the following example: Invoke the following   command to create your database user: You can view the newly-created user in  : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. To delete a database user, pass the  metadata.name  from the user\n  to the following command:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: <resource-name>\nspec:\n  username: <rfc2253-subject>\n  db: \"$external\"\n  mongodbResourceRef:\n    name: '<MongoDB-Resource-name>'\n  roles:\n    - db: <database-name>\n      name: <role-name>\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "openssl x509 -noout \\\n  -subject -in <my-cert.pem> \\\n  -nameopt RFC2253"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: mms-user-1\nspec:\n  username: CN=mms-user,U=My Organizational Unit,O=My Org,L=New York,ST=New York,C=US\n  project: my-project\n  db: \"$external\"\n  roles:\n    - db: admin\n      name: backup\n    - db: admin\n      name: restore\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mdbu <metadata.name>"
                }
            ],
            "preview": "The  supports managing database users for deployments\nrunning with  and X.509 internal cluster authentication enabled.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/k8s-operator-om-specification",
            "title": "Ops Manager Resource Specification",
            "headings": [
                "Example",
                "Required  Resource Settings",
                "Optional  Resource Settings"
            ],
            "paragraphs": "The \ncreates a containerized   deployment from specification files\nthat you write. After you create or update an   resource\nspecification, you direct   to apply this specification to\nyour   environment.   creates the services and custom\n  resources that   requires, then deploys   and its\nbacking application database in containers in your   environment. Each   resource uses an   specification in\n YAML (Yet Another Markup Language)  to define the characteristics\nand settings of the deployment. The following example shows a resource specification for an  \ndeployment: This section describes settings that you must use for all  \nresources. Type : string Required . Version of the MongoDB   resource schema. Type : string Required . Kind of MongoDB Kubernetes resource to create. Set this\nto  MongoDBOpsManager . Type : string Required . Name of the MongoDB   resource you are creating. Resource names must be 44 characters or less. Type : number Required . Number of   instances to run in parallel. The minimum accepted value is  1 . For high availability, set this value to more than  1 . Multiple\n  instances can read from the same Application\nDatabase, ensuring failover if one instance is unavailable and\nenabling you to update the   resource without downtime. Type : number Required . Version of   that you want to install\non this MongoDB   resource. Type : string Required . Name of the     you created for\nthe   admin user. When you deploy the   resource,\n  creates a user with these credentials. The admin user is granted the\n Global Owner \nrole. Type : integer Required . Number of members in the  Application Database  replica set.  resources can use the following settings: Type : collection   Application Database \nresource definition. The following settings from the\n replica set  resource specification are\noptional: spec.applicationDatabase. spec.persistent spec.applicationDatabase. spec.logLevel spec.applicationDatabase. spec.featureCompatibilityVersion spec.applicationDatabase.podSpec. spec.podSpec.cpu spec.applicationDatabase.podSpec. spec.podSpec.cpuRequests spec.applicationDatabase.podSpec. spec.podSpec.memory spec.applicationDatabase.podSpec. spec.podSpec.memoryRequests spec.applicationDatabase.podSpec. spec.podSpec.persistence.single spec.applicationDatabase.podSpec.persistence.multiple. spec.podSpec.persistence.multiple.data spec.applicationDatabase.podSpec.persistence.multiple. spec.podSpec.persistence.multiple.journal spec.applicationDatabase.podSpec.persistence.multiple. spec.podSpec.persistence.multiple.logs spec.applicationDatabase.podSpec. spec.podSpec.podAffinity spec.applicationDatabase.podSpec. spec.podSpec.podAntiAffinityTopologyKey spec.applicationDatabase.podSpec. spec.podSpec.nodeAffinity spec.applicationDatabase.version Type : string Name of the  secret  that contains the\npassword for the   database user  mongodb-ops-manager .\n  uses this password to  authenticate to the Application\nDatabase . Type : string Name of the field in the  secret  that\ncontains the password for the   database user\n mongodb-ops-manager .   uses this password to\n authenticate to the Application Database . The default value is  password . Type : number Version of MongoDB that is installed on the  \n Application Database . Default value is  4.2.2-ent . To deploy   inside   without an Internet connection,\nomit this setting or leave the value empty. The  \ninstalls the  bundled MongoDB Enterprise  version 4.2.2 by default. If you update this value to a later version, consider setting\n spec.featureCompatibilityVersion  to give yourself the\noption to downgrade if necessary. Type : boolean Flag that enables Backup for your   resource. When set to\n false , Backup is disabled. Default value is  true . Type : collection Configuration settings for the  head database .\n  creates a   with the specified configuration. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum size of   that should be mounted. This value is\nexpressed as an integer followed by a unit of storage in\n  notation. Default value is  30Gi . backup-hardware-requirements If the head database requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage specified in a  . You may create\nthis storage type as a   object before using it in this\n  specification. Make sure to set the    reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a   is removed. Type : collection Required if you enable Backup. Array of  oplog stores  used\nfor Backup. Each item in the array references a MongoDB database\nresource deployed in the   cluster by the  . Type : string Required if you enable Backup. Name of the oplog store. Once specified, do not edit the name of the oplog store. Type : string Required if you enable Backup. Name of the MongoDB database resource that you create to store oplog\nslices. You must deploy this database resource in the same namespace\nas the   resource. If a MongoDB database resource with this name doesn't exist, the\n  resource enters a  Pending  state. The  \nretries every 10 seconds until a MongoDB database resource with this\nname is created. If you enable  SCRAM  authentication on the oplog database, you\nmust: Specify a MongoDB version earlier than v4.0 in the oplog database\nresource definition. Create a MongoDB user resource to connect   to the oplog\ndatabase. Specify the  name \nof the user in the   resource definition. Type : string Required if ``SCRAM`` authentication is enabled on the oplog\nstore database. Name of the MongoDB user resource used to connect to the oplog store\ndatabase. Deploy this user resource in the same\nnamespace as the   resource and with the\n readWriteAnyDatabase  and\n dbAdminAnyDatabase  roles. Type : string Maximum CPU capacity that must be available on a  \n  to host the  backup-daemon . This value maps to the\n limits field \nfor CPU for the created pod. Type : string Minimum CPU capacity for the   being created to host\nthe  backup-daemon . If omitted, this value is set to\n spec.backup.podSpec.cpu . This value maps to the\n requests field \nfor CPU for the created pod. The requested value must be less than or equal to\n spec.backup.podSpec.cpu . Type : string Maximum memory capacity that must be available on a\n    to host the  backup-daemon  on  . This\nvalue is expressed as an integer followed by a unit of memory in\n  notation. This value maps to the\n limits field \nfor memory for the created pod. Type : string Minimum memory capacity for the   being created to host\nthe  backup-daemon . If omitted, this value is set to\n spec.backup.podSpec.memory . This value maps to the\n requests field \nfor memory for the created pod. The requested value must be less than or equal to\n spec.backup.podSpec.memory . Set this value to at least  4.5G . Values of less than  4.5G \nmight result in an error. Type : collection    to place the  backup-daemon    on a\nspecific range of  . A user can isolate \"dev\" and \"testing\" environments to ensure\n  go to   with appropriate labels. Type : collection    to determine if multiple\n backup-daemon    must be co-located with other  . The   documentation for use cases on\n affinity and anti-affinity Type : collection Template \nfor the   pods that the   creates for the the\n backup-daemon . Template values take precedence over values specified in\n spec.backup.podSpec . The   doesn't validate the fields you provide\nin  spec.backup.podSpec.podTemplate . Type : collection Metadata for the   pods that the   creates for the\n backup-daemon . To review which fields you can add to\n spec.backup.podSpec.podTemplate.metadata , see\nthe  Kubernetes documentation . Type : collection Specifications of the   pods that the   creates for\nthe  backup-daemon . To review which fields you can add to\n spec.backup.podSpec.podTemplate.spec , see the\n Kubernetes documentation . When you add containers to\n spec.backup.podSpec.podTemplate.spec.containers ,\nthe   adds them to the   pod. These containers are\nappended to the  backup-daemon  containers in the pod. Type : string Required if you enable Backup. Name of the   snapshot store. Once specified, do not edit the name of the   snapshot store. Type : string Required if you enable Backup. Name of the MongoDB database resource that you create to store\nmetadata for the   snapshot store. You must deploy this database\nresource in the same namespace as the   resource. If you enable  SCRAM  authentication on this database, you must: Specify a MongoDB version earlier than v4.0 in the database\nresource definition. Create a MongoDB user resource to connect   to the\ndatabase. Specify the\n name  of the\nuser in the   resource definition. Type : string Required if SCRAM authentication is enabled on the metadata\ndatabase of the S3 snapshot store. Name of the MongoDB user resource used to connect to the metadata\ndatabase of the   snapshot store. Deploy this user resource in the\nsame namespace as the   resource and with the\n readWriteAnyDatabase  and\n dbAdminAnyDatabase  roles. Type : string Required if you enable Backup. Name of the secret that contains the  accessKey  and\n secretKey  fields. The  backup-daemon  uses the values\nof these fields as credentials to access your\n    or  -compatible bucket. The  \nsnapshot store can't be configured if the secret is missimg either\nkey. Type : boolean Indicates the style of the bucket endpoint URL. Default value is  true . Value Description Example true Path-style URL s3.amazonaws.com/<bucket> false Virtual-host-style URL <bucket>.s3.amazonaws.com Type : string Required if you enable Backup. URL of the     bucket or  -compatible bucket that hosts the\nsnapshot store. Type : string Required if you enable Backup. Name of the     bucket or  -compatible bucket that hosts\nthe snapshot store. Type : string  assigns each   a  . The   calculates\nthe   for each   using a provided  clusterDomain .\n  does not provide an   to query these hostnames. Type : string  assigns each   a  . The   calculates\nthe   for each   using a provided  clusterName .  \ndoes not provide an   to query these hostnames. Use  spec.clusterDomain  instead. Type : collection  configuration properties.\nSee  Ops Manager Configuration Settings  for property names and descriptions.\nEach property takes a value of type  string . If   will manage MongoDB resources deployed outside of the\n  cluster it's deployed to, you must add the  mms.centralUrl \nsetting to  spec.configuration . Set the value to the URL by which   is exposed outside of the\n  cluster. Type : collection Configuration object that enables external connectivity to  .\nIf provided, the   creates a    service  that allows traffic\noriginating from outside of the   cluster to reach the  \napplication. If not provided, the   does not create a   service.\nYou must create one manually or use a third-party solution that\nenables you to route external traffic to the   application in\nyour   cluster. Type : string The   service  ServiceType \nthat exposes   outside of  . Required  if  spec.externalConnectivity.type  is\npresent. Accepted values are:  LoadBalancer  and  NodePort .\n LoadBalancer  is recommended if your cloud provider supports it.\nUse  NodePort  for local deployments. Type : integer If  spec.externalConnectivity.type  is  NodePort , the\nport on the   service from which external traffic is routed to\nthe   application. If  spec.externalConnectivity.type  is  LoadBalancer ,\nthe load balancer resource that your cloud provider creates routes\ntraffic to this port on the   service. You don't need to provide\nthis value.   uses an open port within the default range and\nhandles internal traffic routing appropriately. In both cases, if this value is not provided, the   service\nroutes traffic from an available port within the following default\nrange to the   application:  30000 - 32767 . You must configure your network's firewall to allow traffic over\nthis port. Type : string The IP address the  LoadBalancer    service uses when the\n  creates it. This setting can only be used if your cloud provider supports it and\n spec.externalConnectivity.type  is  LoadBalancer . To\nlearn more about the\n Type LoadBalancer , see the\n  documentation. Type : string Routing policy for external traffic to the     service.\nThe service routes external traffic to node-local or cluster-wide\nendpoints depending the value of this setting. Accepted values are:  Cluster  and  Local . To learn which of\nvalues meet your requirements, see  Source IPs in Kubernetes  in the   documentation. If you select  Cluster , the  Source-IP  of your clients are\nlost during the network hops that happen at the  \nnetwork boundary. Type : collection Key-value pairs that allow you to provide cloud provider-specific\nconfiguration settings. To learn more about  Annotations  and\n TLS support on AWS , see the\n  documentation. Type : string Maximum CPU capacity that must be available on a  \n  to host   on  . This value maps to the\n limits field \nfor CPU for the created pod. Type : string Minimum CPU capacity for the   being created to host\n  on  . If omitted, this value is set to\n spec.podSpec.cpu . This value maps to the\n requests field \nfor CPU for the created pod. The requested value must be less than or equal to\n spec.podSpec.cpu . Type : string Maximum memory capacity that must be available on a\n    to host   on  . This value is\nexpressed as an integer followed by a unit of memory in\n  notation. This value maps to the\n limits field \nfor memory for the created pod. If   on   requires 6 gigabytes of memory, set\nthis value to  6G . Type : string Minimum memory capacity for the   being created to host\n  on  . If omitted, this value is set to\n spec.podSpec.memory . This value maps to the\n requests field \nfor memory for the created pod. The requested value must be less than or equal to\n spec.podSpec.memory . MongoDB recommends setting this value to at least  5G . Type : string Sets a   to spread    \nto different locations. A location can be a single node, rack, or\nregion. This key defines which node\n label \nis used to  determine equal location \nfor nodes. By default,   tries to spread pods across\ndifferent hosts. Type : collection    to place     on a\nspecific range of  . A user can isolate \"dev\" and \"testing\" environments to ensure\n  go to   with appropriate labels. Type : collection    to determine if multiple\n    must be co-located with other  . The   documentation for use cases on\n affinity and anti-affinity Type : collection Template \nfor the   pods that the   creates for   on  . Template values take precedence over values specified in\n spec.podSpec . The   doesn't validate the fields you provide\nin  spec.podSpec.podTemplate . Type : collection Metadata for the   pods that the   creates for   on\n . To review which fields you can add to\n spec.podSpec.podTemplate.metadata , see the\n Kubernetes documentation . Type : collection Specifications of the   pods that the   creates for\n  on  . To review which fields you can add to\n spec.podSpec.podTemplate.spec , see the\n Kubernetes documentation . When you add containers to\n spec.podSpec.podTemplate.spec.containers ,\nthe   adds them to the   pod. These containers are\nappended to the   containers in the pod.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: om\nspec:\n replicas: 1\n version: 4.2.6\n adminCredentials: ops-manager-admin\n configuration:\n  mms.fromEmailAddr: admin@example.com\n  mms.security.allowCORS: \"false\"\n backup:\n  enabled: true\n  headDB:\n   storage: 30Gi\n   labelSelector:\n    matchLabels:\n     app: my-app\n  opLogStores:\n   - name: oplog1\n     mongodbResourceRef:\n      name: my-oplog-db\n     mongodbUserRef:\n      name: my-oplog-user\n  s3Stores:\n   - name: s3store1\n     mongodbResourceRef:\n      name: my-s3-metadata-db\n     mongodbUserRef:\n      name: my-s3-store-user\n     s3SecretRef:\n       name: my-s3-credentials\n     pathStyleAccessEnabled: true\n     s3BucketEndpoint: s3.region.amazonaws.com\n     s3BucketName: my-bucket\n\n applicationDatabase:\n   passwordSecretKeyRef:\n    name: om-db-user-secret\n    key: password\n   members: 3\n   version: 4.2.2-ent\n   persistent: true\n   podSpec:\n     cpu: 0.25\n"
                }
            ],
            "preview": "Type: string",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/create-operator-credentials",
            "title": "Create Credentials for the ",
            "headings": [
                "Prerequisites",
                "Procedure"
            ],
            "paragraphs": "For the   to create or update   in your  \nProject, you need to store your\n Programmatic API Key  as a  \n . Creating a secret stores authentication credentials so\nonly   can access them. Multiple secrets can exist in the same namespace. Each user should\nhave their own secret. To create credentials for the  , you must: Have or create an  \n Organization . Unlike earlier   versions, use the Operator to\ncreate your   project. The Operator adds additional metadata\nto Projects that it creates to help manage the deployments. Have or generate a\n Programmatic API Key . Grant this new   the  Project Owner  role. Add the   or   block of any hosts that serve the\n  to the\n API Whitelist . To create your   secret: Make sure you have the Public and Private Keys for your desired\n   . Invoke the following   command to create your secret: The  -n  flag limits the   to which this secret\napplies. All MongoDB   resources must be in the same\nnamespace with the   and  . The\n  does not use either the secrets or ConfigMaps. Invoke the following   command to verify your secret: This command returns a secret description in the shell:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl -n <metadata.namespace> \\\n  create secret generic <myCredentials> \\\n  --from-literal=\"user=<publicKey>\" \\\n  --from-literal=\"publicApiKey=<privateKey>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe secrets/<myCredentials> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Name:         <myCredentials>\nNamespace:    <metadata.namespace>\nLabels:       <none>\nAnnotations:  <none>\n\nType:  Opaque\n\nData\n====\npublicApiKey:  31 bytes\nuser:          22 bytes"
                }
            ],
            "preview": "For the  to create or update  in your \nProject, you need to store your\nProgrammatic API Key as a \n. Creating a secret stores authentication credentials so\nonly  can access them.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-replica-set",
            "title": "Deploy a Replica Set",
            "headings": [
                "Prerequisites",
                "Considerations",
                "Deploy a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example to create a new replica set resource.",
                "Change the highlighted settings to your preferred values.",
                "Add any additional accepted settings for a replica set deployment.",
                "Save this replica set config file with a .yaml file extension.",
                "Start your replica set deployment.",
                "Track the status of your replica set deployment.",
                "Enable External Access for a Replica Set",
                "If you haven't done so already, deploy a replica with the .",
                "Optional: If you already deployed a replica set with the  with  enabled, remove the  for each host in your deployment.",
                "Create a NodePort for each .",
                "Discover the dynamically assigned NodePorts.",
                "Open your replica set resource  file.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Change the highlighted settings to your preferred values.",
                "Confirm the external hostnames and NodePort values in your replica set resource.",
                "Save your replica set config file.",
                "Update and restart your replica set deployment.",
                "Check the status of your deployment.",
                "Retrieve the CSRs for each host and agent in your deployment.",
                "Approve the CSR for each host in your deployment.",
                "Test the connection to the replica set."
            ],
            "paragraphs": "A  replica set  is a group of MongoDB deployments that maintain\nthe same data set. Replica sets provide redundancy and high\navailability and are the basis for all production deployments. To learn more about replica sets, see the\n Replication Introduction  in\nthe MongoDB manual. Use this procedure to deploy a new replica set that   manages.\nAfter deployment, use   to manage the replica set, including such\noperations as adding, removing, and reconfiguring members. You can use the   to deploy MongoDB resources with\n  version 4.0.11 or later and Cloud Manager. At any place in\nthis guide that says  Ops Manager , you can substitute\n Cloud Manager . To deploy a  replica set  using an  , you need to complete the\nfollowing procedures: Install Kubernetes Operator Create One Project using a ConfigMap Create Credentials for the  Starting in   version 1.3.0, you can only have one\nMongoDB resource per project. To learn how to migrate your project to\na single-cluster configuration, see  Migrate to One Resource per Project (Required for Version 1.3.0) . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\ninto a new text file. Key Type Description Example metadata.name string Label for this    replica set   . Resource names must be 44 characters or less. metadata.name Kubernetes documentation on\n names . myproject spec.members integer Number of members of the  replica set . 3 spec.version string Version of MongoDB that this  replica set  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. 3.6.7 string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value  must  match the namespace in which you created the\n   project ConfigMap . The   tracks any changes to the ConfigMap and\nreconciles the state of the  . <myconfigmap> spec.credentials string Name of the     you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . This value  must  match the namespace in which you created the\nsecret and the  name  value you provided for your  \n   Secret . The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. ReplicaSet spec.persistent string Optional. Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. If this value is  true , then\n spec.podSpec.persistence.single  is set to its\ndefault value of  16Gi . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: If you want one   for each  , configure the\n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs Your containers must have permissions to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the deployment item does not fix\nissues with your  , contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n  specification file for a  replica set  deployment: spec.clusterDomain spec.featureCompatibilityVersion spec.logLevel spec.podSpec.cpu spec.podSpec.cpuRequests spec.podSpec.memory spec.podSpec.memoryRequests spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs spec.podSpec.podAffinity spec.podSpec.podAntiAffinityTopologyKey spec.podSpec.nodeAffinity spec.podSpec.podTemplate.metadata spec.podSpec.podTemplate.spec You must set  spec.clusterDomain  if your   cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n  might not function as expected. Invoke the following   command to create your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. Follow the instructions to :ref:` deploy a replica set\n<deploy-replica-set>`. To simplify the configuration, don't enable\n  with the  spec.security.tls.enabled  setting. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Repeat the following command for each host in your deployment to\nremove the  : Remove only the    . Don't remove X.509 or any other\n . Invoke the following commands to create the NodePorts: Discover the dynamically assigned NodePorts: NodePorts range from 30000 to 32767, inclusive. Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. Key Type Necessity Description Example boolean Optional Set this value to  true  to enable   on the MongoDB\ndeployment. By default,   requires hosts to use and accept\n  encrypted connections. To connect to a replica set from outside  , set this\nvalue to  true . true collection Conditional Add this parameter and values if you need your database to be\naccessed outside of  . This setting allows you to provide\ndifferent   settings within the   cluster and to the\n  cluster. The   uses split horizon   for\nreplica set members. This feature allows communication both\nwithin the   cluster and from outside  . You may add multiple external mappings per host. Make sure that each value in this array is unique. Make sure that the number of entries in this array matches\nthe value given in  spec.members . Set the  spec.security.tls.enabled  to  true  to\nenable  . This method to use split horizons requires\nthe Server Name Indication extension of the   protocol. See Setting Confirm that the external hostnames in the\n spec.connectivity.replicaSetHorizons  setting are correct. External hostnames should match the   names of   worker\nnodes. These can be  any  nodes in the   cluster.   do\ninternal routing if the pod is run on another node. Set the ports in  spec.connectivity.replicaSetHorizons  to\nthe NodePort values that you discovered. Invoke the following   command to update and restart your\n replica set : The   creates the MongoDB resources and requests the\n    to approve the database host's certificates. Run the\nfollowing command to verify that the certificates are pending\napproval: The  status  field of the output should resemble the following: If you do not see the  status.message  above, see\n Troubleshooting the   to help diagnose the issue. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the replica set\nexample: If the connection succeeds, you should see: Don't use the --sslAllowInvalidCertificates flag in production. In\nproduction, share the Kubernetes CA files with client tools or\napplications.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmy-secure-rs-0.mongodb                      33s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved, Issued\nmy-secure-rs-1.mongodb                      31s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved, Issued\nmy-secure-rs-2.mongodb                      24s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved, Issued"
                },
                {
                    "lang": "none",
                    "value": "kubectl delete my-secure-rs-0.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl expose pod/<my-replica-set>-0 --type=\"NodePort\" --port 27017\nkubectl expose pod/<my-replica-set>-1 --type=\"NodePort\" --port 27017\nkubectl expose pod/<my-replica-set>-2 --type=\"NodePort\" --port 27017"
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl get svc | grep <my-replica-set>\n<my-replica-set>-0                      NodePort    172.30.39.228    <none>        27017:30907/TCP              16m\n<my-replica-set>-1                      NodePort    172.30.185.136   <none>        27017:32350/TCP              16m\n<my-replica-set>-2                      NodePort    172.30.84.192    <none>        27017:31185/TCP              17m\n<my-replica-set>-svc                    ClusterIP   None             <none>        27017/TCP                    38m"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  type: ReplicaSet\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:10017\"\n      - \"example-website\": \"web2.example.com:10017\"\n      - \"example-website\": \"web3.example.com:10017\"\n..."
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:30907\"\n      - \"example-website\": \"web2.example.com:32350\"\n      - \"example-website\": \"web3.example.com:31185\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "status:\n  lastTransition: 2019-05-01T15:36:59Z\n  message: Not all certificates have been approved by Kubernetes CA\n  phase: Failed\n  type: \"\"\n  version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmy-secure-rs-0.mongodb                      33s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-1.mongodb                      31s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-2.mongodb                      24s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-rs-0.mongodb\nkubectl certificate approve my-secure-rs-1.mongodb\nkubectl certificate approve my-secure-rs-2.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host <my-replica-set>/web1.example.com:30907,web2.example.com:32350,web3.example.com:31185 --ssl --sslAllowInvalidCertificates"
                },
                {
                    "lang": "javascript",
                    "value": "MongoDB Enterprise <my-replica-set>:PRIMARY>"
                }
            ],
            "preview": "A replica set is a group of MongoDB deployments that maintain\nthe same data set. Replica sets provide redundancy and high\navailability and are the basis for all production deployments.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/modify-resource-image",
            "title": "Modify  or MongoDB Kubernetes Resource Containers",
            "headings": [
                "Define a Volume Mount for a MongoDB Kubernetes Resource",
                "Tune MongoDB Kubernetes Resource Docker Images with an InitContainer"
            ],
            "paragraphs": "You can modify the containers in the   in which   and\nMongoDB database resources run using the  template  or\n podTemplate  setting that applies to your deployment: To review which fields you can add to a  template  or a\n podTemplate , see the  Kubernetes documentation . When you create containers with a  template  or  podTemplate , the\n  handles container creation differently based on the\n name  you provide for each container in the  containers  array: MongoDB database:  spec.podSpec.podTemplate :  spec.statefulSet.spec.template backup-daemon :  spec.backup.statefulSet.spec.template If the  name  field  matches  the name of the applicable resource\nimage, the   updates the   or MongoDB database\ncontainer in the   to which the  template  or\n podTemplate  applies: :  mongodb-enterprise-ops-manager backup-daemon :  mongodb-backup-daemon MongoDB database:  mongodb-enterprise-database Application Database:  mongodb-enterprise-appdb If the  name  field  does not match  the name of the applicable\nresource image, the   creates a new container in each\n  to which the  template  or  podTemplate  applies. On-disk files in containers in   don't survive container\ncrashes or restarts. Using the  spec.podSpec.podTemplate \nsetting, you can add a  volume mount \nto persist data in a MongoDB database resource for the life of the\n . To create a volume mount for a MongoDB database resource: Update the MongoDB database resource definition to include a volume\nmount for containers in the database pods that the  \ncreates. Use  spec.podSpec.podTemplate  to define a volume mount: Apply the updated resource definition:  Docker images run on Ubuntu and use Ubuntu's default\nsystem configuration. To tune the underlying Ubuntu system\nconfiguration in the   containers, add a privileged\nInitContainer\n init container \nusing one of the following settings: To tune Docker images for a MongoDB database resource container:  adds a privileged InitContainer to each   that the\n  creates using the   definition. Open a shell session to a running container in your database resource\n  and verify your changes. spec.podSpec.podTemplate : add a privileged InitContainer\nto a MongoDB database resource container. spec.statefulSet.spec.template : add a privileged\nInitContainer to an   resource container. MongoDB database resource Docker images use the Ubuntu default\n keepalive  time of  7200 . MongoDB recommends a shorter\n keepalive  time of  120  for database deployments. You can tune the  keepalive  time in the database resource Docker\nimages if you experience network timeouts or socket errors in\ncommunication between clients and the database resources. Update the MongoDB database resource definition to append a\nprivileged InitContainer to the database pods that the\n  creates. Change  spec.podSpec.podTemplate  the  keepalive \nvalue to the recommended value of  120 : Apply the updated resource definition: To follow the previous  keepalive  example, invoke the following\ncommand to get the current  keepalive  value:",
            "code": [
                {
                    "lang": "yaml",
                    "value": "podSpec:\n  podTemplate:\n    spec:\n      containers:\n      - name: mongodb-enterprise-database\n        volumeMounts:\n        - mountPath: </new/mount/path>\n          name: survives-restart\n      volumes:\n      - name: survives-restart\n        emptyDir: {}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-resource-conf>.yaml -n <namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  podSpec:\n    podTemplate:\n      spec:\n        initContainers:\n        - name: \"adjust-tcp-keepalive\"\n          image: \"busybox:latest\"\n          securityContext:\n            privileged: true\n          command: [\"sysctl\", \"-w\", \"net.ipv4.tcp_keepalive_time=120\"]"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-resource-conf>.yaml -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "> kubectl exec -n <namespace> -it <pod-name> -- cat /proc/sys/net/ipv4/tcp_keepalive_time\n\n\n> 120"
                }
            ],
            "preview": "You can modify the containers in the  in which  and\nMongoDB database resources run using the template or\npodTemplate setting that applies to your deployment:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/install-k8s-operator",
            "title": "Install the ",
            "headings": [
                "Prerequisites and Considerations",
                "Procedure",
                "Change to the directory in which you cloned the repository.",
                "Install the  for MongoDB deployments using the following  command:",
                "You can edit the Operator  file to further customize your Operator before installing it.",
                "Install the  using the following  command:",
                "Change to the directory in which you cloned the repository.",
                "Install the  using the following helm command:",
                "Change to the directory in which you cloned the repository.",
                "Use docker to request the files.",
                "Disconnect from the internet.",
                "Install the  with modified pull policy values using the following helm command:",
                "Use docker to request the files on a host connected to the Internet.",
                "Export the  images as .tar archive files:",
                "Copy these .tar files to the host running the  docker daemon.",
                "Import the .tar files into docker.",
                "Install the  with modified pull policy values using the following helm command:",
                "Change to the directory in which you cloned the repository.",
                "Install the  for MongoDB deployments using the following  command:",
                "You can edit the Operator  file to further customize your Operator before installing it.",
                "Install the  using the following  command:",
                "Change to the directory in which you cloned the repository.",
                "Add the name of your <openshift-pull-secret> to the registry.imagePullSecrets setting in the helm_chart/values-openshift.yaml file:",
                "Install the  using the following helm command:",
                "Change to the directory in which you cloned the repository.",
                "Use docker to request the files.",
                "Disconnect from the internet.",
                "Add the name of your <openshift-pull-secret> to the registry.imagePullSecrets setting in the helm_chart/values-openshift.yaml file:",
                "Install the  with modified pull policy values using the following helm command:",
                "Use docker to request the files on a host connected to the Internet.",
                "Export the  images as .tar archive files:",
                "Copy these .tar files to the host running the  docker daemon.",
                "Import the .tar files into docker.",
                "Add the name of your <openshift-pull-secret> to the registry.imagePullSecrets setting in the helm_chart/values-openshift.yaml file:",
                "Install the  with modified pull policy values using the following helm command:",
                "Verify the Installation",
                "Next Steps"
            ],
            "paragraphs": "Before you install the  , make sure you\n plan for your installation : Choose a  deployment topology . Read the  Considerations . Complete the  Prerequisites . This tutorial presumes some knowledge of  , but does link to\nrelevant   documentation where possible. If you are unfamiliar\nwith  , please review that documentation first. The following steps vary depending on how you want to configure your\nenvironment: The following examples assume that you created a  \nusing the default   namespace of  mongodb .\nIf you specified a different label for your namespace when\nyou  created it , change all\nvalues for  metadata.namespace  to that namespace. To change the label for the namespace for the following\ndeployment to  production , edit all values for\n metadata.namespace  in  mongodb-enterprise.yaml : Open your  mongodb-enterprise.yaml  in your preferred\ntext editor. You may need to add one or more of the following\noptions: Environment Variable When to Use OPERATOR_ENV Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . You can set the following pair of values: If  OPERATOR_ENV  is Log Level is set to Log Format is set to dev debug text prod info json WATCH_NAMESPACE Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . You can set the following pair of values: To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the image for an  Ops\nManager resource  is downloaded. Default value is:\n quay.io/mongodb/mongodb-enterprise-ops-manager OPS_MANAGER_IMAGE_PULL_POLICY Pull policy  for the\nimage deployed to an  Ops Manager resource . Accepted values are:  Always ,  IfNotPresent ,  Never Default value is:  Always MANAGED_SECURITY_CONTEXT Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if you want to run the  \nin OpenShift or in a restrictive environment. Default value is  false . If you have not already installed Helm, follow the\ninstructions on  to install it. You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the  values.yaml  file: Setting When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if your cluster manages the\n securityContext  for your   resources. Default value is  false . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the\nHelm Chart: To install the   on a host not connected to the Internet,\nchoose to download its files from: If you have not already installed Helm, follow the\ninstructions on  to install it. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the  values.yaml  file: Setting When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if your cluster manages the\n securityContext  for your   resources. Default value is  false . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the Helm\nChart: Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the  values.yaml  file: Setting When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if your cluster manages the\n securityContext  for your   resources. Default value is  false . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the Helm\nChart: The following examples assume that you created a  \nusing the default   namespace of  mongodb .\nIf you specified a different label for your namespace when\nyou  created it , change all\nvalues for  metadata.namespace  to that namespace. To change the label for the namespace for the following\ndeployment to  production , edit all values for\n metadata.namespace  in  mongodb-enterprise-openshift.yaml : Open your  mongodb-enterprise-openshift.yaml  in your preferred\ntext editor. You must add your  <openshift-pull-secret>  to the\n ServiceAccount  definitions: You may need to add one or more of the following\noptions: Environment Variable When to Use OPERATOR_ENV Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . You can set the following pair of values: If  OPERATOR_ENV  is Log Level is set to Log Format is set to dev debug text prod info json WATCH_NAMESPACE Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . You can set the following pair of values: To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the image for an  Ops\nManager resource  is downloaded. Default value is:\n registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager OPS_MANAGER_IMAGE_PULL_POLICY Pull policy  for the\nimage deployed to an  Ops Manager resource . Accepted values are:  Always ,  IfNotPresent ,  Never Default value is:  Always MANAGED_SECURITY_CONTEXT Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. For OpenShift,  MANAGED_SECURITY_CONTEXT  must always be\n true . Default value is  true . If you have not already installed Helm, follow the\ninstructions on  to install it. You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the\n values-openshift.yaml  file: Setting When to Use registry.imagePullSecrets  that contains the credentials required to pull imagePullSecrets\nfrom the repository. This setting is mandatory for OpenShift installs. You must\neither define it in this file or pass it when you install the\n  using Helm. namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Default value is  true . For OpenShift,  managedSecurityContext  must always be\n true . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the Helm\nChart: To install the   on a host not connected to the Internet,\nchoose to download its files from: If you have not already installed Helm, follow the\ninstructions on  to install it. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the\n values-openshift.yaml  file: Setting When to Use registry.imagePullSecrets  that contains the credentials required to pull imagePullSecrets\nfrom the repository. This setting is mandatory for OpenShift installs. You must\neither define it in this file or pass it when you install the\n  using Helm. namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Default value is  true . For OpenShift,  managedSecurityContext  must always be\n true . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the Helm\nChart: Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the\n values-openshift.yaml  file: Setting When to Use registry.imagePullSecrets  that contains the credentials required to pull imagePullSecrets\nfrom the repository. This setting is mandatory for OpenShift installs. You must\neither define it in this file or pass it when you install the\n  using Helm. namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Default value is  true . For OpenShift,  managedSecurityContext  must always be\n true . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the Helm\nChart: To verify that the   installed correctly, run the\nfollowing command and verify the output: By default, deployments exist in the  mongodb  namespace. If the\nfollowing error message appears, ensure you use the correct\nnamespace: To troubleshoot your  , see\n Review Logs from the  . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . After installing the  , you can: Create an instance of Ops Manager Configure the Kubernetes Operator to deploy MongoDB resources",
            "code": [
                {
                    "lang": "yaml",
                    "value": "##---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-operator\n  namespace: production\n##---\n# Source: mongodb-enterprise-operator/templates/operator.yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-enterprise-operator\n  namespace: production\n\n---\n# Example truncated\n---\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: OPERATOR_ENV\nspec.template.spec.containers.name.env.value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPERATOR_ENV\n          value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: WATCH_NAMESPACE\nspec.template.spec.containers.name.env.value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: WATCH_NAMESPACE\n          value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMANAGED_SECURITY_CONTEXT\nspec.template.spec.containers.name.env.value:\nfalse"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MANAGED_SECURITY_CONTEXT\n          value: true"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f mongodb-enterprise.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template helm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set namespace=<testNamespace> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "docker pull quay.io/mongodb/mongodb-enterprise-operator:<op-version>; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-database:<op-version>; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-ops-manager:<om-version>-operator<op-version>"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set namespace=<testNamespace> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "docker pull quay.io/mongodb/mongodb-enterprise-operator:<op-version>; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-database:<op-version>; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-ops-manager:<om-version>-operator<op-version>"
                },
                {
                    "lang": "sh",
                    "value": "docker save quay.io/mongodb/mongodb-enterprise-operator:<op-version> -o mongodb-enterprise-operator.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-database:<op-version> -o mongodb-enterprise-database.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-ops-manager:<om-version>-operator<op-version> -o mongodb-enterprise-ops-manager.tar"
                },
                {
                    "lang": "sh",
                    "value": "docker load -i mongodb-enterprise-operator.tar; \\\ndocker load -i mongodb-enterprise-database.tar; \\\ndocker load -i mongodb-enterprise-ops-manager.tar"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set namespace=<testNamespace> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "##---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n  namespace: production\n##---\n# Source: mongodb-enterprise-operator/templates/operator.yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: enterprise-operator\n  namespace: production\n\n---\n# Example truncated\n---\n..."
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-appdb\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-database-pods\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: OPERATOR_ENV\nspec.template.spec.containers.name.env.value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPERATOR_ENV\n          value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: WATCH_NAMESPACE\nspec.template.spec.containers.name.env.value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: WATCH_NAMESPACE\n          value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMANAGED_SECURITY_CONTEXT\nspec.template.spec.containers.name.env.value:\ntrue"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MANAGED_SECURITY_CONTEXT\n          value: true"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f mongodb-enterprise-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "registry:\n# The pull secret must be specified\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "helm template helm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "registry:\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# OpenShift manages security context on its own\nmanagedSecurityContext: true"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.imagePullSecrets=<openshift-pull-secret> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "docker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-operator:<op-version>; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-database:<op-version>; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager:<om-version>-operator<op-version>"
                },
                {
                    "lang": "sh",
                    "value": "registry:\n# The pull secret must be specified\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set registry.imagePullSecrets=<openshift-pull-secret> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "registry:\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# OpenShift manages security context on its own\nmanagedSecurityContext: true"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set registry.imagePullSecrets=<openshift-pull-secret> \\\n--set namespace=<testNamespace> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "docker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-operator:<op-version>; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-database:<op-version>; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager:<om-version>-operator<op-version>"
                },
                {
                    "lang": "sh",
                    "value": "docker save registry.connect.redhat.com/mongodb/mongodb-enterprise-operator:<op-version> -o mongodb-enterprise-operator.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-database:<op-version> -o mongodb-enterprise-database.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager:<om-version>-operator<op-version> -o mongodb-enterprise-ops-manager.tar"
                },
                {
                    "lang": "sh",
                    "value": "docker load -i mongodb-enterprise-operator.tar; \\\ndocker load -i mongodb-enterprise-database.tar; \\\ndocker load -i mongodb-enterprise-ops-manager.tar"
                },
                {
                    "lang": "sh",
                    "value": "registry:\n# The pull secret must be specified\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set registry.imagePullSecrets=<openshift-pull-secret> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "registry:\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# OpenShift manages security context on its own\nmanagedSecurityContext: true"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set registry.imagePullSecrets=<openshift-pull-secret> \\\n--set namespace=<testNamespace> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe deployments mongodb-enterprise-operator -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Error from server (NotFound): deployments.apps \"mongodb-enterprise-operator\" not found"
                }
            ],
            "preview": "Before you install the , make sure you\nplan for your installation:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/upgrade-mdb-version",
            "title": "Upgrade MongoDB Version and FCV",
            "headings": [
                "Example"
            ],
            "paragraphs": "You can upgrade the major, minor, and/or feature compatibility versions\nof your MongoDB resource. These settings are configured in your\nresource's  . To upgrade your resource's major and/or minor versions, set the\n spec.version  setting to the desired MongoDB version. To modify your resource's\n feature compatibility version ,\nset the  spec.featureCompatibilityVersion  setting to the\ndesired version. If you update  spec.version  to a later version, consider setting\n spec.featureCompatibilityVersion  to the current working\nMongoDB version to give yourself the option to downgrade if\nnecessary. To learn more about feature compatibility, see\n setFeatureCompatibilityVersion  in the MongoDB Manual. Consider the following   for a standalone resource: This resource has a MongoDB version of \u200b. The\nfollowing steps upgrade the deployment's MongoDB version to\n 4.2.2-ent :  automatically reconfigures your deployment with the new\nspecifications. You can see these changes reflected in the   or\n Cloud Manager  application. Perform the following modifications to the resource's ConfigMap: Set  spec.version  to the desired MongoDB version. Set  spec.featureCompatibilityVersion  to the current\nworking MongoDB version: Setting  featureCompatibilityVersion  to  4.0  disables\n 4.2 features incompatible with MongoDB 4.0 . Reapply the configuration to  :",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone-downgrade\nspec:\n  version: 4.0.14-ent\n  type: Standalone\n  project: my-project\n  credentials: my-credentials\n  persistent: false\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone-downgrade\nspec:\n  version: 4.2.2-ent\n  featureCompatibilityVersion: 4.0\n  type: Standalone\n  project: my-project\n  credentials: my-credentials\n  persistent: false\n..."
                },
                {
                    "lang": "none",
                    "value": "kubectl apply -f <standalone-config>.yaml"
                }
            ],
            "preview": "You can upgrade the major, minor, and/or feature compatibility versions\nof your MongoDB resource. These settings are configured in your\nresource's .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/create-x509-client-certs",
            "title": "Generate X.509 Client Certificates",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Generate a Private Key and Certificate Signing Request",
                "Create a new directory to complete this tutorial.",
                "Enter your newly created directory.",
                "Copy and save the following example JSON.",
                "Generate a key file.",
                "Generate the Certificate Signing Request.",
                "Submit the New CSR to the Kubernetes ",
                "Create a  in Kubernetes.",
                "View your CSRs.",
                "Approve the CSR.",
                "Verify that your certificate has been approved",
                "Obtain the Newly Issued Certificate from the Kubernetes CA",
                "Generate the X.509 certificate from the CSR (Certificate Signing Request).",
                "Concatenate the user private key and Kubernetes certificate.",
                "Connect to the X.509-Enabled MongoDB Deployment",
                "Configure kubectl to default to your namespace.",
                "Copy and save the following example .",
                "Create the X.509 MongoDB user.",
                "Verify your newly created user",
                "Use your X.509 user to connect to the MongoDB deployment"
            ],
            "paragraphs": "The   can deploy MongoDB instances with\n X.509 authentication \nenabled. If X.509 authentication has been enabled for the deployment,\nyou must generate and use an X.509 certificate to connect to the\ndeployment. This new client certificate must be signed by the\nKubernetes   to be accepted by the MongoDB deployment. Use the procedure outlined in this document to: Generate an X.509 certificate. Get that certificate signed by the Kubernetes\n CA (Certificate Authority) . Use the certificate to connect to your X.509-enabled MongoDB\ndeployment. A full description of Transport Layer Security (TLS), Public Key Infrastructure (PKI)\ncertificates, and Certificate Authorities is beyond the scope of this\ndocument. This page assumes prior knowledge of   and\nX.509 authentication. To complete this tutorial, you must have the  \ninstalled. For instructions on installing the  ,\nsee  Install the  . This tutorial assumes you have a MongoDB deployment which\nrequires X.509 authentication. For instructions on deploying\nMongoDB resources, see  Deploy a MongoDB Database Resource . This tutorial uses  CFSSL  to generate X.509 certificates.  CFSSL \nis a certificate generation tool built by\n Cloudflare . For instructions on\ninstalling  CFSSL , refer to the\n CFSSL GitHub page . The user configuration files used in this tutorial are\nstrictly examples. You may need to adjust the values in the\nexamples to suit your deployment's needs. For more\ninformation on formatting user ConfigMaps,\nsee  Manage Database Users . Run the following command to create a new directory for\nthe configuration files used in this tutorial: In the  client-x509-certs-tutorial  directory, save the following\nJSON as   x509_user.json : Run the following command to pass the JSON from the previous step\nto  CFSSL  and generate a key file: You should see output similar to the following: You now have a file called  x509_user_key.json  containing\na new private key. Run the following command to use your  x509_user_key.json  key\nfile to generate a certificate signing request (CSR): This command generates two files: x509_user-key.pem , the private key for the user x509_user.csr , the CSR that represents the user Kubernetes' own certificate authority provides the trusted  \nfor the Kubernetes cluster. You need the  .csr  and  .pem  files\ngenerated in the previous section to request a new certificate from\nKubernetes. Run the following command to create a CSR\nin Kubernetes: Run the following command to view a list of CSRs: You should see an output similar to the following: The CSR remains in  Pending  condition\nuntil Kubernetes approves it. Run the following command to\napprove the certificate: You should see an output similar to the following: Run the following command to verify that the Kubernetes   has\napproved your certificate: You should see an output similar to the following: You can use the new certificate.\nThe  status.certificate  attribute of the CSR\ngenerated in the  previous section \ncontains the certificate. Run the following command to generate the certificate from the\nCSR object to a file called  client.crt : A   client can use the  client.crt \ncertificate to connect to the X.509-enabled MongoDB deployment. You need both the  x509_user-key.pem  and  client.crt  files\nto connect to the deployment. Run the following command to\nconcatenate the two files into the a new  .pem  file: With the client certificate created, you can create a MongoDB user\nand connect to the X.509-enabled deployment. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Save the following ConfigMap as  x509-mongodb-user.yaml : This ConfigMap  .yaml  file describes a  MongoDBUser  custom object. You\ncan use these custom objects to create MongoDB users. In this example, the ConfigMap describes the user as an X.509\nuser that the client can use to connect to MongoDB with the\ncorresponding X.509 certificate. Run the following command to apply the ConfigMap and create the\nX.509 MongoDB user: You should see an output similar to the following: Run the following command to check the state of the  new-x509-user : You should see an output similar to the following: Once you have created your X.509 user, try to connect to the\ndeployment using the mongo Shell: On Kubernetes Pods, the   file is saved in\n /var/run/secrets/kubernetes.io/serviceaccount/ca.crt , which\nis the file location used for the  --sslCAFile  connection\noption.",
            "code": [
                {
                    "lang": "sh",
                    "value": "mkdir client-x509-certs-tutorial"
                },
                {
                    "lang": "sh",
                    "value": "cd client-x509-certs-tutorial"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"names\": [\n    {\"O\": \"organization\"},\n    {\"OU\": \"organizationalunit\"}\n  ],\n  \"CN\": \"my-x509-authenticated-user\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 4096\n  }\n}"
                },
                {
                    "lang": "sh",
                    "value": "cfssl genkey x509_user.json > x509_user_key.json"
                },
                {
                    "lang": "sh",
                    "value": "2019/06/04 18:12:38 [INFO] generate received request\n2019/06/04 18:12:38 [INFO] received CSR\n2019/06/04 18:12:38 [INFO] generating key: rsa-4096\n2019/06/04 18:12:40 [INFO] encoded CSR"
                },
                {
                    "lang": "sh",
                    "value": "cfssljson -f x509_user_key.json -bare x509_user"
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | kubectl apply -f -\napiVersion: certificates.k8s.io/v1beta1\nkind: CertificateSigningRequest\nmetadata:\n  name: x509-user.some-namespace\nspec:\n  groups:\n  - system:authenticated\n  request: $(cat x509_user.csr | base64 | tr -d '\\n')\n  usages:\n  - digital signature\n  - key encipherment\n  - client auth\nEOF"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                         AGE    REQUESTOR                                 CONDITION\nx509-user.some-namespace     1m     system:serviceaccount:some-namespace      Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve x509-user.some-namespace"
                },
                {
                    "lang": "sh",
                    "value": "certificatesigningrequest.certificates.k8s.io/x509-user.some-namespace approved"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                       AGE   REQUESTOR                              CONDITION\nx509-user.some-namespace   45m   system:serviceaccount:some-namespace   Approved,Issued"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr x509-user.some-namespace -o jsonpath='{.status.certificate}' | base64 --decode > client.crt"
                },
                {
                    "lang": "sh",
                    "value": "cat x509_user-key.pem client.crt > x509-full.pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "none",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: new-x509-user\nspec:\n  username: \"CN=my-x509-authenticated-user, OU=organizationalunit, O=organization\"\n  db: \"$external\"\n  mongodbResourceRef:\n    name: '<name of the MongoDB resource>'\n  roles:\n    - db: \"admin\"\n      name: \"clusterAdmin\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f x509-mongodb-user.yaml"
                },
                {
                    "lang": "sh",
                    "value": "mongodbuser.mongodb.com/new-x509-user created"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdbu/new-x509-user -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "NAME            CREATED AT\nnew-x509-user   8m"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host {host} --tls --tlsCAFile /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --tlsCertificateKeyFile x509-full.pem --authenticationMechanism MONGODB-X509 --authenticationDatabase '$external'"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host {host} --ssl --sslCAFile /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --sslPEMKeyFile x509-full.pem --authenticationMechanism MONGODB-X509 --authenticationDatabase '$external'"
                }
            ],
            "preview": "The  can deploy MongoDB instances with\nX.509 authentication\nenabled. If X.509 authentication has been enabled for the deployment,\nyou must generate and use an X.509 certificate to connect to the\ndeployment. This new client certificate must be signed by the\nKubernetes  to be accepted by the MongoDB deployment.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/connect-from-outside-k8s",
            "title": "Connect to a MongoDB Database Resource from Outside Kubernetes",
            "headings": [
                "Prerequisite",
                "Procedure",
                "Open your standalone resource  file.",
                "Copy the highlighted section of this standalone resource.",
                "Paste the copied example section into your existing standalone resource.",
                "Change the highlighted settings to your preferred values.",
                "Save your standalone config file.",
                "Update and restart your standalone deployment.",
                "Discover the dynamically assigned NodePorts.",
                "Test the connection to the standalone.",
                "If you haven't done so already, deploy a replica with the .",
                "Optional: If you already deployed a replica set with the  with  enabled, remove the  for each host in your deployment.",
                "Create a NodePort for each .",
                "Discover the dynamically assigned NodePorts.",
                "Open your replica set resource  file.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Change the highlighted settings to your preferred values.",
                "Confirm the external hostnames and NodePort values in your replica set resource.",
                "Save your replica set config file.",
                "Update and restart your replica set deployment.",
                "Check the status of your deployment.",
                "Retrieve the CSRs for each host and agent in your deployment.",
                "Approve the CSR for each host in your deployment.",
                "Test the connection to the replica set.",
                "Open your sharded cluster resource  file.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Change the highlighted settings to your preferred values.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Discover the dynamically assigned NodePorts.",
                "Test the connection to the sharded cluster."
            ],
            "paragraphs": "The following procedure describes how to connect to a MongoDB resource\ndeployed by   from outside of the   cluster. For your databases to be accessed outside of  , they must run one\nof the following versions of MongoDB: 3.6.17 or later 4.0.15 or later 4.2.3 or later How you connect to a MongoDB resource that the   deployed\nfrom outside of the   cluster depends on the resource. This procedure uses the following example: To connect to your  -deployed MongoDB standalone\nresource from outside of the   cluster: Change the highlighted settings of this   file to match your\ndesired  standalone  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. Key Type Necessity Description Example spec.exposedExternally Boolean Optional Set this value to  true  to allow external services to connect\nto the MongoDB deployment. This results in   creating a\n NodePort service . true Invoke the following   command to update and restart your\n standalone : Discover the dynamically assigned NodePort: The list output should contain an entry similar to the following:  exposes   on port  27017  within the  \ncontainer. The NodePort service exposes the  mongod  via port  30994 .\nNodePorts range from 30000 to 32767, inclusive. To connect to your deployment from outside of the   cluster, run\nthe   command with the external   of a   as the\n --host  flag. If a node in the   cluster has an external   of\n ec2-54-212-23-143.us-west-2.compute.amazonaws.com , you can\nconnect to this standalone instance from outside of the  \ncluster using the following command: To obtain the external  DNS (Domain Name System)  of your\nKubernetes cluster, you can run the following command: This command displays the external DNS in the\n Addresses.ExternalDNS  section of the output. Alternatively, you can output the external DNS directly by running: To connect to your  -deployed MongoDB replica set\nresource from outside of the   cluster: This procedure explains the least complicated way to enable\nexternal connectivity. Other utilities can be used in\nproduction. Follow the instructions to :ref:` deploy a replica set\n<deploy-replica-set>`. To simplify the configuration, don't enable\n  with the  spec.security.tls.enabled  setting. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Repeat the following command for each host in your deployment to\nremove the  : Remove only the    . Don't remove X.509 or any other\n . Invoke the following commands to create the NodePorts: Discover the dynamically assigned NodePorts: NodePorts range from 30000 to 32767, inclusive. Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. Key Type Necessity Description Example boolean Optional Set this value to  true  to enable   on the MongoDB\ndeployment. By default,   requires hosts to use and accept\n  encrypted connections. To connect to a replica set from outside  , set this\nvalue to  true . true collection Conditional Add this parameter and values if you need your database to be\naccessed outside of  . This setting allows you to provide\ndifferent   settings within the   cluster and to the\n  cluster. The   uses split horizon   for\nreplica set members. This feature allows communication both\nwithin the   cluster and from outside  . You may add multiple external mappings per host. Make sure that each value in this array is unique. Make sure that the number of entries in this array matches\nthe value given in  spec.members . Set the  spec.security.tls.enabled  to  true  to\nenable  . This method to use split horizons requires\nthe Server Name Indication extension of the   protocol. See Setting Confirm that the external hostnames in the\n spec.connectivity.replicaSetHorizons  setting are correct. External hostnames should match the   names of   worker\nnodes. These can be  any  nodes in the   cluster.   do\ninternal routing if the pod is run on another node. Set the ports in  spec.connectivity.replicaSetHorizons  to\nthe NodePort values that you discovered. Invoke the following   command to update and restart your\n replica set : The   creates the MongoDB resources and requests the\n    to approve the database host's certificates. Run the\nfollowing command to verify that the certificates are pending\napproval: The  status  field of the output should resemble the following: If you do not see the  status.message  above, see\n Troubleshooting the   to help diagnose the issue. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the replica set\nexample: If the connection succeeds, you should see: Don't use the --sslAllowInvalidCertificates flag in production. In\nproduction, share the Kubernetes CA files with client tools or\napplications. This procedure uses the following example: To connect to your  -deployed MongoDB sharded\ncluster resource from outside of the   cluster: Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. Key Type Necessity Description Example spec.exposedExternally Boolean Optional Set this value to  true  to allow external services to connect\nto the MongoDB deployment. This results in   creating a\n NodePort service . true Key Type Necessity Description Example spec.exposedExternally Boolean Optional Set this value to  true  to allow external services to connect\nto the MongoDB deployment. This results in   creating a\n NodePort service . true boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true collection Optional List of every domain that should be added to   certificates\nto each pod in this deployment. When you set this parameter,\nevery   that the   transforms into a  \ncertificate includes a   in the form  <pod\nname>.<additional cert domain> . true Invoke the following   command to update and restart your\n sharded cluster : Discover the dynamically assigned NodePort: The list output should contain an entry similar to the following:  exposes   on port  27017  within the  \ncontainer. The NodePort service exposes the  mongod  via port  30078 .\nNodePorts range from 30000 to 32767, inclusive. To connect to your deployment from outside of the   cluster, run\nthe   command with the external   of a   as the\n --host  flag. If a node in the   cluster has an external   of\n ec2-54-212-23-143.us-west-2.compute.amazonaws.com , you can\nconnect to this sharded cluster instance from outside of the  \ncluster using the following command: To obtain the external  DNS (Domain Name System)  of your\nKubernetes cluster, you can run the following command: This command displays the external DNS in the\n Addresses.ExternalDNS  section of the output. Alternatively, you can output the external DNS directly by running:",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\nspec:\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true\n  exposedExternally: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\nspec:\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  exposedExternally: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <standalone-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get services -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\n\n<my-standalone>           NodePort    10.102.27.116   <none>        27017:30994/TCP   8m30s"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host ec2-54-212-23-143.us-west-2.compute.amazonaws.com --port 30994"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe nodes"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get nodes -o jsonpath='{ $.items[*].status.addresses[?(@.type==\"ExternalDNS\")].address }'"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmy-secure-rs-0.mongodb                      33s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved, Issued\nmy-secure-rs-1.mongodb                      31s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved, Issued\nmy-secure-rs-2.mongodb                      24s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved, Issued"
                },
                {
                    "lang": "none",
                    "value": "kubectl delete my-secure-rs-0.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl expose pod/<my-replica-set>-0 --type=\"NodePort\" --port 27017\nkubectl expose pod/<my-replica-set>-1 --type=\"NodePort\" --port 27017\nkubectl expose pod/<my-replica-set>-2 --type=\"NodePort\" --port 27017"
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl get svc | grep <my-replica-set>\n<my-replica-set>-0                      NodePort    172.30.39.228    <none>        27017:30907/TCP              16m\n<my-replica-set>-1                      NodePort    172.30.185.136   <none>        27017:32350/TCP              16m\n<my-replica-set>-2                      NodePort    172.30.84.192    <none>        27017:31185/TCP              17m\n<my-replica-set>-svc                    ClusterIP   None             <none>        27017/TCP                    38m"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  type: ReplicaSet\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:10017\"\n      - \"example-website\": \"web2.example.com:10017\"\n      - \"example-website\": \"web3.example.com:10017\"\n..."
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:30907\"\n      - \"example-website\": \"web2.example.com:32350\"\n      - \"example-website\": \"web3.example.com:31185\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "status:\n  lastTransition: 2019-05-01T15:36:59Z\n  message: Not all certificates have been approved by Kubernetes CA\n  phase: Failed\n  type: \"\"\n  version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmy-secure-rs-0.mongodb                      33s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-1.mongodb                      31s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-2.mongodb                      24s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-rs-0.mongodb\nkubectl certificate approve my-secure-rs-1.mongodb\nkubectl certificate approve my-secure-rs-2.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host <my-replica-set>/web1.example.com:30907,web2.example.com:32350,web3.example.com:31185 --ssl --sslAllowInvalidCertificates"
                },
                {
                    "lang": "javascript",
                    "value": "MongoDB Enterprise <my-replica-set>:PRIMARY>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  credentials: my-secret\n  type: ShardedCluster\n  persistent: true\n  exposedExternally: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  credentials: my-secret\n  type: ShardedCluster\n  persistent: true\n  exposedExternally: true\n  security:\n    tls:\n      enabled: true\n      additionalCertificateDomains:\n        - \"additional-cert-test.com\"\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  credentials: my-secret\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  exposedExternally: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  credentials: my-secret\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  exposedExternally: true\n  security:\n    tls:\n      enabled: true\n      additionalCertificateDomains:\n        - \"additional-cert-test.com\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get services -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\n\n<my-sharded cluster>      NodePort    10.106.44.30    <none>        27017:30078/TCP   10s"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host ec2-54-212-23-143.us-west-2.compute.amazonaws.com --port 30078"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe nodes"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get nodes -o jsonpath='{ $.items[*].status.addresses[?(@.type==\"ExternalDNS\")].address }'"
                }
            ],
            "preview": "The following procedure describes how to connect to a MongoDB resource\ndeployed by  from outside of the  cluster.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-standalone",
            "title": "Deploy a Standalone MongoDB Instance",
            "headings": [
                "Prerequisites",
                "Considerations",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the following example standalone  .",
                "Open your preferred text editor and paste the  specification into a new text file.",
                "Configure the settings highlighted in the preceeding step as follows.",
                "Add any additional accepted settings for a Standalone deployment.",
                "Save this file with a .yaml file extension.",
                "Start your Standalone deployment.",
                "Track the status of your standalone deployment."
            ],
            "paragraphs": "You can deploy a  standalone  MongoDB instance for   to\nmanage. Use standalone instances for testing and development.\n Do not  use these deployments for production systems as they lack\nreplication and high availability. For all production deployments\nuse replica sets. To learn about replica sets, see\n Deploy a Replica Set . You can use the   to deploy MongoDB resources with\n  version 4.0.11 or later and Cloud Manager. At any place in\nthis guide that says  Ops Manager , you can substitute\n Cloud Manager . To deploy a  standalone  using an  , you need to complete the\nfollowing procedures: Install Kubernetes Operator Create One Project using a ConfigMap Create Credentials for the  Starting in   version 1.3.0, you can only have one\nMongoDB resource per project. To learn how to migrate your project to\na single-cluster configuration, see  Migrate to One Resource per Project (Required for Version 1.3.0) . To troubleshoot your sharded cluster, see: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : This is a   file that you can modify to meet your desired\nconfiguration. Change the highlighted settings to match your desired\nstandalone configuration. Key Type Description Example metadata.name string Label for this   standalone  . Resource names must be 44 characters or less. metadata.name  documentation on  names . my-project spec.version string Version of MongoDB that is installed on this\nstandalone. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. 4.2.2-ent string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value  must  match the namespace in which you created the\n   project ConfigMap . The   tracks any changes to the ConfigMap and\nreconciles the state of the  . <myproject> spec.credentials string Name of the     you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . This value  must  match the namespace in which you created the\nsecret and the  name  value you provided for your  \n   Secret . The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. Standalone spec.persistent string Optional. If this value is  true , then\n spec.podSpec.persistence.single  is set to its\ndefault value of  16Gi . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: If you want one   for each  , configure the\n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs Your containers must have permissions to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the deployment item does not fix\nissues with your  , contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n  specification file for a Standalone deployment: spec.exposedExternally spec.logLevel spec.featureCompatibilityVersion spec.podSpec.cpu spec.podSpec.cpuRequests spec.podSpec.memory spec.podSpec.memoryRequests spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs spec.podSpec.podAffinity spec.podSpec.nodeAffinity Invoke the following   command to create your standalone: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. Find a Specific Pod Review Logs from Specific Pod",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\nspec:\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <standalone-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "You can deploy a standalone MongoDB instance for  to\nmanage. Use standalone instances for testing and development.\nDo not use these deployments for production systems as they lack\nreplication and high availability. For all production deployments\nuse replica sets. To learn about replica sets, see\nDeploy a Replica Set.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-tls",
            "title": "Secure Deployments using TLS",
            "headings": [
                "General Prerequisites",
                "Configure TLS for a Replica Set",
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the TLS settings for your replica set resource.",
                "Save your replica set config file.",
                "Update and restart your replica set deployment.",
                "Check the status of your deployment.",
                "Retrieve the CSRs for each host and agent in your deployment.",
                "Approve the CSR for each host in your deployment.",
                "Track the status of your deployment.",
                "Configure kubectl to default to your namespace.",
                "Create the  for your TLS certificates.",
                "Create the  to link your  with your deployment.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the TLS settings for your replica set resource using a Custom Certificate Authority.",
                "Save your replica set config file.",
                "Apply your changes to your replica set deployment.",
                "Track the status of your deployment.",
                "Configure TLS for a Sharded Cluster",
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the TLS settings for your sharded cluster resource.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Check the status of your deployment.",
                "Retrieve the CSRs for each host and agent in your deployment.",
                "Approve the CSR for each host in your deployment.",
                "Track the status of your deployment.",
                "Configure kubectl to default to your namespace.",
                "Create the  for your Shards' TLS certificates.",
                "Create the  for your config server's TLS certificates.",
                "Create the  for your mongos server's TLS certificates.",
                "Create the  to link your  with your deployment.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the TLS settings for your sharded cluster resource using a Custom Certificate Authority.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Track the status of your deployment."
            ],
            "paragraphs": "The   can use   certificates to encrypt connections\nbetween: This guide instructs you on how to configure the   to use\n  for its MongoDB instances. To secure your deployment using  , you may use  's  \nor your own  : MongoDB hosts in a replica set or sharded cluster Client applications and MongoDB deployments Before you secure your MongoDB deployment using   encryption,\ncomplete the following: Install the Kubernetes Operator Create Credentials for the Kubernetes Operator Before you secure your replica set using   encryption, complete the\nfollowing: Deploy the Replica Set \nthat you want to secure Deploy the Replica Set \nthat you want to secure Create a   file for each of the following components: PEM file purpose Save File As... Your custom  ca-pem Each member of your replica set <metadata.name>-<X>-pem Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional Set this value to  true  to enable   on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true Invoke the following   command to update and restart your\n replica set : The   creates the MongoDB resources and requests the\n    to approve the database host's certificates. Run the\nfollowing command to verify that the certificates are pending\napproval: The  status  field of the output should resemble the following: If you do not see the  status.message  above, see\n Troubleshooting the   to help diagnose the issue. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the replica set\nexample: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create the   that stores\nthe replica set's certificates: This example covers a three-member replica set. If you have more than\nthree members, you can add them to the certificate using the\n --from-file  option. Run this  kubectl  command to link your   to your replica\nset: Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Optional If you use a custom   and have created the  \nthat stores it, add the secret's name. <custom-ca> Invoke the following   command to updated your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. Before you secure your replica set using   encryption, complete the\nfollowing: Deploy the Sharded Cluster \nthat you want to secure Create a   file for each of the following components: PEM file purpose Save File As... Your custom  ca-pem Each shard in your sharded cluster <metadata.name>-<Y>-<X>-pem Each member of your config server replica set <metadata.name>-config-<X>-pem Each  <metadata.name>-mongos-<X>-pem Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. Deploy the Sharded Cluster \nthat you want to secure Create a   file for each of the following components: PEM file purpose Save File As... Your custom  ca-pem Each shard in your sharded cluster <metadata.name>-<Y>-<X>-pem Each member of your config server replica set <metadata.name>-config-<X>-pem Each  <metadata.name>-mongos-<X>-pem Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional Set this value to  true  to enable   on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true Invoke the following   command to update and restart your\n sharded cluster : The   creates the MongoDB resources and requests the\n    to approve the database host's certificates. Run the\nfollowing command to verify that the certificates are pending\napproval: The  status  field of the output should resemble the following: If you do not see the  status.message  above, see\n Troubleshooting the   to help diagnose the issue. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the sharded cluster\nexample: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create the   that stores\nthe sharded cluster shards' certificates: This example covers a two-shard sharded cluster with five members per\nshard. If you have more than two shards or five members per shard,\nyou can add them to the certificate using the  --from-file  option. Run this  kubectl  command to create the   that stores\nthe sharded cluster config server's certificates: Run this  kubectl  command to create the   that stores\nthe sharded cluster   certificates: Run this  kubectl  command to link your   to your replica\nset: Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Optional If you use a custom   and have created the  \nthat stores it, add the secret's name. <custom-ca> Invoke the following   command to update and restart your\n sharded cluster : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "status:\n  lastTransition: 2019-05-01T15:36:59Z\n  message: Not all certificates have been approved by Kubernetes CA\n  phase: Failed\n  type: \"\"\n  version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmy-secure-rs-0.mongodb                      33s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-1.mongodb                      31s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-2.mongodb                      24s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-rs-0.mongodb\nkubectl certificate approve my-secure-rs-1.mongodb\nkubectl certificate approve my-secure-rs-2.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <metadata.name>-cert \\\n  --from-file=<metadata.name>-0-pem \\\n  --from-file=<metadata.name>-1-pem \\\n  --from-file=<metadata.name>-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap custom-ca --from-file=ca-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "status:\n  lastTransition: 2019-05-01T15:36:59Z\n  message: Not all certificates have been approved by Kubernetes CA\n  phase: Failed\n  type: \"\"\n  version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmy-secure-sc-0-0.mongodb                    30s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-1.mongodb                    28s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-2.mongodb                    27s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-0.mongodb                    22s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-1.mongodb                    13s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-2.mongodb                    6s        system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-0.mongodb               36s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-1.mongodb               34s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-2.mongodb               32s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-0.mongodb               49s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-1.mongodb               42s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-sc-0-0.mongodb\nkubectl certificate approve my-secure-sc-0-1.mongodb\nkubectl certificate approve my-secure-sc-0-2.mongodb\nkubectl certificate approve my-secure-sc-1-0.mongodb\nkubectl certificate approve my-secure-sc-1-1.mongodb\nkubectl certificate approve my-secure-sc-1-2.mongodb\nkubectl certificate approve my-secure-sc-config-0.mongodb\nkubectl certificate approve my-secure-sc-config-1.mongodb\nkubectl certificate approve my-secure-sc-config-2.mongodb\nkubectl certificate approve my-secure-sc-mongos-0.mongodb\nkubectl certificate approve my-secure-sc-mongos-1.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-0-cert \\\n       --from-file=<metadata.name>-0-0-pem \\\n       --from-file=<metadata.name>-0-1-pem \\\n       --from-file=<metadata.name>-0-2-pem \\\n       --from-file=<metadata.name>-0-3-pem \\\n       --from-file=<metadata.name>-0-4-pem\n\nkubectl -n mongodb create secret generic <metadata.name>-1-cert \\\n       --from-file=<metadata.name>-1-0-pem \\\n       --from-file=<metadata.name>-1-1-pem \\\n       --from-file=<metadata.name>-1-2-pem \\\n       --from-file=<metadata.name>-1-3-pem \\\n       --from-file=<metadata.name>-1-4-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-config-cert \\\n       --from-file=<metadata.name>-config-0-pem \\\n       --from-file=<metadata.name>-config-1-pem \\\n       --from-file=<metadata.name>-config-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-mongos-cert \\\n       --from-file=<metadata.name>-mongos-0-pem \\\n       --from-file=<metadata.name>-mongos-1-pem \\\n       --from-file=<metadata.name>-mongos-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap custom-ca --from-file=ca-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "The  can use  certificates to encrypt connections\nbetween:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/scale-resources",
            "title": "Scale a Deployment",
            "headings": [
                "Considerations",
                "Examples"
            ],
            "paragraphs": "You can scale your  replica set  and  sharded cluster \ndeployments up or down to match your desired configuration. Scaling up\nincreases the number of members and/or shards in the deployment,\nthereby improving your deployment's redundancy and availability. The\nscale of your deployment is configured in its corresponding\n . To scale your replica set deployment, set the  spec.members \nsetting to the desired number of replica set members. To learn more\nabout replication, see  Replication  in the\nMongoDB manual. To scale your sharded cluster deployment, set the following settings\nas desired: To learn more about sharded cluster configurations, see\n Sharded Cluster Components  in the MongoDB manual. Setting Description spec.shardCount Number of  shards  in the sharded cluster. spec.mongodsPerShardCount Number of members per shard. spec.mongosCount Number of Shard Routers. spec.configServerCount Number of members in the Config Server. The   does not support modifying deployment types.\nFor example, you cannot convert a standalone deployment to a\nreplica set. To modify the type of a deployment,\nwe recommend the following procedure: Create the new deployment with the desired configuration. Back up the data  from\nyour current deployment. Restore the data  from your current\ndeployment to the new deployment. Test your application connections to the new deployment as needed. Once you have verified that the new deployment contains the\nrequired data and can be reached by your application(s), bring\ndown the old deployment. Select the desired tab based on the deployment configuration you\nwant to scale: Consider a replica set resource with the following\n : To scale up this replica set and add more members: Adjust the  spec.members  setting to the desired\nnumber of members: Reapply the configuration to  : Consider a sharded cluster resource with the following\n : To scale up this sharded cluster: Adjust the following settings to the desired values: spec.shardCount spec.mongodsPerShardCount spec.mongosCount spec.configServerCount Reapply the configuration to  :",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-secure-replica-set>\nspec:\n  members: 4\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <repl-set-config>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-secure-sharded-cluster>\nspec:\n  shardCount: 3\n  mongodsPerShardCount: 3\n  mongosCount: 3\n  configServerCount: 4\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-config>.yaml"
                }
            ],
            "preview": "You can scale your replica set and sharded cluster\ndeployments up or down to match your desired configuration. Scaling up\nincreases the number of members and/or shards in the deployment,\nthereby improving your deployment's redundancy and availability. The\nscale of your deployment is configured in its corresponding\n.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-internal-auth",
            "title": "Secure Internal Authentication with X.509 and TLS",
            "headings": [
                "General Prerequisites",
                "Configure X.509 Internal Authentication for a Replica Set",
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the TLS settings for your replica set resource.",
                "Configure the general X.509 settings for your replica set resource.",
                "Configure the internal X.509 settings for your replica set resource.",
                "Save your replica set config file.",
                "Update and restart your replica set deployment.",
                "Check the status of your deployment.",
                "Retrieve the CSRs for each host and agent in your deployment.",
                "Approve the CSR for each host in your deployment.",
                "Retrieve the cluster file CSR for each host in your deployment.",
                "Approve the cluster file CSR for each host in your deployment.",
                "Approve the CSR for each agent in your deployment.",
                "Track the status of your deployment.",
                "Configure kubectl to default to your namespace.",
                "Create the  for your TLS certificates.",
                "Create the  for your X.509 certificates.",
                "Create the  to link your  with your deployment.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the TLS settings for your replica set resource using a Custom Certificate Authority.",
                "Configure the general X.509 settings for your replica set resource.",
                "Configure the internal X.509 settings for your replica set resource.",
                "Save your replica set config file.",
                "Apply your changes to your replica set deployment.",
                "Track the status of your deployment.",
                "Configure X.509 Internal Authentication for a Sharded Cluster",
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the TLS settings for your sharded cluster resource.",
                "Configure the general X.509 settings for your sharded cluster resource.",
                "Configure the internal X.509 settings for your sharded cluster resource.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Check the status of your deployment.",
                "Retrieve the CSRs for each host in your deployment.",
                "Approve the CSR for each host in your deployment.",
                "Retrieve the cluster file CSR for each host in your deployment.",
                "Approve the cluster file CSR for each host in your deployment.",
                "Approve the CSR for each agent in your deployment.",
                "Track the status of your deployment.",
                "Configure kubectl to default to your namespace.",
                "Create the  for your Shards' TLS certificates.",
                "Create the  for your config server's TLS certificates.",
                "Create the  for your mongos server's TLS certificates.",
                "Create the  for your Shards' X.509 certificates.",
                "Create the  for your config server's X.509 certificates.",
                "Create the  for your mongos server's X.509 certificates.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the TLS settings for your sharded cluster resource using a Custom Certificate Authority.",
                "Configure the general X.509 settings for your sharded cluster resource.",
                "Configure the internal X.509 settings for your sharded cluster resource.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Track the status of your deployment."
            ],
            "paragraphs": "This guide instructs you on how to configure: To secure your deployment using   and X.509 authentication,\nyou may use  's   or your own  : X.509 internal authentication between MongoDB nodes in a cluster. X.509 authentication from clients to your MongoDB instances.  to encrypt connections between MongoDB hosts in a replica set\nor sharded cluster.  to encrypt connections client applications and MongoDB\ndeployments. Before you secure any of your MongoDB deployments using  \nencryption, complete the following: Install the Kubernetes Operator Create Credentials for the Kubernetes Operator Enabling X.509 authentication at the project level configures all\nagents to use X.509 client authentication when communicating with\nMongoDB deployments. X.509 client authentication requires one of the following:  4.1.7 or later  4.0.11 or later Before you secure your replica set using   encryption, complete the\nfollowing: Deploy the Replica Set \nthat you want to secure Deploy the Replica Set \nthat you want to secure Create a   file for each of the following components: PEM file purpose Save File As... Your custom  ca-pem Each member of your replica set <metadata.name>-<X>-pem Your project's Automation or MongoDB Agent mms-automation-agent-pem Your project's Backup Agent (if needed) mms-backup-agent-pem Your project's Monitoring Agent (if needed) mms-monitoring-agent-pem Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional Set this value to  true  to enable   on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true , authentication is enabled on the\nMongoDB deployment. true array Conditional If you enabled authentication, you must set an authentication\nmechanism. Accepted values are  X509 . X509 To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example string Conditional If you enabled authentication, you can enable\n X.509 internal cluster authentication .\nAccepted values are  X509 . Once internal cluster authentication is enabled, it can not\nbe disabled. X509 Invoke the following   command to update and restart your\n replica set : The   creates the MongoDB resources and requests the\n    to approve the database host's certificates. Run the\nfollowing command to verify that the certificates are pending\napproval: The  status  field of the output should resemble the following: If you do not see the  status.message  above, see\n Troubleshooting the   to help diagnose the issue. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the replica set\nexample: When  spec.security.clusterAuthenticationMode  is set to\n x509 ,   generates an additional   per host for\nthe clusterfile. After the first batch of certificates are approved, run the\ncommand to retrieve the   again: The clusterfile   are now present in the output: The   generates   for the \u200bs when\nMongoDB resources are present. Approve the clusterfile   using the same command: The following commands approve the clusterfile certificates: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the replica set\nexample: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create the   that stores\nthe replica set's certificates: This example covers a three-member replica set. If you have more than\nthree members, you can add them to the certificate using the\n --from-file  option. Run this  kubectl  command to create the   that stores\nthe replica set's certificates: This example covers a three-member replica set. If you have more than\nthree members, you can add them to the certificate using the\n --from-file  option. Run this  kubectl  command to link your   to your replica\nset: Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Optional If you use a custom   and have created the  \nthat stores it, add the secret's name. <custom-ca> To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true , authentication is enabled on the\nMongoDB deployment. true array Conditional If you enabled authentication, you must set an authentication\nmechanism. Accepted values are  X509 . X509 To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example string Conditional If you enabled authentication, you can enable\n X.509 internal cluster authentication .\nAccepted values are  X509 . Once internal cluster authentication is enabled, it can not\nbe disabled. X509 Invoke the following   command to updated your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. Before you secure your sharded cluster using   encryption, complete\nthe following: Deploy the Sharded Cluster \nthat you want to secure Create a   file for each of the following components: PEM file purpose Save File As... Your custom  ca-pem Each shard in your sharded cluster <metadata.name>-<Y>-<X>-pem Each member of your config server replica set <metadata.name>-config-<X>-pem Each  <metadata.name>-mongos-<X>-pem Your project's Automation or MongoDB Agent mms-automation-agent-pem Your project's Backup Agent (if needed) mms-backup-agent-pem Your project's Monitoring Agent (if needed) mms-monitoring-agent-pem Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. Deploy the Sharded Cluster \nthat you want to secure Create a   file for each of the following components: PEM file purpose Save File As... Your custom  ca-pem Each shard in your sharded cluster <metadata.name>-<Y>-<X>-pem Each member of your config server replica set <metadata.name>-config-<X>-pem Each  <metadata.name>-mongos-<X>-pem Your project's Automation or MongoDB Agent mms-automation-agent-pem Your project's Backup Agent (if needed) mms-backup-agent-pem Your project's Monitoring Agent (if needed) mms-monitoring-agent-pem Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional Set this value to  true  to enable   on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true , authentication is enabled on the\nMongoDB deployment. true array Conditional If you enabled authentication, you must set an authentication\nmechanism. Accepted values are  X509 . X509 To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example string Conditional If you enabled authentication, you can enable\n X.509 internal cluster authentication .\nAccepted values are  X509 . Once internal cluster authentication is enabled, it can not\nbe disabled. X509 Invoke the following   command to update and restart your\n sharded cluster : The   creates the MongoDB resources and requests the\n    to approve the database host's certificates. Run the\nfollowing command to verify that the certificates are pending\napproval: The  status  field of the output should resemble the following: If you do not see the  status.message  above, see\n Troubleshooting the   to help diagnose the issue. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the sharded cluster\nexample: When  spec.security.clusterAuthenticationMode  is set to\n x509 ,   generates an additional   per host for\nthe clusterfile. After the first batch of certificates are approved, run the\ncommand to retrieve the   again: The clusterfile   are now present in the output: The   generates   for the \u200bs when\nMongoDB resources are present. Approve the clusterfile   using the same command: The following commands approve the clusterfile certificates: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the sharded cluster\nexample: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create the   that stores\nthe sharded cluster shards' certificates: This example covers a two-shard sharded cluster with five members per\nshard. If you have more than two shards or five members per shard,\nyou can add them to the certificate using the  --from-file  option. Run this  kubectl  command to create the   that stores\nthe sharded cluster config server's certificates: Run this  kubectl  command to create the   that stores\nthe sharded cluster   certificates: Run this  kubectl  command to create the   that stores\nthe sharded cluster shards' certificates: This example covers a two-shard sharded cluster with five members per\nshard. If you have more than two shards or five members per shard,\nyou can add them to the certificate using the  --from-file  option. Run this  kubectl  command to create the   that stores\nthe sharded cluster config server's certificates: Run this  kubectl  command to create the   that stores\nthe sharded cluster   certificates: Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Optional If you use a custom   and have created the  \nthat stores it, add the secret's name. <custom-ca> To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true , authentication is enabled on the\nMongoDB deployment. true array Conditional If you enabled authentication, you must set an authentication\nmechanism. Accepted values are  X509 . X509 To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example string Conditional If you enabled authentication, you can enable\n X.509 internal cluster authentication .\nAccepted values are  X509 . Once internal cluster authentication is enabled, it can not\nbe disabled. X509 Invoke the following   command to update and restart your\n sharded cluster : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "status:\n  lastTransition: 2019-05-01T15:36:59Z\n  message: Not all certificates have been approved by Kubernetes CA\n  phase: Failed\n  type: \"\"\n  version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmms-automation-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-backup-agent.mongodb                    15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-monitoring-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-rs-0.mongodb                      33s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-1.mongodb                      31s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-2.mongodb                      24s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-rs-0.mongodb\nkubectl certificate approve my-secure-rs-1.mongodb\nkubectl certificate approve my-secure-rs-2.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmms-automation-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-backup-agent.mongodb                    15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-monitoring-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-rs-0-clusterfile.mongodb          13s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-0.mongodb                      105s      system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-rs-1-clusterfile.mongodb          7s        system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-1.mongodb                      103s      system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-rs-2-clusterfile.mongodb          3s        system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-2.mongodb                      100s      system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-rs-0-clusterfile.mongodb\nkubectl certificate approve my-secure-rs-1-clusterfile.mongodb\nkubectl certificate approve my-secure-rs-2-clusterfile.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve mms-automation-agent.mongodb\nkubectl certificate approve mms-backup-agent.mongodb\nkubectl certificate approve mms-monitoring-agent.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <metadata.name>-cert \\\n  --from-file=<metadata.name>-0-pem \\\n  --from-file=<metadata.name>-1-pem \\\n  --from-file=<metadata.name>-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <metadata.name>-clusterfile \\\n  --from-file=<metadata.name>-0-pem \\\n  --from-file=<metadata.name>-1-pem \\\n  --from-file=<metadata.name>-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap custom-ca --from-file=ca-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "status:\n  lastTransition: 2019-05-01T15:36:59Z\n  message: Not all certificates have been approved by Kubernetes CA\n  phase: Failed\n  type: \"\"\n  version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmms-automation-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-backup-agent.mongodb                    15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-monitoring-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-0-0.mongodb                    30s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-1.mongodb                    28s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-2.mongodb                    27s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-0.mongodb                    22s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-1.mongodb                    13s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-2.mongodb                    6s        system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-0.mongodb               36s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-1.mongodb               34s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-2.mongodb               32s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-0.mongodb               49s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-1.mongodb               42s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-sc-0-0.mongodb\nkubectl certificate approve my-secure-sc-0-1.mongodb\nkubectl certificate approve my-secure-sc-0-2.mongodb\nkubectl certificate approve my-secure-sc-1-0.mongodb\nkubectl certificate approve my-secure-sc-1-1.mongodb\nkubectl certificate approve my-secure-sc-1-2.mongodb\nkubectl certificate approve my-secure-sc-config-0.mongodb\nkubectl certificate approve my-secure-sc-config-1.mongodb\nkubectl certificate approve my-secure-sc-config-2.mongodb\nkubectl certificate approve my-secure-sc-mongos-0.mongodb\nkubectl certificate approve my-secure-sc-mongos-1.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmms-automation-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-backup-agent.mongodb                    15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-monitoring-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-0-0-clusterfile.mongodb        40s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-0.mongodb                    2m22s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-0-1-clusterfile.mongodb        36s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-1.mongodb                    2m20s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-0-2-clusterfile.mongodb        32s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-2.mongodb                    2m19s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-1-0-clusterfile.mongodb        27s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-0.mongodb                    2m14s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-1-1-clusterfile.mongodb        23s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-1.mongodb                    2m5s      system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-1-2-clusterfile.mongodb        20s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-2.mongodb                    118s      system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-config-0-clusterfile.mongodb   10s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-0.mongodb               2m28s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-config-1-clusterfile.mongodb   5s        system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-1.mongodb               2m26s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-config-2-clusterfile.mongodb   2s        system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-2.mongodb               2m24s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-mongos-0-clusterfile.mongodb   18s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-0.mongodb               2m41s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-sc-mongos-1-clusterfile.mongodb   12s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-1.mongodb               2m34s     system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-sc-0-0-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-0-1-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-0-2-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-1-0-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-1-1-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-1-2-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-config-0-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-config-1-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-config-2-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-mongos-0-clusterfile.mongodb\nkubectl certificate approve my-secure-sc-mongos-1-clusterfile.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve mms-automation-agent.mongodb\nkubectl certificate approve mms-backup-agent.mongodb\nkubectl certificate approve mms-monitoring-agent.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-0-cert \\\n       --from-file=<metadata.name>-0-0-pem \\\n       --from-file=<metadata.name>-0-1-pem \\\n       --from-file=<metadata.name>-0-2-pem \\\n       --from-file=<metadata.name>-0-3-pem \\\n       --from-file=<metadata.name>-0-4-pem\n\nkubectl -n mongodb create secret generic <metadata.name>-1-cert \\\n       --from-file=<metadata.name>-1-0-pem \\\n       --from-file=<metadata.name>-1-1-pem \\\n       --from-file=<metadata.name>-1-2-pem \\\n       --from-file=<metadata.name>-1-3-pem \\\n       --from-file=<metadata.name>-1-4-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-config-cert \\\n       --from-file=<metadata.name>-config-0-pem \\\n       --from-file=<metadata.name>-config-1-pem \\\n       --from-file=<metadata.name>-config-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-mongos-cert \\\n       --from-file=<metadata.name>-mongos-0-pem \\\n       --from-file=<metadata.name>-mongos-1-pem \\\n       --from-file=<metadata.name>-mongos-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-0-clusterfile \\\n       --from-file=<metadata.name>-0-0-pem \\\n       --from-file=<metadata.name>-0-1-pem \\\n       --from-file=<metadata.name>-0-2-pem \\\n       --from-file=<metadata.name>-0-3-pem \\\n       --from-file=<metadata.name>-0-4-pem\n\nkubectl -n mongodb create secret generic <metadata.name>-1-clusterfile \\\n       --from-file=<metadata.name>-1-0-pem \\\n       --from-file=<metadata.name>-1-1-pem \\\n       --from-file=<metadata.name>-1-2-pem \\\n       --from-file=<metadata.name>-1-3-pem \\\n       --from-file=<metadata.name>-1-4-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-config-clusterfile \\\n       --from-file=<metadata.name>-config-0-pem \\\n       --from-file=<metadata.name>-config-1-pem \\\n       --from-file=<metadata.name>-config-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-mongos-clusterfile \\\n       --from-file=<metadata.name>-mongos-0-pem \\\n       --from-file=<metadata.name>-mongos-1-pem \\\n       --from-file=<metadata.name>-mongos-2-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "This guide instructs you on how to configure:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-sharded-cluster",
            "title": "Deploy a Sharded Cluster",
            "headings": [
                "Prerequisites",
                "Considerations",
                "Do Not Deploy Monitoring Agents inside and outside ",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example to create a new sharded cluster resource.",
                "Configure the settings highlighted in the preceding step as follows.",
                "Add any additional accepted settings for a sharded cluster  deployment.",
                "Save this file with a .yaml file extension.",
                "Start your sharded cluster deployment.",
                "Track the status of your sharded cluster deployment."
            ],
            "paragraphs": "Sharded clusters  provide horizontal scaling\nfor large data sets and enable high throughput operations by\ndistributing the data set across a group of servers. To learn more about sharding, see\n Sharding Introduction  in the\nMongoDB manual. Use this procedure to deploy a new sharded cluster that   manages.\nLater, you can use   to add shards and perform other maintenance\noperations on the cluster. You can use the   to deploy MongoDB resources with\n  version 4.0.11 or later and Cloud Manager. At any place in\nthis guide that says  Ops Manager , you can substitute\n Cloud Manager . To deploy a  sharded cluster  using an  , you need to complete the\nfollowing procedures: Install Kubernetes Operator Create One Project using a ConfigMap Create Credentials for the  Starting in   version 1.3.0, you can only have one\nMongoDB resource per project. To learn how to migrate your project to\na single-cluster configuration, see  Migrate to One Resource per Project (Required for Version 1.3.0) . Do not mix MongoDB deployments outside   with ones insider  \nin the same Project. Due to   network translation, a Monitoring Agent outside  \ncannot monitor MongoDB instances inside  . For this reason, k8s\nand non-k8s deployments in the same Project is not supported. Use\nseparate projects. The procedure for deploying a sharded cluster depends on whether you\nrequire the deployment to run with   enabled for intra-cluster\ncommunication and clients connecting to the database: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. This is a   file that you can modify to meet your desired\nconfiguration. Change the highlighted settings to match your desired\n sharded cluster  configuration. Open your preferred text editor and paste the   specification\ninto a new text file. Key Type Description Example metadata.name string Label for this    sharded cluster   . Resource names must be 44 characters or less. metadata.name  documentation on  names . myproject spec.shardCount integer Number of shards to deploy. 2 spec.mongodsPerShardCount integer Number of shard members per shard. 3 spec.mongosCount integer Number of shard routers to deploy. 2 spec.configServerCount integer Number of members of the config server replica set. 3 spec.version string Version of MongoDB that this  sharded cluster  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. 4.2.2-ent spec.opsManager.configMapRef.name string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value  must  match the namespace in which you created the\n   project ConfigMap . The   tracks any changes to the ConfigMap and\nreconciles the state of the  . <myproject> spec.credentials string Name of the     you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . This value  must  match the namespace in which you created the\nsecret and the  name  value you provided for your  \n   Secret . The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. ShardedCluster spec.persistent string Optional. Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. If this value is  true , then the following values are set\nto their default value of  16Gi : To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: spec.shardPodSpec.persistence.single spec.configSrvPodSpec.persistence.single If you want one   for each  , configure the\n spec.shardPodSpec.persistence.single  and\n spec.configSrvPodSpec.persistence.single \ncollections. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: In the  spec.configSrvPodSpec.persistence.multiple \ncollection:\n-  .data \n-  .journal \n-  .logs In the  spec.configSrvPodSpec.persistence.multiple  collection:\n-  .data \n-  .journal \n-  .logs Your containers must have permissions to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the deployment item does not fix\nissues with your  , contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n  specification file for a  sharded cluster \ndeployment: For config server For shard routers For shard members spec.clusterDomain spec.exposedExternally spec.logLevel spec.featureCompatibilityVersion spec.connectivity.replicaSetHorizons You must set  spec.clusterDomain  if your   cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n  might not function as expected. spec.configSrvPodSpec.cpu spec.configSrvPodSpec.cpuRequests spec.configSrvPodSpec.memory spec.configSrvPodSpec.memoryRequests spec.configSrvPodSpec.persistence.single spec.configSrvPodSpec.persistence.multiple.data spec.configSrvPodSpec.persistence.multiple.journal spec.configSrvPodSpec.persistence.multiple.logs spec.configSrvPodSpec.nodeAffinity spec.configSrvPodSpec.podAffinity spec.configSrvPodSpec.podAntiAffinityTopologyKey spec.configSrvPodSpec.podTemplate.metadata spec.configSrvPodSpec.podTemplate.spec spec.mongosPodSpec.cpu spec.mongosPodSpec.cpuRequests spec.mongosPodSpec.memory spec.mongosPodSpec.memoryRequests spec.mongosPodSpec.nodeAffinity spec.mongosPodSpec.podAffinity spec.mongosPodSpec.podAntiAffinityTopologyKey spec.mongosPodSpec.podTemplate.metadata spec.mongosPodSpec.podTemplate.spec spec.shardPodSpec.cpu spec.shardPodSpec.cpuRequests spec.shardPodSpec.memory spec.shardPodSpec.memoryRequests spec.shardPodSpec.nodeAffinity spec.shardPodSpec.persistence.single spec.shardPodSpec.persistence.multiple.data spec.shardPodSpec.persistence.multiple.journal spec.shardPodSpec.persistence.multiple.logs spec.shardPodSpec.podAffinity spec.shardPodSpec.podAntiAffinityTopologyKey spec.shardPodSpec.podTemplate.metadata spec.shardPodSpec.podTemplate.spec Invoke the following   command to create your\n sharded cluster : Check the log  after running this\ncommand. If the creation was successful, you should see a message\nsimilar to the following: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "2018-06-26T10:30:30.346Z INFO operator/shardedclusterkube.go:52 Created! {\"sharded cluster\": \"my-sharded-cluster\"}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "Sharded clusters provide horizontal scaling\nfor large data sets and enable high throughput operations by\ndistributing the data set across a group of servers.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/upgrade-k8s-operator",
            "title": "Upgrade from Operator Version 0.10 or Later",
            "headings": [
                "Procedure",
                "Change to the directory in which you cloned the repository.",
                "Upgrade the  for MongoDB deployments using the following  command:",
                "You can edit the Operator  file to further customize your Operator before upgrading it.",
                "Upgrade the  using the following  command:",
                "Upgrade the  using the following helm command:",
                "Upgrade the latest version of the  with modified pull policy values using the following helm command:",
                "Upgrade the latest version of the  with modified pull policy values using the following helm command:",
                "Change to the directory in which you cloned the repository.",
                "Upgrade the  for MongoDB deployments using the following  command:",
                "You can edit the Operator  file to further customize your Operator before upgrading it.",
                "Upgrade the  using the following  command:",
                "Upgrade the  using the following helm command:",
                "Upgrade the latest version of the  with modified pull policy values using the following helm command:",
                "Upgrade the latest version of the  with modified pull policy values using the following helm command:"
            ],
            "paragraphs": "Starting in   version 1.3.0, you can only have one\nMongoDB resource per project. To learn how to migrate your project to\na single-cluster configuration, see  Migrate to One Resource per Project (Required for Version 1.3.0) . The following steps depend on how your environment is configured: Open your  mongodb-enterprise.yaml  in your preferred\ntext editor. You may need to add one or more of the following\noptions: Environment Variable When to Use OPERATOR_ENV Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . You can set the following pair of values: If  OPERATOR_ENV  is Log Level is set to Log Format is set to dev debug text prod info json WATCH_NAMESPACE Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . You can set the following pair of values: To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the image for an  Ops\nManager resource  is downloaded. Default value is:\n quay.io/mongodb/mongodb-enterprise-ops-manager OPS_MANAGER_IMAGE_PULL_POLICY Pull policy  for the\nimage deployed to an  Ops Manager resource . Accepted values are:  Always ,  IfNotPresent ,  Never Default value is:  Always MANAGED_SECURITY_CONTEXT Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if you want to run the  \nin OpenShift or in a restrictive environment. Default value is  false . You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the  values.yaml  file: Setting When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if your cluster manages the\n securityContext  for your   resources. Default value is  false . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the\nHelm Chart: To upgrade the   on a host not connected to the\nInternet, you have two options, you can download the\n  files from either: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the  values.yaml  file: Setting When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if your cluster manages the\n securityContext  for your   resources. Default value is  false . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the Helm\nChart: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the  values.yaml  file: Setting When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if your cluster manages the\n securityContext  for your   resources. Default value is  false . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the Helm\nChart: Open your  mongodb-enterprise-openshift.yaml  in your preferred\ntext editor. You must add your  <openshift-pull-secret>  to the\n ServiceAccount  definitions: You may need to add one or more of the following\noptions: Environment Variable When to Use OPERATOR_ENV Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . You can set the following pair of values: If  OPERATOR_ENV  is Log Level is set to Log Format is set to dev debug text prod info json WATCH_NAMESPACE Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . You can set the following pair of values: To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the image for an  Ops\nManager resource  is downloaded. Default value is:\n registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager OPS_MANAGER_IMAGE_PULL_POLICY Pull policy  for the\nimage deployed to an  Ops Manager resource . Accepted values are:  Always ,  IfNotPresent ,  Never Default value is:  Always MANAGED_SECURITY_CONTEXT Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. For OpenShift,  MANAGED_SECURITY_CONTEXT  must always be\n true . Default value is  true . You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the\n values-openshift.yaml  file: Setting When to Use registry.imagePullSecrets  that contains the credentials required to pull imagePullSecrets\nfrom the repository. This setting is mandatory for OpenShift installs. You must\neither define it in this file or pass it when you install the\n  using Helm. namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Default value is  true . For OpenShift,  managedSecurityContext  must always be\n true . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the Helm\nChart: To upgrade the   on a host not connected to the\nInternet, you have two options, you can download the\n  files from either: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the\n values-openshift.yaml  file: Setting When to Use registry.imagePullSecrets  that contains the credentials required to pull imagePullSecrets\nfrom the repository. This setting is mandatory for OpenShift installs. You must\neither define it in this file or pass it when you install the\n  using Helm. namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Default value is  true . For OpenShift,  managedSecurityContext  must always be\n true . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the Helm\nChart: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the\n values-openshift.yaml  file: Setting When to Use registry.imagePullSecrets  that contains the credentials required to pull imagePullSecrets\nfrom the repository. This setting is mandatory for OpenShift installs. You must\neither define it in this file or pass it when you install the\n  using Helm. namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Default value is  true . For OpenShift,  managedSecurityContext  must always be\n true . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the Helm\nChart: To troubleshoot your  , see\n Review Logs from the  . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: OPERATOR_ENV\nspec.template.spec.containers.name.env.value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPERATOR_ENV\n          value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: WATCH_NAMESPACE\nspec.template.spec.containers.name.env.value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: WATCH_NAMESPACE\n          value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMANAGED_SECURITY_CONTEXT\nspec.template.spec.containers.name.env.value:\nfalse"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MANAGED_SECURITY_CONTEXT\n          value: true"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f mongodb-enterprise.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template helm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set namespace=<testNamespace> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set namespace=<testNamespace> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set namespace=<testNamespace> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-appdb\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-database-pods\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: OPERATOR_ENV\nspec.template.spec.containers.name.env.value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPERATOR_ENV\n          value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: WATCH_NAMESPACE\nspec.template.spec.containers.name.env.value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: WATCH_NAMESPACE\n          value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMANAGED_SECURITY_CONTEXT\nspec.template.spec.containers.name.env.value:\ntrue"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MANAGED_SECURITY_CONTEXT\n          value: true"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f mongodb-enterprise-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template helm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "registry:\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# OpenShift manages security context on its own\nmanagedSecurityContext: true"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.imagePullSecrets=<openshift-pull-secret> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set registry.imagePullSecrets=<openshift-pull-secret> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "registry:\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# OpenShift manages security context on its own\nmanagedSecurityContext: true"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set registry.imagePullSecrets=<openshift-pull-secret> \\\n--set namespace=<testNamespace> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set registry.imagePullSecrets=<openshift-pull-secret> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "registry:\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# OpenShift manages security context on its own\nmanagedSecurityContext: true"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set registry.imagePullSecrets=<openshift-pull-secret> \\\n--set namespace=<testNamespace> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                }
            ],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-om-container",
            "title": "Deploy an  Resource",
            "headings": [
                "Prerequisites and Considerations",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the following example   .",
                "Open your preferred text editor and paste the  specification into a new text file.",
                "Configure the settings highlighted in the prior example.",
                "(Optional) Configure any additional settings for an  deployment.",
                "Save this file with a .yaml file extension.",
                "Create your  instance.",
                "Track the status of your  instance.",
                "Access the  application.",
                "Create credentials for the Kubernetes Operator.",
                "Create a project using a .",
                "Deploy MongoDB database resources to complete the Backup configuration.",
                "Confirm that the  resource is running."
            ],
            "paragraphs": "You can deploy   in a container with the  . Before you deploy an   resource, make sure you  plan for\nyour Ops Manager resource : Complete the  Prerequisites Read the  Considerations . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the highlighted settings to match your desired\n  configuration. Key Type Description Example metadata.name string Name for this      . Resource names must be 44 characters or less. metadata.name  documentation on  names . om spec.replicas number Number of   instances to run in parallel. The minimum valid value is  1 . For high availability, set this value to more than  1 . Multiple\n  instances can read from the same Application\nDatabase, ensuring failover if one instance is unavailable and\nenabling you to update the   resource without downtime. 1 spec.version string Version of   to be installed. The format should be  X.Y.Z .\nTo view available   versions, view the\n container registry . \u200b spec.adminCredentials string Name of the   you  created \nfor the   admin user. Configure the secret to use the same   as the\n  resource. om-admin-secret boolean Flag that indicates that Backup is enabled for your You must\nspecify  spec.backup.enabled: true  to configure settings\nfor the head database, oplog store, and snapshot store. true string Name of the oplog store. oplog1 string Name of the MongoDB database resource for the oplog store. my-oplog-db string Name of the   snapshot store. s3store1 string Name of the MongoDB database resource for the   snapshot\nstore metadata. my-s3-metadata-db string Name of the   that contains the  accessKey  and\n secretKey  fields. The  backup-daemon  uses the\nvalues of these fields as credentials to access the   or\n -compatible bucket. my-s3-credentials string  of the   or  -compatible bucket that\n stores  the\ndatabase Backup snapshots. s3.us-east-1.amazonaws.com string Name of the   or  -compatible bucket that stores the\ndatabase Backup snapshots. my-bucket string Optional . The   service  ServiceType \nthat exposes   outside of  . Exclude the\n spec. externalConnectivity  setting\nand its children if you don't want the   to\ncreate a   service to route external traffic to the\n  application. LoadBalancer integer Number of members of the  mms-application-database \nreplica set. 3 string Optional . Version of MongoDB that the  mms-application-database \nshould run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the  Enterprise edition . To learn more about MongoDB versioning, see see\n release-version-numbers  in the MongoDB Manual. To deploy   inside   without an Internet connection,\nomit this setting or leave the value empty. The  \ninstalls the  bundled MongoDB Enterprise  version 4.2.2 by default. 4.2.2-ent boolean Optional. Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. If this value is  true , then\n spec.applicationDatabase.podSpec.persistence. \n spec.podSpec.persistence.single \nis set to its default value of  16Gi . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: If you want one   for each  , configure the\n spec.applicationDatabase. \n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: Grant your containers permission to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the resource does not fix\nissues with your  , contact MongoDB support. true Add any  optional settings  that you\nwant to apply to your deployment to the   specification file. Invoke the following  kubectl  command on the filename of the\n  resource definition: To check the status of your   resource, invoke the following\ncommand: The command returns the following output under the  status  field\nwhile the resource deploys: After the resource completes the  Reconciling  phase, the command\nreturns the following output under the  status  field: The   remains in a  Pending  state until you configure\nthe Backup databases. The  status.opsManager.url  field states the resource's\nconnection  . Using this  , you can reach   from\ninside the   cluster or  create a project using a\nConfigMap . The steps you take differ based on how you are routing traffic to the\n  application in  . If you configured the   to\ncreate a   service for you, or you created a   service\nmanually, use one of the following methods to access the  \napplication: To learn how to access the   application using a third-party\nservice, refer to the documentation for your solution. Query your cloud provider to get the   of the load\nbalancer service. See your cloud provider's documentation\nfor details. Open a browser window and navigate to the  \napplication using the   and port number of your load\nbalancer service. Log in to   using the  admin user credentials . Set your firewall rules to allow access from the Internet to\nthe  spec.externalConnectivity. port \non the host on which your   cluster is running. Open a browser window and navigate to the  \napplication using the   and the\n spec.externalConnectivity. port . Log in to   using the  admin user credentials . To configure credentials, you must create an   organization,\ngenerate programmatic API keys, and create a  . These\nactivities follow the prerequisites and procedure on the\n Create Credentials for the   page. To create a project, follow the prerequisites and procedure on the\n Create One Project using a ConfigMap  page. You must set  data.baseUrl  in the ConfigMap to the  's  . To find this  , invoke the following command: The command returns the URL of the   in the\n status.opsManager.url  field. If you deploy   with the   and   will\nmanage MongoDB database resources deployed  outside  of the  \ncluster it's deployed to, you must set  data.baseUrl  to the same\nvalue of the\n spec.configuration.mms.centralUrl \nsetting in the   resource specification. By default,   enables  mms-backup-functional-overview .\nCreate a MongoDB database resource for the oplog and snapshot stores\nto complete the configuration. Deploy a  MongoDB database resource  for the oplog store in the same\nnamespace as the   resource. Match the  metadata.name  of the resource with the\n spec.backup.opLogStores.mongodbResourceRef.name  that you specified\nin your   resource definition. Create this database as a three-member  replica set . Deploy a  MongoDB database resource  for the   snapshot store in the\nsame namespace as the   resource. Match the  metadata.name  of the resource to the\n spec.backup.s3Stores.mongodbResourceRef.name \nthat you specified in your   resource definition. Create the   snapshot store as a replica set. To check the status of your   resource, invoke the following\ncommand: When the   is running, the command returns the following\noutput under the  status  field: See  Troubleshooting the   for information about the\nresource deployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: <myopsmanager>\nspec:\n  replicas: 1\n  version: <opsmanagerversion>\n  adminCredentials: <adminusercredentials> # Should match metadata.name\n                                           # in the Kubernetes secret\n                                           # for the admin user\n  backup:\n    enabled: true\n    opLogStores:\n      - name: <oplogname>\n        mongodbResourceRef:\n          name: <oplogresourcename> # Should match metadata.name\n                                    # in the MongoDB database resource\n                                    # for the oplog store\n    s3Stores:\n      - name: <s3storename>\n        mongodbResourceRef:\n          name: <s3storeresourcename> # Should match metadata.name\n                                      # in the MongoDB database resource\n                                      # for the snapshot store\n        s3SecretRef:\n          name: <awss3credentials> # Should match metadata.name\n                                   # in the Kubernetes secret\n                                   # for your |aws| credentials\n        s3BucketEndpoint: <s3.region.amazonaws.com>\n        s3BucketName: <bucketname>\n\n  externalConnectivity:\n    type: LoadBalancer\n\n  applicationDatabase:\n    members: 3\n    version: <mongodbversion>\n    persistent: true\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n applicationDatabase:\n  lastTransition: \"2019-11-15T19:48:01Z\"\n  message: AppDB Statefulset is not ready yet\n  phase: Reconciling\n  type: \"\"\n  version: \"\"\n opsManager:\n  lastTransition: \"2019-11-15T19:48:01Z\"\n  message: Ops Manager is still starting\n  phase: Reconciling\n  version: \"\""
                },
                {
                    "lang": "yaml",
                    "value": " status:\n   applicationDatabase:\n     lastTransition: \"2019-12-06T18:23:22Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: 4.2.2-ent\n   opsManager:\n     lastTransition: \"2019-12-06T18:23:26Z\"\n     message: The MongoDB object namespace/oplogdbname doesn't exist\n     phase: Pending\n     url: http://om-svc.dev.svc.cluster.local:8080\n     version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "http://ops.example.com:8080"
                },
                {
                    "lang": "sh",
                    "value": "http://ops.example.com:30036"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": " status:\n   applicationDatabase:\n     lastTransition: \"2019-12-06T18:23:22Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: 4.2.2-ent\n   opsManager:\n     lastTransition: \"2019-12-06T18:23:26Z\"\n     message: The MongoDB object namespace/oplogdbname doesn't exist\n     phase: Pending\n     url: http://om-svc.dev.svc.cluster.local:8080\n     version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2019-12-06T17:46:15Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: 4.2.2-ent\n  opsManager:\n    lastTransition: \"2019-12-06T17:46:32Z\"\n    phase: Running\n    replicas: 1\n    url: http://om-backup-svc.dev.svc.cluster.local:8080\n    version: 4.2.6"
                }
            ],
            "preview": "You can deploy  in a container with the .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/connect-from-inside-k8s",
            "title": "Connect to a MongoDB Database Resource from Inside Kubernetes",
            "headings": [
                "Considerations",
                "Procedure",
                "Open the Topology view for your deployment.",
                "Click  for the deployment to which you want to connect.",
                "Click Connect to this instance.",
                "Copy the connection command displayed in the Connect to your Deployment dialog.",
                "Run the connection command in a terminal to connect to the deployment."
            ],
            "paragraphs": "The following procedure describes how to connect to a MongoDB resource\ndeployed by   from inside of the   cluster. You must be able to connect to the host and port where you deployed your\n  resource. To learn more about connecting to your deployment, see\n Connect to a MongoDB Process . Perform the following steps in the   or\n Cloud Manager \napplication, depending on where your clusters are hosted: When connecting to a resource from inside of  , the\nhostname to which you connect has the following form: Click  Deployment  in the left navigation. To connect to a sharded cluster resource named\n shardedcluster , you might use the following connection\nstring:",
            "code": [
                {
                    "lang": "sh",
                    "value": "<k8s-pod-name>.<k8s-internal-service-name>.<k8s-namespace>.<cluster-name>"
                },
                {
                    "lang": "none",
                    "value": "mongo --host shardedcluster-mongos-0.shardedcluster-svc.mongodb.svc.cluster.local --port 27017"
                }
            ],
            "preview": "The following procedure describes how to connect to a MongoDB resource\ndeployed by  from inside of the  cluster.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/upgrade-k8s-operator-v9-and-earlier",
            "title": "Upgrade from Operator Version 0.9 or Earlier",
            "headings": [
                "Prerequisites",
                "Upgrade the ",
                "Change to the directory in which you cloned the repository.",
                "Upgrade the  for MongoDB deployments using the following  command:",
                "You can edit the Operator  file to further customize your Operator before upgrading it.",
                "Upgrade the  using the following  command:",
                "Upgrade the  using the following helm command:",
                "Upgrade the latest version of the  with modified pull policy values using the following helm command:",
                "Upgrade the latest version of the  with modified pull policy values using the following helm command:",
                "Change to the directory in which you cloned the repository.",
                "Upgrade the  for MongoDB deployments using the following  command:",
                "You can edit the Operator  file to further customize your Operator before upgrading it.",
                "Upgrade the  using the following  command:",
                "Upgrade the  using the following helm command:",
                "Upgrade the latest version of the  with modified pull policy values using the following helm command:",
                "Upgrade the latest version of the  with modified pull policy values using the following helm command:",
                "Recreate MongoDB Resources and Delete the Version 0.9 CRDs",
                "Troubleshooting"
            ],
            "paragraphs": "Version 0.10 of the   consolidated the\n MongoDbStandalone ,  MongoDbShardedCluster , and\n MongoDbReplicaSet    into a\nsingle  CustomResourceDefinition  called  MongoDB . Version 0.10 of the   included breaking changes and\nrequires some additional preparation before upgrading. The\nfollowing procedure outlines the upgrade process for  \nversions 0.9 and earlier. If you are already running version 0.10\nor later, see  Upgrade from Operator Version 0.10 or Later  for upgrade instructions. The following upgrade procedure allows you to keep data stored in\npersistent volumes from previous deployments that the  \nmanaged. If you do not wish to retain data from previous\ndeployments and plan on deploying new resources, skip to the\n Upgrade  section. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Verify you have the  .yaml  configuration file for each MongoDB\nresource you have deployed. If you have standalone resources but do not have the  .yaml \nconfiguration file for them, run the following command to generate\nthe configuration file: If you have replica set resources but do not have the  .yaml \nconfiguration file for them, run the following command to generate\nthe configuration file: If you have sharded cluster resources but do not have the  .yaml \nconfiguration file for them, run the following command to generate\nthe configuration file: Edit each  .yaml  configuration file match the new  : After you edit each  .yaml  file, they should look like the\nfollowing example: Change the  kind  to  MongoDB Add the  spec.type  field and set it to  Standalone ,\n ReplicaSet , or  ShardedCluster  depending on your resource. The   does not support changing the type of an existing\nconfiguration even though it will accept a valid configuration for a\ndifferent type. For example, if your MongoDB resource is a\nstandalone, you cannot set the value of  spec.type  to\n ReplicaSet  and set  spec.members . If you do, the\n  throws an error and requires you to revert to the\npreviously working configuration. If you change the  metadata.name  field you will lose your\nresource's data. To upgrade to the latest version of the   from version v0.9\nor earlier: The following steps depend on how your environment is configured: Open your  mongodb-enterprise.yaml  in your preferred\ntext editor. You may need to add one or more of the following\noptions: Environment Variable When to Use OPERATOR_ENV Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . You can set the following pair of values: If  OPERATOR_ENV  is Log Level is set to Log Format is set to dev debug text prod info json WATCH_NAMESPACE Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . You can set the following pair of values: To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the image for an  Ops\nManager resource  is downloaded. Default value is:\n quay.io/mongodb/mongodb-enterprise-ops-manager OPS_MANAGER_IMAGE_PULL_POLICY Pull policy  for the\nimage deployed to an  Ops Manager resource . Accepted values are:  Always ,  IfNotPresent ,  Never Default value is:  Always MANAGED_SECURITY_CONTEXT Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if you want to run the  \nin OpenShift or in a restrictive environment. Default value is  false . You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the  values.yaml  file: Setting When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if your cluster manages the\n securityContext  for your   resources. Default value is  false . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the\nHelm Chart: To upgrade the   on a host not connected to the\nInternet, you have two options, you can download the\n  files from either: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the  values.yaml  file: Setting When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if your cluster manages the\n securityContext  for your   resources. Default value is  false . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the Helm\nChart: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the  values.yaml  file: Setting When to Use namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if your cluster manages the\n securityContext  for your   resources. Default value is  false . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the Helm\nChart: Open your  mongodb-enterprise-openshift.yaml  in your preferred\ntext editor. You must add your  <openshift-pull-secret>  to the\n ServiceAccount  definitions: You may need to add one or more of the following\noptions: Environment Variable When to Use OPERATOR_ENV Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . You can set the following pair of values: If  OPERATOR_ENV  is Log Level is set to Log Format is set to dev debug text prod info json WATCH_NAMESPACE Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . You can set the following pair of values: To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the image for an  Ops\nManager resource  is downloaded. Default value is:\n registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager OPS_MANAGER_IMAGE_PULL_POLICY Pull policy  for the\nimage deployed to an  Ops Manager resource . Accepted values are:  Always ,  IfNotPresent ,  Never Default value is:  Always MANAGED_SECURITY_CONTEXT Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. For OpenShift,  MANAGED_SECURITY_CONTEXT  must always be\n true . Default value is  true . You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the\n values-openshift.yaml  file: Setting When to Use registry.imagePullSecrets  that contains the credentials required to pull imagePullSecrets\nfrom the repository. This setting is mandatory for OpenShift installs. You must\neither define it in this file or pass it when you install the\n  using Helm. namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Default value is  true . For OpenShift,  managedSecurityContext  must always be\n true . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the Helm\nChart: To upgrade the   on a host not connected to the\nInternet, you have two options, you can download the\n  files from either: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the\n values-openshift.yaml  file: Setting When to Use registry.imagePullSecrets  that contains the credentials required to pull imagePullSecrets\nfrom the repository. This setting is mandatory for OpenShift installs. You must\neither define it in this file or pass it when you install the\n  using Helm. namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Default value is  true . For OpenShift,  managedSecurityContext  must always be\n true . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the Helm\nChart: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the\n values-openshift.yaml  file: Setting When to Use registry.imagePullSecrets  that contains the credentials required to pull imagePullSecrets\nfrom the repository. This setting is mandatory for OpenShift installs. You must\neither define it in this file or pass it when you install the\n  using Helm. namespace To use a different namespace, you need to specify that\n namespace . Default value is:  mongodb . managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Default value is  true . For OpenShift,  managedSecurityContext  must always be\n true . operator.env Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json operator.watchNamespace Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. You can also pass these values as options when you apply the Helm\nChart: To troubleshoot your  , see\n Review Logs from the  . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . Once the version 0.9   are deleted, the   upgrade\nis complete. After you upgrade the  , verify you have four CRDs by\nrunning the following command: The following output contains the new  mongodb.mongodb.com  CRD and\nthe version 0.9 CRDs: Remove the old resources from Kubernetes. Run each of the following commands to remove all MongoDB resources: Removing MongoDB resources will remove the database server pods\nand drop any client connections to the database. Connections are\nreestablished when the new MongoDB resources are created in\nKubernetes. MongoDB resources that have  persistent: true  set in their\n .yaml  configuration file will not lose data as it is stored in\npersistent volumes. The previous command only deletes pods\ncontaining MongoDB and not the persistent volumes containing the\ndata. Persistent volume claims referencing persistent volumes stay\nalive and are reused by the new MongoDB resources. Create the MongoDB resources again. Use the  .yaml  resource configuration file to recreate each\nresource: Run the following command to check the status of each resource and\nverify that the  phase  reaches the  Running  status: For an example of this command's output, see\n Get Status of a Deployed Resource . If the old resources had  persistent: true  set and the\n metadata.name  haven't changed, the new MongoDB pods will\nreuse the data from the old pods. Delete the old CRDs. Once all the resources are up and running, delete all of the v0.9\nCRDs as the   no longer watches them: Run the following command to verify the old CRDs were removed: The output of the command above should look similar to the following: To troubleshoot your  , see\n Review Logs from the  . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl mst <standalone-name> -o yaml > <standalone-conf-name>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mrs <replicaset-name> -o yaml > <replicaset-conf-name>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get msc <shardedcluster-name> -o yaml > <shardedcluster-conf-name>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\nspec:\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: OPERATOR_ENV\nspec.template.spec.containers.name.env.value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPERATOR_ENV\n          value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: WATCH_NAMESPACE\nspec.template.spec.containers.name.env.value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: WATCH_NAMESPACE\n          value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMANAGED_SECURITY_CONTEXT\nspec.template.spec.containers.name.env.value:\nfalse"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MANAGED_SECURITY_CONTEXT\n          value: true"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f mongodb-enterprise.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template helm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set namespace=<testNamespace> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set namespace=<testNamespace> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set namespace=<testNamespace> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values.yaml\nkubectl apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-appdb\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-database-pods\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: OPERATOR_ENV\nspec.template.spec.containers.name.env.value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPERATOR_ENV\n          value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: WATCH_NAMESPACE\nspec.template.spec.containers.name.env.value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: WATCH_NAMESPACE\n          value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMANAGED_SECURITY_CONTEXT\nspec.template.spec.containers.name.env.value:\ntrue"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MANAGED_SECURITY_CONTEXT\n          value: true"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f mongodb-enterprise-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template helm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "registry:\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# OpenShift manages security context on its own\nmanagedSecurityContext: true"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.imagePullSecrets=<openshift-pull-secret> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set registry.imagePullSecrets=<openshift-pull-secret> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "registry:\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# OpenShift manages security context on its own\nmanagedSecurityContext: true"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set registry.imagePullSecrets=<openshift-pull-secret> \\\n--set namespace=<testNamespace> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set registry.imagePullSecrets=<openshift-pull-secret> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "registry:\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# OpenShift manages security context on its own\nmanagedSecurityContext: true"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n # Execution environment for the operator, dev or prod. Use dev for more verbose logging\n env: prod"
                },
                {
                    "lang": "sh",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set registry.pullPolicy=IfNotPresent \\\n--set registry.imagePullSecrets=<openshift-pull-secret> \\\n--set namespace=<testNamespace> \\\nhelm_chart > operator.yaml \\\n-- values helm_chart/values-openshift.yaml\noc apply -f operator.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get crds"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                 CREATED AT\nmongodb.mongodb.com                  2019-03-27T19:30:09Z\nmongodbreplicasets.mongodb.com       2018-12-07T18:25:42Z\nmongodbshardedclusters.mongodb.com   2018-12-07T18:25:42Z\nmongodbstandalones.mongodb.com       2018-12-07T18:25:42Z"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mst --all"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mrs --all"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete msc --all"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <resource-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete crd mongodbreplicasets.mongodb.com"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete crd mongodbshardedclusters.mongodb.com"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete crd mongodbstandalones.mongodb.com"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get crds"
                },
                {
                    "lang": "sh",
                    "value": "NAME                  CREATED AT\nmongodb.mongodb.com   2019-03-27T19:30:09Z"
                }
            ],
            "preview": "Version 0.10 of the  consolidated the\nMongoDbStandalone, MongoDbShardedCluster, and\nMongoDbReplicaSet  into a\nsingle CustomResourceDefinition called MongoDB.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/manage-database-users-scram",
            "title": "Manage Database Users using SCRAM Authentication",
            "headings": [
                "Considerations",
                "Supported SCRAM Implementations",
                "Supported Authentication Mechanisms",
                "Prerequisites",
                "Add a Database User",
                "Create User Secret",
                "Configure kubectl to default to your namespace.",
                "Copy the following example .",
                "Create a new User Secret YAML file.",
                "Change the highlighted lines.",
                "Save the User Secret file with a .yaml extension.",
                "Create MongoDBUser",
                "Copy the following example MongoDBUser.",
                "Create a new MongoDBUser file.",
                "Change the highlighted lines.",
                "Add any additional roles for the user to the MongoDBUser.",
                "Save the MongoDBUser file with a .yaml extension.",
                "Create the user.",
                "View the newly created user in .",
                "Delete a Database User",
                "Change Authentication Mechanism"
            ],
            "paragraphs": "The   supports managing database users using SCRAM\nauthentication on MongoDB deployments. When you specify  SCRAM  as the authentication mechanism, the\nimplementation of SCRAM used depends upon: The version of MongoDB and If the database is the Application Database or another database. MongoDB Version Database SCRAM Implementation 3.6 or earlier Any except Application Database SCRAM-SHA-1 4.0 or later Any except Application Database SCRAM-SHA-256 Any Application Database SCRAM-SHA-1 The   supports only SCRAM and X.509 authentication\nmechanisms in deployments it creates. In an Operator-created\ndeployment, you cannot use   to: After enabling SCRAM authentication, you can add SCRAM users using the\n  interface or the MongoDBUser  . Configure other authentication mechanisms for deployments. Manage users  not  using SCRAM or X.509 authentication. Before managing database users, you must deploy a\n standalone ,\n replica set , or\n sharded cluster . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : You can choose to use a cleartext password: or you can choose to use a Base64-encoded password: Make sure to copy the desired password configuration. Plaintext\npasswords use  stringData.password  and Base64-encoded\npasswords use  data.password Open your preferred text editor. Paste this User Secret into a new text file. Use the following table to guide you through changing the highlighted\nlines in the Secret: Key Type Description Example metadata.name string Name of the database password secret. Resource names must be 44 characters or less. mms-scram-user-1-password stringData.password string Plaintext password for the desired user. Use this option and value  or   data.password . You\ncan't use both. <my-plain-text-password> data.password string Base64-encoded password for the desired user. Use this option and value  or   stringData.password .\nYou can't use both. You must encode your password into Base64 yourself then\npaste the resulting value with this option. There are\ntools for most every platform and multiple web-based\ntools as well. <my-base64-encoded-password> Open your preferred text editor. Paste this MongoDBUser into a new YAML file. Use the following table to guide you through changing the highlighted\nlines in the MongoDBUser YAML file: Key Type Description Example metadata.name string Name of the database user resource. Resource names must be 44 characters or less. mms-scram-user-1 spec.username string Name of the database user. mms-scram-user-1 spec.passwordSecretKeyRef.name string metadata.name  value of the   that stores the\nuser's password. my-resource spec.mongodbResourceRef.name string Name of the  MongoDB resource  to\nwhich this user is associated. my-resource spec.roles.db string Database on which the  role  can act. admin spec.roles.name string Name of the  role  to grant the database\nuser. The role name can be any\n built-in MongoDB role  or\n custom role  that\nexists in  . readWriteAnyDatabase You may grant additional roles to this user. Invoke the following   command to create your database user: You can view the newly-created user in  : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. To delete a database user, pass the  metadata.name  from the user\nMongoDBUser to the following command: To change your user authenication from SCRAM to X.509: Disable authentication. Under  spec.security.authentication , change  enabled  to\n false . Reapply the user's resource definition. Wait for the MongoDBResource to reach the  running  state. Enable SCRAM authentication. Under  spec.security.authentication , change  enabled  to\n true  and set  modes  to  [\"SCRAM\"] . Reapply the MongoDBUser resource. Wait for the MongoDBResource to reach the  running  state.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: <mms-scram-user-1>\nspec:\n  passwordSecretKeyRef:\n    name: <mms-user-1-password>\n    # Match to metadata.name of the User Secret\n    key: password\n  username: \"<mms-scram-user-1>\"\n  db: \"admin\" #\n  mongodbResourceRef:\n    name: \"<my-replica-set>\"\n    # Match to MongoDB resource using authenticaiton\n  roles:\n    - db: \"admin\"\n      name: \"clusterAdmin\"\n    - db: \"admin\"\n      name: \"userAdminAnyDatabase\"\n    - db: \"admin\"\n      name: \"readWrite\"\n    - db: \"admin\"\n      name: \"userAdminAnyDatabase\"\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mdbu <metadata.name>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  security:\n    authentication:\n      enabled : false"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  security:\n    authentication:\n      enabled : true\n      modes: [\"SCRAM\"]"
                }
            ],
            "preview": "The  supports managing database users using SCRAM\nauthentication on MongoDB deployments.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-om-resource",
            "title": "Plan Your Ops Manager Resource",
            "headings": [
                "Architecture",
                "Considerations",
                "Encryption Key",
                "Application Database",
                "Topology",
                "Authentication",
                "Offline Deployments",
                "Backup",
                "Oplog Store",
                "S3 Snapshot Store",
                "Disable Backup",
                "Ops Manager Application Access",
                "Managing External MongoDB Deployments",
                "Prerequisites"
            ],
            "paragraphs": "MongoDB   is an enterprise application that manages, backs\nup, and monitors MongoDB deployments. With  , you can scale and\nupgrade MongoDB, optimize queries, perform point-in-time restores,\nreceive performance alerts, and more. To easily manage and maintain\n  and its underlying database, you can use the   to run\n  as a container on  . Before you deploy an   resource, make sure you read the\n considerations  and complete\nthe  prerequisites . The   manages and monitors each  MongoDBOpsManager \n custom resource  through\na  resource definition file  that you\n create . Every time you create or update a\nresource definition, the   performs the following\nreconciliation process: The   creates or updates the   that\nruns the  mms-application-database . The Application Database\nis always deployed as a  replica-set  with\n SCRAM-SHA authentication  enabled. Each database\n  runs an instance of the MongoDB Agent which is configured\ndirectly by the  . The   creates or updates the   that\nruns the    . The   instance connects to the\nApplication Database. The   ensures the   for the\n backup-daemon  is running unless  backup  is disabled. The   consists of a\nsingle  . The Backup Daemon connects to the Application\nDatabase. The   registers the  first user \nwith the  Global Owner  role and saves a public API key to a secret for later use. The   configures the Backup Daemon using\n    according to the  spec.backup.enabled \nspecification in the    resource definition . The   generates an encryption key to protect sensitive\ninformation in the  mms-application-database . The  \nsaves this key in a   in the same namespace as the  \nresource. The   names the secret\n <om-resource-name>-gen-key . If you remove the   resource, the key remains stored in the\nsecret on the   cluster. If you stored the Application Database in\na   and you create another   resource with the same name,\nthe   reuses the secret. If you create an  \nresource with a different name, then   creates a new\nsecret and Application Database, and the old secret isn't reused. When you create an instance of   through the  , the\n mms-application-database  is deployed as a  replica set .\nYou can't configure the Application Database as a  standalone \ndatabase or  sharded cluster . If you have concerns about\nperformance or size requirements for the Application Database, contact\n MongoDB Support . The   enforces  SCRAM-SHA-1 \n authentication  on\nthe Application Database. The   creates the database user which   uses to\nconnect to the Application Database. This database user has the\nfollowing attributes: The   database user's name and roles cannot be modified. However,\nyou can set the database user's password by  creating a\nsecret  and can later update the password by editing\nthat secret. If you don't create a secret, or if you delete a previously\ncreated secret, then the   automatically generates a password and stores it internally. If you need to authenticate to the Application Database as a\ndifferent user: Username mongodb-ops-manager Authentication Database admin Roles Deploy the   resource Add a new user  to\nthe database using the  mongo  shell. The   bundles MongoDB Enterprise version 4.2.2\nwith the  Application Database  image to enable offline\ndeployments of   resources. To deploy   inside   without an Internet connection, omit\nthe  spec.applicationDatabase.version  setting or leave the\nvalue empty. When you upgrade the  , the Application\nDatabase performs a rolling upgrade for new patch releases of MongoDB,\nsuch as from  4.2.2-ent  to  4.2.3-ent . However, the Application\nDatabase does not automatically upgrade between major versions,\nsuch as from  4.2.2-ent  to  4.4.0-ent .  enables  mms-backup-functional-overview  by\ndefault. The   deploys a   comprised of\none pod to host the  backup-daemon , and then creates a  \nand   for the Backup Daemon's  head database  . The\n  uses the  Ops Manager API  to\nenable the Backup Daemon and configure the head database. To configure Backup, you must create MongoDB database resources\nfor the  oplog store  and  \n snapshot store . The   resource\nremains in a  Pending  state until these Backup resources are\nconfigured. You must deploy a three-member replica set to store your  oplog\nslices . If you enable  SCRAM  authentication on the oplog database, you\nmust: Specify a MongoDB version earlier than v4.0 in the oplog database\nresource definition. Create a MongoDB user resource to connect   to the oplog\ndatabase. Specify the  name \nof the user in the   resource definition. To configure an    snapshot store , you must: You can update any additional    configuration settings \nthat are not managed by the   through the  . Have an oplog store already configured. Create an     or  -compatible bucket to store your database\nBackup  snapshots . Deploy a three-member replica set to store snapshot metadata. To disable backup after you enabled it: Set the        spec.backup.enabled \nsetting to  false . Disable backups  in the  . Delete the  backup-daemon   : The   and   for the Backup Daemon's\n head database  are not deleted when you delete the\n backup-daemon   . You can retrieve stored data\nbefore you delete these   resources. To learn about reclaiming  , see the\n Kubernetes documentation . By default, the   doesn't create a   service to route\ntraffic originating from outside of the   cluster to the  \napplication. To access the   application, you can: The simplest method is configuring the   to create a  \nservice that routes external traffic to the   application. The\n  deployment procedure instructs you to add the following\nsettings to the   specification that configures the\n  to create a service: Configure the   to create a   service. Create a   service manually. MongoDB recommends using a\n LoadBalancer    service if your cloud provider supports it. If you're using OpenShift, use\n routes . Use a third-party service, such as Istio. spec. externalConnectivity spec.externalConnectivity. type When you deploy   with the  ,   can manage\nMongoDB database resources deployed: If   will manage MongoDB database resources deployed to\ndifferent   clusters than   or outside of   clusters,\nyou must: To the same   cluster as  . Outside of   clusters. Add the  mms.centralUrl  setting to  spec.configuration  in the\n  resource specification. Set the value to the URL by which   is exposed outside of the\n  cluster: Update the ConfigMaps  referenced by\nall MongoDB database resources inside the   cluster that you\ndeployed with the  . Set  data.baseUrl  to the same value of the\n spec.configuration.mms.centralUrl \nsetting in the   resource specification. This includes the ConfigMaps that the  MongoDB database resources\nfor the oplog and snapshot stores reference . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Install  the   1.4.1\nor newer. Ensure that the host on which you want to deploy   has a\nminimum of five gigabytes of memory. Create a     for an admin user in the same  \nas the   resource. When you deploy the   resource,   creates a user with\nthese credentials and grants it the  Global Owner  role.\nUse these credentials to log in to   for the first time. Once\n  is deployed, you should change the password or remove this\nsecret. ( Optional ) To set the password for the   database user,\ncreate a   in the same   as the   resource. The   creates the database user that   uses to\nconnect to the  mms-application-database . You can set the\npassword for this database user by invoking the following command to\ncreate a secret: If you don't create a secret, then the   automatically\ngenerates a password and stores it internally. For more information,\nsee  Authentication . If you choose to create a secret for the   database user,\nyou must specify the secret's\n name \nin the   resource definition. By default, the\n  looks for the password value in the  password \nkey. If you stored the password value in a different key, you\nmust also specify that\n key \nname in the   resource definition. ( Optional ). To configure Backup to an   snapshot store, create\na   in the same namespace as the   resource. This secret stores your   credentials so that the\n  can connect   to your     or\n -compatible bucket. The secret must contain the following\nkey-value pairs: To create the secret, invoke the following command: To learn more about managing   blockstore snapshot storage, see the  Prerequisites . Key Value accessKey The access key ID of the user who owns the   or\n -compatible bucket. secretKey The secret key of the user who owns the   or\n -compatible bucket.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl delete statefulset <metadata.name>-backup-daemon -n <namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  configuration:\n    mms.centralUrl: https://a9a8f8566e0094380b5c257746627b82-1037623671.us-east-1.elb.example.com:8080/"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <adminusercredentials> \\\n  --from-literal=Username=\"<username>\" \\\n  --from-literal=Password=\"<password>\" \\\n  --from-literal=FirstName=\"<firstname>\" \\\n  --from-literal=LastName=\"<lastname>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <om-db-user-secret-name> \\\n  --from-literal=password=\"<om-db-user-password>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <my-aws-s3-credentials> \\\n  --from-literal=accessKey=\"<AKIAIOSFODNN7EXAMPLE>\" \\\n  --from-literal=secretKey=\"<wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY>\""
                }
            ],
            "preview": "MongoDB  is an enterprise application that manages, backs\nup, and monitors MongoDB deployments. With , you can scale and\nupgrade MongoDB, optimize queries, perform point-in-time restores,\nreceive performance alerts, and more. To easily manage and maintain\n and its underlying database, you can use the  to run\n as a container on .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-operator-install",
            "title": "Plan your  Installation",
            "headings": [
                "Considerations",
                " Compatibility",
                "MANAGED_SECURITY_CONTEXT for  OpenShift Deployments",
                "Docker Container Details",
                "Validation Webhook",
                " Deployment Scopes",
                "Operator in Same Namespace as Resources",
                "Operator in Different Namespace Than Resources",
                "Cluster-Wide Scope",
                "Prerequisites"
            ],
            "paragraphs": "Use the   to deploy: To deploy MongoDB resources with the  , you need an\n  instance. Deploy this instance to   using the Operator or\noutside   using\n traditional installation methods . The\nOperator uses     methods to deploy then manage MongoDB\nresources. Ops Manager resources MongoDB standalone, replica set, and sharded cluster resources You can use the   to deploy MongoDB resources with\n  version 4.0.11 or later and Cloud Manager. At any place in\nthis guide that says  Ops Manager , you can substitute\n Cloud Manager .  is compatible with   v1.13 or later. When you deploy the   to OpenShift, you must set the\n MANAGED_SECURITY_CONTEXT  flag to  true . This value is set for you\nin the \nand \nfiles included in the . For more information on modifying this value, see the  instructions  for the installation method you want to use. MongoDB builds the container images from the latest builds of the\nfollowing operating systems: MongoDB, Inc. updates all packages on these images before releasing\nthem every three weeks. If you get your   from... ...the Container uses quay.io \nor  Ubuntu 16.04 OpenShift Red Hat Enterprise Linux 7 The   uses a webhook to prevent users from applying invalid\nresource definitions. The webhook rejects these requests immediately and\nthe   doesn't create or update the resource. The  ClusterRole  and  ClusterRoleBinding  for the webhook are\nincluded in the default configuration files that you apply during\ninstallation. To create the role and binding, you must have\n cluster-admin privileges . If you apply an invalid resource definition, the webhook returns\na message that describes the error to the shell: The validation webhook is not required to create or update resources. If\nyou omit the validation webhook, remove its role and binding from the\ndefault configuration, or have insufficient privileges to run it, the\n  performs the same validations when it reconciles each\nresource. The   marks resources as  Failed  if validation\nencounters a critical error. For non-critical errors, the  \nissues warnings. You can deploy the   with different scopes based on where\nyou want to deploy   and   resources: Operator in Same Namespace as Resources   (Default) Operator in Different Namespace Than Resources Cluster-Wide Scope You scope the   to a namespace. The   watches\n  and   in that same  . This is the default scope when you install the   using the\n installation instructions . You scope the   to a namespace. The   watches\n  and   in the   you specify. You must use  helm  to install the   with this scope.\nFollow the relevant  helm   installation instructions ,\nbut use the following command to set the namespace for the\n  to watch: Setting the namespace ensures that: The namespace you want the   to watch has the correct\nroles and role bindings. The   can watch and create resources in the namespace. You scope the   to a cluster. The   watches\n  and   in all   in the   cluster. You must use  helm  to install the   with this scope.\nFollow the relevant  helm   installation instructions ,\nbut make the following adjustments: You can deploy only one Operator with a cluster-wide scope per  \ncluster. Use the following command to set the   to watch all\nnamespaces: Create the required service accounts for each namespace where you\nwant to deploy   and  : To install the MongoDB  , you must: Have a   solution available to use. If you need a   solution, see the  \n documentation on picking the right solution . Have a running  . Your   installation must run an active   service. If\nthe   host's clock falls out of sync, that host can't\ncommunicate with the  . To learn how to check your   service for your Ops Manager\nhost, see the documentation for\n Ubuntu  or\n RHEL . Clone the . You can use  Helm  to install the\n . To learn how to install Helm, see its\n. Create a   for your   deployment. By default, The\n  uses the  mongodb  namespace. To simplify your\ninstallation, consider creating a namespace labeled  mongodb \nusing the following   command: If you do not want to use the  mongodb  namespace, you can label\nyour namespace anything you like: ( Required for OpenShift Installs ) Create a   that\ncontains credentials authorized to pull images from the\n registry.connect.redhat.com  repository: If you have not already, obtain a Red Hat subscription. Create a  Registry Service Account . Click on your Registry Service Account, then click the\n Docker Configuration  tab. Download the  <account-name>-auth.json  file and open it in a\ntext editor. Copy the  registry.redhat.io  object, and paste another instance\nof this object into the file. Remember to add a comma after the\nfirst object. Rename the second object\n registry.connect.redhat.com , then save the file: Create a  openshift-pull-secret.yaml  file with the contents of\nthe modified  <account-name>-auth.json  file as  stringData \nnamed  .dockerconfigjson : The value you provide in the  metadata.name  field contains\nthe secret name. Provide this value when asked for the\n <openshift-pull-secret> . Create a   from the  openshift-pull-secret.yaml \nfile:",
            "code": [
                {
                    "lang": "none",
                    "value": "Error from server (shardPodSpec field is not configurable for\napplication databases as it is for sharded clusters and appdbs are\nreplica sets): error when creating \"my-ops-manager.yaml\":\nadmission webhook \"ompolicy.mongodb.com\" denied the request:\nshardPodSpec field is not configurable for application databases as\nit is for sharded clusters and appdbs are replica sets"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set operator.watchNamespace=<namespace> \\\nhelm_chart | kubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set operator.watchNamespace=* \\\nhelm_chart | kubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set namespace=<namespace> \\\nhelm_chart -x templates/database-roles.yaml | kubectl apply -f -"
                },
                {
                    "lang": "sh",
                    "value": "git clone https://github.com/mongodb/mongodb-enterprise-kubernetes.git"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace <namespaceName>"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"auths\": {\n   \"registry.redhat.io\": {\n    \"auth\": \"<encoded-string>\"\n   },\n  \"auths\": {\n   \"registry.connect.redhat.com\": {\n    \"auth\": \"<encoded-string>\"\n   }\n  }\n}"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: openshift-pull-secret\nstringData:\n  .dockerconfigjson: |\n      {\n        \"auths\": {\n          \"registry.redhat.io\": {\n            \"auth\": \"<encoded-string>\"\n          },\n          \"registry.connect.redhat.com\": {\n            \"auth\": \"<encoded-string>\"\n          }\n        }\n      }\ntype: kubernetes.io/dockerconfigjson"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f openshift-pull-secret.yaml -n <namespace>"
                }
            ],
            "preview": "Use the  to deploy:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-x509-auth",
            "title": "Secure Client Authentication with X.509",
            "headings": [
                "General Prerequisites",
                "Configure X.509 Client Authentication for a Replica Set",
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the TLS settings for your replica set resource.",
                "Configure the general X.509 settings for your replica set resource.",
                "Save your replica set config file.",
                "Update and restart your replica set deployment.",
                "Check the status of your deployment.",
                "Retrieve the CSRs for each host and agent in your deployment.",
                "Approve the CSR for each host in your deployment.",
                "Approve the CSR for each agent in your deployment.",
                "Track the status of your deployment.",
                "Configure kubectl to default to your namespace.",
                "Create the  for your TLS certificates.",
                "Create the  to link your  with your deployment.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the TLS settings for your replica set resource using a Custom Certificate Authority.",
                "Configure the general X.509 settings for your replica set resource.",
                "Save your replica set config file.",
                "Apply your changes to your replica set deployment.",
                "Track the status of your deployment.",
                "Configure X.509 Client Authentication for a Sharded Cluster",
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the TLS settings for your sharded cluster resource.",
                "Configure the general X.509 settings for your sharded cluster resource.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Check the status of your deployment.",
                "Retrieve the CSRs for each host and agent in your deployment.",
                "Approve the CSR for each host in your deployment.",
                "Approve the CSR for each agent in your deployment.",
                "Track the status of your deployment.",
                "Configure kubectl to default to your namespace.",
                "Create the  for your Shards' TLS certificates.",
                "Create the  for your config server's TLS certificates.",
                "Create the  for your mongos server's TLS certificates.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the TLS settings for your sharded cluster resource using a Custom Certificate Authority.",
                "Configure the general X.509 settings for your sharded cluster resource.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Check the status of your deployment.",
                "Retrieve the CSRs for each host and agent in your deployment.",
                "Approve the CSR for each host in your deployment.",
                "Approve the CSR for each agent in your deployment.",
                "Track the status of your deployment."
            ],
            "paragraphs": "The   can use X.509 certificates to authenticate your\nclient applications to your MongoDB deployments. This guide instructs you on how to configure: To secure your deployment using   and client X.509 authentication,\nyou may use  's   or your own  : X.509 authentication from clients to your MongoDB instances.  to encrypt connections between MongoDB hosts in a replica set\nor sharded cluster.  to encrypt connections client applications and MongoDB\ndeployments. Before you secure your MongoDB deployment using   encryption,\ncomplete the following: Install the Kubernetes Operator Create Credentials for the Kubernetes Operator Enabling X.509 authentication at the project level configures all\nagents to use X.509 client authentication when communicating with\nMongoDB deployments. X.509 client authentication requires one of the following:  4.1.7 or later  4.0.11 or later Before you secure your replica set using   encryption, complete the\nfollowing: Deploy the Replica Set \nthat you want to secure Deploy the Replica Set \nthat you want to secure Create a   file for each of the following components: PEM file purpose Save File As... Your custom  ca-pem Each member of your replica set <metadata.name>-<X>-pem Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional Set this value to  true  to enable   on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true , authentication is enabled on the\nMongoDB deployment. true array Conditional If you enabled authentication, you must set an authentication\nmechanism. Accepted values are  X509 . X509 Invoke the following   command to update and restart your\n replica set : The   creates the MongoDB resources and requests the\n    to approve the database host's certificates. Run the\nfollowing command to verify that the certificates are pending\napproval: The  status  field of the output should resemble the following: If you do not see the  status.message  above, see\n Troubleshooting the   to help diagnose the issue. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the replica set\nexample: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the replica set\nexample: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create the   that stores\nthe replica set's certificates: This example covers a three-member replica set. If you have more than\nthree members, you can add them to the certificate using the\n --from-file  option. Run this  kubectl  command to link your   to your replica\nset: Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Optional If you use a custom   and have created the  \nthat stores it, add the secret's name. <custom-ca> To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true , authentication is enabled on the\nMongoDB deployment. true array Conditional If you enabled authentication, you must set an authentication\nmechanism. Accepted values are  X509 . X509 Invoke the following   command to updated your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. Before you secure your replica set using   encryption, complete the\nfollowing: Deploy the Sharded Cluster \nthat you want to secure Create a   file for each of the following components: PEM file purpose Save File As... Your custom  ca-pem Each shard in your sharded cluster <metadata.name>-<Y>-<X>-pem Each member of your config server replica set <metadata.name>-config-<X>-pem Each  <metadata.name>-mongos-<X>-pem Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. Deploy the Sharded Cluster \nthat you want to secure Create a   file for each of the following components: PEM file purpose Save File As... Your custom  ca-pem Each shard in your sharded cluster <metadata.name>-<Y>-<X>-pem Each member of your config server replica set <metadata.name>-config-<X>-pem Each  <metadata.name>-mongos-<X>-pem Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional Set this value to  true  to enable   on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true , authentication is enabled on the\nMongoDB deployment. true array Conditional If you enabled authentication, you must set an authentication\nmechanism. Accepted values are  X509 . X509 Invoke the following   command to update and restart your\n sharded cluster : The   creates the MongoDB resources and requests the\n    to approve the database host's certificates. Run the\nfollowing command to verify that the certificates are pending\napproval: The  status  field of the output should resemble the following: If you do not see the  status.message  above, see\n Troubleshooting the   to help diagnose the issue. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the sharded cluster\nexample: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the sharded cluster\nexample: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create the   that stores\nthe sharded cluster shards' certificates: This example covers a two-shard sharded cluster with five members per\nshard. If you have more than two shards or five members per shard,\nyou can add them to the certificate using the  --from-file  option. Run this  kubectl  command to create the   that stores\nthe sharded cluster config server's certificates: Run this  kubectl  command to create the   that stores\nthe sharded cluster   certificates: Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Optional If you use a custom   and have created the  \nthat stores it, add the secret's name. <custom-ca> To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true , authentication is enabled on the\nMongoDB deployment. true array Conditional If you enabled authentication, you must set an authentication\nmechanism. Accepted values are  X509 . X509 Invoke the following   command to update and restart your\n sharded cluster : The   creates the MongoDB resources and requests the\n    to approve the database host's certificates. Run the\nfollowing command to verify that the certificates are pending\napproval: The  status  field of the output should resemble the following: If you do not see the  status.message  above, see\n Troubleshooting the   to help diagnose the issue. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the sharded cluster\nexample: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the sharded cluster\nexample: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "status:\n  lastTransition: 2019-05-01T15:36:59Z\n  message: Not all certificates have been approved by Kubernetes CA\n  phase: Failed\n  type: \"\"\n  version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmms-automation-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-backup-agent.mongodb                    15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmms-monitoring-agent.mongodb                15m       system:serviceaccount:mongodb:mongodb-enterprise-operator   Approved,Issued\nmy-secure-rs-0.mongodb                      33s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-1.mongodb                      31s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-rs-2.mongodb                      24s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-rs-0.mongodb\nkubectl certificate approve my-secure-rs-1.mongodb\nkubectl certificate approve my-secure-rs-2.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve mms-automation-agent.mongodb\nkubectl certificate approve mms-backup-agent.mongodb\nkubectl certificate approve mms-monitoring-agent.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <metadata.name>-cert \\\n  --from-file=<metadata.name>-0-pem \\\n  --from-file=<metadata.name>-1-pem \\\n  --from-file=<metadata.name>-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap custom-ca --from-file=ca-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "status:\n  lastTransition: 2019-05-01T15:36:59Z\n  message: Not all certificates have been approved by Kubernetes CA\n  phase: Failed\n  type: \"\"\n  version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmy-secure-sc-0-0.mongodb                    30s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-1.mongodb                    28s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-2.mongodb                    27s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-0.mongodb                    22s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-1.mongodb                    13s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-2.mongodb                    6s        system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-0.mongodb               36s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-1.mongodb               34s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-2.mongodb               32s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-0.mongodb               49s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-1.mongodb               42s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-sc-0-0.mongodb\nkubectl certificate approve my-secure-sc-0-1.mongodb\nkubectl certificate approve my-secure-sc-0-2.mongodb\nkubectl certificate approve my-secure-sc-1-0.mongodb\nkubectl certificate approve my-secure-sc-1-1.mongodb\nkubectl certificate approve my-secure-sc-1-2.mongodb\nkubectl certificate approve my-secure-sc-config-0.mongodb\nkubectl certificate approve my-secure-sc-config-1.mongodb\nkubectl certificate approve my-secure-sc-config-2.mongodb\nkubectl certificate approve my-secure-sc-mongos-0.mongodb\nkubectl certificate approve my-secure-sc-mongos-1.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve mms-automation-agent.mongodb\nkubectl certificate approve mms-backup-agent.mongodb\nkubectl certificate approve mms-monitoring-agent.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-0-cert \\\n       --from-file=<metadata.name>-0-0-pem \\\n       --from-file=<metadata.name>-0-1-pem \\\n       --from-file=<metadata.name>-0-2-pem \\\n       --from-file=<metadata.name>-0-3-pem \\\n       --from-file=<metadata.name>-0-4-pem\n\nkubectl -n mongodb create secret generic <metadata.name>-1-cert \\\n       --from-file=<metadata.name>-1-0-pem \\\n       --from-file=<metadata.name>-1-1-pem \\\n       --from-file=<metadata.name>-1-2-pem \\\n       --from-file=<metadata.name>-1-3-pem \\\n       --from-file=<metadata.name>-1-4-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-config-cert \\\n       --from-file=<metadata.name>-config-0-pem \\\n       --from-file=<metadata.name>-config-1-pem \\\n       --from-file=<metadata.name>-config-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-mongos-cert \\\n       --from-file=<metadata.name>-mongos-0-pem \\\n       --from-file=<metadata.name>-mongos-1-pem \\\n       --from-file=<metadata.name>-mongos-2-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "status:\n  lastTransition: 2019-05-01T15:36:59Z\n  message: Not all certificates have been approved by Kubernetes CA\n  phase: Failed\n  type: \"\"\n  version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmy-secure-sc-0-0.mongodb                    30s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-1.mongodb                    28s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-2.mongodb                    27s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-0.mongodb                    22s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-1.mongodb                    13s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-2.mongodb                    6s        system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-0.mongodb               36s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-1.mongodb               34s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-2.mongodb               32s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-0.mongodb               49s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-1.mongodb               42s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-sc-0-0.mongodb\nkubectl certificate approve my-secure-sc-0-1.mongodb\nkubectl certificate approve my-secure-sc-0-2.mongodb\nkubectl certificate approve my-secure-sc-1-0.mongodb\nkubectl certificate approve my-secure-sc-1-1.mongodb\nkubectl certificate approve my-secure-sc-1-2.mongodb\nkubectl certificate approve my-secure-sc-config-0.mongodb\nkubectl certificate approve my-secure-sc-config-1.mongodb\nkubectl certificate approve my-secure-sc-config-2.mongodb\nkubectl certificate approve my-secure-sc-mongos-0.mongodb\nkubectl certificate approve my-secure-sc-mongos-1.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve mms-automation-agent.mongodb\nkubectl certificate approve mms-backup-agent.mongodb\nkubectl certificate approve mms-monitoring-agent.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "The  can use X.509 certificates to authenticate your\nclient applications to your MongoDB deployments.",
            "tags": null,
            "facets": null
        }
    ]
}