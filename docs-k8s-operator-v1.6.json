{
    "url": "http://mongodb.com/docs/kubernetes-operator/v1.6",
    "includeInGlobalSearch": false,
    "documents": [
        {
            "slug": "openshift-quick-start",
            "title": "OpenShift Quick Start",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Clone the .",
                "Create a  for your  deployment.",
                "Configure kubectl to default to your namespace.",
                "Create a  that contains credentials authorized to pull images from the registry.connect.redhat.com repository.",
                "Install the ",
                "Create credentials and store them as a secret.",
                "Invoke the following command to create a ConfigMap.",
                "Deploy the replica set resource.",
                "Create a secret with your database user password",
                "Create a database user.",
                "Optional: View the newly created user in .",
                "Connect to the replica set."
            ],
            "paragraphs": " uses the   API and tools to manage MongoDB\nclusters.   works together with MongoDB  . This\ntutorial demonstrates how to deploy and connect to your first replica\nset in   from OpenShift with  . This tutorial requires: A running   cluster. By default, The   uses the  mongodb  namespace. To\nsimplify your installation, consider creating a namespace labeled\n mongodb  using the following   command: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you created: If you have not already, obtain a Red Hat subscription. Create a  Registry Service Account . Click on your Registry Service Account, then click the\n Docker Configuration  tab. Download the  <account-name>-auth.json  file and open it in a\ntext editor. Copy the  registry.redhat.io  object, and paste another instance\nof this object into the file. Remember to add a comma after the\nfirst object. Rename the second object\n registry.connect.redhat.com , then save the file: Create a  openshift-pull-secret.yaml  file with the contents of\nthe modified  <account-name>-auth.json  file as  stringData \nnamed  .dockerconfigjson : The value you provide in the  metadata.name  field contains\nthe secret name. Provide this value when asked for the\n <openshift-pull-secret> . Create a   from the  openshift-pull-secret.yaml \nfile: Invoke the following   command to install the   for\nMongoDB deployments: Add your  <openshift-pull-secret>  to the  ServiceAccount \ndefinitions in the     file: Invoke the following   command to install  : Run the following command: Provide your Public and Private Key values for the following\nparameters. To learn more, see  Create Credentials for the  . Provide your values for the following parameters. To learn more,\nsee the  parameter descriptions . Run the following command: You can choose to use a cleartext password or a Base64-encoded\npassword. Plaintext passwords use  stringData.password  and\nBase64-encoded passwords use  data.password . For a cleartext password, create and save the following YAML file: For a Base64-encoded password, create and save the following YAML\nfile: Provide your values for the following parameters. To learn more,\nsee the  parameter descriptions . Run the following command: You can view the newly-created user in  : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. Perform the following steps in the   or  \napplication, depending on where your clusters are hosted: Click  Deployment  in the left navigation. Click   for the deployment to which you want\nto connect. Click  Connect to this instance . Run the connection command in a terminal to connect to the\ndeployment.",
            "code": [
                {
                    "lang": "sh",
                    "value": "git clone https://github.com/mongodb/mongodb-enterprise-kubernetes.git"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=mongodb"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"auths\": {\n    \"registry.redhat.io\": {\n      \"auth\": \"<encoded-string>\"\n    },\n    \"registry.connect.redhat.com\": {\n      \"auth\": \"<encoded-string>\"\n    }\n  }\n}"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: openshift-pull-secret\nstringData:\n  .dockerconfigjson: |\n      {\n        \"auths\": {\n          \"registry.redhat.io\": {\n            \"auth\": \"<encoded-string>\"\n          },\n          \"registry.connect.redhat.com\": {\n            \"auth\": \"<encoded-string>\"\n          }\n        }\n      }\ntype: kubernetes.io/dockerconfigjson"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f openshift-pull-secret.yaml -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-appdb\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-database-pods\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f mongodb-enterprise-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb \\\n  create secret generic ops-manager-admin-key \\\n  --from-literal=\"user=<publicKey>\" \\\n  --from-literal=\"publicApiKey=<privateKey>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap myconfigmap \\\n  --from-literal=\"baseUrl=<myOpsManagerURL>\" \\\n  --from-literal=\"projectName=<myOpsManagerProjectName>\" \\ #Optional\n  --from-literal=\"orgId=<orgID>\" #Required for Global API Keys"
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | kubectl apply -f -\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: demo-mongodb-cluster-1\n  namespace: mongodb\nspec:\n  members: 3\n  version: 4.4.5-ent\n  type: ReplicaSet\n  authentication:\n    enabled: true\n    modes: [\"SHA\"]\n  opsManager:\n    configMapRef:\n      name: myconfigmap\n  credentials: ops-manager-admin-key\n  persistent: true\n  podSpec:\n    podTemplate:\n      spec:\n        containers:\n        - name: mongodb-enterprise-database\n          resources:\n            limits:\n              cpu: 2\n              memory: 1.5G\n            requests:\n              cpu: 1\n              memory: 1G\n            persistence:\n              single:\n                storage: 10Gi\nEOF"
                },
                {
                    "lang": "sh",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mms-user-1-password\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\nstringData:\n  password: <my-plain-text-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key"
                },
                {
                    "lang": "sh",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mms-user-1-password\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\ndata:\n  password: <base-64-encoded-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key"
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | kubectl apply -f -\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: mms-scram-user-1\nspec:\n  passwordSecretKeyRef:\n    name: mms-user-1-password\n    # Match to metadata.name of the User Secret\n    key: password\n  username: \"mms-scram-user-1\"\n  db: \"admin\" #\n  mongodbResourceRef:\n    name: \"demo-mongodb-cluster-1\"\n    # Match to MongoDB resource using authenticaiton\n  roles:\n  - db: \"admin\"\n    name: \"clusterAdmin\"\n  - db: \"admin\"\n    name: \"userAdminAnyDatabase\"\n  - db: \"admin\"\n    name: \"readWrite\"\n  - db: \"admin\"\n    name: \"userAdminAnyDatabase\"\nEOF"
                }
            ],
            "preview": " uses the  API and tools to manage MongoDB\nclusters.  works together with MongoDB . This\ntutorial demonstrates how to deploy and connect to your first replica\nset in  from OpenShift with .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "deploy",
            "title": "Deploy a MongoDB Database Resource",
            "headings": [],
            "paragraphs": "Use   to deploy a new standalone MongoDB instance. Use   to deploy a replica set. Use   to deploy a sharded cluster.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "manage-users",
            "title": "Manage Database Users",
            "headings": [],
            "paragraphs": "Manage database users using SCRAM authentication on MongoDB\ndeployments. Manage database users for deployments running with TLS and X.509\ninternal cluster authentication enabled.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "kind-quick-start",
            "title": "Kind Quick Start",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Clone the .",
                "Create a  for your  deployment.",
                "Configure kubectl to default to your namespace.",
                "Install the ",
                "Create credentials and store them as a secret.",
                "Invoke the following command to create a ConfigMap.",
                "Deploy the replica set resource.",
                "Create a secret with your database user password",
                "Create a database user.",
                "Optional: View the newly created user in .",
                "Connect to the replica set."
            ],
            "paragraphs": " uses the   API and tools to manage MongoDB\nclusters.   works together with MongoDB  . This\ntutorial demonstrates how to deploy and connect to your first replica\nset in   from   with  . This tutorial requires: A running   cluster. By default, The   uses the  mongodb  namespace. To\nsimplify your installation, consider creating a namespace labeled\n mongodb  using the following   command: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you created: Change to the directory in which you cloned the repository. Install the   for MongoDB deployments using the\nfollowing   command: Install the   using the following   command: Run the following command: Provide your Public and Private Key values for the following\nparameters. To learn more, see  Create Credentials for the  . Provide your values for the following parameters. To learn more,\nsee the  parameter descriptions . Run the following command: You can choose to use a cleartext password or a Base64-encoded\npassword. Plaintext passwords use  stringData.password  and\nBase64-encoded passwords use  data.password . For a cleartext password, create and save the following YAML file: For a Base64-encoded password, create and save the following YAML\nfile: Provide your values for the following parameters. To learn more,\nsee the  parameter descriptions . Run the following command: You can view the newly-created user in  : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. Perform the following steps in the   or  \napplication, depending on where your clusters are hosted: Click  Deployment  in the left navigation. Click   for the deployment to which you want\nto connect. Click  Connect to this instance . Run the connection command in a terminal to connect to the\ndeployment.",
            "code": [
                {
                    "lang": "sh",
                    "value": "git clone https://github.com/mongodb/mongodb-enterprise-kubernetes.git"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f crds.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f mongodb-enterprise.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb \\\n  create secret generic ops-manager-admin-key \\\n  --from-literal=\"user=<publicKey>\" \\\n  --from-literal=\"publicApiKey=<privateKey>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap myconfigmap \\\n  --from-literal=\"baseUrl=<myOpsManagerURL>\" \\\n  --from-literal=\"projectName=<myOpsManagerProjectName>\" \\ #Optional\n  --from-literal=\"orgId=<orgID>\" #Required for Global API Keys"
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | kubectl apply -f -\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: demo-mongodb-cluster-1\n  namespace: mongodb\nspec:\n  members: 3\n  version: 4.4.5-ent\n  type: ReplicaSet\n  authentication:\n    enabled: true\n    modes: [\"SHA\"]\n  opsManager:\n    configMapRef:\n      name: myconfigmap\n  credentials: ops-manager-admin-key\n  persistent: true\n  podSpec:\n    podTemplate:\n      spec:\n        containers:\n        - name: mongodb-enterprise-database\n          resources:\n            limits:\n              cpu: 2\n              memory: 1.5G\n            requests:\n              cpu: 1\n              memory: 1G\n            persistence:\n              single:\n                storage: 10Gi\nEOF"
                },
                {
                    "lang": "sh",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mms-user-1-password\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\nstringData:\n  password: <my-plain-text-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key"
                },
                {
                    "lang": "sh",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: mms-user-1-password\n  # corresponds to user.spec.passwordSecretKeyRef.name\ntype: Opaque\ndata:\n  password: <base-64-encoded-password>\n  # corresponds to user.spec.passwordSecretKeyRef.key"
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | kubectl apply -f -\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: mms-scram-user-1\nspec:\n  passwordSecretKeyRef:\n    name: mms-user-1-password\n    # Match to metadata.name of the User Secret\n    key: password\n  username: \"mms-scram-user-1\"\n  db: \"admin\" #\n  mongodbResourceRef:\n    name: \"demo-mongodb-cluster-1\"\n    # Match to MongoDB resource using authenticaiton\n  roles:\n  - db: \"admin\"\n    name: \"clusterAdmin\"\n  - db: \"admin\"\n    name: \"userAdminAnyDatabase\"\n  - db: \"admin\"\n    name: \"readWrite\"\n  - db: \"admin\"\n    name: \"userAdminAnyDatabase\"\nEOF"
                }
            ],
            "preview": " uses the  API and tools to manage MongoDB\nclusters.  works together with MongoDB . This\ntutorial demonstrates how to deploy and connect to your first replica\nset in  from  with .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "secure",
            "title": "Secure a Database Resource",
            "headings": [],
            "paragraphs": "Configure   for   deployments. Configure X.509 for client authentication. Configure   and X.509 for internal authentication.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference",
            "title": "Reference",
            "headings": [],
            "paragraphs": "Review the     object specification. Review the MongoDB   object specifications. Review settings that only the   can set. Find solutions to   issues. Review the EOL dates for   versions. Open Source licenses that the   uses.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "mdb-resources",
            "title": "Deploy and Configure MongoDB Database Resources",
            "headings": [],
            "paragraphs": "You can use the   to deploy and manage MongoDB clusters\nfrom the    , without having to configure them in\n  or  . Configure the   to deploy MongoDB database resources. Deploy a standalone, replica set, or sharded cluster resource. Modify the configuration of a MongoDB database resource. Configure authentication for client applications and encrypt\nconnections to your MongoDB resources. Configure authentication for MongoDB database users. Access database resources from inside or outside  .",
            "code": [],
            "preview": "You can use the  to deploy and manage MongoDB clusters\nfrom the  , without having to configure them in\n or .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "connect",
            "title": "Access Database Resources",
            "headings": [],
            "paragraphs": "The following pages describe how to connect to a MongoDB\ndatabase resource that is deployed to  : Connect to a MongoDB database resource from inside\nof the   cluster. Connect to a MongoDB database resource from outside\nof the   cluster.",
            "code": [],
            "preview": "The following pages describe how to connect to a MongoDB\ndatabase resource that is deployed to :",
            "tags": null,
            "facets": null
        },
        {
            "slug": "",
            "title": "MongoDB Enterprise Kubernetes Operator",
            "headings": [],
            "paragraphs": "The   translates the human knowledge of creating a MongoDB\ninstance into a scalable, repeatable, and standardized method.\nKubernetes needs help creating and managing stateful applications like\ndatabases. It needs to configure the network, persist storage, and\ndedicate computing capacity without additional human effort on each\ncontainer. The   manages the typical lifecycle events for a MongoDB\ncluster: provisioning storage and computing power, configuring network\nconnections, setting up users, and changing these settings as needed.\nIt accomplishes this using the Kubernetes API and tools. You provide the Operator with the specifications for your MongoDB\ncluster. The Operator uses this information to tell Kubernetes how to\nconfigure that cluster including provisioning storage, setting up the\nnetwork connections, and configuring other resources. The   works together with MongoDB  , which further\nconfigures to MongoDB clusters. When MongoDB is deployed and running in\nKubernetes, you can manage MongoDB tasks using  . You can then deploy MongoDB databases as you deploy them now after the\ncluster is created. You can use the   console to run MongoDB at\noptimal performance.",
            "code": [],
            "preview": "The  translates the human knowledge of creating a MongoDB\ninstance into a scalable, repeatable, and standardized method.\nKubernetes needs help creating and managing stateful applications like\ndatabases. It needs to configure the network, persist storage, and\ndedicate computing capacity without additional human effort on each\ncontainer.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "installation",
            "title": "Install and Configure the ",
            "headings": [],
            "paragraphs": "Review   deployment scopes, considerations, and\nprerequisites. Install the  . Upgrade from earlier versions of  .",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "upgrade",
            "title": "Upgrade the  from Prior Versions",
            "headings": [],
            "paragraphs": "Follow this upgrade procedure if you are running version 0.10 or\nlater. Follow this upgrade procedure if you are running version 0.9 or\nearlier. Starting in   version 1.3.0, you can only have one MongoDB\nresource per project. This document explains how to migrate existing\nprojects which have multiple MongoDB resources into configurations with\na single resource per project.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "configure-k8s-operator-for-mdb-resources",
            "title": "Configure the  for MongoDB Database Resources",
            "headings": [],
            "paragraphs": "Create a   so the   can create and update\n  in your   Project. Create a   to link the   to your  \nProject. Create an X.509 certificate to connect to an X.509-enabled\nMongoDB deployment.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "om-resources",
            "title": "Deploy and Configure Ops Manager Resources",
            "headings": [],
            "paragraphs": "Review the   resource architecture, considerations, and\nprerequisites. Use the   to deploy an   instance. Use the   to configure   to operate in\n Remote  mode. In Remote mode, the Backup Daemons and managed\nMongoDB resources download installation archives from HTTP endpoints\non a web server or S3-compatible file store deployed to your  \ncluster instead of from the Internet. Use the   to configure   to operate in\n Local  mode. In Local mode, the Backup Daemons and managed\nMongoDB resources download installation archives from a   that\nyou create for the   StatefulSet instead of from the Internet. Encrypt the connection between the application database replica set\nmembers.",
            "code": [],
            "preview": null,
            "tags": null,
            "facets": null
        },
        {
            "slug": "third-party-licenses",
            "title": "Third-Party Licenses",
            "headings": [
                "Apache License 2.0",
                "BSD (Berkeley Software Distribution) 2-Clause",
                "BSD (Berkeley Software Distribution) 3-Clause",
                "ISC (Internet Systems Consortium) License",
                "MIT (Massachusetts Institute of Technology) License",
                "MPL (Mozilla Public License) 2.0"
            ],
            "paragraphs": "MongoDB   uses third-party libraries or other resources\nthat may be distributed under licenses different than the MongoDB\nsoftware.  depends upon the following third-party packages. These\npackages are licensed as shown in the following list. Should MongoDB\nhave accidentally failed to list a required license, please\n contact the MongoDB Legal Department . License:  TL;DR  |  Full Text k8s.io/api k8s.io/apiextensions-apiserver k8s.io/apimachinery k8s.io/client-go k8s.io/kube-openapi License:  TL;DR  |  Full Text License:  TL;DR  |  Full Text fsnotify.v1 inf.v0 net oauth2 text time/rate ssh/terminal unix xerrors License:  TL;DR  |  Full Text License:  TL;DR  |  Full Text atomic multierr zap yaml License:  TL;DR  |  Full Text",
            "code": [],
            "preview": "MongoDB  uses third-party libraries or other resources\nthat may be distributed under licenses different than the MongoDB\nsoftware.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "release-notes",
            "title": "Release Notes for ",
            "headings": [
                " 1.6.1",
                "Ops Manager Resource Changes",
                "Docker Image Changes",
                "Bug Fixes",
                " 1.6.0",
                "MongoDB Resource Changes",
                "Kubernetes Operator Changes",
                "Bug Fixes",
                " 1.5.5",
                "MongoDB Resource Changes",
                "Bug Fixes",
                " 1.5.4",
                "MongoDB Resource Changes",
                "Kubernetes Operator Changes",
                "Bug Fixes",
                " 1.5.3",
                "Kubernetes Operator Changes",
                "Bug Fixes",
                " 1.5.2",
                "Ops Manager Resource Changes",
                "Kubernetes Operator Changes",
                "Bug Fixes",
                " 1.5.1",
                "Bug Fixes",
                "Known Issues",
                " 1.5.0",
                " Changes",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes",
                " 1.4.5",
                "CVE Description",
                "Common Weakness Enumeration",
                "Affected Versions",
                "Fixed Versions",
                " 1.4.4",
                "MongoDB Resource Changes",
                " Resource Changes (Beta Release)",
                "Bug Fixes",
                " 1.4.3",
                " Changes",
                "MongoDB Resource Changes",
                " Resource Changes (Beta Release)",
                "Bug Fixes",
                " 1.4.2",
                "MongoDB Resource Changes",
                " Resource Changes (Beta Release)",
                "Bug Fixes",
                " 1.4.1",
                " 1.4.0",
                "MongoDB Resource Changes",
                " Resource Changes (Beta Release)",
                "Bug Fixes",
                " 1.3.1",
                "MongoDB Resource Changes",
                "Ops Manager Resource Changes (Alpha Release)",
                " 1.3.0",
                "Specification Schema Changes",
                "Ops Manager Resource Changes (Alpha Release)",
                "Bug Fixes",
                " 1.2.5",
                "CVE Description",
                "Common Weakness Enumeration",
                "Affected Versions",
                "Fixed Versions",
                " 1.2.4",
                " 1.2.3",
                " 1.2.2",
                " 1.2.1",
                " 1.2",
                "GA Release",
                "Alpha Release",
                " 1.1",
                " 1.0",
                " 0.12",
                " 0.11",
                " 0.10",
                " 0.9",
                " 0.8",
                " 0.7",
                " 0.6",
                " 0.5",
                " 0.4",
                " 0.3",
                " 0.2",
                " 0.1"
            ],
            "paragraphs": "Released 2020-07-30  image for version 4.4.0 is available. The Red Hat  database  and  operator  Docker images are now based\non the latest UBI 7 release. Two high criticality issues have been\nresolved. The following Docker images have been released: Image Type Ubuntu 16.04 Red Hat UBI 7 quay.io/mongodb/mongodb-enterprise-operator:1.6.1 quay.io/mongodb/mongodb-enterprise-operator-ubi:1.6.1 MongoDB Database quay.io/mongodb/mongodb-enterprise-database:1.6.1 quay.io/mongodb/mongodb-enterprise-database-ubi:1.6.1 quay.io/mongodb/mongodb-enterprise-ops-manager:4.4.0 quay.io/mongodb/mongodb-enterprise-ops-manager-ubi:4.4.0 Fixes a bug where the   did not store a configuration of\nyour deployed resources in a  . Fixes a bug where the   did not allow passwords of any\nlength or complexity for Application Database, oplog store, and\nblockstore database resources defined in   resources. Fixes a bug where the authentication configuration was not removed\nfrom   or   projects when you remove a MongoDB\ndatabase resource. Released 2020-07-16 Supports LDAP as an authentication mechanism for MongoDB database\nresources you deploy with the  . For more information,\nsee the sample LDAP configurations on . LDAP authorization is not yet supported. Preserves backup history by retaining   cluster records when\nyou enable backup. Fixes a bug that prevented the   from raising errors when\na  projectName  contained spaces. Fixes a bug that prevented   to monitor for all MongoDB\ndatabase resources that you deploy with the  . Released 2020-07-02 Provides additional options for more granular configuration of\n mongod  /  mongos  processes. You can find an\nexample of how to apply these options in the\n /samples/mongodb/mongodb-options  file of the\n. Fixes a bug introduced in 1.5.4 where   would not tag\nprojects correctly when working on   versions older than 4.2.2.\nIn this version,   tags the projects correctly. Released 2020-06-22 Allows modification of authentication settings using the   UI if\nthe  spec.security.authentication  setting is not provided\nin the MongoDB resource object definition. Supports Helm  installation  with\n helm install  in addition to  helm template | kubectl apply .\n helm install  is now the recommended way to install with Helm. Supports configuring the MongoDB Agent authentication mechanism\nindependently from the cluster authentication mechanism. Supports configuring monitoring for the Application Database to send\nmetrics to  . To learn more about the monitoring function of\nthe MongoDB Agent, see\n MongoDB Agent . Fixes a bug that affected transitioning authentication mechanisms\nfrom X.509 to SCRAM. Fixes a bug that prevented the MongoDB Agent from reaching a goal\nstate if SCRAM configuration was changed in the   UI. Released 2020-05-29 Passes   and MongoDB deployment configuration properties as\n Secret environment variables . Correctly configures shutdown timeouts for   and the Backup\nDaemon. Fixes an issue where  -watched Secrets and ConfigMaps\ntriggered unnecessary reconciliations. Fixes an issue where the status of custom resources failed to update\nin OpenShift 3.11. Released 2020-05-08 Runs   and Backup Daemon pods under a dedicated service\naccount. Can configure the   to watch a subset of provided\n . You can find more information in the documentation. Can generate   without using subresources. Some versions of\nOpenshift 3.11 require this capability. To avoid using subresources,\nuse  --set subresourceEnabled=false  when installing the\n  with helm. Fixes setting the  spec.statefulSet  and\n spec.backup.statefulSet  fields on the\n MongoDBOpsManager  Resource. Fixes an issue that requires a restart of the   during\nsetup of webhook. Fixes an issue that could make an   resource to reach an\nunrecoverable state if the provided admin password has insufficient\nstrength. Released 2020-04-30 Deprecates the generation of   certificates by the  .\nIf you use  -generated certificates, warning messages now\nappear in the   logs. To configure secure deployments, see\n Secure a Database Resource . Fixes an issue where, when no authentication is configured by the\n , the   disables authentication in  .\nThe   no longer disables authentication unless you\nexplicitly set  spec.security.authentication.enabled  to\n false . When you configure the\n spec.statefulSet.spec  and\n spec.backup.statefulSet.spec  settings of the\n MongoDBOpsManager  resource, you can only\nconfigure the  spec.statefulSet.spec.template  and\n spec.backup.statefulSet.spec.template  fields. Any other\n spec.statefulSet.spec  or\n spec.backup.statefulSet.spec  field has no effect. Released 2020-04-24 Adds the ability to start the   with some but not all\nMongoDB   installed. Administrators can specify the container\nargument  watch-resource  to limit the   to deploy either\nMonogDB instances or  , or both. Adds the following new   configuration properties: When using a private docker registry, these properties must point\nto the relevant registries after you copy the images from the MongoDB distribution channels. INIT_OPS_MANAGER_IMAGE_REPOSITORY INIT_APPDB_IMAGE_REPOSITORY APPDB_IMAGE_REPOSITORY Increases support for custom   certificates with the\n spec.security.tls.secretRef  and  spec.security.tls.ca  configuration\nsettings. Deprecates   certificate generation by the  .\nMigrating to custom   certificates is recommended. See the \nfor new feature usage examples. Releases the  MongoDBOpsManager  resource as\nGenerally Available (GA). MongoDB now supports using the  \nto deploy   resources to   in production environments. Supports Backup Blockstore Snapshot Stores. Defaults to the Application Database as a metadata database for Backup\n  Snapshot Stores. Supports  spec.jvmParameters  and  spec.backup.jvmParameters  to add or\noverride JVM parameters in   and Backup Daemon processes. Automatically configures   and Backup Daemon JVM memory parameters\nbased on pod memory availability. Supports   for   and the Application Database. Adds more detailed information to the  status  field. Supports   Local Mode for  MongoDBOpsManager  resources with\nmultiple replicas by enabling users to specify\n PersistentVolumeClaimTemplates  in  spec.statefulSet.spec . Implements a new image versioning scheme. Known Issue : To enable   Snapshot stores in   4.2.10\nand 4.2.12, you must set  brs.s3.validation.testing: disabled  in\nthe  spec.configuration  property of your  \nresource specification. Removes the  spec.podSpec  configuration setting. Use\n spec.statefulSet.spec  instead. Removes the  spec.backup.podSpec  configuration setting. Use\n spec.backup.statefulSet.spec  instead. Fixes CVE-2020-7922:   Operator generates potentially insecure certificates. X.509 certificates generated by the   may allow an attacker with\naccess to the   cluster improper access to MongoDB instances.\nCustomers who do not use X.509 authentication, and those who do not use\nthe   to generate their X.509 certificates are unaffected. CWE-295: Improper Certificate Validation\nCVSS score: 6.4\nCVSS:3.1/AV:A/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:N 1.0, 1.1 1.2.0 - 1.2.4 1.3.0 - 1.3.1 1.4.0 - 1.4.4 1.2.5 1.4.5 and above Supports changes in the  Cloud Manager API . Properly terminates resources with a termination hook. Implements stricter validations. MongoDB resources: Fixes an issue when working with   with custom  \ncertificates. Released 2020-02-24 Adds a  webhook  to validate\na   configuration. Adds support for sidecars for   pods using the\n spec.podSpec.podTemplate  setting. Allows users to change the  PodSecurityContext  to allow privileged\nsidecar containers. Adds the  spec.podSpec  configuration settings for\n , the Backup Daemon, and the Application Database. See\n Ops Manager Resource Specification .  image for version 4.2.8 is available. See the  for new\nfeature usage examples. MongoDB resources: Fixes potential race conditions when deleting  .  resources: Supports the  spec.clusterDomain  setting for  \nand Application Database resources. No longer starts monitoring and backup processes for the Application\nDatabase. Released 2020-01-24 Runs MongoDB database   pods under a dedicated   service\naccount:  mongodb-enterprise-database-pods . Adds the  spec.podSpec.podTemplate  setting, which allows\nyou to apply templates to   pods that the  \ngenerates for each database  . Renames the  spec.clusterName  setting to\n spec.clusterDomain . Adds  offline mode support  for the Application\nDatabase. Bundles MongoDB Enterprise version 4.2.2 with the\nApplication Database image. Internet access is not required to\ninstall the application database if\n spec.applicationDatabase.version  is set to\n 4.2.2-ent  or omitted. Renames the  spec.clusterName  setting to\n spec.clusterDomain .  images for versions 4.2.6 and 4.2.7 are available. See the  for new\nfeature usage examples. MongoDB resources: Fixes the order of sharded cluster component creation. Allows   to be enabled on Amazon EKS.  resources: Enables the   to use the  spec.clusterDomain  setting. Released 2019-12-13 Includes  CVE fixes  and\n RHSA security fixes . Fixes an issue that prevented backup from starting on MongoDB 4.0. Released 2019-12-09 Adds split horizon DNS support for MongoDB replica sets, which allows\nclients to connect to a replica set from outside of the  \ncluster. Supports requests for  -generated certificates for\nadditional certificate domains, which makes them valid for the\nspecified subdomains. For more information on how to enable new features, see the sample YAML\nfiles in the . Promotes the  MongoDBOpsManager   resource  to Beta.   version\n4.2.4 is available. Supports Backup and restore in  -deployed  \ninstances. This is a semi-automated process that deploys everything\nyou need to enable backups in  . You can enable Backup by\nsetting the  spec.backup.enabled  setting in the  \ncustom resource. You can configure the Head Database, Oplog Store, and\nS3 Snapshot Store by using the  MongoDBOpsManager   resource\nspecification . Supports access to   from outside the  \ncluster through the  spec.externalConnectivity  setting. Enables SCRAM-SHA-1 authentication  on  's\nApplication Database by default. Adds support for OpenShift (Red Hat UBI Images). Improves overall stability of X.509 user management. Released 2019-11-08 This release introduces significant changes that may not be\ncompatible with previous deployments or resource configurations. Read\n Migrate to One Resource per Project (Required for Version 1.3.0)  before\nupgrading the  . Requires one MongoDB resource per   project. If you\nhave more than one MongoDB resource in a project, all resources will\nchange to a  Pending  status and the   won\u2019t perform\nany changes on them. The existing MongoDB databases will still be\naccessible. You must  migrate to one resource per project . Supports  SCRAM-SHA  authentication mode. See \nfor examples. Requires that the project ( ConfigMap ) and\ncredentials ( secret )\nreferenced from a MongoDB resource be in the same namespace. Adds OpenShift installation files (  file and Helm chart\nconfiguration). Supports highly available  Ops Manager resources  by introducing the  spec.replicas \nsetting. Runs   as a non-root user. Released 2019-10-25 This release introduces significant changes that may not be\ncompatible with previous deployments or resource configurations. Read\n Migrate to One Resource per Project (Required for Version 1.3.0)  before installing or\nupgrading the  . Moves to a\n one cluster per project configuration .\nThis follows the warnings introduced in a\n previous version of the operator .\nThe operator now requires each cluster to be contained within a new\nproject. Authentication settings are now contained within the\n security section  of the MongoDB resource\nspecification rather than the project ConfigMap. Replaces the  project  field with the\n spec.opsManager.configMapRef.name  or\n spec.cloudManager.configMapRef.name  fields. User resources  now refer to MongoDB resources\nrather than project ConfigMaps. No longer requires  data.projectName  in the project ConfigMap. The\nname of the project defaults to the name of the MongoDB resource in\n . This release introduces signficant changes to the   resource's\narchitecture. The   application database is now managed by\nthe  , not by  . Stops unnecessary recreation of NodePorts. Fixes logging so it's always in JSON format. Sets  USER  in the   Docker image. Fixes CVE-2020-7922:   Operator generates potentially insecure certificates. X.509 certificates generated by the   may allow an attacker with\naccess to the   cluster improper access to MongoDB instances.\nCustomers who do not use X.509 authentication, and those who do not use\nthe   to generate their X.509 certificates are unaffected. CWE-295: Improper Certificate Validation\nCVSS score: 6.4\nCVSS:3.1/AV:A/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:N 1.0, 1.1 1.2.0 - 1.2.4 1.3.0 - 1.3.1 1.4.0 - 1.4.4 1.2.5 1.4.5 and above Released 2019-10-02 Increases stability of Sharded Cluster deployments. Improves internal testing infrastructure. Released 2019-09-13 Update:  The   will remove support for multiple\nclusters per project in a future release. If a project contains more\nthan one cluster, a warning will be added to the status of the\nMongoDB Resources. Additionally, any new cluster being added to a\nnon-empty project will result in a  Failed  state, and won't\nbe processed. Fix:  The overall stability of the operator has been improved. The\noperator is now more conservative in resource updates both on\n  and  . Released 2019-08-30 Security Fix:  Clusters configured by   versions\n1.0 through 1.2.1 used an insufficiently strong keyfile for internal\ncluster authentication between  mongod  processes. This only affects\nclusters which are using X.509 for user authentication, but are not\nusing X.509 for internal cluster authentication. Users are advised to\nupgrade to version 1.2.2, which will replace all managed keyfiles. Security Fix:  Clusters configured by   versions 1.0\nthrough 1.2.1 used an insufficiently strong password to authenticate\nthe MongoDB Agent. This only affects clusters which have been manually\nconfigured to enable  SCRAM-SHA-1 , which is not a supported\nconfiguration. Users are advised to upgrade to version 1.2.2, which\nwill reset these passwords. Released 2019-08-23 Fix:  The   no longer recreates   when X.509\nauthentication is enabled and the approved   have been deleted. Fix:  If the  OPERATOR_ENV  environment variable is set to\nsomething unrecognized by the  , it will no longer result\nin a  CrashLoopBackOff  of the pod. A default value of  prod  is\nused. The   now supports more than 100 agents in a given\nproject. Released 2019-08-13 Adds a\n readinessprobe \nto the MongoDB Pods to improve the reliability of rolling upgrades. This feature is an alpha release. It is not ready for production use. Can use the   to manage   4.2. To  deploy an\n|onprem| instance , you use a new\n resource :  MongoDBOpsManager . Released 2019-07-19 Fix:  Sample yaml files, in particular, the attribute related to\n featureCompatibilityVersion . Fix:    can be disabled in a deployment. Improvement:  Added\n script  in the\n support  directory that can gather information of\nyour MongoDB resources in Kubernetes. Improvement:  In a   environment, the   can use a\ncustom Certificate Authority. All the certificates must be passed as\nKubernetes Secret objects. Released 2019-06-18 Supports Kubernetes v1.11 or later. Provisions any kind of MongoDB deployment in the Kubernetes Cluster\nof your Organization: Standalone Replica Set Sharded Cluster Configures   on the MongoDB deployments and encrypt all traffic.\nHosts and clients can verify each other\u2019s identities. Manages MongoDB users. Supports X.509 authentication to your MongoDB databases. If you have any questions regarding this release, use the\n #enterprise-kubernetes \nSlack channel. Released 2019-06-07 Rolling upgrades of MongoDB resources ensure that  rs.stepDown() \nis called for the primary member. Requires MongoDB patch version 4.0.8 and\nlater or MongoDB patch version 4.1.10 and later. During a MongoDB major version upgrade, the\n featureCompatibilityVersion  field can be set. Fixed a bug where replica sets with more than seven members could\nnot be created. X.509 Authentication can be enabled at the\n Project level . Requires  ,  \npatch version 4.0.11 and later, or   patch version 4.1.7 and later. Internal cluster authentication based on X.509 can be enabled at the\n deployment  level. MongoDB users with X.509 authentication can be created, using the\nnew  MongoDBUser  custom resource. Released 2019-04-29 NodePort  service creation can be disabled.  can be enabled for internal authentication between MongoDB in\nreplica sets and sharded clusters. The   certificates are created\nautomatically by the  . Please refer to the sample\n .yaml  files in the\n GitHub repository \nfor examples. Wide or asterisk roles have been replaced with strict listing of\nverbs in  roles.yaml . Printing  mdb  objects with  kubectl  will provide more\ninformation about the MongoDB object: type, state, and MongoDB server\nversion. Released 2019-04-02 The   and database images are now based on ubuntu:16.04. The   now uses a single   named  MongoDB \ninstead of the  MongoDbReplicaSet ,  MongoDbShardedCluster , and\n MongoDbStandalone  CRDs. Follow the  upgrade procedure  to\ntransfer existing  MongoDbReplicaSet ,  MongoDbShardedCluster ,\nand  MongoDbStandalone  resources to the new format. For a list of the packages installed and any security vulnerabilities\ndetected in our build process, see: MongoDB Enterprise Operator MongoDB Enterprise Database Released 2019-03-19 The Operator and Database images are now based on\n debian:stretch-slim  which is the latest and up-to-date Docker\nimage for Debian 9. Released 2019-02-26 Perform   clean-up on deletion of MongoDB resource without the\nuse of finalisers. Bug fix:  Race conditions when communicating with  . Bug fix:   ImagePullSecrets  being incorrectly initialized in\nOpenShift. Bug fix:  Unintended fetching of closed projects. Bug fix:  Creation of duplicate organizations. Bug fix:  Reconciliation could fail for the MongoDB resource if\nsome other resources in   were in error state. Released 2019-02-01 Improved detailed status field for MongoDB resources. The   watches changes to configuration parameters in a\nproject configMap and the credentials secret then performs a rolling\nupgrade for relevant Kubernetes resources. Added   structured logging for Automation Agent pods. Support   records for MongoDB access. Bug fix: Avoiding unnecessary reconciliation. Bug fix: Improved Ops Manager/Cloud Manager state management for\ndeleted resources. Released 2018-12-17 Refactored code to use the  controller-runtime  library to fix issues\nwhere Operator could leave resources in inconsistent state. This also\nintroduced a proper reconciliation process. Added new  status  field for all MongoDB Kubernetes resources. Can configure Operator to watch any single namespace or all\nnamespaces in a cluster (requires cluster role). Improved database logging by adding a new configuration property\n logLevel . This property is set to  INFO  by default.\nAutomation Agent and MongoDB logs are merged in to a single log\nstream. Added new configuration Operator timeout. It defines waiting time\nfor database pods start while updating  . Fix:  Fixed failure detection for  mongos . Released 2018-11-14 Image for database no longer includes the binary for the Automation\nAgent. The container downloads the Automation Agent binary from\n  when it starts. Fix:  Communication with   failed if the project with the same\nname existed in different organization. Released 2018-10-04 If a backup was enabled in   for a Replica Set or Sharded\nCluster that the   created, then the  \ndisables the backup before removing a resource. Improved persistence support: The data, journal and log directories are mounted to three\nmountpoints in one or three volumes depending upon the\n podSpec.persistence  setting. Prior to this release, only the data directory was mounted to\npersistent storage. Setting Mount Directories to podSpec.persistence.single One volume podSpec.persistence.multiple Three volumes A new parameter,  labelSelector , allows you to specify the\nselector for volumes that   should consider mounting. If   is not specified in the  persistence \nconfiguration, then the default  StorageClass  for the cluster is\nused. In most of public cloud providers, this results in dynamic\nvolume provisioning. Released 2018-08-07 The Operator no longer creates the CustomResourceDefinition objects.\nThe user needs to create them manually. Download and apply\n this new yaml file \n( crd.yaml ) to create/configure these objects. ClusterRoles are no longer required. How the Operator watches\nresources has changed. Until the last release, the Operator would\nwatch for any resource on any  . With 0.3, the Operator\nwatches for resources in the same namespace in which it was created.\nTo support multiple namespaces, multiple Operators can be installed.\nThis allows isolation of MongoDB deployments. Permissions changes were made to how PersistentVolumes are mounted. Added configuration to Operator to not create\n SecurityContexts \nfor  . This solves an issue with OpenShift which does not\nallow this setting when  SecurityContextContraints  are used. If you are using Helm, set  managedSecurityContext  to  true .\nThis tells the Operator to not create  SecurityContext  for\n , satisfying the OpenShift requirement. The combination of  projectName  and  orgId  replaces\n projectId  alone to configure the connection to  .\nThe project is created if it doesn't exist. Released 2018-08-03 Calculates WiredTiger memory cache. Released 2018-06-27 Initial Release Can deploy standalone instances, replica sets, sharded clusters\nusing   configuration files.",
            "code": [],
            "preview": "Released 2020-07-30",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/k8s-op-exclusive-settings",
            "title": "MongoDB  Exclusive Settings",
            "headings": [
                "Kubernetes Operator Overrides Some Ops Manager Settings"
            ],
            "paragraphs": "At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . Some settings that you configure using   cannot be set or\noverridden in the  . Settings that the   does\nnot manage are accepted. The following list of settings are exclusive to  . This list may\nchange at a later date. These settings can be found on the\n Automation Configuration \npage: In addition to the list of Automation settings, the   uses attributes\noutside of the deployment from the Monitoring and Backup Agent configurations. auth.autoAuthMechanisms auth.authoritativeSet auth.autoPwd auth.autoUser auth.deploymentAuthMechanisms auth.disabled auth.key auth.keyfile auth.keyfileWindows auth.usersWanted auth.usersWanted[n].mechanisms auth.usersWanted[n].roles auth.usersWanted[n].roles[m].role auth.usersWanted[n].roles[m].db auth.usersWanted[n].user auth.usersWanted[n].authenticationRestrictions processes.args2_6.net.port processes.args2_6.net.tls.certificateKeyFile processes.args2_6.net.tls.clusterFile processes.args2_6.net.tls.PEMKeyFile processes.args2_6.replication.replSetName processes.args2_6.sharding.clusterRole processes.args2_6.security.clusterAuthMode processes.args2_6.storage.dbPath processes.args2_6.systemLog.destination processes.args2_6.systemLog.path processes.authSchemaVersion processes.cluster  (mongos processes) processes.featureCompatibilityVersion processes.hostname processes.name processes.version replicaSets._id replicaSets.members._id replicaSets.members.host replicaSets.members replicaSets.version sharding.clusterRole  (config server) sharding.configServerReplica sharding.name sharding.shards._id sharding.shards.rs ssl.CAFilePath ssl.autoPEMKeyFilePath ssl.clientCertificateMode backupAgentTemplate.username backupAgentTemplate.sslPEMKeyFile monitoringAgentTemplate.username monitoringAgentTemplate.sslPEMKeyFile  creates a replica set of 3 members. You changed  storage.wiredTiger.engineConfig.cacheSizeGB \nto  40 . This setting is not in the   exclusive settings\nlist. You then use the   to scale the replica set to\n5 members. The  storage.wiredTiger.engineConfig.cacheSizeGB  on the\nnew members should still be  40 .",
            "code": [],
            "preview": "Some settings that you configure using  cannot be set or\noverridden in the . Settings that the  does\nnot manage are accepted.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/support-lifecycle",
            "title": "Support Lifecycle",
            "headings": [],
            "paragraphs": "The support lifecycle of the   is governed by the  MongoDB\nSupport Policy , where the\n  is considered an \"extension to  \". The following\ndates are derived from this policy and are published here for guidance\nonly. These dates might be subject to change.  Version End of Life Date 1.0 2020-03-15 1.1 2020-04-23 1.2.x 2020-05-21 1.3.x 2020-07-30 1.4.x 2020-09-13 1.5.x 2021-01-28",
            "code": [],
            "preview": "The support lifecycle of the  is governed by the MongoDB\nSupport Policy, where the\n is considered an \"extension to \". The following\ndates are derived from this policy and are published here for guidance\nonly. These dates might be subject to change.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/known-issues",
            "title": "Known Issues in the ",
            "headings": [
                "Update Google Firewall Rules to Fix WebHook Issues",
                "Enable  Snapshot Stores in  4.2.10 and 4.2.12",
                "Configure Persistent Storage Correctly",
                "Remove Resources before Removing ",
                "Create Separate Namespaces for  and MongoDB Resources",
                "HTTPS Enabled After Deployment",
                "Difficulties with Updates",
                "Machine Memory vs. Container Memory",
                "Changes to Avoid"
            ],
            "paragraphs": "When you deploy   to   private clusters, the\n  or MongoDBOpsManager resource creation could time out.\nThe following message might appear in the logs: Error setting state to reconciling: Timeout: request did not\ncomplete within requested timeout 30s\". Google configures its firewalls to restrict access to your  \n . To use the webhook service,\n add a new firewall rule \nto grant   control plane access to your webhook service. The   webhook service runs on port 443. To enable S3 Snapshot stores in   4.2.10 and 4.2.12, you must\nset  brs.s3.validation.testing: disabled  in\nthe  spec.configuration  property of your  \nresource specification. If there are no\n persistent volumes \navailable when you create a resource, the resulting   stays in\ntransient state and the Operator fails  (after 20 retries) with the\nfollowing error: To prevent this error, either: For testing only, you may also set  persistent : false . This\n must not be used in production , as data is not preserved between\nrestarts. Provide   or Set  persistent : false  for the resource Sometimes   can diverge from  . This mostly occurs when\n  resources are removed manually.   can keep displaying an\nAutomation Agent which has been shut down. If you want to remove deployments of MongoDB on  , use the\nresource specification to delete resources first so no dead Automation\nAgents remain. The best strategy is to create   and its resources in\ndifferent namespaces so that the following operations would work\ncorrectly: or If the   and resources sit in the same  mongodb \n , then operator would also be removed in the same operation.\nThis would mean that it could not clean the configurations, which\nwould have to be done in the  . We recommend that you enable    before  deploying your   resources.\nHowever, if you enable   after deployment,\nyour managed resources can no longer communicate with   and\nthe   reports your resources' status as  Failed . To resolve this issue, you must delete your   by\nrunning the following command for each Pod: After deletion,   automatically restarts the deleted Pods.\nDuring this period, the resource is unreachable and incurs\ndowntime. Configure   to Run over HTTPS Troubleshooting the  In some cases, the   can stop receiving change events. As\nthis problem is hard to reproduce, the recommended workaround is to\ndelete the operator pod.   starts the new  \nautomatically and starts working correctly: Kubernetes Operator installation MongoDB versions older than 3.6.13, 4.0.9, and 4.1.9 report host system RAM, not\ncontainer RAM. The   will not be able to apply the following change on a MongoDB Deployment simultaneously: If both operations are applied at the same time, the MongoDB Resource could go into a unrecoverable state. The TLS configuration is disabled ( security.tls.enabled: false ) The number of members in a Replica Set is increased",
            "code": [
                {
                    "lang": "sh",
                    "value": "Failed to update Ops Manager automation config: Some agents failed to register"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete pods --all"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete pod <replicaset-pod-name>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods;\nkubectl delete pod mongodb-enterprise-operator-<podId>`"
                }
            ],
            "preview": "When you deploy  to  private clusters, the\n or MongoDBOpsManager resource creation could time out.\nThe following message might appear in the logs:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/troubleshooting",
            "title": "Troubleshooting the ",
            "headings": [
                "Get Status of a Deployed Resource",
                "Review the Logs",
                "Review Logs from the ",
                "Find a Specific Pod",
                "Review Logs from Specific Pod",
                "View All  Specifications",
                "Restore StatefulSet that Failed to Deploy",
                "Replace a ConfigMap to Reflect Changes",
                "Remove  Components",
                "Remove a ",
                "Remove the ",
                "Remove the ",
                "Remove the ",
                "Disable  Feature Controls",
                "Debugging a Failing Container"
            ],
            "paragraphs": "To find the status of a resource deployed with the  ,\ninvoke one of the following commands: The following key-value pairs describe the resource deployment statuses: For   resource deployments: The  status.applicationDatabase.phase  field displays the\nApplication Database resource deployment status. The  status.backup.phase  displays the backup daemon resource\ndeployment status. The  status.opsManager.phase  field displays the   resource\ndeployment status. The  opsManager  controller watches the database resources\ndefined in the following settings: spec.backup.opLogStores spec.backup.s3Stores spec.backup.blockStores For MongoDB resource deployments: The  status.phase  field displays the MongoDB resource deployment\nstatus. Key Value message Message explaining why the resource is in a  Pending  or\n Failed  state. phase Status Meaning Pending The   is unable to reconcile the resource\ndeployment state. This happens when a reconciliation\ntimes out or if the   requires you to take\naction for the resource to enter a running state. If a resource is pending because a reconciliation timed\nout, the   attempts to reconcile the\nresource state in 10 seconds. Reconciling The   is reconciling the resource state. Resources enter this state after you create or update\nthem or if the   is attempting to reconcile\na resource previously in a  Pending  or  Failed \nstate. The   attempts to reconcile the resource\nstate in 10 seconds. Running The resource is running properly. Failed The resource is not running properly. The  message \nfield provides additional details. The   attempts to reconcile the resource\nstate in 10 seconds. lastTransition  when the last reconciliation happened. link Deployment   in  . Resource specific fields For descriptions of these fields, see\n MongoDB Database Resource Specification . If you want to see what the status of a replica set named\n my-replica-set  in the  developer  namespace, run: If  my-replica-set  is running, you should see: If  my-replica-set  is not running, you should see: To review the   logs, invoke this command: You could check the  Ops Manager Logs  as\nwell to see if any issues were reported to  . To find which pods are available, invoke this command first: If you want to narrow your review to a specific  , you can\ninvoke this command: If your  replica set  is labeled  myrs , the   log\ncommand is invoked as: This returns the  Automation Agent Log  for this\nreplica set. To view all   specifications in the provided\n : To read details about the  dublin  standalone resource, invoke\nthis command: This returns the following response: A StatefulSet   may hang with a status of  Pending  if it\nencounters an error during deployment. Pending    do not automatically terminate, even if you\nmake  and apply  configuration changes to resolve the error. To return the StatefulSet to a healthy state, apply the configuration\nchanges to the MongoDB resource in the  Pending  state, then delete\nthose pods. A host system has a number of running  : my-replica-set-2  is stuck in the  Pending  stage. To gather\nmore data on the error, run the following: The output indicates an error in memory allocation. Updating the memory allocations in the MongoDB resource is\ninsufficient, as the pod does not terminate automatically after\napplying configuration updates. To remedy this issue, update the configuration, apply the\nconfiguration, then delete the hung pod: Once this hung pod is deleted, the other pods restart with your new\nconfiguration as part of rolling upgrade of the Statefulset. To learn more about this issue, see\n Kubernetes Issue 67250 . If you are unable to modify or redeploy an already-deployed configMap file using the  kubectl apply  command, invoke the following: This will delete and re-create a resource. This command is useful in cases where you need to update resource files that cannot be updated\nonce initialized or want to make an immediate recursive change. To remove any component, you need the following permissions: Cluster Roles mongodb-enterprise-operator-mongodb-webhook mongodb-enterprise-operator-mongodb-certs Cluster Role Bindings mongodb-enterprise-operator-mongodb-webhook-binding mongodb-enterprise-operator-mongodb-certs To remove any instance that   deployed, you must use  . You can only use the   to remove  -deployed\ninstances. If you use   to remove the instance,   throws an\nerror. To remove a single MongoDB instance you created using  : To remove all MongoDB instances you created using  : To remove the  : Remove all Kubernetes resources : Remove the  : To remove the  : Remove all Kubernetes resources : Remove the  : To remove the  : Remove all Kubernetes resources : Remove the  : When you manage an   project through the  , the\n  places the  EXTERNALLY_MANAGED_LOCK \n feature control policy \non the project. This policy disables certain features in the  \napplication that might compromise your   configuration. If\nyou need to use these blocked features, you can remove the policy\nthrough the  feature controls API ,\nmake changes in the   application, and then restore the original\npolicy through the  API . The following procedure enables you to use features in the  \napplication that are otherwise blocked by the  . Retrieve the feature control policies \nfor your   project. Save the response that the API returns. After you make changes in\nthe   application, you must add these policies back to\nthe project. Your response should be similar to: Note the highlighted fields and values in the following sample\nresponse. You must send these same fields and values in later\nsteps when you remove and add feature control policies. The  externalManagementSystem.version  field corresponds to the\n  version. You must send the exact same field value\nin your requests later in this task. Update \nthe  policies  array with an empty list: The previously blocked features are now available in the\n  application. The values you provide for the  externalManagementSystem \nobject, like the  externalManagementSystem.version  field, must\nmatch values that you received in the response in Step 1. Make your changes in the   application. Update \nthe  policies  array with the original feature control policies: The features are now blocked again, preventing you from making\nfurther changes through the   application. However, the\n  retains any changes you made in the  \napplication while features were available. The values you provide for the  externalManagementSystem \nobject, like the  externalManagementSystem.version  field, must\nmatch values that you received in the response in Step 1. A container might fail with an error that results in   restarting\nthat container in a loop. You may need to interact with that container to inspect files or run\ncommands. This requires you to prevent the container from restarting. In your preferred text editor, open the MongoDB resource you need to\nrepair. To this resource, add a  podSpec  collection that resembles the\nfollowing. The  sleep  command in the\n spec.podSpec.podTemplate.spec  instructs the container to\nwait for the number of seconds you specify. In this example, the\ncontainer will wait for 1 hour. Apply this change to the resource. Invoke the shell inside the container.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl get <resource-name> -n <namespace> -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -n <namespace> -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb my-replica-set -n developer -o yaml"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n    lastTransition: \"2019-01-30T10:51:40Z\"\n    link: http://ec2-3-84-128-187.compute-1.amazonaws.com:9080/v2/5c503a8a1b90141cbdc60a77\n    members: 1\n    phase: Running\n    version: 4.2.2-ent"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  lastTransition: 2019-02-01T13:00:24Z\n  link: http://ec2-34-204-36-217.compute-1.amazonaws.com:9080/v2/5c51c040d6853d1f50a51678\n  members: 1\n  message: 'Failed to create/update replica set in Ops Manager: Status: 400 (Bad Request),\n    Detail: Something went wrong validating your Automation Config. Sorry!'\n  phase: Failed\n  version: 4.2.2-ent"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs -f deployment/mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs <podName> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl logs myrs-0 -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl get mdb -n <namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl get mdb dublin -n <namespace> -o yaml"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"mongodb.com/v1\",\"kind\":\"MongoDB\",\"metadata\":{\"annotations\":{},\"name\":\"dublin\",\"namespace\":\"mongodb\"},\"spec\":{\"credentials\":\"credentials\",\"persistent\":false,\"podSpec\":{\"memory\":\"1Gi\"},\"project\":\"my-om-config\",\"type\":\"Standalone\",\"version\":\"4.0.0-ent\"}}\n  clusterDomain: \"\"\n  creationTimestamp: 2018-09-12T17:15:32Z\n  generation: 1\n  name: dublin\n  namespace: mongodb\n  resourceVersion: \"337269\"\n  selfLink: /apis/mongodb.com/v1/namespaces/mongodb/mongodbstandalones/dublin\n  uid: 7442095b-b6af-11e8-87df-0800271b001d\nspec:\n  credentials: my-credentials\n  type: Standalone\n  persistent: false\n  podSpec:\n    memory: 1Gi\n  project: my-om-config\n  version: 4.2.2-ent"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods\n\nmy-replica-set-0     1/1 Running 2 2h\nmy-replica-set-1     1/1 Running 2 2h\nmy-replica-set-2     0/1 Pending 0 2h"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe pod my-replica-set-2\n\n<describe output omitted>\n\nWarning FailedScheduling 15s (x3691 over 3h) default-scheduler 0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient memory."
                },
                {
                    "lang": "sh",
                    "value": "vi <my-replica-set>.yaml\n\nkubectl apply -f <my-replica-set>.yaml\n\nkubectl delete pod my-replica-set-2"
                },
                {
                    "lang": "shell",
                    "value": "kubectl replace -f <my-config-map>.yaml"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb <name> -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete deployment mongodb-enterprise-operator -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete crd MongoDB\nkubectl delete crd MongoDBUSer\nkubectl delete crd MongoDBOpsManager"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete mdb --all -n <metadata.namespace>"
                },
                {
                    "lang": "shell",
                    "value": "kubectl delete namespace <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "curl --user \"{USERNAME}:{APIKEY}\" --digest \\\n     --header \"Accept: application/json\" \\\n     --header \"Content-Type: application/json\" \\\n     --include \\\n     --request GET \"https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true\""
                },
                {
                    "lang": "json",
                    "value": "{\n \"created\": \"2020-02-25T04:09:42Z\",\n \"externalManagementSystem\": {\n   \"name\": \"mongodb-enterprise-operator\",\n   \"systemId\": null,\n   \"version\": \"1.4.2\"\n },\n \"policies\": [\n   {\n     \"disabledParams\": [],\n     \"policy\": \"EXTERNALLY_MANAGED_LOCK\"\n   },\n   {\n     \"disabledParams\": [],\n     \"policy\": \"DISABLE_AUTHENTICATION_MECHANISMS\"\n   }\n ],\n \"updated\": \"2020-02-25T04:10:12Z\"\n}"
                },
                {
                    "lang": "sh",
                    "value": "curl --user \"{USERNAME}:{APIKEY}\" --digest \\\n     --header \"Accept: application/json\" \\\n     --header \"Content-Type: application/json\" \\\n     --include \\\n     --request PUT \"https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true\" \\\n     --data\n       '{\n         \"externalManagementSystem\": {\n           \"name\": \"mongodb-enterprise-operator\",\n           \"systemId\": null,\n           \"version\": \"1.4.2\"\n         },\n         \"policies\": []\n       }'"
                },
                {
                    "lang": "sh",
                    "value": "curl --user \"{USERNAME}:{APIKEY}\" --digest \\\n     --header \"Accept: application/json\" \\\n     --header \"Content-Type: application/json\" \\\n     --include \\\n     --request PUT \"https://{OPSMANAGER-HOST}:{PORT}/api/public/v1.0/groups/{PROJECT-ID}/controlledFeature?pretty=true\" \\\n     --data\n       '{\n         \"externalManagementSystem\": {\n           \"name\": \"mongodb-enterprise-operator\",\n           \"systemId\": null,\n           \"version\": \"1.4.2\"\n         },\n         \"policies\": [\n           {\n             \"disabledParams\": [],\n             \"policy\": \"EXTERNALLY_MANAGED_LOCK\"\n           },\n           {\n             \"disabledParams\": [],\n             \"policy\": \"DISABLE_AUTHENTICATION_MECHANISMS\"\n           }\n         ]\n       }'"
                },
                {
                    "lang": "yaml",
                    "value": "podSpec:\n  podTemplate:\n    spec:\n      containers:\n      - name: mongodb-enterprise-database\n        command: ['sh', '-c', 'echo \"Hello!\" && sleep 3600' ]"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl exec -it <pod-name> bash"
                }
            ],
            "preview": "To find the status of a resource deployed with the ,\ninvoke one of the following commands:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/production-notes",
            "title": " Production Notes",
            "headings": [
                "Ensure Proper Persistence Configuration",
                "Name Your MongoDB Service with its Purpose",
                "Specify Resource Requirements",
                "Use Multiple Availability Zones",
                "Co-locate mongos Pods with Your Applications",
                "Manage Multitenancy with Labels",
                "Enable HTTPS",
                "Enable TLS",
                "Enable Authentication",
                "Example Deployment CRD",
                "Example User CRD"
            ],
            "paragraphs": "This page details system configuration recommendations for the\n  when running in production. You use a   to ensure stateful configurations of  \ndeployments. The storage of your   deployment must persist, so\nverify that the   are configured to meet your storage needs. The  : Supports mounting storage devices to one or more directories\ncalled mount points. Creates one   per MongoDB mount point. Sets the default path in each container to  /data . spec.persistent spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data If using your own MongoDB Service, set the  spec.service  parameter\nto something that helps you identify this deployment's purpose. For the replica sets, sharded clusters, and config servers you create\nusing the  , set the resource utilization bounds for both\ncompute and memory.   refers to the lower bound of a resource as a\n request  and the upper bound as a  limit . Set  CPU requests and limits \nto guarantee the CPU allocation and resource reporting. Set  Memory requests and limits \nto guarantee the requested memory allocation for the WiredTiger\ncache and resource reporting. Monitoring tools report the size of the   rather than the\nactual size of the container. Set the   and   to distribute all members\nof one replica set to different   to ensure high\navailability. The lightweight  mongos  instance can be run in the same  \nas your apps using MongoDB. The   supports standard  \n node-affinity and node anti-affinity \nfeatures. Using these features, you can force install the  mongos \non the same pod as your application. The  podAffinity  key determines if an application should be\ninstalled on the same pod, node, or data center as another\napplication. You add a label and value in the  spec.template.metadata.labels \n  collection to tag the deployment. In this example, the  web-store  value for the  app  key labels\nthe pod on which the  web-server  is installed. The\n labelSelector  key declares that the  mongos  app must be\ninstalled on the same pod that has its  app  label set to be\n In  values that include  web-store . If you need to physically separate different MongoDB resources (such\nas  test  and  staging  environments) or want to place  \non some specific nodes (such as   support) use the\n pod affinity    feature. The   supports configuring   to run over\n HTTPS . Enable   before deploying your   resources to avoid a situation\nwhere the   reports your resources' status as  Failed . HTTPS Enabled After Deployment The   supports   encryption. Use   with your\nMongoDB deployment to encrypt your data over the network. The   supports X.509 user authentication. You must create\nan additional   for your MongoDB users and the MongoDB Agents.\nThe Operator generates and distributes the certificate.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.0.14\n  service: my-service\n  featureCompatibilityVersion: \"3.6\"\n  project: my-project\n  credentials: my-credentials\n  type: ShardedCluster\n  persistent: true\n  configSrvPodSpec:\n    cpu: '0.5'\n    memory: 512M\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n  mongosPodSpec:\n    cpu: '0.8'\n    memory: 1Gi\n    podAntiAffinityTopologyKey: rackId\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n  shardPodSpec:\n    cpu: '0.6'\n    memory: 3Gi\n    persistence:\n      multiple:\n        data:\n          storage: 20Gi\n        logs:\n          storage: 4Gi\n          storageClass: standard\n    podAntiAffinityTopologyKey: kubernetes.io/hostname"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: 4.0.14\n  service: drilling-pumps-geosensors\n  featureCompatibilityVersion: \"3.6\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.0.14\n  service: my-service\n  featureCompatibilityVersion: \"3.6\"\n  project: my-project\n  credentials: my-credentials\n  type: ShardedCluster\n  persistent: true\n  configSrvPodSpec:\n    cpu: '0.5'\n    memory: 512M\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n  mongosPodSpec:\n    cpu: '0.8'\n    memory: 1Gi\n    podAntiAffinityTopologyKey: rackId\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n  shardPodSpec:\n    cpu: '0.6'\n    memory: 3Gi\n    persistence:\n      multiple:\n        data:\n          storage: 20Gi\n        logs:\n          storage: 4Gi\n          storageClass: standard\n    podAntiAffinityTopologyKey: kubernetes.io/hostname"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.0.14\n  service: my-service\n  featureCompatibilityVersion: \"3.6\"\n  project: my-project\n  credentials: my-credentials\n  type: ShardedCluster\n  persistent: true\n  configSrvPodSpec:\n    cpu: '0.5'\n    memory: 512M\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n  mongosPodSpec:\n    cpu: '0.8'\n    memory: 1Gi\n    podAntiAffinityTopologyKey: rackId\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n  shardPodSpec:\n    cpu: '0.6'\n    memory: 3Gi\n    persistence:\n      multiple:\n        data:\n          storage: 20Gi\n        logs:\n          storage: 4Gi\n          storageClass: standard\n    podAntiAffinityTopologyKey: kubernetes.io/hostname"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-server\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web-store\n\nmongosPodSpec:\n  podAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n        matchExpressions:\n        - key: app\n          operator: In\n          values:\n          - web-store"
                },
                {
                    "lang": "yaml",
                    "value": "mongosPodSpec:\n  podAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n        matchExpressions:\n        - key: app\n          operator: In\n          values:\n          - web-store"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-tls-enabled-rs\nspec:\n  type: ReplicaSet\n  members: 3\n  version: 4.0.14\n  project: my-project\n  credentials: my-credentials\n  security:\n    tls:\n      enabled: true\n  additionalMongodConfig:\n    net:\n      ssl:\n        mode: \"preferSSL\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-tls-enabled-rs\nspec:\n  type: ReplicaSet\n  members: 3\n  version: 4.0.14\n  project: my-project\n  credentials: my-credentials\n  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\""
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: user-with-roles\nspec:\n  username: \"CN=mms-user-1,OU=cloud,O=MongoDB,L=New York,ST=New York,C=US\"\n  db: \"$external\"\n  project: my-project\n  roles:\n    - db: \"admin\"\n      name: \"clusterAdmin\""
                }
            ],
            "preview": "This page details system configuration recommendations for the\n when running in production.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-op-prerequisites",
            "title": "Prerequisites",
            "headings": [
                "Procedure",
                "Have a  solution available to use.",
                "Clone the .",
                "Create a  for your  deployment.",
                "Optional: Have a running .",
                "Required for OpenShift Installs: Create a  that contains credentials authorized to pull images from the registry.connect.redhat.com repository."
            ],
            "paragraphs": "To install the MongoDB  , you must: If you need a   solution, see the  \n documentation on picking the right solution . You can use  Helm  to install the\n . To learn how to install Helm, see its\n. By default, The   uses the  mongodb  namespace. To\nsimplify your installation, consider creating a namespace labeled\n mongodb  using the following   command: If you do not want to use the  mongodb  namespace, you can label\nyour namespace anything you like: If you don't deploy an   resource with the\n , you must have an   running outside of your\n  cluster. If you will deploy an   resource in   with\nthe  , skip this prerequisite. Your   installation must run an active   service. If\nthe   host's clock falls out of sync, that host can't\ncommunicate with the  . To learn how to check your   service for your Ops Manager\nhost, see the documentation for\n Ubuntu  or\n RHEL . If you have not already, obtain a Red Hat subscription. Create a  Registry Service Account . Click on your Registry Service Account, then click the\n Docker Configuration  tab. Download the  <account-name>-auth.json  file and open it in a\ntext editor. Copy the  registry.redhat.io  object, and paste another instance\nof this object into the file. Remember to add a comma after the\nfirst object. Rename the second object\n registry.connect.redhat.com , then save the file: Create a  openshift-pull-secret.yaml  file with the contents of\nthe modified  <account-name>-auth.json  file as  stringData \nnamed  .dockerconfigjson : The value you provide in the  metadata.name  field contains\nthe secret name. Provide this value when asked for the\n <openshift-pull-secret> . Create a   from the  openshift-pull-secret.yaml \nfile:",
            "code": [
                {
                    "lang": "sh",
                    "value": "git clone https://github.com/mongodb/mongodb-enterprise-kubernetes.git"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create namespace <namespaceName>"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"auths\": {\n    \"registry.redhat.io\": {\n      \"auth\": \"<encoded-string>\"\n    },\n    \"registry.connect.redhat.com\": {\n      \"auth\": \"<encoded-string>\"\n    }\n  }\n}"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: openshift-pull-secret\nstringData:\n  .dockerconfigjson: |\n      {\n        \"auths\": {\n          \"registry.redhat.io\": {\n            \"auth\": \"<encoded-string>\"\n          },\n          \"registry.connect.redhat.com\": {\n            \"auth\": \"<encoded-string>\"\n          }\n        }\n      }\ntype: kubernetes.io/dockerconfigjson"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f openshift-pull-secret.yaml -n <namespace>"
                }
            ],
            "preview": "To install the MongoDB , you must:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/k8s-operator-specification",
            "title": "MongoDB Database Resource Specification",
            "headings": [
                "Common Resource Settings",
                "Required",
                "Conditional",
                "Optional",
                "Deployment-Specific Resource Settings",
                "Standalone Settings",
                "Replica Set Settings",
                "Sharded Cluster Settings",
                "Security Settings",
                "Examples"
            ],
            "paragraphs": "The \ncreates     from specification files that you\nwrote. MongoDB resources are created in Kubernetes as\n custom resources .\nAfter you create or update a   specification, you direct\n  to apply this specification to your   environment.\n  creates the defined  , services and\nother Kubernetes resources. After the Operator finishes creating those\nobjects, it updates the   deployment configuration to\nreflect changes. Each   uses an object specification in   to define the\ncharacteristics and settings of the MongoDB object: standalone,\n replica set , and  sharded cluster . At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . Deployment Type StatefulSets Size of StatefulSet Standalone 1 1  Replica Set 1 1   per member Sharded Cluster <numberOfShards> + 2 1   per  , shard, or config server member Every resource type must use the following settings: Every resource must use  one  of the following settings: Every resource type may use the following settings: Other settings you can and must use in a   specification\ndepend upon which MongoDB deployment item you want to create: Standalone Settings Replica Set Settings Sharded Cluster Settings All of the  Standalone Settings  also apply to replica set\nresources. The following settings only apply to replica set resource types: All of the  Standalone Settings  also apply to replica set\nresources. The following settings only apply to sharded cluster resource types: The following security settings only apply to replica set and sharded\ncluster resource types: The following example shows a resource specification for a\nstandlone deployment with every setting provided: The following example shows a resource specification for a\n replica set  with every setting provided: The following example shows a resource specification for a\n sharded cluster  with every setting provided:",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone\nspec:\n  version: 4.2.2-ent\n  service: my-service\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  persistent: true\n  type: Standalone\n  additionalMongodConfig:\n    systemLog:\n      logAppend: true\n      verbosity: 4\n    operationProfiling:\n      mode: slowOp\n  podSpec:\n    cpu: '0.25'\n    memory: 512M\n    persistence:\n      single:\n        storage: 12Gi\n        storageClass: standard\n        labelSelector:\n          matchExpressions:\n          - {key: environment, operator: In, values: [dev]}\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/e2e-az-name\n            operator: In\n            values:\n            - e2e-az1\n            - e2e-az2\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-replica-set\nspec:\n  members: 3\n  version: 4.2.2-ent\n  service: my-service\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n  persistent: true\n  type: ReplicaSet\n  podSpec:\n    cpu: '0.25'\n    memory: 512M\n    persistence:\n      multiple:\n        data:\n          storage: 10Gi\n        journal:\n          storage: 1Gi\n          labelSelector:\n            matchLabels:\n              app: \"my-app\"\n        logs:\n          storage: 500M\n          storageClass: standard\n    podAntiAffinityTopologyKey: nodeId\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/e2e-az-name\n            operator: In\n            values:\n            - e2e-az1\n            - e2e-az2\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n  additionalMongodConfig:\n    net:\n      ssl:\n        mode: preferSSL\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-sharded-cluster\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  service: my-service\n  type: ShardedCluster\n\n  ## Please Note: The default Kubernetes cluster name is\n  ## `cluster.local`.\n  ## If your cluster has been configured with another name, you can\n  ## specify it with the `clusterDomain` attribute.\n\n  opsManager: # Alias of cloudManager\n    configMapRef:\n      name: my-project\n  credentials: my-credentials\n\n  persistent: true\n  configSrvPodSpec:\n    cpu: '0.5'\n    memory: 512M\n\n    # if \"persistence\" element is omitted then Operator uses the\n    # default size (5Gi) for mounting single Persistent Volume\n\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n  mongosPodSpec:\n    cpu: '0.8'\n    memory: 1Gi\n    podAntiAffinityTopologyKey: rackId\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n    podTemplate:\n      metadata:\n        labels:\n          label1: mycustomlabel\n      spec:\n        affinity:\n          podAntiAffinity:\n            preferredDuringSchedulingIgnoredDuringExecution:\n              - podAffinityTerm:\n                  topologyKey: \"mykey\"\n                weight: 50\n  shardPodSpec:\n    cpu: '0.6'\n    memory: 3Gi\n    persistence:\n      multiple:\n        # if the child of \"multiple\" is omitted then the default size will be used.\n        # 16GB for \"data\", 1GB for \"journal\", 3GB for \"logs\"\n        data:\n          storage: 20Gi\n        logs:\n          storage: 4Gi\n          storageClass: standard\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n  mongos:\n    additionalMongodConfig:\n      systemLog:\n        logAppend: true\n        verbosity: 4\n  configSrv:\n    additionalMongodConfig:\n      operationProfiling:\n        mode: slowOp\n  shard:\n    additionalMongodConfig:\n      storage:\n        journal:\n          commitIntervalMs: 50\n  security:\n    tls:\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n...\n"
                }
            ],
            "preview": "The \ncreates   from specification files that you\nwrote.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/resize-pv-storage",
            "title": "Resize Storage for One Database Resource",
            "headings": [
                "Prerequisites",
                "Storage Class Must Support Resizing",
                "Procedure",
                "Create or identify a persistent .",
                "Insert data to the database that the resource serves.",
                "Patch each persistence volume.",
                "Remove the StatefulSets.",
                "Update the database resource with a new storage value.",
                "Update the pods in a rolling fashion.",
                "Validate data exists on the updated ."
            ],
            "paragraphs": "Make sure the   and volume plugin provider that the  \nuse supports resize: If you don't have a StorageClass that supports resizing, ask your  \nadministrator to help. Use an existing database resource or create a new one with persistent\nstorage. Wait until the peristent volume gets to the  Running \nstate. A database resource with persistent storage would include: Start   in the   cluster. Insert data into the  test  database. Invoke the following commands for the entire replica set: Wait until each   gets to the following condition: Delete a   resource. This step removes the   only. The pods remain\nunchanged and running. Update the disk size. Open your preferred text editor and make\nchanges similar to this example: To update the disk size of the replica set to 2 GB, change the\n storage  value in database resource specification: Recreate a   resource with the new volume size. Wait until this StatefulSet achieves the  Running  state. Invoke the following command: The new pods mount the resized volume. If the   were reused, the data that you inserted in  Step\n2  can be found on the databases stored in  :",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl patch storageclass/<my-storageclass> --type='json' \\\n        -p='[{\"op\": \"add\", \"path\": \"/allowVolumeExpansion\", \"value\": true }]'"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.0.4\n  project: my-project\n  credentials: my-credentials\n  type: ReplicaSet\n  podSpec:\n    memory: 300M\n    persistence:\n      single:\n        storage: 1Gi"
                },
                {
                    "lang": "sh",
                    "value": "$kubectl exec -it <my-replica-set>-0 \\\n         /var/lib/mongodb-mms-automation/mongodb-linux-x86_64-4.0.4/bin/mongo"
                },
                {
                    "lang": "javascript",
                    "value": "<my-replica-set>:PRIMARY> use test\n\nswitched to db test\n\n<my-replica-set>:PRIMARY> db.tmp.insert({\"foo\":\"bar\"})\n\nWriteResult({ \"nInserted\" : 1 })"
                },
                {
                    "lang": "sh",
                    "value": "kubectl patch pvc/\"data-<my-replica-set>-0\" -p='{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"2Gi\"}}}}'\nkubectl patch pvc/\"data-<my-replica-set>-1\" -p='{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"2Gi\"}}}}'\nkubectl patch pvc/\"data-<my-replica-set>-2\" -p='{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"2Gi\"}}}}'"
                },
                {
                    "lang": "yaml",
                    "value": "- lastProbeTime: null\n  lastTransitionTime: \"2019-08-01T12:11:39Z\"\n  message: Waiting for user to (re-)start a pod to finish file\n           system resize of volume on node.\n  status: \"True\"\n  type: FileSystemResizePending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete sts --cascade=false <my-replica-set>"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.0.4\n  project: my-project\n  credentials: my-credentials\n  type: ReplicaSet\n  podSpec:\n    memory: 300M\n    persistence:\n      single:\n        storage: 2Gi"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f my-replica-set-vol.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl rollout restart sts <my-replica-set>"
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl exec -it <my-replica-set>-1 \\\n          /var/lib/mongodb-mms-automation/mongodb-linux-x86_64-4.0.4/bin/mongo"
                },
                {
                    "lang": "javascript",
                    "value": "<my-replica-set>:PRIMARY> use test\nswitched to db test\n\n<my-replica-set>:PRIMARY> db.tmp.count()\n1"
                }
            ],
            "preview": "Make sure the  and volume plugin provider that the \nuse supports resize:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/edit-deployment",
            "title": "Edit a Database Resource",
            "headings": [
                "Considerations",
                "Prerequisites",
                "Procedure"
            ],
            "paragraphs": "You can modify a deployment's configuration and topology, including its\nMongoDB versions, storage engines, and numbers of hosts or shards. You\ncan make modifications at all levels of a deployment's topology from a\ntop-level  sharded cluster  or\n replica set  to lower levels, such as a replica set within a\nsharded cluster, or an individual process within a replica set. You can\nalso modify  standalone  processes. Changes cannot be made to individual members of a replica set or\nsharded cluster, only to the whole set or cluster. If a setting isn't available for a MongoDB Kubernetes resource,\nthen the change must be made in the  Ops Manager  or\n Cloud Manager  application. MongoDB custom resources do not support all\n mongod  command line options. If you use an\nunsupported option in your object specification file, the backing\n MongoDB Agent \noverrides the unsupported options. For a complete list of options\nsupported by MongoDB custom resources, see  MongoDB Database Resource Specification . Certain settings can only be configured using  . To\nreview the list of settings, see\n MongoDB   Exclusive Settings . To update a MongoDB  , you need to have completed the following procedures: Install and Configure the  Create Credentials for the  Create One Project using a ConfigMap Deploy a database Edit the   resource specification file. Modify or add any settings you need added or changed. Save your specification file. Invoke the following   command to update your resource.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <standalone-conf>.yaml"
                }
            ],
            "preview": "You can modify a deployment's configuration and topology, including its\nMongoDB versions, storage engines, and numbers of hosts or shards. You\ncan make modifications at all levels of a deployment's topology from a\ntop-level sharded cluster or\nreplica set to lower levels, such as a replica set within a\nsharded cluster, or an individual process within a replica set. You can\nalso modify standalone processes.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/create-project-using-configmap",
            "title": "Create One Project using a ConfigMap",
            "headings": [
                "Considerations",
                "Prerequisites",
                "Kubernetes",
                "Create One Project Using a ConfigMap",
                "Configure kubectl to default to your namespace.",
                "Invoke the following command to create a ConfigMap.",
                "Find the organization's ID.",
                "Copy and paste the orgId into the URL.",
                "Invoke the following  command to verify your .",
                "Connect to HTTPS-enabled Ops Manager Using a Custom CA",
                "Create a ConfigMap for the Certificate Authority certificate.",
                "Copy the highlighted section of the following example ConfigMap.",
                "Add the highlighted section to your project's ConfigMap.",
                "Specify the TLS settings",
                "Save your updated ConfigMap.",
                "Invoke the  command to verify your .",
                "Next Steps"
            ],
            "paragraphs": "The   uses a     to create or link your\n   Project . To create a\n  ConfigMap, you need to edit a few lines of the\n example ConfigMap    file and apply\nthe ConfigMap. Starting in   version 1.3.0, you can only deploy one\nMongoDB resource per project. See  Deploy a MongoDB Database Resource . You can use the   to deploy MongoDB resources with\n  version 4.0.11 or later and Cloud Manager. You can't use the   to deploy MongoDB resources to\n . Kubernetes version 1.11 or later or Openshift version\n3.11 or later.  version 0.11 or later\n installed . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Key Type Description Example <configmap-name> string Name of the    . Resource names must be 44 characters or less. metadata.name  documentation on  names .\nThis name must follow  RFC1123  naming\nconventions, using only lowercase alphanumeric\ncharacters, '-' or '.', and must start and end with an\nalphanumeric character. myconfigmap baseUrl string  to your   including the   and port\nnumber. If you deploy   with the   and   will\nmanage MongoDB database resources deployed  outside  of the  \ncluster it's deployed to, you must set  data.baseUrl  to the same\nvalue of the\n spec.configuration.mms.centralUrl \nsetting in the   resource specification. If you're using  , set the  data.baseUrl  value\nto  https://cloud.mongodb.com . https://ops.example.com:8443 projectName string Label for your  \n Project . The   creates the   project if it does\nnot exist. If you omit the  projectName , the  \ncreates a project with the same name as your\n  resource. To use an existing project in a  \norganization, locate\nthe  projectName  by clicking the  All Clusters \nlink at the top left of the   page, and\nsearching by name in the  Search \nbox, or scrolling to find the name in the list.\nEach card in this list represents the\ncombination of one    Organization  and  Project . Development orgId string 24 character hex string that uniquely identifies your\nMongoDB  Organization . This field is  optional . If you omit the  orgId ,\n  creates an organization called  projectName \nthat contains a project also called  projectName .\nIf specified, the   links to the organization. You must have the  Organization Project Creator \nrole to create a new project within an existing\n  organization. To find and use the  orgID  of your organization: Do one of the following: If you are using   4.4 or later or\n , click\n Settings  in the left navigation bar. If you are using   4.2 or earlier, click the\n Context  menu. Select your organization, view the current  \nin your browser and copy the value displayed in\nthe  <orgId>  placeholder as follows: https://ops.example.com:8443/ \n v2#/org/<orgId>/projects 5cc9b333dde384a625a6615 This command returns a ConfigMap description in the shell: You might have chosen to use your own   certificate to enable\n  for your   instance. If you used a custom certificate,\nyou need to add the CA that signed that custom certificate to the\n . To add your custom CA, complete the following: The   requires the root   certificate of the\nCertificate Authority that issued the   host's certificate. Run\nthe following command to create a   containing the root\nCA certificate in the same namespace of your database pods: The   requires that the certificate is named\n mms-ca.crt  in the ConfigMap. Invoke the following command to edit your project's ConfigMap in\nthe default configured editor: Paste the highlighted section in the example   at\nthe end of the project ConfigMap. Change the following   keys: Key Type Description Example sslMMSCAConfigMap string Name of the   created in the first step\ncontaining the root   certificate used to sign the\n  host's certificate. This mounts the CA certificate\nto the   and database resources. my-root-ca sslRequireValidMMSServerCertificates boolean Forces the Operator to require a valid   certificate\nfrom  . The value must be enclosed in single quotes or the\noperator will throw an error. 'true' This command returns a ConfigMap description in the shell:  defaults to an empty namespace if you do not specify the\n -n  option, resulting in deployment failures. The\n ,  , and  s should run in the\nsame unique namespace. Now that you created your ConfigMap,  Create Credentials for the   before\nyou start  deploying MongoDB resources .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap <configmap-name> \\\n  --from-literal=\"baseUrl=<myOpsManagerURL>\" \\\n  --from-literal=\"projectName=<myOpsManagerProjectName>\" \\ #Optional\n  --from-literal=\"orgId=<orgID>\" #Optional"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe configmaps <configmap-name>"
                },
                {
                    "lang": "sh",
                    "value": "Name:           <configmap-name>\nNamespace:      <namespace>\nLabels:         <none>\nAnnotations:    <none>\n\nData\n====\nbaseUrl:\n----\n<myOpsManagerURL>\nEvents:  <none>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n <namespace> create configmap <root-ca-configmap-name> \\\n  --from-file=mms-ca.crt"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: <my-configmap>\n  namespace: <my-namespace>\ndata:\n  projectName: <my-ops-manager-project-name>\n  orgId: <org-id> # Optional\n  baseUrl: https://<my-ops-manager-URL>\n\n"
                },
                {
                    "lang": "yaml",
                    "value": "  sslMMSCAConfigMap: <root-ca-configmap-name>\n  sslRequireValidMMSServerCertificates: \u2018true\u2019\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl edit configmaps <my-configmap> -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe configmaps <my-configmap> -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Name:           <my-configmap>\nNamespace:      <namespace>\nLabels:         <none>\nAnnotations:    <none>\n\nData\n====\nsslMMSCAConfigMap:\n----\n<root-ca-configmap-name>\nsslRequireValidMMSServerCertificates:\n----\ntrue\nEvents:  <none>"
                }
            ],
            "preview": "The  uses a   to create or link your\n Project. To create a\n ConfigMap, you need to edit a few lines of the\nexample ConfigMap  file and apply\nthe ConfigMap.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/migrate-to-single-resource",
            "title": "Migrate to One Resource per Project (Required for Version 1.3.0)",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Create a new ConfigMap for each MongoDB resource in the project.",
                "Update MongoDB resource objects.",
                "Update MongoDB user objects.",
                "Delete the original project ConfigMap.",
                "(Optional) Remove Orphaned Clusters from ."
            ],
            "paragraphs": "Starting in   version 1.3.0, you can only have one MongoDB\nresource per project. This document explains how to migrate existing\nprojects which have multiple MongoDB resources into configurations with\na single resource per project. Before completing this procedure, ensure that you have upgraded your\n  to version 1.3.0. For upgrade instructions, see\n Upgrade from Operator Version 0.10 or Later . Complete the following steps for each project that contains multiple\nMongoDB resources: To associate each MongoDB resource with a single project, each\nresource must have a distinct  . The new ConfigMaps: All other fields can remain the same as the original project\nConfigMap. Invoke the following command for each ConfigMap to apply\nthem to  : To learn more about creating a project using a ConfigMap, see\n Create One Project using a ConfigMap . Must have unique  projectName  fields. Cannot contain the  credentials  or  authenticationMode \nfields. For each MongoDB resource in the project: If  X.509 authentication  is enabled, add the\nfollowing fields to the    : Field Type Description spec.security.authentication object Contains authentication specifications for the\ndeployment. spec.security.authentication.enabled boolean Specifies whether authentication is enabled for the\ndeployment. Set this value to  true . spec.security.authentication.modes array Specifies supported authentication mechanisms for the\ndeployment. Set this value to  [\"X509\"] If internal cluster authentication is enabled, set\n spec.security.authentication.internalCluster  to  X509 . Add the  spec.opsManager.configMapRef.name  field to the\n    and set the value to the  metadata.name  value\nof the corresponding ConfigMap you created in step 1. Remove the  spec.project  field from the resource object. Invoke the following command for each resource object to apply\nthe updated configuration(s). When you apply a new configuration,\nthe Operator creates a new project in   containing the\ndeployment from the corresponding MongoDB resource. All\ndata on the resource database remains the same after the migration. For each  MongoDB user  resource: Remove the  spec.project   field. Add the  spec.mongodbResourceRef.name  field and set the value\nto the name of the relevant MongoDB resource in the same namespace. This may require duplicating your  MongoDBUser  resource if\nyou wish to have the same user in multiple clusters. Invoke the following command to delete the original project ConfigMap\nfrom your   namespace: After reconfiguring your deployments to exist in dedicated clusters,\nyou may have clusters remaining in the original project which are no\nlonger managed by the  . You can remove these clusters if\nyou wish. Removing clusters will delete their historical backups and\nmonitoring data.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <myconfigmap.yaml>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <configuration>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete -f <configMap>.yaml"
                }
            ],
            "preview": "Before completing this procedure, ensure that you have upgraded your\n to version 1.3.0. For upgrade instructions, see\nUpgrade from Operator Version 0.10 or Later.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/manage-database-users-x509",
            "title": "Manage Database Users Using X.509 Authentication",
            "headings": [
                "Prerequisites",
                "Add a Database User",
                "Configure kubectl to default to your namespace.",
                "Copy the following example .",
                "Open your preferred text editor and paste the example ConfigMap into a new text file.",
                "Change the five highlighted lines.",
                "Add any additional roles for the user to the ConfigMap.",
                "Create the user.",
                "View the newly created user in .",
                "Delete a Database User"
            ],
            "paragraphs": "The   supports managing database users for deployments\nrunning with   and X.509 internal cluster authentication enabled. The   does not support other authentication mechanisms\nin deployments it creates. In an Operator-created deployment, you\ncannot use   to: After enabling X.509 authentication, you can add X.509 users using the   interface or the  . Add other authentication methods to users. Manage users  not  using X.509 authentication. Before managing database users, you must deploy a\n replica set  or\n sharded cluster  with   and X.509\nenabled. If you need to generate X.509 certificates for your MongoDB users,\nsee  Generate X.509 Client Certificates . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Use the following table to guide you through changing the highlighted\nlines in the ConfigMap: Key Type Description Example metadata.name string The name of the database user resource. Resource names must be 44 characters or less. mms-user-1 spec.username string The subject line of the x509 client certificate signed\nby the     (Kube CA). To get the subject line of the X.509 certificate, run the\nfollowing command: The username must comply with the\n RFC 2253 \nLDAPv3 Distinguished Name standard. CN=mms-user,U=My Organizational Unit,O=My Org,L=New York,ST=New York,C=US spec.opsManager.configMapRef.name string The name of the project containing the MongoDB database\nwhere user will be added. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. my-project spec.roles.db string The database the  role  can act on. admin spec.mongodbResourceRef.name string The name of the  MongoDB resource  to\nwhich this user is associated. my-resource spec.roles.name string The name of the  role  to grant the database\nuser. The role name can be any\n built-in MongoDB role  or\n custom role  that exists\nin  . readWriteAnyDatabase You may grant additional roles to this user using the format defined\nin the following example: Invoke the following   command to create your database user: You can view the newly-created user in  : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. To delete a database user, pass the  metadata.name  from the user\n  to the following command:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: <resource-name>\nspec:\n  username: <rfc2253-subject>\n  db: \"$external\"\n  mongodbResourceRef:\n    name: '<MongoDB-Resource-name>'\n  roles:\n    - db: <database-name>\n      name: <role-name>\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "openssl x509 -noout \\\n  -subject -in <my-cert.pem> \\\n  -nameopt RFC2253"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: mms-user-1\nspec:\n  username: CN=mms-user,U=My Organizational Unit,O=My Org,L=New York,ST=New York,C=US\n  project: my-project\n  db: \"$external\"\n  roles:\n    - db: admin\n      name: backup\n    - db: admin\n      name: restore\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mdbu <metadata.name>"
                }
            ],
            "preview": "The  supports managing database users for deployments\nrunning with  and X.509 internal cluster authentication enabled.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "reference/k8s-operator-om-specification",
            "title": "Ops Manager Resource Specification",
            "headings": [
                "Example",
                "Required  Resource Settings",
                "Optional  Resource Settings"
            ],
            "paragraphs": "The \ncreates a containerized   deployment from specification files\nthat you write. After you create or update an   resource specification, you\ndirect   to apply this specification to your  \nenvironment.   creates the services and custom  \nresources that   requires, then deploys   and its backing\napplication database in containers in your   environment. Each   resource uses an   specification in   to\ndefine the characteristics and settings of the deployment. The following example shows a resource specification for an  \ndeployment: This section describes settings that you must use for all  \nresources. Type : string Required . Version of the MongoDB   resource schema. Type : string Required . Kind of MongoDB Kubernetes resource to create. Set this\nto  MongoDBOpsManager . Type : string Required . Name of the MongoDB   resource you are creating. Resource names must be 44 characters or less. Type : number Required . Number of   instances to run in parallel. The minimum accepted value is  1 . For high availability, set this value to more than  1 . Multiple\n  instances can read from the same Application\nDatabase, ensuring failover if one instance is unavailable and\nenabling you to update the   resource without downtime. Type : number Required . Version of   that you want to install\non this MongoDB   resource. Type : string Required . Name of the     you created for\nthe   admin user. When you deploy the   resource,\n  creates a user with these credentials. The admin user is granted the\n Global Owner \nrole. Type : integer Required . Number of members in the\n Application Database \nreplica set. Type : number Optional . Version of MongoDB installed on the  \n Application Database .\nIf you don't specify a specific version,   uses the\ndefault value. This value defaults to  4.2.2-ent . To deploy   inside   without an Internet connection,\nomit this setting or leave the value empty. The  \ninstalls the  bundled MongoDB Enterprise  version 4.2.2 by default. If you update this value to a later version, consider setting\n spec.featureCompatibilityVersion  to give yourself the\noption to downgrade if necessary.  resources can use the following settings: Type : collection   Application Database \nresource definition. The following settings from the\n replica set  resource specification are\noptional: spec.applicationDatabase. spec.additionalMongodConfig spec.applicationDatabase. spec.featureCompatibilityVersion spec.applicationDatabase. spec.logLevel spec.applicationDatabase. spec.persistent spec.applicationDatabase.podSpec. spec.podSpec.cpu spec.applicationDatabase.podSpec. spec.podSpec.cpuRequests spec.applicationDatabase.podSpec. spec.podSpec.memory spec.applicationDatabase.podSpec. spec.podSpec.memoryRequests spec.applicationDatabase.podSpec. spec.podSpec.persistence.single spec.applicationDatabase.podSpec.persistence.multiple. spec.podSpec.persistence.multiple.data spec.applicationDatabase.podSpec.persistence.multiple. spec.podSpec.persistence.multiple.journal spec.applicationDatabase.podSpec.persistence.multiple. spec.podSpec.persistence.multiple.logs spec.applicationDatabase.podSpec. spec.podSpec.podAffinity spec.applicationDatabase.podSpec. spec.podSpec.podAntiAffinityTopologyKey spec.applicationDatabase.podSpec. spec.podSpec.nodeAffinity spec.applicationDatabase.version Type : string Name of the  secret  that contains the\npassword for the   database user  mongodb-ops-manager .\n  uses this password to  authenticate to the Application\nDatabase . Type : string Name of the field in the  secret  that\ncontains the password for the   database user\n mongodb-ops-manager .   uses this password to\n authenticate to the Application Database . The default value is  password . Type : string Name of the     containing the   file for\nthe application database. Type : string Name of the     object created to secure the\napplication database resources. Type : boolean Flag that enables Backup for your   resource. When set to\n false , Backup is disabled. Default value is  true . Type : collection Configuration settings for the  head database .\n  creates a   with the specified configuration. Scalar Data Type Description labelSelector string Tag \nused to bind mounted volumes to directories. storage string Minimum size of   that should be mounted. This value is\nexpressed as an integer followed by a unit of storage in\n  notation. Default value is  30Gi . backup-hardware-requirements If the head database requires 60 gigabytes of storage\nspace, set this value to  60Gi . storageClass string Type of storage specified in a  . You may create\nthis storage type as a   object before using it in this\n  specification. Make sure to set the    reclaimPolicy  to\n Retain .\nThis ensures that data is retained when a   is removed. Type : array of strings Optional .   parameters passed to the   backup service\nin the container. This   parameter defaults to an empty list.  calculates the   memory heap values of the\nbackup service based on the container's memory. Changing the\n -Xms  and  -Xmx  values can cause issues with  . Type : collection Required if you enable Backup. Array of  oplog stores  used\nfor Backup. Each item in the array references a MongoDB database\nresource deployed in the   cluster by the  . Type : string Required if you enable Backup. Name of the oplog store. Once specified, do not edit the name of the oplog store. Type : string Required if you enable Backup. Name of the MongoDB database resource that you create to store oplog\nslices. You must deploy this database resource in the same namespace\nas the   resource. The Oplog database only supports the  SCRAM  authentication mechanism.\nYou cannot enable other authentication mechanisms. If you enable  SCRAM  authentication on the oplog database, you\nmust: Specify a MongoDB version earlier than v4.0 in the oplog database\nresource definition. Create a MongoDB user resource to connect   to the oplog\ndatabase. Specify the  name \nof the user in the   resource definition. If a MongoDB database resource with this name doesn't exist, the\n backup  resource enters a  Pending  state. The  \nretries every 10 seconds until a MongoDB database resource with this\nname is created. The   begins to reconcile the   resource\nautomatically when you make security changes to the database\nresources you reference in this setting. The   updates\nthe  mongoURI  and  ssl  flags in the   configuration\nbased on your changes. Type : string Required if SCRAM authentication is enabled on the oplog\nstore database. Name of the MongoDB user resource used to connect to the oplog store\ndatabase. Deploy this user resource in the same\nnamespace as the   resource and with the\n readWriteAnyDatabase  and\n dbAdminAnyDatabase  roles. Type : collection Required if you enable Backup using a blockstore. Array of  blockstores  used\nfor Backup. Each item in the array references a MongoDB database\nresource deployed in the   cluster by the  . Type : string Required if you enable Backup using a blockstore. Name of the blockstore. Once specified, do not edit the name of the blockstore. Type : string Required if you enable Backup using a blockstore. Name of the MongoDB database resource that you create for the\nblockstore. You must deploy this database resource in the same\nnamespace as the   resource. The blockstore database only supports the  SCRAM  authentication\nmechanism. You cannot enable other authentication mechanisms. If you enable  SCRAM  authentication on the blockstore database,\nyou must: Specify a MongoDB version earlier than v4.0 in the blockstore\ndatabase resource definition. Create a MongoDB user resource to connect   to the\nblockstore database. Specify the  name \nof the user in the   resource definition. If a MongoDB database resource with this name doesn't exist, the\n backup  resource enters a  Pending  state. The  \nretries every 10 seconds until a MongoDB database resource with this\nname is created. The   begins to reconcile the   resource\nautomatically when you make security changes to the database\nresources you reference in this setting. The   updates\nthe  mongoURI  and  ssl  flags in the   configuration\nbased on your changes. Type : string Required if SCRAM authentication is enabled on the blockstore database. Name of the MongoDB user resource used to connect to the blockstore\ndatabase. Deploy this user resource in the same\nnamespace as the   resource and with the\n readWriteAnyDatabase  and\n dbAdminAnyDatabase  roles. Type : collection Specification for the   that the   creates\nfor the  backup-daemon . To review which fields you can add to\n spec.backup.statefulSet.spec , see the\n Kubernetes documentation . Type : collection Template \nfor the   pods in the   that the   creates\nfor the  backup-daemon . The   doesn't validate the fields you provide\nin  spec.backup.statefulSet.spec.template . Type : collection Metadata for the   pods in the   that the\n  creates for the  backup-daemon . To review which fields you can add to\n spec.backup.statefulSet.spec.template.metadata , see\nthe  Kubernetes documentation . Type : collection Specifications of the   pods in the   that the\n  creates for the  backup-daemon . To review the complete list of fields you can add to\n spec.backup.statefulSet.spec.template.spec , see the\n Kubernetes documentation . The following example  spec.backup.statefulSet.spec.template.spec \ndefines minimum and maximum CPU and memory capacity for one\n backup-daemon  container the   deploys: Type : collection List of containers that belong to the   pods in the\n  that the   creates for the\n backup-daemon . To modify the specifications of the  backup-daemon  container,\nyou must provide the exact name of the container using the  name \nfield, as shown in the following example: When you add containers to\n spec.backup.statefulSet.spec.template.spec.containers ,\nthe   adds them to the   pod. These containers\nare appended to the  backup-daemon  containers in the pod. Type : string Minimum CPU capacity that must be available on a     to\nhost the  backup-daemon . The requested value must be less than or equal to\n spec.backup.statefulSet.spec.template.spec.containers.resources.limits.cpu . Type : string Maximum CPU capacity for the   being created to host\nthe  backup-daemon . If omitted, this value is set to\n spec.backup.statefulSet.spec.template.spec.containers.resources.requests.cpu . Type : string Minimum memory capacity that must be available on a    \nto host the  backup-daemon  on  . This value is expressed as\nan integer followed by a unit of memory in   notation. The requested value must be less than or equal to\n spec.backup.statefulSet.spec.template.spec.containers.resources.limits.memory . Set this value to at least  4.5Gi . Values of less than  4.5Gi \nmight result in an error. Type : string Maximum memory capacity for the   being created to host\nthe  backup-daemon . If omitted, this value is set to\n spec.backup.statefulSet.spec.template.spec.containers.resources.requests.memory . The   calculates and sets parameters for Java heap size\nbased on the container's memory. Setting this value to a value greater than 32 GB ( 32Gi ) can\ncause issues with the backup service. Excessive heaps can cause\nunpredictable results in  . Type : string Required if you enable Backup using an S3 store. Name of the   snapshot store. Once specified, do not edit the name of the   snapshot store. This change will likely fail if\nbackups use the old name. The consequences of\na successful change are unpredictable. Type : string Name of the MongoDB database resource that you create to store\nmetadata for the   snapshot store. You must deploy this database\nresource in the same namespace as the   resource. If you enable  SCRAM  authentication on this database, you must: Omit this setting to use the application database to store\nmetadata for the   snapshot store. If you omit this setting, you must also omit the\n spec.backup.s3Stores.mongodbUserRef.name  setting.\nThe   handles  SCRAM  user authentication\ninternally. Specify a MongoDB version earlier than v4.0 in the database\nresource definition. Create a MongoDB user resource to connect   to the\ndatabase. Specify the\n name  of the\nuser in the   resource definition. Once specified, do not edit the name of the   snapshot store.\nThis change will likely fail if backups use the old name. The\nconsequences of a successful change are unpredictable. If a MongoDB database resource with this name doesn't exist, the\n backup  resource enters a  Pending  state. The  \nretries every 10 seconds until a MongoDB database resource with this\nname is created. The   begins to reconcile the   resource\nautomatically when you make security changes to the database\nresources you reference in this setting. The   updates\nthe  mongoURI  and  ssl  flags in the   configuration\nbased on your changes. Type : string Required if you created a MongoDB database resource to store\n|s3| snapshot metadata and SCRAM is enabled on this database. Name of the MongoDB user resource used to connect to the metadata\ndatabase of the   snapshot store. Deploy this user resource in the\nsame namespace as the   resource and with the\n readWriteAnyDatabase  and\n dbAdminAnyDatabase  roles. Once specified, do not edit the name of the   metadata snapshot\nstore username. Type : string Required if you enable Backup using an S3 store. Name of the secret that contains the  accessKey  and\n secretKey  fields. The  backup-daemon  uses the values\nof these fields as credentials to access your\n    or  -compatible bucket. The  \nsnapshot store can't be configured if the secret is missimg either\nkey. Type : boolean Indicates the style of the bucket endpoint URL. Default value is  true . Value Description Example true Path-style URL s3.amazonaws.com/<bucket> false Virtual-host-style URL <bucket>.s3.amazonaws.com Type : string Required if you enable Backup using an S3 store. URL of the     bucket or  -compatible bucket that hosts the\nsnapshot store. Type : string Required if you enable Backup using an S3 store. Name of the     bucket or  -compatible bucket that hosts\nthe snapshot store. Type : string  assigns each   a  . The   calculates\nthe   for each   using a provided  clusterDomain .\n  does not provide an   to query these hostnames. Type : string  assigns each   a  . The   calculates\nthe   for each   using a provided  clusterName .  \ndoes not provide an   to query these hostnames. Use  spec.clusterDomain  instead. Type : collection  configuration properties.\nSee  Ops Manager Configuration Settings  for property names and descriptions.\nEach property takes a value of type  string . If   will manage MongoDB resources deployed outside of the\n  cluster it's deployed to, you must add the  mms.centralUrl \nsetting to  spec.configuration . Set the value to the URL by which   is exposed outside of the\n  cluster. Type : string The   service's  default server type . Accepted values are:  PRODUCTION_SERVER ,  TEST_SERVER ,  DEV_SERVER , and\n RAM_POOL . Type : collection Configuration object that enables external connectivity to  .\nIf provided, the   creates a    service  that allows traffic\noriginating from outside of the   cluster to reach the  \napplication. If not provided, the   does not create a   service.\nYou must create one manually or use a third-party solution that\nenables you to route external traffic to the   in your\n  cluster. Type : string The   service  ServiceType \nthat exposes   outside of  . Required  if  spec.externalConnectivity.type  is\npresent. Accepted values are:  LoadBalancer  and  NodePort .\n LoadBalancer  is recommended if your cloud provider supports it.\nUse  NodePort  for local deployments. Type : integer If  spec.externalConnectivity.type  is  NodePort , the\nport on the   service from which external traffic is routed to\nthe  . If  spec.externalConnectivity.type  is  LoadBalancer ,\nthe load balancer resource that your cloud provider creates routes\ntraffic to this port on the   service. You don't need to provide\nthis value.   uses an open port within the default range and\nhandles internal traffic routing appropriately. In both cases, if this value is not provided, the   service\nroutes traffic from an available port within the following default\nrange to the  :  30000 - 32767 . You must configure your network's firewall to allow traffic over\nthis port. Type : string The IP address the  LoadBalancer    service uses when the\n  creates it. This setting can only be used if your cloud provider supports it and\n spec.externalConnectivity.type  is  LoadBalancer . To\nlearn more about the\n Type LoadBalancer , see the\n  documentation. Type : string Routing policy for external traffic to the     service.\nThe service routes external traffic to node-local or cluster-wide\nendpoints depending the value of this setting. Accepted values are:  Cluster  and  Local . To learn which of\nvalues meet your requirements, see  Source IPs in Kubernetes  in the   documentation. If you select  Cluster , the  Source-IP  of your clients are\nlost during the network hops that happen at the  \nnetwork boundary. Type : collection Key-value pairs that allow you to provide cloud provider-specific\nconfiguration settings. To learn more about\n Annotations \nand\n TLS support on AWS ,\nsee the   documentation. Type : array of strings Optional .   parameters passed to the   in the\ncontainer. Any parameters given replace the default   parameters\nfor the  . This   parameter defaults to an empty list.  calculates its   memory heap values of the\n  based on the container's memory. Changing the\n -Xms  and  -Xmx  values can cause issues with  . Type : string Name of the     you created for your  \ncertificate. Used when creating an   instance which runs\nover  . To learn how to configure your   instance to run over\n , see  Deploy an   Resource . Type : collection Specification for the   that the   creates\nfor  . To review which fields you can add to\n spec.statefulSet.spec , see the\n Kubernetes documentation . Type : collection Template \nfor the   pods in the   that the   creates\nfor the  . The   doesn't validate the fields you provide\nin  spec.statefulSet.spec.template . Type : collection Metadata for the   pods in the   that the\n  creates for the  . To review which fields you can add to\n spec.statefulSet.spec.template.metadata , see\nthe  Kubernetes documentation . Type : collection Specifications of the   pods in the   that the\n  creates for the  . To review the complete list of fields you can add to\n spec.statefulSet.spec.template.spec , see the\n Kubernetes documentation . The following example  spec.statefulSet.spec.template.spec  defines\nminimum and maximum CPU and memory capacity for one  \ncontainer the   deploys: Type : collection List of containers that belong to the   pods in the\n  that the   creates for the\n . To modify the specifications of the   container,\nyou must provide the exact name of the container using the  name \nfield, as shown in the following example: When you add containers to\n spec.statefulSet.spec.template.spec.containers ,\nthe   adds them to the   pod. These containers\nare appended to the   containers in the pod. Type : string Minimum CPU capacity that must be available on a     to\nhost the  . The requested value must be less than or equal to\n spec.statefulSet.spec.template.spec.containers.resources.limits.cpu . Type : string Maximum CPU capacity for the   being created to host\nthe  . If omitted, this value is set to\n spec.statefulSet.spec.template.spec.containers.resources.requests.cpu . Type : string Minimum memory capacity that must be available on a    \nto host the   on  . This value is expressed as\nan integer followed by a unit of memory in   notation. The requested value must be less than or equal to\n spec.statefulSet.spec.template.spec.containers.resources.limits.memory . If   on   requires 6 gigabytes of memory, set\nthis value to  6Gi . MongoDB recommends setting this value to at least  5Gi . Type : string Maximum memory capacity for the   being created to host\nthe  . If omitted, this value is set to\n spec.statefulSet.spec.template.spec.containers.resources.requests.memory . The   calculates and sets parameters for Java heap size\nbased on the container's memory. Setting this value to a value greater than 32 GB ( 32Gi ) can\ncause issues with the backup service. Excessive heaps can cause\nunpredictable results in  .",
            "code": [
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: om\nspec:\n replicas: 1\n version: 4.2.6\n adminCredentials: ops-manager-admin\n configuration:\n  mms.fromEmailAddr: admin@example.com\n  mms.security.allowCORS: \"false\"\n backup:\n  enabled: true\n  headDB:\n   storage: 30Gi\n   labelSelector:\n    matchLabels:\n     app: my-app\n  opLogStores:\n   - name: oplog1\n     mongodbResourceRef:\n      name: my-oplog-db\n     mongodbUserRef:\n      name: my-oplog-user\n  s3Stores:\n   - name: s3store1\n     mongodbResourceRef:\n      name: my-s3-metadata-db\n     mongodbUserRef:\n      name: my-s3-store-user\n     s3SecretRef:\n       name: my-s3-credentials\n     pathStyleAccessEnabled: true\n     s3BucketEndpoint: s3.region.amazonaws.com\n     s3BucketName: my-bucket\n\n applicationDatabase:\n   passwordSecretKeyRef:\n    name: om-db-user-secret\n    key: password\n   members: 3\n   version: 4.2.2-ent\n   persistent: true\n   podSpec:\n     cpu: 0.25\n"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  backup:\n    jvmParameters: [\"-XX:+UseStringCache\"]"
                },
                {
                    "lang": "yaml",
                    "value": "statefulSet:\n  spec:\n    template:\n      spec:\n        containers:\n        - name: mongodb-backup-daemon\n          resources:\n            requests:\n              cpu: '0.50'\n              memory: '4500M'\n            limits:\n              cpu: '1'\n              memory: '6000M'"
                },
                {
                    "lang": "yaml",
                    "value": "backup:\n statefulSet:\n   spec:\n     template:\n       spec:\n         containers:\n         - name: mongodb-backup-daemon"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  jvmParameters: [\"-XX:+HeapDumpOnOutOfMemoryError\",\"-XX:HeapDumpPath=/tmp\"]"
                },
                {
                    "lang": "yaml",
                    "value": "statefulSet:\n  spec:\n    template:\n      spec:\n        containers:\n          - name: mongodb-ops-manager\n            resources:\n              requests:\n                cpu: '0.70'\n                memory: '6Gi'\n              limits:\n                cpu: '1'\n                memory: '7000M'"
                },
                {
                    "lang": "yaml",
                    "value": "backup:\n statefulSet:\n   spec:\n     template:\n       spec:\n         containers:\n         - name: mongodb-ops-manager"
                }
            ],
            "preview": "Type: string",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/install-k8s-operator",
            "title": "Install the ",
            "headings": [
                "Prerequisites and Considerations",
                "Procedure",
                "Change to the directory in which you cloned the repository.",
                "Install the  for MongoDB deployments using the following  command:",
                "You can edit the Operator  file to further customize your Operator before installing it.",
                "Install the  using the following  command:",
                "Change to the directory in which you cloned the repository.",
                "Install the  using the following helm command:",
                "Use docker to request the files on a host connected to the Internet.",
                "Export the  images as .tar archive files:",
                "Copy these .tar files to the host running the  docker daemon.",
                "Import the .tar files into docker.",
                "Install the  with modified pull policy values using the following helm command:",
                "Change to the directory in which you cloned the repository.",
                "Install the  for MongoDB deployments.",
                "You can edit the Operator  file to further customize your Operator before installing it.",
                "Install the  using the following  command:",
                "Change to the directory in which you cloned the repository.",
                "Add your OpenShift Pull Secret to the OpenShift Values file.",
                "Install the  using helm.",
                "Use docker to request the files on a host connected to the Internet.",
                "Export the  images as .tar archive files:",
                "Copy these .tar files to the host running the  docker daemon.",
                "Import the .tar files into docker.",
                "Add your OpenShift Pull Secret to the OpenShift Values file.",
                "Install the  with modified pull policy values.",
                "Verify the Installation",
                "Next Steps"
            ],
            "paragraphs": "Before you install the  , make sure you\n plan for your installation : Choose a  deployment topology . Read the  Considerations . Complete the  Prerequisites . This tutorial presumes some knowledge of  , but does link to\nrelevant   documentation where possible. If you are unfamiliar\nwith  , please review that documentation first. The install procedure varies based on how you want to configure your\nenvironment: The following examples assume that you created a  \nusing the default   namespace of  mongodb .\nIf you specified a different label for your namespace when\nyou  created it , change all\nvalues for  metadata.namespace  to that namespace. To change the label for the namespace for the following\ndeployment to  production , edit all values for\n metadata.namespace  in  mongodb-enterprise.yaml : Open your  mongodb-enterprise.yaml  in your preferred\ntext editor. You may need to add one or more of the following\noptions: Environment Variable When to Use OPERATOR_ENV Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . You can set the following pair of values: If  OPERATOR_ENV  is Log Level is set to Log Format is set to dev debug text prod info json WATCH_NAMESPACE Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . You can set the following pair of values: To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. MONGODB_ENTERPRISE_DATABASE_IMAGE  of the MongoDB Enterprise Database image the  \ndeploys. Default value is\n quay.io/mongodb/mongodb-enterprise-database . IMAGE_PULL_POLICY Pull policy  for the\nMongoDB Enterprise database image the   deploys. Accepted values are  Always ,  IfNotPresent ,  Never . Default value is  Always . OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the image for an  Ops\nManager resource  is downloaded. Default value is\n quay.io/mongodb/mongodb-enterprise-ops-manager . OPS_MANAGER_IMAGE_PULL_POLICY Pull policy  for the\n  images the   deploys. Accepted values are:  Always ,  IfNotPresent ,  Never . Default value is  Always . INIT_OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the initContainer image that\ncontains   start-up scripts and the readiness probe is\ndownloaded. Default value is\n quay.io/mongodb/mongodb-enterprise-ops-manager-init . INIT_OPS_MANAGER_VERSION Version of the initContainer image that contains  \nstart-up scripts and the readiness probe. Default value is 1.0.1. APPDB_IMAGE_REPOSITORY  of the repository from which the Application Database image\nis downloaded. Default value is\n quay.io/mongodb/mongodb-enterprise-appdb . INIT_APPDB_IMAGE_REPOSITORY  of the repository from which the initContainer image that\ncontains Application Database start-up scripts and the readiness\nprobe is downloaded. Default value is\n quay.io/mongodb/mongodb-enterprise-appdb-init . INIT_APPDB_VERSION Version of the initContainer image that contains  \nstart-up scripts and the readiness probe. Default value is 1.0.2. MANAGED_SECURITY_CONTEXT Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if you want to run the  \nin OpenShift or in a restrictive environment. Default value is  false . You can install the   with  Helm 3 . Invoke the following  helm  command: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the  values.yaml  file: Setting Purpose Default namespace To use a different namespace, specify that  namespace . mongodb managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if your cluster manages the\n securityContext  for your   resources. false Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json prod Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. <metadata.namespace> Custom resources that the   watches. The   installs the   for and watches only\nthe resources you specify. Accepted values are: Value Description mongodb Install the CustomResourceDefinitions for the database resources and also\nwatch those resources. mongodbusers Install the CustomResourceDefinitions for the MongoDB user resources and\nalso watch those resources. opsmanagers Install the CustomResourceDefinitions for the   resources and also\nwatch those resources. mongodbusers mongodb opsmanagers Repository from which the Application Database image is pulled.\nSpecify this value if you want to pull the   image from a\nprivate repository. Repository from which the Application Database initContainer\nimage is pulled. This image contains the start-up scripts and\nreadiness probe for the Application Database. Specify this value if you want to pull the Application Database\ninitContainer image from a private repository. Repository from which the   initContainer image is\npulled. This image contains the start-up scripts and readiness\nprobe for  . Specify this value if you want to pull the  \n initContainer  image from a private repository. Repository from which the   image is pulled.\nSpecify this value if you want to pull the   image\nfrom a private repository. Repository from which the   image is pulled. Specify this\nvalue if you want to pull the   image from a private\nrepository. Alternatively, you can pass these values as options when you apply the\nHelm Chart: To install the   on a host not connected to the Internet: You can install the   with  Helm 3 . Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the MongoDB Enterprise database version you're\ninstalling. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the MongoDB Enterprise database version you're\ninstalling. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the MongoDB Enterprise database version you're\ninstalling. Invoke the following  helm  command: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the  values.yaml  file: Setting Purpose Default namespace To use a different namespace, specify that  namespace . mongodb managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if your cluster manages the\n securityContext  for your   resources. false Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json prod Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. <metadata.namespace> Custom resources that the   watches. The   installs the   for and watches only\nthe resources you specify. Accepted values are: Value Description mongodb Install the CustomResourceDefinitions for the database resources and also\nwatch those resources. mongodbusers Install the CustomResourceDefinitions for the MongoDB user resources and\nalso watch those resources. opsmanagers Install the CustomResourceDefinitions for the   resources and also\nwatch those resources. mongodbusers mongodb opsmanagers Repository from which the Application Database image is pulled.\nSpecify this value if you want to pull the   image from a\nprivate repository. Repository from which the Application Database initContainer\nimage is pulled. This image contains the start-up scripts and\nreadiness probe for the Application Database. Specify this value if you want to pull the Application Database\ninitContainer image from a private repository. Repository from which the   initContainer image is\npulled. This image contains the start-up scripts and readiness\nprobe for  . Specify this value if you want to pull the  \n initContainer  image from a private repository. Repository from which the   image is pulled.\nSpecify this value if you want to pull the   image\nfrom a private repository. Repository from which the   image is pulled. Specify this\nvalue if you want to pull the   image from a private\nrepository. Alternatively, you can pass these values as options when you apply the\nHelm Chart: The following examples assume that you created a  \nusing the default   namespace of  mongodb .\nIf you specified a different label for your namespace when\nyou  created it , change all\nvalues for  metadata.namespace  to that namespace. To change the label for the namespace for the following\ndeployment to  production , edit all values for\n metadata.namespace  in  mongodb-enterprise-openshift.yaml : Invoke the following   command: If you run OpenShift 3.11 or earlier, you must first manually edit the   to remove subresources. In each  , remove the\nfollowing option: Open your  mongodb-enterprise-openshift.yaml  in your preferred\ntext editor. You must add your  <openshift-pull-secret>  to the\n ServiceAccount  definitions: You may need to add one or more of the following\noptions: Environment Variable Purpose OPERATOR_ENV Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . You can set the following pair of values: If  OPERATOR_ENV  is Log Level is set to Log Format is set to dev debug text prod info json WATCH_NAMESPACE Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . You can set the following pair of values: To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. MONGODB_ENTERPRISE_DATABASE_IMAGE  of the MongoDB Enterprise Database image the\n  deploys. Default value is\n registry.connect.redhat.com/mongodb/mongodb-enterprise-database . IMAGE_PULL_POLICY Pull policy \nfor the MongoDB Enterprise database image the  \ndeploys. Accepted values are  Always ,  IfNotPresent ,  Never . Default value is  Always . OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the image for an\n Ops Manager resource  is\ndownloaded. Default value is\n registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager . OPS_MANAGER_IMAGE_PULL_POLICY Pull policy \nfor the image deployed to an\n Ops Manager resource . Accepted values are  Always ,  IfNotPresent ,  Never . Default value is  Always . INIT_OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the initContainer image that\ncontains   start-up scripts and the readiness probe is\ndownloaded. Default value is\n registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager-init . INIT_OPS_MANAGER_VERSION Version of the initContainer image that contains  \nstart-up scripts and the readiness probe. Default value is 1.0.1. APPDB_IMAGE_REPOSITORY  of the repository from which the Application Database\nimage is downloaded. Default value is\n registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb . INIT_APPDB_IMAGE_REPOSITORY  of the repository from which the  initContainer  image\nthat contains Application Database start-up scripts and the\nreadiness probe is downloaded. Default value is\n registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb-init . INIT_APPDB_VERSION Version of the  initContainer  image that contains  \nstart-up scripts and the readiness probe. Default value is 1.0.2. MANAGED_SECURITY_CONTEXT Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. For OpenShift,  MANAGED_SECURITY_CONTEXT  must always be\n true . Default value is  true . You can install the   with  Helm 3 . Add the name of your  <openshift-pull-secret>  to the\n registry.imagePullSecrets  setting in the\n helm_chart/values-openshift.yaml  file: Invoke the following  helm  command: If you run OpenShift 3.11 or earlier, you must first manually edit the   to remove subresources. In each  , remove the\nfollowing option: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the\n values-openshift.yaml  file: Setting Purpose Default namespace To use a different namespace, specify that  namespace . mongodb managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. For OpenShift,  managedSecurityContext  must always be\n true . true Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json prod Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. <metadata.namespace> Custom resources that the   watches. The   installs the   for and watches only\nthe resources you specify. Accepted values are: Value Description mongodb Install the CustomResourceDefinitions for the database resources and also\nwatch those resources. mongodbusers Install the CustomResourceDefinitions for the MongoDB user resources and\nalso watch those resources. opsmanagers Install the CustomResourceDefinitions for the   resources and also\nwatch those resources. mongodbusers mongodb opsmanagers Repository from which the Application Database image is pulled.\nSpecify this value if you want to pull the   image from a\nprivate repository.  that contains the credentials required to pull\nimagePullSecrets from the repository. OpenShift requires this setting. Define it in this file or\npass it when you install the   using Helm. Repository from which the   image is pulled.\nSpecify this value if you want to pull the   image\nfrom a private repository. Repository from which OpenShift pulls the   image.\nSpecify this value if you want to pull the   image from a\nprivate repository. Repository from which the Application Database  initContainer \nimage is pulled. This image contains the start-up scripts and\nreadiness probe for the Application Database. Specify this value if you want to pull the Application Database\n initContainer  image from a private repository. Repository from which the    initContainer  image is\npulled. This image contains the start-up scripts and readiness\nprobe for  . Specify this value if you want to pull the  \n initContainer  image from a private repository. Alternatively, you can pass these values as options when you apply the\nHelm Chart: To install the   on a host not connected to the Internet: You can install the   with  Helm 3 . Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the MongoDB Enterprise database version you're\ninstalling. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the MongoDB Enterprise database version you're\ninstalling. Replace the following values: <op-version>  with the   version you're installing <om-version>  with the   version you're installing. <db-version>  with the MongoDB Enterprise database version you're\ninstalling. Add the name of your  <openshift-pull-secret>  to the\n registry.imagePullSecrets  setting in the\n helm_chart/values-openshift.yaml  file: Invoke the following  helm  command: If you run OpenShift 3.11 or earlier, you must first manually edit the   to remove subresources. In each  , remove the\nfollowing option: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the\n values-openshift.yaml  file: Setting Purpose Default namespace To use a different namespace, specify that  namespace . mongodb managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. For OpenShift,  managedSecurityContext  must always be\n true . true Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json prod Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. <metadata.namespace> Custom resources that the   watches. The   installs the   for and watches only\nthe resources you specify. Accepted values are: Value Description mongodb Install the CustomResourceDefinitions for the database resources and also\nwatch those resources. mongodbusers Install the CustomResourceDefinitions for the MongoDB user resources and\nalso watch those resources. opsmanagers Install the CustomResourceDefinitions for the   resources and also\nwatch those resources. mongodbusers mongodb opsmanagers Repository from which the Application Database image is pulled.\nSpecify this value if you want to pull the   image from a\nprivate repository.  that contains the credentials required to pull\nimagePullSecrets from the repository. OpenShift requires this setting. Define it in this file or\npass it when you install the   using Helm. Repository from which the   image is pulled.\nSpecify this value if you want to pull the   image\nfrom a private repository. Repository from which OpenShift pulls the   image.\nSpecify this value if you want to pull the   image from a\nprivate repository. Repository from which the Application Database  initContainer \nimage is pulled. This image contains the start-up scripts and\nreadiness probe for the Application Database. Specify this value if you want to pull the Application Database\n initContainer  image from a private repository. Repository from which the    initContainer  image is\npulled. This image contains the start-up scripts and readiness\nprobe for  . Specify this value if you want to pull the  \n initContainer  image from a private repository. Alternatively, you can pass these values as options when you apply the\nHelm Chart: To verify that the   installed correctly, run the\nfollowing command and verify the output: By default, deployments exist in the  mongodb  namespace. If the\nfollowing error message appears, ensure you use the correct\nnamespace: To troubleshoot your  , see\n Review Logs from the  . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . After installing the  , you can: Create an instance of Ops Manager Configure the Kubernetes Operator to deploy MongoDB resources",
            "code": [
                {
                    "lang": "yaml",
                    "value": "##---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-operator\n  namespace: production\n##---\n# Source: mongodb-enterprise-operator/templates/operator.yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-enterprise-operator\n  namespace: production\n\n---\n# Example truncated\n---\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: OPERATOR_ENV\nspec.template.spec.containers.name.env.value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPERATOR_ENV\n          value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: WATCH_NAMESPACE\nspec.template.spec.containers.name.env.value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: WATCH_NAMESPACE\n          value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMONGODB_ENTERPRISE_DATABASE_IMAGE\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-database"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n          value: quay.io/mongodb/mongodb-enterprise-database\n        - name: IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nIMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n          value: quay.io/mongodb/mongodb-enterprise-database\n        - name: IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_OPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-ops-manager-init"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager-init"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_OPS_MANAGER_VERSION\nspec.template.spec.containers.name.env.value:\n1.0.1"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_OPS_MANAGER_VERSION\n          value: 1.0.1"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nAPPDB_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: APPDB_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_APPDB_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-init-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_APPDB_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-init-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_APPDB_VERSION\nspec.template.spec.containers.name.env.value:\n1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_APPDB_VERSION\n          value: 1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMANAGED_SECURITY_CONTEXT\nspec.template.spec.containers.name.env.value:\nfalse"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MANAGED_SECURITY_CONTEXT\n          value: true"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f mongodb-enterprise.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --values helm_chart/values.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  # Execution environment for the operator, dev or prod.\n  # Use dev for more verbose logging\n  env: prod"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchedResources:\n    - mongodbusers\n    - mongodb\n    - opsmanagers"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: quay.io/mongodb"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --values helm_chart/values.yaml \\\n     --set namespace=<testNamespace>"
                },
                {
                    "lang": "sh",
                    "value": "docker pull quay.io/mongodb/mongodb-enterprise-operator:<op-version>; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-database:<db-version>; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-ops-manager:<om-version>; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-appdb:latest; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-ops-manager-init:1.0.1; \\\ndocker pull quay.io/mongodb/mongodb-enterprise-init-appdb:1.0.2"
                },
                {
                    "lang": "sh",
                    "value": "docker save quay.io/mongodb/mongodb-enterprise-operator:<op-version> -o mongodb-enterprise-operator.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-database:<db-version> -o mongodb-enterprise-database.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-ops-manager:<om-version> -o mongodb-enterprise-ops-manager.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-appdb:latest -o mongodb-enterprise-appdb.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-ops-manager-init:1.0.1 -o mongodb-enterprise-ops-manager-init.tar; \\\ndocker save quay.io/mongodb/mongodb-enterprise-init-appdb:1.0.2 -o mongodb-enterprise-init-appdb.tar"
                },
                {
                    "lang": "sh",
                    "value": "docker load -i mongodb-enterprise-operator.tar; \\\ndocker load -i mongodb-enterprise-database.tar; \\\ndocker load -i mongodb-enterprise-ops-manager.tar; \\\ndocker load -i mongodb-enterprise-appdb.tar; \\\ndocker load -i mongodb-enterprise-ops-manager-init.tar\ndocker load -i mongodb-enterprise-init-appdb.tar"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --values helm_chart/values.yaml \\\n     --set registry.pullPolicy=IfNotPresent"
                },
                {
                    "lang": "yaml",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  # Execution environment for the operator, dev or prod.\n  # Use dev for more verbose logging\n  env: prod"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchedResources:\n    - mongodbusers\n    - mongodb\n    - opsmanagers"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: quay.io/mongodb"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --values helm_chart/values.yaml \\\n     --set registry.pullPolicy=IfNotPresent \\\n     --set namespace=<testNamespace>"
                },
                {
                    "lang": "yaml",
                    "value": "##---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n  namespace: production\n##---\n# Source: mongodb-enterprise-operator/templates/operator.yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: enterprise-operator\n  namespace: production\n\n---\n# Example truncated\n---\n..."
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  subresources:\n    status: {}\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-appdb\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-database-pods\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: OPERATOR_ENV\nspec.template.spec.containers.name.env.value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPERATOR_ENV\n          value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: WATCH_NAMESPACE\nspec.template.spec.containers.name.env.value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: WATCH_NAMESPACE\n          value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMONGODB_ENTERPRISE_DATABASE_IMAGE\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-database"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-database\n        - name: IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nIMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-database\n        - name: IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_OPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager-init"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_OPS_MANAGER_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager-init"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_OPS_MANAGER_VERSION\nspec.template.spec.containers.name.env.value:\n1.0.1"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_OPS_MANAGER_VERSION\n          value: 1.0.1"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nAPPDB_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: APPDB_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_APPDB_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-init-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_APPDB_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-init-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_APPDB_VERSION\nspec.template.spec.containers.name.env.value:\n1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_APPDB_VERSION\n          value: 1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMANAGED_SECURITY_CONTEXT\nspec.template.spec.containers.name.env.value:\ntrue"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MANAGED_SECURITY_CONTEXT\n          value: true"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f mongodb-enterprise-openshift.yaml"
                },
                {
                    "lang": "sh",
                    "value": "registry:\n# The pull secret must be specified\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --values helm_chart/values-openshift.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  subresources:\n    status: {}\n"
                },
                {
                    "lang": "yaml",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# OpenShift manages security context on its own\nmanagedSecurityContext: true"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  # Execution environment for the operator, dev or prod.\n  # Use dev for more verbose logging\n  env: prod"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchedResources:\n    - mongodbusers\n    - mongodb\n    - opsmanagers"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --values helm_chart/values-openshift.yaml \\\n     --set registry.imagePullSecrets=<openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "docker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-operator:<op-version>; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-database:<db-version>; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager:<om-version>; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb:latest; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager-init:1.0.1; \\\ndocker pull registry.connect.redhat.com/mongodb/mongodb-enterprise-init-appdb:1.0.2"
                },
                {
                    "lang": "sh",
                    "value": "docker save registry.connect.redhat.com/mongodb/mongodb-enterprise-operator:<op-version> -o mongodb-enterprise-operator.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-database:<db-version> -o mongodb-enterprise-database.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager:<om-version> -o mongodb-enterprise-ops-manager.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb:latest -o mongodb-enterprise-appdb.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager-init:1.0.1 -o mongodb-enterprise-ops-manager-init.tar; \\\ndocker save registry.connect.redhat.com/mongodb/mongodb-enterprise-init-appdb:1.0.2 -o mongodb-enterprise-init-appdb.tar"
                },
                {
                    "lang": "sh",
                    "value": "docker load -i mongodb-enterprise-operator.tar; \\\ndocker load -i mongodb-enterprise-database.tar; \\\ndocker load -i mongodb-enterprise-ops-manager.tar; \\\ndocker load -i mongodb-enterprise-appdb.tar; \\\ndocker load -i mongodb-enterprise-ops-manager-init.tar\ndocker load -i mongodb-enterprise-init-appdb.tar"
                },
                {
                    "lang": "sh",
                    "value": "registry:\n# The pull secret must be specified\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --values helm_chart/values-openshift.yaml \\\n     --set registry.pullPolicy=IfNotPresent \\\n     --set registry.imagePullSecrets=<openshift-pull-secret>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  subresources:\n    status: {}\n"
                },
                {
                    "lang": "yaml",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# OpenShift manages security context on its own\nmanagedSecurityContext: true"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  # Execution environment for the operator, dev or prod.\n  # Use dev for more verbose logging\n  env: prod"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchedResources:\n    - mongodbusers\n    - mongodb\n    - opsmanagers"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --values helm_chart/values-openshift.yaml \\\n     --set registry.pullPolicy=IfNotPresent \\\n     --set registry.imagePullSecrets=<openshift-pull-secret> \\\n     --set namespace=<testNamespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe deployments mongodb-enterprise-operator -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Error from server (NotFound): deployments.apps \"mongodb-enterprise-operator\" not found"
                }
            ],
            "preview": "Before you install the , make sure you\nplan for your installation:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/create-operator-credentials",
            "title": "Create Credentials for the ",
            "headings": [
                "Prerequisites",
                "Procedure"
            ],
            "paragraphs": "For the   to create or update   in your  \nProject, you need to store your\n Programmatic API Key  as a  \n . Creating a secret stores authentication credentials so\nonly   can access them. Multiple secrets can exist in the same namespace. Each user should\nhave their own secret. To create credentials for the  , you must: Have or create an  \n Organization . Unlike earlier   versions, use the Operator to\ncreate your   project. The Operator adds additional metadata\nto Projects that it creates to help manage the deployments. Have or generate a\n Programmatic API Key . Grant this new   the  Project Owner  role. Add the   or   block of any hosts that serve the\n  to the\n API Whitelist . To create your   secret: Make sure you have the Public and Private Keys for your desired\n   . Invoke the following   command to create your secret: The  -n  flag limits the   to which this secret\napplies. All MongoDB   resources must be in the same\nnamespace with the   and  . The\n  does not use either the secrets or ConfigMaps. Invoke the following   command to verify your secret: This command returns a secret description in the shell:",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl -n <metadata.namespace> \\\n  create secret generic <myCredentials> \\\n  --from-literal=\"user=<publicKey>\" \\\n  --from-literal=\"publicApiKey=<privateKey>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe secrets/<myCredentials> -n <metadata.namespace>"
                },
                {
                    "lang": "sh",
                    "value": "Name:         <myCredentials>\nNamespace:    <metadata.namespace>\nLabels:       <none>\nAnnotations:  <none>\n\nType:  Opaque\n\nData\n====\npublicApiKey:  31 bytes\nuser:          22 bytes"
                }
            ],
            "preview": "For the  to create or update  in your \nProject, you need to store your\nProgrammatic API Key as a \n. Creating a secret stores authentication credentials so\nonly  can access them.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-op-considerations",
            "title": "Considerations",
            "headings": [
                "MANAGED_SECURITY_CONTEXT for  OpenShift Deployments",
                "Docker Container Details",
                "Validation Webhook",
                " Deployment Scopes",
                "Operator in Same Namespace as Resources",
                "Operator in Different Namespace Than Resources",
                "Cluster-Wide Scope",
                "Customize the CustomResourceDefinitions that the  Watches"
            ],
            "paragraphs": "When you deploy the   to OpenShift, you must set the\n MANAGED_SECURITY_CONTEXT  flag to  true . This value is set for you\nin the \nand \nfiles included in the . For more information on modifying this value, see the  instructions  for the installation method you want to use. MongoDB builds the container images from the latest builds of the\nfollowing operating systems: MongoDB, Inc. updates all packages on these images before releasing\nthem every three weeks. If you get your   from... ...the Container uses quay.io \nor  Ubuntu 16.04 OpenShift Red Hat Enterprise Linux 7 The   uses a webhook to prevent users from applying\ninvalid resource definitions. The webhook rejects invalid requests.\nThe   doesn't create or update the resource. The  ClusterRole  and  ClusterRoleBinding  for the webhook are\nincluded in the default configuration files that you apply during\ninstallation. To create the role and binding, you must have\n cluster-admin privileges . If you apply an invalid resource definition, the webhook returns\na message that describes the error to the shell: Error from server (shardPodSpec field is not configurable for\napplication databases as it is for sharded clusters and appdbs are\nreplica sets): error when creating \"my-ops-manager.yaml\":\nadmission webhook \"ompolicy.mongodb.com\" denied the request:\nshardPodSpec field is not configurable for application databases as\nit is for sharded clusters and appdbs are replica sets The   doesn't require the validation webhook to create or\nupdate resources. If you omit the validation webhook, remove its role\nand binding from the default configuration, or have insufficient\nprivileges to run it, the   performs the same validations\nwhen it reconciles each resource. The   marks resources as\n Failed  if validation encounters a critical error. For non-critical\nerrors, the   issues warnings.  has a known issue with the webhook when deploying to private\nclusters. To learn more, see  Update Google Firewall Rules to Fix WebHook Issues You can deploy the   with different scopes based on where\nyou want to deploy   and   resources: Operator in Same Namespace as Resources   (Default) Operator in Different Namespace Than Resources Cluster-Wide Scope You scope the   to a namespace. The   watches\n  and   in that same  . This is the default scope when you install the   using the\n installation instructions . You scope the   to a namespace. The   watches\n  and   in the   you specify. You must use  helm  to install the   with this scope.\nFollow the relevant  helm   installation instructions ,\nbut use the following command to set the namespace for the\n  to watch: Setting the namespace ensures that: The namespace you want the   to watch has the correct\nroles and role bindings. The   can watch and create resources in the namespace. You scope the   to a cluster. The   watches\n  and   in all   in the   cluster. You must use  helm  to install the   with this scope.\nFollow the relevant  helm \n installation instructions , but make the\nfollowing adjustments: If you install a cluster-wide   without  helm : You can deploy only one Operator with a cluster-wide scope per  \ncluster. To set the   to watch all namespaces, invoke the\nfollowing command: Create the required service accounts for each namespace where you\nwant to deploy   and  : Ensure that  spec.template.spec.containers.name.env.name:\nWATCH_NAMESPACE  is set to  *  in . If you deploy the   to OpenShift, ensure that you\ncreate all required local   service accounts and secrets. Use  \nor the OpenShift Container Platform UI to apply the following  \nfile before you deploy the  : In the sample   file, replace  <namespace>  with the\nnamespace that you want to deploy the   to. Earlier versions of the   would crash on startup if any\none of the MongoDB   was not present in the cluster. For\ninstance, you had to install the CustomResourceDefinition for  \neven if you did not plan to deploy it with the  . You can now specify which custom resources you want the  \nto watch. This allows you to install the CustomResourceDefinition for\nonly the resources that you want the   to manage. You must use  helm  to configure the   to watch only the\ncustom resources you specify. Follow the relevant  helm \n installation instructions ,\nbut make the following adjustments: Decide which CustomResourceDefinitions you want to install. You can\ninstall any number of the following: Value Description mongodb Install the CustomResourceDefinitions for the database resources and also\nwatch those resources. mongodbusers Install the CustomResourceDefinitions for the MongoDB user resources and\nalso watch those resources. opsmanagers Install the CustomResourceDefinitions for the   resources and also\nwatch those resources. Install each CustomResourceDefinition that you want the\n  to manage from the \ndirectory: Install the Helm Chart and specify which\nCustomResourceDefinitions you want the\n  to watch. Separate each custom resource with a comma:",
            "code": [
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --set operator.watchNamespace=<namespace> \\"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --set operator.watchNamespace=*"
                },
                {
                    "lang": "sh",
                    "value": "helm template --set namespace=<namespace> \\\nhelm_chart --show-only templates/database-roles.yaml | kubectl apply -f -"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-database-pods\n  namespace: <namespace>\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-ops-manager\n  namespace: <namespace>\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - secrets\n    verbs:\n      - get\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: mongodb-enterprise-appdb\n  namespace: <namespace>\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: mongodb-enterprise-appdb\nsubjects:\n  - kind: ServiceAccount\n    name: mongodb-enterprise-appdb\n    namespace: <namespace>\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f helm_chart/crds/{value}.mongodb.com.yaml"
                },
                {
                    "lang": "sh",
                    "value": "helm install <chart-name> helm_chart \\\n     --operator.watchedResources=\"{mongodb,mongodbusers}\" \\\n     --skip-crds"
                }
            ],
            "preview": "When you deploy the  to OpenShift, you must set the\nMANAGED_SECURITY_CONTEXT flag to true. This value is set for you\nin the \nand \nfiles included in the .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-om-container-remote-mode",
            "title": "Configure an  Resource to use Remote Mode",
            "headings": [
                "Prerequisites and Considerations",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Create a  for Nginx.",
                "Deploy Nginx to your  cluster.",
                "Create a  service to make Nginx accessible from other pods in your cluster.",
                "Copy and update the highlighted fields of this  resource.",
                "Paste the copied example section into your existing  resource.",
                "Save your  config file.",
                "Apply changes to your  deployment.",
                "Track the status of your  instance.",
                "Deploy a MongoDB Database Resource."
            ],
            "paragraphs": "In a default configuration, the \u200bs and Backup Daemons\naccess MongoDB installation archives over the Internet from  You can configure   to run in  Remote Mode  with the\n  if the nodes in your   cluster don't have access to\nthe Internet. The Backup Daemons and managed MongoDB resources download\ninstallation archives only from  , which proxies download\nrequests to an HTTP endpoint on a local web server or S3-compatible\nstore deployed to your   cluster. This procedure covers deploying an Nginx HTTP server to your  \ncluster to host the MongoDB installation archives. Deploy an   Resource . The following procedure shows you how to\nupdate your       to enable Remote Mode. You can enable Remote Mode only on   4.4 or later. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : The ConfigMap in this tutorial configures Nginx to: Run an HTTP server named  localhost  listening on port  80  on a\nnode in your   cluster, and Route HTTP requests for specific resources to locations that serve\nthe the MongoDB Server and MongoDB Database Tools installation\narchives. Paste the following example Nginx ConfigMap into a text editor: Save this file with a  .yaml  file extension. Create the Nginx ConfigMap by invoking the following\n kubectl  command on the ConfigMap file you created: The Nginx resource configuration in this tutorial: Deploys one Nginx replica, Creates volume mounts to store MongoDB Server and MongoDB Database\nTools installation archives, and Defines  init containers  that use  curl \ncommands to download the installation archives that Nginx serves to\nMongoDB Database resources you deploy in your   cluster. Paste the following example Nginx resource configuration\ninto a text editor: Modify the lines highlighted in the example to specify the\nMongoDB Server versions that you want to install. For example, to replace MongoDB version  4.0.2  with\na different database version, update the following block: Update this block to modify the MongoDB Database Tools\nversion: to the appropriate initContainer for each version you want\nNginx to serve. For example, to configure Nginx to serve MongoDB  4.2.0 \nand  4.4.0 : Save this file with a  .yaml  file extension. Deploy Nginx by invoking the following  kubectl \ncommand on the Nginx resource file you created: Paste the following example Nginx resource configuration\ninto a text editor: Modify the lines highlighted in the example to specify the\nMongoDB Server versions that you want to install. For example, to replace MongoDB version  4.0.2  with\na different database version, update the following block: Update this block to modify the MongoDB Database Tools\nversion: To load multiple versions, append  curl  commands\nto the appropriate initContainer for each version you want\nNginx to serve. For example, to configure Nginx to serve MongoDB  4.2.0 \nand  4.4.0 : Save this file with a  .yaml  file extension. Deploy Nginx by invoking the following  oc \ncommand on the Nginx resource file you created: The service in this tutorial exposes Nginx to traffic from other nodes\nin your   cluster over port  80 . This allows the MongoDB\nDatabase resource pods you deploy using the   to download\nthe installation archives from Nginx. Run the following command to create a service your Nginx deployment: Paste the following example service into a text editor: Save this file with a  .yaml  file extension. Create the service by invoking the following\n kubectl  command on the service file you created: The highlighted section uses the following   configuration\nsettings: automation.versions.source: remote  in\n spec.configuration  to enable Remote Mode. automation.versions.download.baseUrl  in\n spec.configuration  to provide the base URL of the\nHTTP resources that serve the MongoDB installation archives. Update this line to replace  <namespace>  with the namespace to\nwhich you deploy resources with the  . automation.versions.download.baseUrl.allowOnlyAvailableBuilds:\n\"false\"  in  spec.configuration  to help ensure\nenterprise builds have no issues. You must set this parameter only if you use a version of\n  before 4.4.11. Open your preferred text editor and paste the  \nspecification into the appropriate location in your resource file. Invoke the following  kubectl  command on the filename of the\n  resource  definition: To check the status of your   resource, invoke the following\ncommand: See  Troubleshooting the   for information about the\nresource deployment statuses. After the   resource completes the  Reconciling  phase, the\ncommand returns output similar to the following: Copy the value of the  status.opsManager.url  field, which states\nthe resource's connection  . You use this value when you create a\n  later in the procedure. \u200bs running in MongoDB database resource containers that\nyou create with the   download the installation archives\nfrom   via Nginx instead of from the Internet. If you have not done so already, complete the following\nprerequisites: Create Credentials for the  Create One Project using a ConfigMap Deploy a  MongoDB Database resource \nin the same namespace to which you deployed  .\nEnsure that you: Match the  spec.opsManager.configMapRef.name  of the resource\nto the  metadata.name  of your ConfigMap. Match the  spec.credentials  of the resource to the name of\nthe secret you created that contains an   programmatic\nAPI key pair.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-conf\ndata:\n  nginx.conf: |\n    events {}\n    http {\n      server {\n        server_name localhost;\n        listen 80;\n        location /linux/ {\n          alias /mongodb-ops-manager/mongodb-releases/linux/;\n        }\n        location /tools/ {\n          alias /tools/;\n        }\n      }\n    }\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <nginix-configmap>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - image: nginx:1.14.2\n          imagePullPolicy: IfNotPresent\n          name: nginx\n          ports:\n            - containerPort: 80\n          volumeMounts:\n            - mountPath: /mongodb-ops-manager/mongodb-releases/linux\n              name: mongodb-versions\n            - mountPath: /tools/db/\n              name: mongodb-tools\n            - name: nginx-conf\n              mountPath: /etc/nginx/nginx.conf\n              subPath: nginx.conf\n      initContainers:\n        - name: setting-up-ubuntu-mongodb\n          image: curlimages/curl:latest\n          command:\n            - curl\n            - -L\n            - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-ubuntu1604-4.2.0.tgz\n            - -o\n            - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-ubuntu1604-4.2.0.tgz\n          volumeMounts:\n            - name: mongodb-versions\n              mountPath: /mongodb-ops-manager/mongodb-releases/linux\n        - name: setting-up-ubuntu-mongodb-tools\n          image: curlimages/curl:latest\n          command:\n            - curl\n            - -L\n            - https://fastdl.mongodb.org/tools/db/mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz\n            - -o\n            - /tools/db/mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz\n          volumeMounts:\n            - name: mongodb-tools\n              mountPath: /tools/db/\n      restartPolicy: Always\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n        - name: mongodb-versions\n          emptyDir: {}\n        - name: mongodb-tools\n          emptyDir: {}\n        - configMap:\n            name: nginx-conf\n          name: nginx-conf\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "- name: setting-up-ubuntu-mongodb\n  image: curlimages/curl:latest\n  command:\n    - curl\n    - -L\n    - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-ubuntu1604-4.2.0.tgz\n    - -o\n    - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-ubuntu1604-4.2.0.tgz\n    - &&\n    - curl\n    - -L\n    - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-ubuntu1604-4.4.0.tgz\n    - -o\n    - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-ubuntu1604-4.4.0.tgz"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <nginix>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - image: nginx:1.14.2\n          imagePullPolicy: IfNotPresent\n          name: nginx\n          ports:\n            - containerPort: 80\n          volumeMounts:\n            - mountPath: /mongodb-ops-manager/mongodb-releases/linux\n              name: mongodb-versions\n            - mountPath: /tools/db/\n              name: mongodb-tools\n            - name: nginx-conf\n              mountPath: /etc/nginx/nginx.conf\n              subPath: nginx.conf\n      initContainers:\n        - name: setting-up-rhel-mongodb\n          image: curlimages/curl:latest\n          command:\n            - curl\n            - -L\n            - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-4.2.0.tgz\n            - -o\n            - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel70-4.2.0.tgz\n          volumeMounts:\n            - name: mongodb-versions\n              mountPath: /mongodb-ops-manager/mongodb-releases/linux\n        - name: setting-up-rhel-mongodb-tools\n          image: curlimages/curl:latest\n          command:\n            - curl\n            - -L\n            - https://fastdl.mongodb.org/tools/db/mongodb-database-tools-rhel70-x86_64-100.1.0.tgz\n            - -o\n            - /tools/db/mongodb-database-tools-rhel70-x86_64-100.1.0.tgz\n          volumeMounts:\n            - name: mongodb-tools\n              mountPath: /tools/db/\n      restartPolicy: Always\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n        - name: mongodb-versions\n          emptyDir: {}\n        - name: mongodb-tools\n          emptyDir: {}\n        - configMap:\n            name: nginx-conf\n          name: nginx-conf\n...\n"
                },
                {
                    "lang": "yaml",
                    "value": "initContainers:\n  - name: setting-up-rhel-mongodb\n    image: curlimages/curl:latest\n    command:\n      - curl\n      - -L\n      - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-4.2.0.tgz\n      - -o\n      - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel70-4.2.0.tgz\n      - &&\n      - curl\n      - -L\n      - https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-4.4.0.tgz\n      - -o\n      - /mongodb-ops-manager/mongodb-releases/linux/mongodb-linux-x86_64-rhel70-4.4.0.tgz"
                },
                {
                    "lang": "sh",
                    "value": "oc apply -f <nginix>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: v1\nkind: Service\nmetadata:\n name: nginx-svc\n labels:\n   app: nginx\nspec:\n ports:\n - port: 80\n   protocol: TCP\n selector:\n   app: nginx\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <nginix-service>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: ops-manager-localmode\nspec:\n replicas: 1\n version: 4.4.0-rc1\n adminCredentials: ops-manager-admin-secret\n configuration:\n   # this enables local mode in Ops Manager\n   automation.versions.source: remote\n   automation.versions.download.baseUrl: \"http://nginx-svc.<namespace>.svc.cluster.local:80\"\n   # set the following only if you use a version before Ops Manager 4.4.11\n   automation.versions.download.baseUrl.allowOnlyAvailableBuilds: \"false\"\n\n backup:\n   enabled: false\n\n applicationDatabase:\n   members: 3\n   persistent: true\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2020-05-15T16:20:22Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: 4.2.2-ent\n  backup:\n    phase: \"\"\n  opsManager:\n    lastTransition: \"2020-05-15T16:20:26Z\"\n    phase: Running\n    replicas: 1\n    url: http://ops-manager-localmode-svc.mongodb.svc.cluster.local:8080\n    version: 4.2.12"
                }
            ],
            "preview": "In a default configuration, the \u200bs and Backup Daemons\naccess MongoDB installation archives over the Internet from ",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/scale-resources",
            "title": "Scale a Deployment",
            "headings": [
                "Considerations",
                "Examples"
            ],
            "paragraphs": "You can scale your  replica set  and  sharded cluster \ndeployments up or down to match your desired configuration. Scaling up\nincreases the number of members and/or shards in the deployment,\nthereby improving your deployment's redundancy and availability. The\nscale of your deployment is configured in its corresponding\n . To scale your replica set deployment, set the  spec.members \nsetting to the desired number of replica set members. To learn more\nabout replication, see  Replication  in the\nMongoDB manual. To scale your sharded cluster deployment, set the following settings\nas desired: To learn more about sharded cluster configurations, see\n Sharded Cluster Components  in the MongoDB manual. Setting Description spec.shardCount Number of  shards  in the sharded cluster. spec.mongodsPerShardCount Number of members per shard. spec.mongosCount Number of Shard Routers. spec.configServerCount Number of members in the Config Server. The   does not support modifying deployment types.\nFor example, you cannot convert a standalone deployment to a\nreplica set. To modify the type of a deployment,\nwe recommend the following procedure: Create the new deployment with the desired configuration. Back up the data  from\nyour current deployment. Restore the data  from your current\ndeployment to the new deployment. Test your application connections to the new deployment as needed. Once you have verified that the new deployment contains the\nrequired data and can be reached by your application(s), bring\ndown the old deployment. Select the desired tab based on the deployment configuration you\nwant to scale: Consider a replica set resource with the following\n : To scale up this replica set and add more members: Adjust the  spec.members  setting to the desired\nnumber of members: Reapply the configuration to  : Consider a sharded cluster resource with the following\n : To scale up this sharded cluster: Adjust the following settings to the desired values: spec.shardCount spec.mongodsPerShardCount spec.mongosCount spec.configServerCount Reapply the configuration to  :",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-secure-replica-set>\nspec:\n  members: 4\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <repl-set-config>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-secure-sharded-cluster>\nspec:\n  shardCount: 3\n  mongodsPerShardCount: 3\n  mongosCount: 3\n  configServerCount: 4\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-config>.yaml"
                }
            ],
            "preview": "You can scale your replica set and sharded cluster\ndeployments up or down to match your desired configuration. Scaling up\nincreases the number of members and/or shards in the deployment,\nthereby improving your deployment's redundancy and availability. The\nscale of your deployment is configured in its corresponding\n.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/connect-from-outside-k8s",
            "title": "Connect to a MongoDB Database Resource from Outside Kubernetes",
            "headings": [
                "Prerequisite",
                "Compatible MongoDB Versions",
                "Procedure",
                "Open your standalone resource  file.",
                "Copy the highlighted section of this standalone resource.",
                "Paste the copied example section into your existing standalone resource.",
                "Change the highlighted settings to your preferred values.",
                "Save your standalone config file.",
                "Update and restart your standalone deployment.",
                "Discover the dynamically assigned NodePorts.",
                "Test the connection to the standalone.",
                "Optional: Deploy a replica set with the .",
                "Optional: Add Subject Alternate Names to your  certificates.",
                "Create a NodePort for each .",
                "Discover the dynamically assigned NodePorts.",
                "Open your replica set resource  file.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Change the highlighted settings to your preferred values.",
                "Confirm the external hostnames and NodePort values in your replica set resource.",
                "Save your replica set config file.",
                "Update and restart your replica set deployment.",
                "Test the connection to the replica set.",
                "Optional: Deploy a replica set with the .",
                "Configure services to ensure connectivity.",
                "Configure routes to ensure  terminination passthrough.",
                "Optional: Add Subject Alternate Names to your  certificates.",
                "Open your replica set resource  file.",
                "Configure your replica set resource  file.",
                "Change the settings to your preferred values.",
                "Save your replica set config file.",
                "Optional: Create the necessary  certificates and  secrets.",
                "Optional: Approve  requests.",
                "Update and restart your replica set deployment.",
                "Test the connection to the replica set.",
                "Open your sharded cluster resource  file.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Change the highlighted settings to your preferred values.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Discover the dynamically assigned NodePorts.",
                "Test the connection to the sharded cluster."
            ],
            "paragraphs": "The following procedure describes how to connect to a MongoDB resource\ndeployed by   from outside of the   cluster, including how to\ncontrol port mapping. For your databases to be accessed outside of  , they must run one\nof the following versions of MongoDB: 3.6.17 or later 4.0.15 or later 4.2.3 or later How you connect to a MongoDB resource that the   deployed\nfrom outside of the   cluster depends on the resource. This procedure uses the following example: To connect to your  -deployed MongoDB\nstandalone resource from outside of the   cluster: Change the highlighted settings of this   file to match your\ndesired  standalone  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. Key Type Necessity Description Example spec.exposedExternally Boolean Optional Set this value to  true  to allow external services to connect\nto the MongoDB deployment. This results in   creating a\n NodePort service . true Invoke the following   command to update and restart your\n standalone : Discover the dynamically assigned NodePort: The list output should contain an entry similar to the following:  exposes   on port  27017  within the  \ncontainer. The NodePort service exposes the  mongod  via port  30994 .\nNodePorts range from 30000 to 32767, inclusive. To connect to your deployment from outside of the   cluster, run\nthe   command with the external   of a   as the\n --host  flag. If a node in the   cluster has an external   of\n ec2-54-212-23-143.us-west-2.compute.amazonaws.com , you can\nconnect to this standalone instance from outside of the  \ncluster using the following command: To obtain the external  DNS (Domain Name System)  of your\nKubernetes cluster, you can run the following command: This command displays the external DNS in the\n Addresses.ExternalDNS  section of the output. Alternatively, you can output the external DNS directly by running: To connect to your  -deployed MongoDB replica\nset resource from outside of the   cluster: This procedure explains the least complicated way to\nenable external connectivity. Other utilities can be\nused in production. If you haven't deployed a replica set, follow the instructions to\n deploy one . To simplify the configuration, don't enable   with the\n spec.security.tls.enabled  setting. If the  -deployed replica set has   enabled and\nuses a custom certificate stored with\n spec.security.tls.ca ,\nadd each external   name to the certificate  . Invoke the following commands to create the NodePorts: Discover the dynamically assigned NodePorts: NodePorts range from 30000 to 32767, inclusive. Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. Key Type Necessity Description Example boolean Optional Set this value to  true  to enable   on the MongoDB\ndeployment. By default,   requires hosts to use and accept\n  encrypted connections. To connect to a replica set from outside  , set this\nvalue to  true . true collection Conditional Add this parameter and values if you need your database to be\naccessed outside of  . This setting allows you to provide\ndifferent   settings within the   cluster and to the\n  cluster. The   uses split horizon   for\nreplica set members. This feature allows communication both\nwithin the   cluster and from outside  . You may add multiple external mappings per host. Make sure that each value in this array is unique. Make sure that the number of entries in this array matches\nthe value given in  spec.members . Set the  spec.security.tls.enabled  to  true  to\nenable  . This method to use split horizons requires\nthe Server Name Indication extension of the   protocol. See Setting Confirm that the external hostnames in the\n spec.connectivity.replicaSetHorizons  setting are correct. External hostnames should match the   names of   worker nodes.\nThese can be  any  nodes in the   cluster.   do internal\nrouting if the pod is run on another node. Set the ports in  spec.connectivity.replicaSetHorizons  to\nthe NodePort values that you discovered. Invoke the following   command to update and restart your\n replica set : If the connection succeeds, you should see: Don't use the  --sslAllowInvalidCertificates  flag in production.\nIn production, share the     files with client tools\nor applications. To connect to your  -deployed MongoDB replica\nset resource from outside of the   cluster with OpenShift: If you haven't deployed a replica set, follow the instructions to\n deploy one . To simplify the configuration, don't enable   with the\n spec.security.tls.enabled  setting. Paste the following example services into a text editor: If the  spec.selector  has entries that target headless\nservices or applications, OpenShift may create a software\nfirewall rule explicitly dropping connectivity.  Review the\nselectors carefully and consider targeting the stateful set pod\nmembers directly as seen in the example.  Routes in OpenShift\noffer port 80 or port 443. This example service uses\nport 443. Change the settings to your preferred values. Save this file with a  .yaml  file extension. To create the services, invoke the following  kubectl  command\non the services file you created: Paste the following example routes into a text editor: To ensure the     negotiation with   necessary\nfor   to respond with the correct horizon replica set\ntopology for the drivers to use, you must set  \ntermination passthrough. Change the settings to your preferred values. Save this file with a  .yaml  file extension. To create the routes, invoke the following  kubectl  command on\nthe routes file you created: If the  -deployed replica set has   enabled and\nuses a custom certificate stored with\n spec.security.tls.ca ,\nadd each external   name to the certificate  . Use the following example to edit your replica set resource  \nfile: OpenShift clusters require localhost horizons if you intend to use\nthe   to create each  . If you manually create\nyour   certificates, ensure you include localhost in\nthe   list. Key Type Necessity Description Example boolean Optional Set this value to  true  to enable   on the MongoDB\ndeployment. By default,   requires hosts to use and accept\n  encrypted connections. To connect to a replica set from outside  , set this\nvalue to  true . true collection Conditional Add this parameter and values if you need your database to be\naccessed outside of  . This setting allows you to provide\ndifferent   settings within the   cluster and to the\n  cluster. The   uses split horizon   for\nreplica set members. This feature allows communication both\nwithin the   cluster and from outside  . You may add multiple external mappings per host. Make sure that each value in this array is unique. Make sure that the number of entries in this array matches\nthe value given in  spec.members . Set the  spec.security.tls.enabled  to  true  to\nenable  . This method to use split horizons requires\nthe Server Name Indication extension of the   protocol. See Setting Configure TLS for your replica set . Create one secret for the MongoDB replica set\nand one for the certificate authority. The   uses these\nsecrets to place the   files in the pods for MongoDB to use. If you do not manually create the   certificates for the\ndeployment, check for pending   approval requests: When the requests come through, approve them: Invoke the following   command to update and restart your\n replica set : The   should deploy the MongoDB replica set,\nconfigured with the horizon routes created for ingress. After\nthe   completes the deployment, you may connect with the\nhorizon using   connectivity.  If the certificate authority is\nnot present on your workstation, you can view and copy it from a\nMongoDB pod using the following command: To test the connections, run the following command: If the connection succeeds, you should see: In the following example, use your replica set names and replace  {redacted}  with the domain that you manage. Don't use the  --tlsAllowInvalidCertificates  flag in production.\nIn production, share the     files with client tools\nor applications. This procedure uses the following example: To connect to your  -deployed MongoDB sharded\ncluster resource from outside of the   cluster: If the   deployed a  -enabled\nsharded cluster, provide the external   names\n( s) for each member. The   for each MongoDB hosts corresponds to: Each   certificate requires the  \n( ) that corresponds to the   that\nthis host has outside the   cluster. Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. Key Type Necessity Description Example spec.exposedExternally Boolean Optional Set this value to  true  to allow external services to connect\nto the MongoDB deployment. This results in   creating a\n NodePort service . true Key Type Necessity Description Example spec.exposedExternally Boolean Optional Set this value to  true  to allow external services to connect\nto the MongoDB deployment. This results in   creating a\n NodePort service . true boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true collection Optional List of every domain that should be added to   certificates\nto each pod in this deployment. When you set this parameter,\nevery   that the   transforms into a  \ncertificate includes a   in the form  <pod\nname>.<additional cert domain> . true Invoke the following   command to update and restart your\n sharded cluster : Discover the dynamically assigned NodePort: The list output should contain an entry similar to the following:  exposes   on port  27017  within the  \ncontainer. The NodePort service exposes the  mongod  via port  30078 .\nNodePorts range from 30000 to 32767, inclusive. To connect to your deployment from outside of the   cluster, run\nthe   command with the external   of a   as the\n --host  flag. If a node in the   cluster has an external   of\n ec2-54-212-23-143.us-west-2.compute.amazonaws.com , you can\nconnect to this sharded cluster instance from outside of the  \ncluster using the following command: To obtain the external  DNS (Domain Name System)  of your\nKubernetes cluster, you can run the following command: This command displays the external DNS in the\n Addresses.ExternalDNS  section of the output. Alternatively, you can output the external DNS directly by running:",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\nspec:\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true\n  exposedExternally: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\nspec:\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  exposedExternally: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <standalone-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get services -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\n\n<my-standalone>           NodePort    10.102.27.116   <none>        27017:30994/TCP   8m30s"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host ec2-54-212-23-143.us-west-2.compute.amazonaws.com --port 30994"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe nodes"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get nodes -o jsonpath='{ $.items[*].status.addresses[?(@.type==\"ExternalDNS\")].address }'"
                },
                {
                    "lang": "sh",
                    "value": "kubectl expose pod/<my-replica-set>-0 --type=\"NodePort\" --port 27017\nkubectl expose pod/<my-replica-set>-1 --type=\"NodePort\" --port 27017\nkubectl expose pod/<my-replica-set>-2 --type=\"NodePort\" --port 27017"
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl get svc | grep <my-replica-set>\n<my-replica-set>-0     NodePort   172.30.39.228   <none>  27017:30907/TCP  16m\n<my-replica-set>-1     NodePort   172.30.185.136  <none>  27017:32350/TCP  16m\n<my-replica-set>-2     NodePort   172.30.84.192   <none>  27017:31185/TCP  17m\n<my-replica-set>-svc   ClusterIP  None            <none>  27017/TCP        38m"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  type: ReplicaSet\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:30907\"\n      - \"example-website\": \"web2.example.com:32350\"\n      - \"example-website\": \"web3.example.com:31185\"\n..."
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:30907\"\n      - \"example-website\": \"web2.example.com:32350\"\n      - \"example-website\": \"web3.example.com:31185\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host <my-replica-set>/web1.example.com:30907,web2.example.com:32350,web3.example.com:31185 \\\n      --ssl \\\n      --sslAllowInvalidCertificates"
                },
                {
                    "lang": "javascript",
                    "value": "MongoDB Enterprise <my-replica-set>:PRIMARY"
                },
                {
                    "lang": "",
                    "value": "---\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-external-0\nspec:\n  ports:\n    - name: mongodb\n      protocol: TCP\n      port: 443\n      targetPort: 27017\n  selector:\n    statefulset.kubernetes.io/pod-name: my-external-0\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-external-1\nspec:\n  ports:\n    - name: mongodb\n      protocol: TCP\n      port: 443\n      targetPort: 27017\n  selector:\n    statefulset.kubernetes.io/pod-name: my-external-1\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-external-2\nspec:\n  ports:\n    - name: mongodb\n      protocol: TCP\n      port: 443\n      targetPort: 27017\n  selector:\n    statefulset.kubernetes.io/pod-name: my-external-2\n\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <my-external-services>.yaml"
                },
                {
                    "lang": "",
                    "value": "---\napiVersion: v1\nkind: Route\nmetadata:\n  name: my-external-0\nspec:\n  host: my-external-0.{redacted}\n  to:\n    kind: Service\n    name: my-external-0\n  tls:\n    termination: passthrough\n---\napiVersion: v1\nkind: Route\nmetadata:\n  name: my-external-1\nspec:\n  host: my-external-1.{redacted}\n  to:\n    kind: Service\n    name: my-external-1\n  tls:\n    termination: passthrough\n---\napiVersion: v1\nkind: Route\nmetadata:\n  name: my-external-2\nspec:\n  host: my-external-2.{redacted}\n  to:\n    kind: Service\n    name: my-external-2\n  tls:\n    termination: passthrough\n\n...\n "
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <my-external-routes>.yaml"
                },
                {
                    "lang": "",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-external\n  namespace: mongodb\nspec:\n  type: ReplicaSet\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: {redacted}\n  credentials: {redacted}\n  persistent: false\n  security:\n    tls:\n      # TLS must be enabled to allow external connectivity\n      enabled: true\n    authentication:\n      enabled: true\n      modes: [\"SCRAM\",\"X509\"]\n  connectivity:\n    # The \"localhost\" routes are there just to make sure the localhost \n    # TLS SAN is created in the CSR, per OpenShift route requirements.\n    # \"ocroute\" is the configured route in openshift\n    replicaSetHorizons:\n      - \"ocroute\": \"my-external-0.{redacted}:443\"\n        \"localhost\": \"localhost:27017\"\n      - \"ocroute\": \"my-external-1.{redacted}:443\"\n        \"localhost\": \"localhost:27018\"\n      - \"ocroute\": \"my-external-2.{redacted}:443\"\n        \"localhost\": \"localhost:27019\"\n\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "oc get csr"
                },
                {
                    "lang": "sh",
                    "value": "oc adm certificate approve {certificate-0}.{namespace} ... {certificate-n}.{namespace}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "oc exec -it my-external-0 -- cat /mongodb-automation/ca.pem"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host my-external/my-external-0.{redacted}:443,my-external-1.{redacted}:443,my-external-2.{redacted}:443 \\\n      --tls \\\n      --tlsAllowInvalidCertificates"
                },
                {
                    "lang": "javascript",
                    "value": "MongoDB Enterprise <my-replica-set>:PRIMARY"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  credentials: my-secret\n  type: ShardedCluster\n  persistent: true\n  exposedExternally: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "<mdb-resource-name><shard><pod-index>.<external-domain>\n<mdb-resource-name><config><pod-index>.<external-domain>\n<mdb-resource-name><mongos><pod-index>.<external-domain>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  credentials: my-secret\n  type: ShardedCluster\n  persistent: true\n  exposedExternally: true\n  security:\n    tls:\n      enabled: true\n      additionalCertificateDomains:\n        - \"additional-cert-test.com\"\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  credentials: my-secret\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  exposedExternally: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  credentials: my-secret\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  exposedExternally: true\n  security:\n    tls:\n      enabled: true\n      additionalCertificateDomains:\n        - \"additional-cert-test.com\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get services -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\n\n<my-sharded cluster>      NodePort    10.106.44.30    <none>        27017:30078/TCP   10s"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host ec2-54-212-23-143.us-west-2.compute.amazonaws.com --port 30078"
                },
                {
                    "lang": "sh",
                    "value": "kubectl describe nodes"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get nodes -o jsonpath='{ $.items[*].status.addresses[?(@.type==\"ExternalDNS\")].address }'"
                }
            ],
            "preview": "The following procedure describes how to connect to a MongoDB resource\ndeployed by  from outside of the  cluster, including how to\ncontrol port mapping.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-replica-set",
            "title": "Deploy a Replica Set",
            "headings": [
                "Prerequisites",
                "Considerations",
                "Deploy a Replica Set",
                "Configure kubectl to default to your namespace.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example to create a new replica set resource.",
                "Change the highlighted settings to your preferred values.",
                "Add any additional accepted settings for a replica set deployment.",
                "Save this replica set config file with a .yaml file extension.",
                "Start your replica set deployment.",
                "Track the status of your replica set deployment.",
                "Enable External Access for a Replica Set",
                "Optional: Deploy a replica set with the .",
                "Optional: Add Subject Alternate Names to your  certificates.",
                "Create a NodePort for each .",
                "Discover the dynamically assigned NodePorts.",
                "Open your replica set resource  file.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Change the highlighted settings to your preferred values.",
                "Confirm the external hostnames and NodePort values in your replica set resource.",
                "Save your replica set config file.",
                "Update and restart your replica set deployment.",
                "Test the connection to the replica set."
            ],
            "paragraphs": "A  replica set  is a group of MongoDB deployments that maintain\nthe same data set. Replica sets provide redundancy and high\navailability and are the basis for all production deployments. To learn more about replica sets, see the\n Replication Introduction  in\nthe MongoDB manual. Use this procedure to deploy a new replica set that   manages.\nAfter deployment, use   to manage the replica set, including such\noperations as adding, removing, and reconfiguring members. At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . You can use the   to deploy MongoDB resources with\n  version 4.0.11 or later and Cloud Manager. You can't use the   to deploy MongoDB resources to\n . To deploy a  replica set  using an  , you need to complete the\nfollowing procedures: Install Kubernetes Operator Create One Project using a ConfigMap Create Credentials for the  Starting in   version 1.3.0, you can only have one\nMongoDB resource per project. To learn how to migrate your project to\na single-cluster configuration, see  Migrate to One Resource per Project (Required for Version 1.3.0) . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\ninto a new text file. Key Type Description Example metadata.name string Label for this    replica set   . Resource names must be 44 characters or less. metadata.name Kubernetes documentation on\n names . myproject spec.members integer Number of members of the  replica set . 3 spec.version string Version of MongoDB that this  replica set  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. 3.6.7 string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value  must  match the namespace in which you created the\n   project ConfigMap . The   tracks any changes to the ConfigMap and\nreconciles the state of the  . <myconfigmap> spec.credentials string Name of the     you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . This value  must  match the namespace in which you created the\nsecret and the  name  value you provided for your  \n   Secret . The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. ReplicaSet spec.persistent string Optional. Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. If this value is  true , then\n spec.podSpec.persistence.single  is set to its\ndefault value of  16Gi . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: If you want one   for each  , configure the\n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs Your containers must have permissions to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the deployment item does not fix\nissues with your  , contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n  specification file for a  replica set  deployment: spec.additionalMongodConfig spec.clusterDomain spec.featureCompatibilityVersion spec.logLevel spec.podSpec.cpu spec.podSpec.cpuRequests spec.podSpec.memory spec.podSpec.memoryRequests spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs spec.podSpec.podAffinity spec.podSpec.podAntiAffinityTopologyKey spec.podSpec.nodeAffinity spec.podSpec.podTemplate.metadata spec.podSpec.podTemplate.spec You must set  spec.clusterDomain  if your   cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n  might not function as expected. Invoke the following   command to create your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. If you haven't deployed a replica set, follow the instructions to\n deploy one . To simplify the configuration, don't enable   with the\n spec.security.tls.enabled  setting. If the  -deployed replica set has   enabled and\nuses a custom certificate stored with\n spec.security.tls.ca ,\nadd each external   name to the certificate  . Invoke the following commands to create the NodePorts: Discover the dynamically assigned NodePorts: NodePorts range from 30000 to 32767, inclusive. Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. Key Type Necessity Description Example boolean Optional Set this value to  true  to enable   on the MongoDB\ndeployment. By default,   requires hosts to use and accept\n  encrypted connections. To connect to a replica set from outside  , set this\nvalue to  true . true collection Conditional Add this parameter and values if you need your database to be\naccessed outside of  . This setting allows you to provide\ndifferent   settings within the   cluster and to the\n  cluster. The   uses split horizon   for\nreplica set members. This feature allows communication both\nwithin the   cluster and from outside  . You may add multiple external mappings per host. Make sure that each value in this array is unique. Make sure that the number of entries in this array matches\nthe value given in  spec.members . Set the  spec.security.tls.enabled  to  true  to\nenable  . This method to use split horizons requires\nthe Server Name Indication extension of the   protocol. See Setting Confirm that the external hostnames in the\n spec.connectivity.replicaSetHorizons  setting are correct. External hostnames should match the   names of   worker nodes.\nThese can be  any  nodes in the   cluster.   do internal\nrouting if the pod is run on another node. Set the ports in  spec.connectivity.replicaSetHorizons  to\nthe NodePort values that you discovered. Invoke the following   command to update and restart your\n replica set : If the connection succeeds, you should see: Don't use the  --sslAllowInvalidCertificates  flag in production.\nIn production, share the     files with client tools\nor applications.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl expose pod/<my-replica-set>-0 --type=\"NodePort\" --port 27017\nkubectl expose pod/<my-replica-set>-1 --type=\"NodePort\" --port 27017\nkubectl expose pod/<my-replica-set>-2 --type=\"NodePort\" --port 27017"
                },
                {
                    "lang": "sh",
                    "value": "$ kubectl get svc | grep <my-replica-set>\n<my-replica-set>-0     NodePort   172.30.39.228   <none>  27017:30907/TCP  16m\n<my-replica-set>-1     NodePort   172.30.185.136  <none>  27017:32350/TCP  16m\n<my-replica-set>-2     NodePort   172.30.84.192   <none>  27017:31185/TCP  17m\n<my-replica-set>-svc   ClusterIP  None            <none>  27017/TCP        38m"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  type: ReplicaSet\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n  credentials: <mycredentials>\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:30907\"\n      - \"example-website\": \"web2.example.com:32350\"\n      - \"example-website\": \"web3.example.com:31185\"\n..."
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n  connectivity:\n    replicaSetHorizons:\n      - \"example-website\": \"web1.example.com:30907\"\n      - \"example-website\": \"web2.example.com:32350\"\n      - \"example-website\": \"web3.example.com:31185\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host <my-replica-set>/web1.example.com:30907,web2.example.com:32350,web3.example.com:31185 \\\n      --ssl \\\n      --sslAllowInvalidCertificates"
                },
                {
                    "lang": "javascript",
                    "value": "MongoDB Enterprise <my-replica-set>:PRIMARY"
                }
            ],
            "preview": "A replica set is a group of MongoDB deployments that maintain\nthe same data set. Replica sets provide redundancy and high\navailability and are the basis for all production deployments.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/modify-resource-image",
            "title": "Modify  or MongoDB Kubernetes Resource Containers",
            "headings": [
                "Define a Volume Mount for a MongoDB Kubernetes Resource",
                "Tune MongoDB Kubernetes Resource Docker Images with an InitContainer"
            ],
            "paragraphs": "You can modify the containers in the   in which   and\nMongoDB database resources run using the  template  or\n podTemplate  setting that applies to your deployment: To review which fields you can add to a  template  or a\n podTemplate , see the  Kubernetes documentation . When you create containers with a  template  or  podTemplate , the\n  handles container creation differently based on the\n name  you provide for each container in the  containers  array: MongoDB database:  spec.podSpec.podTemplate :  spec.statefulSet.spec.template backup-daemon :  spec.backup.statefulSet.spec.template If the  name  field  matches  the name of the applicable resource\nimage, the   updates the   or MongoDB database\ncontainer in the   to which the  template  or\n podTemplate  applies: :  mongodb-enterprise-ops-manager backup-daemon :  mongodb-backup-daemon MongoDB database:  mongodb-enterprise-database Application Database:  mongodb-enterprise-appdb If the  name  field  does not match  the name of the applicable\nresource image, the   creates a new container in each\n  to which the  template  or  podTemplate  applies. On-disk files in containers in   don't survive container\ncrashes or restarts. Using the  spec.podSpec.podTemplate \nsetting, you can add a  volume mount \nto persist data in a MongoDB database resource for the life of the\n . To create a volume mount for a MongoDB database resource: Update the MongoDB database resource definition to include a volume\nmount for containers in the database pods that the  \ncreates. Use  spec.podSpec.podTemplate  to define a volume mount: Apply the updated resource definition:  Docker images run on Ubuntu and use Ubuntu's default\nsystem configuration. To tune the underlying Ubuntu system\nconfiguration in the   containers, add a privileged\nInitContainer\n init container \nusing one of the following settings: To tune Docker images for a MongoDB database resource container:  adds a privileged InitContainer to each   that the\n  creates using the   definition. Open a shell session to a running container in your database resource\n  and verify your changes. spec.podSpec.podTemplate : add a privileged InitContainer\nto a MongoDB database resource container. spec.statefulSet.spec.template : add a privileged\nInitContainer to an   resource container. MongoDB database resource Docker images use the Ubuntu default\n keepalive  time of  7200 . MongoDB recommends a shorter\n keepalive  time of  120  for database deployments. You can tune the  keepalive  time in the database resource Docker\nimages if you experience network timeouts or socket errors in\ncommunication between clients and the database resources. Update the MongoDB database resource definition to append a\nprivileged InitContainer to the database pods that the\n  creates. Change  spec.podSpec.podTemplate  the  keepalive \nvalue to the recommended value of  120 : Apply the updated resource definition: To follow the previous  keepalive  example, invoke the following\ncommand to get the current  keepalive  value:",
            "code": [
                {
                    "lang": "yaml",
                    "value": "podSpec:\n  podTemplate:\n    spec:\n      containers:\n      - name: mongodb-enterprise-database\n        volumeMounts:\n        - mountPath: </new/mount/path>\n          name: survives-restart\n      volumes:\n      - name: survives-restart\n        emptyDir: {}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-resource-conf>.yaml -n <namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  podSpec:\n    podTemplate:\n      spec:\n        initContainers:\n        - name: \"adjust-tcp-keepalive\"\n          image: \"busybox:latest\"\n          securityContext:\n            privileged: true\n          command: [\"sysctl\", \"-w\", \"net.ipv4.tcp_keepalive_time=120\"]"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-resource-conf>.yaml -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "> kubectl exec -n <namespace> -it <pod-name> -- cat /proc/sys/net/ipv4/tcp_keepalive_time\n\n\n> 120"
                }
            ],
            "preview": "You can modify the containers in the  in which  and\nMongoDB database resources run using the template or\npodTemplate setting that applies to your deployment:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/upgrade-mdb-version",
            "title": "Upgrade MongoDB Version and FCV",
            "headings": [
                "Example"
            ],
            "paragraphs": "You can upgrade the major, minor, and/or feature compatibility versions\nof your MongoDB resource. These settings are configured in your\nresource's  . To upgrade your resource's major and/or minor versions, set the\n spec.version  setting to the desired MongoDB version. To modify your resource's\n feature compatibility version ,\nset the  spec.featureCompatibilityVersion  setting to the\ndesired version. If you update  spec.version  to a later version, consider setting\n spec.featureCompatibilityVersion  to the current working\nMongoDB version to give yourself the option to downgrade if\nnecessary. To learn more about feature compatibility, see\n setFeatureCompatibilityVersion  in the MongoDB Manual. Consider the following   for a standalone resource: This resource has a MongoDB version of \u200b. The\nfollowing steps upgrade the deployment's MongoDB version to\n 4.2.2-ent :  automatically reconfigures your deployment with the new\nspecifications. You can see these changes reflected in the   or\n Cloud Manager  application. Perform the following modifications to the resource's ConfigMap: Set  spec.version  to the desired MongoDB version. Set  spec.featureCompatibilityVersion  to the current\nworking MongoDB version: Setting  featureCompatibilityVersion  to  \"4.0\"  disables\n 4.2 features incompatible with MongoDB 4.0 . Reapply the configuration to  :",
            "code": [
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone-downgrade\nspec:\n  version: 4.0.14-ent\n  type: Standalone\n  project: my-project\n  credentials: my-credentials\n  persistent: false\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: my-standalone-downgrade\nspec:\n  version: 4.2.2-ent\n  featureCompatibilityVersion: 4.0\n  type: Standalone\n  project: my-project\n  credentials: my-credentials\n  persistent: false\n..."
                },
                {
                    "lang": "none",
                    "value": "kubectl apply -f <standalone-config>.yaml"
                }
            ],
            "preview": "You can upgrade the major, minor, and/or feature compatibility versions\nof your MongoDB resource. These settings are configured in your\nresource's .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-tls",
            "title": "Secure Deployments using TLS",
            "headings": [
                "General Prerequisites",
                "Configure TLS for a Replica Set",
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Create the  for your TLS certificates.",
                "Create the  to link your  with your deployment.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the TLS settings for your replica set resource using a Custom Certificate Authority.",
                "Save your replica set config file.",
                "Apply your changes to your replica set deployment.",
                "Track the status of your deployment.",
                "Configure TLS for a Sharded Cluster",
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Create the  for your Shards' TLS certificates.",
                "Create the  for your config server's TLS certificates.",
                "Create the  for your mongos server's TLS certificates.",
                "Create the  to link your  with your deployment.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the TLS settings for your sharded cluster resource using a Custom Certificate Authority.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Track the status of your deployment."
            ],
            "paragraphs": "The   can use   certificates to encrypt connections\nbetween: This guide instructs you on how to configure the   to use\n  for its MongoDB instances. MongoDB hosts in a replica set or sharded cluster Client applications and MongoDB deployments Automatically generating   certificates with the  \nis deprecated and will be removed in a future release. You must provide certificates from your own CA, as described in the\nfollowing procedures, for production environments. Before you secure your MongoDB deployment using   encryption,\ncomplete the following: Install the Kubernetes Operator Create Credentials for the Kubernetes Operator Before you secure your replica set using   encryption, complete the\nfollowing: Deploy the Replica Set \nthat you want to secure Create a   file for each of the following components: PEM file purpose Save File As... Your custom  ca-pem Each member of your replica set <metadata.name>-<X>-pem To create the   file, concatenate the   certificate and the\nPrivate Key. An example of a   file would resemble: Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create the   that stores\nthe replica set's certificates: This example covers a three-member replica set. If you have more than\nthree members, you can add them to the certificate using the\n --from-file  option. Run this  kubectl  command to link your   to your replica\nset: Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Optional If you use a custom   and have created the  \nthat stores it, add the secret's name. <custom-ca> Invoke the following   command to updated your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. Before you secure your replica set using   encryption, complete the\nfollowing: Deploy the Sharded Cluster \nthat you want to secure Create a   file for each of the following components: PEM file purpose Save File As... Your custom  ca-pem Each shard in your sharded cluster <metadata.name>-<Y>-<X>-pem Each member of your config server replica set <metadata.name>-config-<X>-pem Each  <metadata.name>-mongos-<X>-pem Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create the   that stores\nthe sharded cluster shards' certificates: This example covers a two-shard sharded cluster with five members per\nshard. If you have more than two shards or five members per shard,\nyou can add them to the certificate using the  --from-file  option. Run this  kubectl  command to create the   that stores\nthe sharded cluster config server's certificates: Run this  kubectl  command to create the   that stores\nthe sharded cluster   certificates: Run this  kubectl  command to link your   to your replica\nset: Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Optional If you use a custom   and have created the  \nthat stores it, add the secret's name. <custom-ca> Invoke the following   command to update and restart your\n sharded cluster : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "text",
                    "value": "-----BEGIN CERTIFICATE-----\n...\n... your TLS certificate\n...\n-----END CERTIFICATE-----\n-----BEGIN RSA PRIVATE KEY-----\n...\n... your private key\n...\n-----END RSA PRIVATE KEY----"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <metadata.name>-cert \\\n  --from-file=<metadata.name>-0-pem \\\n  --from-file=<metadata.name>-1-pem \\\n  --from-file=<metadata.name>-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap custom-ca --from-file=ca-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-0-cert \\\n       --from-file=<metadata.name>-0-0-pem \\\n       --from-file=<metadata.name>-0-1-pem \\\n       --from-file=<metadata.name>-0-2-pem \\\n       --from-file=<metadata.name>-0-3-pem \\\n       --from-file=<metadata.name>-0-4-pem\n\nkubectl -n mongodb create secret generic <metadata.name>-1-cert \\\n       --from-file=<metadata.name>-1-0-pem \\\n       --from-file=<metadata.name>-1-1-pem \\\n       --from-file=<metadata.name>-1-2-pem \\\n       --from-file=<metadata.name>-1-3-pem \\\n       --from-file=<metadata.name>-1-4-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-config-cert \\\n       --from-file=<metadata.name>-config-0-pem \\\n       --from-file=<metadata.name>-config-1-pem \\\n       --from-file=<metadata.name>-config-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-mongos-cert \\\n       --from-file=<metadata.name>-mongos-0-pem \\\n       --from-file=<metadata.name>-mongos-1-pem \\\n       --from-file=<metadata.name>-mongos-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap custom-ca --from-file=ca-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "The  can use  certificates to encrypt connections\nbetween:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-om-with-tls",
            "title": "Secure Application Database using TLS",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Verify your new  certificates.",
                "Create a Secret with your new  certificates.",
                "Optional: Create a ConfigMap containing the Certificate Authority.",
                "Specify the Secret with certs to the  yaml definition.",
                "Apply changes to your  deployment.",
                "Track the status of your  instance."
            ],
            "paragraphs": "The   can use   certificates to encrypt connections\nbetween members of the application database replica set. Before you secure your application database using   encryption,\ncomplete the following: Install the Kubernetes Operator . Deploy the Ops Manager application  that\nyou want to secure. Create a   certificate for each member of the Application\nDatabase's  replica set . These   certificates require two attributes: DNS Names Each certificate must include a   or Subject Name\nwith the name of the   in  . These names must\nresemble this format: Key Usages MongoDB requires the   certs to include two specific\nkey-usages ( 5280 ): \"server auth\" \"client auth\" If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Verify that each member of the Replica Set has one   certificate named with the following format: <resource-name>-db-<index>-pem Where  <index>  is a 0-based index number to the total amount of\nmembers minus one. ( 0  to  n-1 ) Create a new Secret from these files: kubectl  creates one Secret containing the three certificates. You need to provide a   certificate when the   that\nsigned the certificates might be not \"recognized\" as an official\nauthority. Recognized and valid certificates can be created with\n cert-manager  or  HashiCorp Vault . If you signed the certificates using a   certicate management\ntool like  cert-manager  or\n HashiCorp Vault , you must create a\n  containing the  's certificate file. If you output the certificate as a file, name this file  ca-pem .\nThis simplifies creating the  . This creates a   named  appdb-ca . This\n  contains one entry called  ca-pem  with the\ncontents of the   file. The   mounts the   you add using the\n spec.applicationDatabase.security.tls.ca  setting to\nboth the   and the Application Database pods. Invoke the following  kubectl  command on the filename of the\n  resource definition: To check the status of your   resource, invoke the following\ncommand: When   is running, the command returns the following\noutput under the  status  field: See  Troubleshooting the   for information about the\nresource deployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "<opsmgr-name>-db-<index>.<opsmgr-name>-db-svc.<namespace>.svc.cluster.local"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic appdb-certs \\\n        --from-file=om-appdb-tls-enabled-db-0-pem \\\n        --from-file=om-appdb-tls-enabled-db-1-pem \\\n        --from-file=om-appdb-tls-enabled-db-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap appdb-ca --from-file=ca-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: om-appdb-tls-enabled\nspec:\n  replicas: 1\n  version: 4.2.6\n  adminCredentials: ops-manager-admin-secret\n  configuration:\n    mms.fromEmailAddr: admin@example.com\n    mms.security.allowCORS: \"false\"\n  applicationDatabase:\n    members: 3\n    version: 4.2.2-ent\n    persistent: true\n    security:\n      tls:\n        ca: \"appdb-ca\" # Optional. Name of the ConfigMap file\n                       # containing the certicate authority that\n                       # signs the certificates that the application\n                       # database uses.\n        secretRef:\n          name: \"appdb-certs\" # Name of the Secret object\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2019-12-06T17:46:15Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: 4.2.2-ent\n  opsManager:\n    lastTransition: \"2019-12-06T17:46:32Z\"\n    phase: Running\n    replicas: 1\n    url: https://om-appdb-tls-enabled-svc.dev.svc.cluster.local:8443\n    version: 4.2.6"
                }
            ],
            "preview": "The  can use  certificates to encrypt connections\nbetween members of the application database replica set.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-standalone",
            "title": "Deploy a Standalone MongoDB Instance",
            "headings": [
                "Prerequisites",
                "Considerations",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the following example standalone  .",
                "Open your preferred text editor and paste the  specification into a new text file.",
                "Configure the settings highlighted in the preceeding step as follows.",
                "Add any additional accepted settings for a Standalone deployment.",
                "Save this file with a .yaml file extension.",
                "Start your Standalone deployment.",
                "Track the status of your standalone deployment."
            ],
            "paragraphs": "You can deploy a  standalone  MongoDB instance for   to\nmanage. Use standalone instances for testing and development.\n Do not  use these deployments for production systems as they lack\nreplication and high availability. For all production deployments\nuse replica sets. To learn about replica sets, see\n Deploy a Replica Set . At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . You can use the   to deploy MongoDB resources with\n  version 4.0.11 or later and Cloud Manager. You can't use the   to deploy MongoDB resources to\n . To deploy a  standalone  using an  , you need to complete the\nfollowing procedures: Install Kubernetes Operator Create One Project using a ConfigMap Create Credentials for the  Starting in   version 1.3.0, you can only have one\nMongoDB resource per project. To learn how to migrate your project to\na single-cluster configuration, see  Migrate to One Resource per Project (Required for Version 1.3.0) . To troubleshoot your sharded cluster, see: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : This is a   file that you can modify to meet your desired\nconfiguration. Change the highlighted settings to match your desired\nstandalone configuration. Key Type Description Example metadata.name string Label for this   standalone  . Resource names must be 44 characters or less. metadata.name  documentation on  names . my-project spec.version string Version of MongoDB that is installed on this\nstandalone. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. 4.2.2-ent string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value  must  match the namespace in which you created the\n   project ConfigMap . <myproject> spec.credentials string Name of the     you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . This value  must  match the namespace in which you created the\nsecret and the  name  value you provided for your  \n   Secret . The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. Standalone spec.persistent string Optional. If this value is  true , then\n spec.podSpec.persistence.single  is set to its\ndefault value of  16Gi . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: If you want one   for each  , configure the\n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs Your containers must have permissions to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the deployment item does not fix\nissues with your  , contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n  specification file for a Standalone deployment: spec.additionalMongodConfig spec.exposedExternally spec.logLevel spec.featureCompatibilityVersion spec.podSpec.cpu spec.podSpec.cpuRequests spec.podSpec.memory spec.podSpec.memoryRequests spec.podSpec.persistence.single spec.podSpec.persistence.multiple.data spec.podSpec.persistence.multiple.journal spec.podSpec.persistence.multiple.logs spec.podSpec.podAffinity spec.podSpec.nodeAffinity Invoke the following   command to create your standalone: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. Find a Specific Pod Review Logs from Specific Pod",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\nspec:\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <standalone-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "You can deploy a standalone MongoDB instance for  to\nmanage. Use standalone instances for testing and development.\nDo not use these deployments for production systems as they lack\nreplication and high availability. For all production deployments\nuse replica sets. To learn about replica sets, see\nDeploy a Replica Set.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-operator-install",
            "title": "Plan your  Installation",
            "headings": [],
            "paragraphs": "Use the   to deploy: To deploy MongoDB resources with the  , you need an\n  instance. Deploy this instance to   using the Operator or\noutside   using\n traditional installation methods . The\nOperator uses     methods to deploy then manage MongoDB\nresources. Ops Manager resources MongoDB standalone, replica set, and sharded cluster resources Review compatible versions of  , OpenShift, MongoDB, and  . Review   deployment scopes, Docker container details, and\nother preparation information. Review the prerequisites before you install the  .",
            "code": [],
            "preview": "Use the  to deploy:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-internal-auth",
            "title": "Secure Internal Authentication with X.509 and TLS",
            "headings": [
                "General Prerequisites",
                "Configure X.509 Internal Authentication for a Replica Set",
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Create the  for your TLS certificates.",
                "Create the  for your X.509 certificates.",
                "Create the  to link your  with your deployment.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the TLS settings for your replica set resource using a Custom Certificate Authority.",
                "Configure the general X.509 settings for your replica set resource.",
                "Configure the internal X.509 settings for your replica set resource.",
                "Save your replica set config file.",
                "Apply your changes to your replica set deployment.",
                "Track the status of your deployment.",
                "Configure X.509 Internal Authentication for a Sharded Cluster",
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Create the  for your Shards' TLS certificates.",
                "Create the  for your config server's TLS certificates.",
                "Create the  for your mongos server's TLS certificates.",
                "Create the  for your Shards' X.509 certificates.",
                "Create the  for your config server's X.509 certificates.",
                "Create the  for your mongos server's X.509 certificates.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the TLS settings for your sharded cluster resource using a Custom Certificate Authority.",
                "Configure the general X.509 settings for your sharded cluster resource.",
                "Configure the internal X.509 settings for your sharded cluster resource.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Track the status of your deployment."
            ],
            "paragraphs": "This guide instructs you on how to configure: X.509 internal authentication between MongoDB nodes in a cluster. X.509 authentication from clients to your MongoDB instances.  to encrypt connections between MongoDB hosts in a replica set\nor sharded cluster.  to encrypt connections client applications and MongoDB\ndeployments. Automatically generating   certificates with the  \nis deprecated and will be removed in a future release. You must provide certificates from your own CA, as described in the\nfollowing procedures, for production environments. Before you secure any of your MongoDB deployments using  \nencryption, complete the following: Install the Kubernetes Operator Create Credentials for the Kubernetes Operator Enabling X.509 authentication at the project level configures all\nagents to use X.509 client authentication when communicating with\nMongoDB deployments. X.509 client authentication requires one of the following:  4.1.7 or later  4.0.11 or later Before you secure your replica set using   encryption, complete the\nfollowing: Deploy the Replica Set \nthat you want to secure Create a   file for each of the following components: PEM file purpose Save File As... Your custom  ca-pem Each member of your replica set <metadata.name>-<X>-pem Your project's Automation or MongoDB Agent mms-automation-agent-pem Your project's Backup Agent (if needed) mms-backup-agent-pem Your project's Monitoring Agent (if needed) mms-monitoring-agent-pem Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create the   that stores\nthe replica set's certificates: This example covers a three-member replica set. If you have more than\nthree members, you can add them to the certificate using the\n --from-file  option. Run this  kubectl  command to create the   that stores\nthe replica set's certificates: This example covers a three-member replica set. If you have more than\nthree members, you can add them to the certificate using the\n --from-file  option. Run this  kubectl  command to link your   to your replica\nset: Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Optional If you use a custom   and have created the  \nthat stores it, add the secret's name. <custom-ca> To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true , authentication is enabled on the\nMongoDB deployment. true array Conditional If you enabled authentication, you must set an authentication\nmechanism. Accepted values are  X509 . X509 To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example string Conditional If you enabled authentication, you can enable\n X.509 internal cluster authentication .\nAccepted values are  X509 . Once internal cluster authentication is enabled, it can not\nbe disabled. X509 Invoke the following   command to updated your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. Before you secure your sharded cluster using   encryption, complete\nthe following: Deploy the Sharded Cluster \nthat you want to secure Create a   file for each of the following components: PEM file purpose Save File As... Your custom  ca-pem Each shard in your sharded cluster <metadata.name>-<Y>-<X>-pem Each member of your config server replica set <metadata.name>-config-<X>-pem Each  <metadata.name>-mongos-<X>-pem Your project's Automation or MongoDB Agent mms-automation-agent-pem Your project's Backup Agent (if needed) mms-backup-agent-pem Your project's Monitoring Agent (if needed) mms-monitoring-agent-pem Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create the   that stores\nthe sharded cluster shards' certificates: This example covers a two-shard sharded cluster with five members per\nshard. If you have more than two shards or five members per shard,\nyou can add them to the certificate using the  --from-file  option. Run this  kubectl  command to create the   that stores\nthe sharded cluster config server's certificates: Run this  kubectl  command to create the   that stores\nthe sharded cluster   certificates: Run this  kubectl  command to create the   that stores\nthe sharded cluster shards' certificates: This example covers a two-shard sharded cluster with five members per\nshard. If you have more than two shards or five members per shard,\nyou can add them to the certificate using the  --from-file  option. Run this  kubectl  command to create the   that stores\nthe sharded cluster config server's certificates: Run this  kubectl  command to create the   that stores\nthe sharded cluster   certificates: Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Optional If you use a custom   and have created the  \nthat stores it, add the secret's name. <custom-ca> To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true , authentication is enabled on the\nMongoDB deployment. true array Conditional If you enabled authentication, you must set an authentication\nmechanism. Accepted values are  X509 . X509 To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example string Conditional If you enabled authentication, you can enable\n X.509 internal cluster authentication .\nAccepted values are  X509 . Once internal cluster authentication is enabled, it can not\nbe disabled. X509 Invoke the following   command to update and restart your\n sharded cluster : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <metadata.name>-cert \\\n  --from-file=<metadata.name>-0-pem \\\n  --from-file=<metadata.name>-1-pem \\\n  --from-file=<metadata.name>-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <metadata.name>-clusterfile \\\n  --from-file=<metadata.name>-0-pem \\\n  --from-file=<metadata.name>-1-pem \\\n  --from-file=<metadata.name>-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap custom-ca --from-file=ca-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-0-cert \\\n       --from-file=<metadata.name>-0-0-pem \\\n       --from-file=<metadata.name>-0-1-pem \\\n       --from-file=<metadata.name>-0-2-pem \\\n       --from-file=<metadata.name>-0-3-pem \\\n       --from-file=<metadata.name>-0-4-pem\n\nkubectl -n mongodb create secret generic <metadata.name>-1-cert \\\n       --from-file=<metadata.name>-1-0-pem \\\n       --from-file=<metadata.name>-1-1-pem \\\n       --from-file=<metadata.name>-1-2-pem \\\n       --from-file=<metadata.name>-1-3-pem \\\n       --from-file=<metadata.name>-1-4-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-config-cert \\\n       --from-file=<metadata.name>-config-0-pem \\\n       --from-file=<metadata.name>-config-1-pem \\\n       --from-file=<metadata.name>-config-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-mongos-cert \\\n       --from-file=<metadata.name>-mongos-0-pem \\\n       --from-file=<metadata.name>-mongos-1-pem \\\n       --from-file=<metadata.name>-mongos-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-0-clusterfile \\\n       --from-file=<metadata.name>-0-0-pem \\\n       --from-file=<metadata.name>-0-1-pem \\\n       --from-file=<metadata.name>-0-2-pem \\\n       --from-file=<metadata.name>-0-3-pem \\\n       --from-file=<metadata.name>-0-4-pem\n\nkubectl -n mongodb create secret generic <metadata.name>-1-clusterfile \\\n       --from-file=<metadata.name>-1-0-pem \\\n       --from-file=<metadata.name>-1-1-pem \\\n       --from-file=<metadata.name>-1-2-pem \\\n       --from-file=<metadata.name>-1-3-pem \\\n       --from-file=<metadata.name>-1-4-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-config-clusterfile \\\n       --from-file=<metadata.name>-config-0-pem \\\n       --from-file=<metadata.name>-config-1-pem \\\n       --from-file=<metadata.name>-config-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-mongos-clusterfile \\\n       --from-file=<metadata.name>-mongos-0-pem \\\n       --from-file=<metadata.name>-mongos-1-pem \\\n       --from-file=<metadata.name>-mongos-2-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n      internalCluster: \"X509\"\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "This guide instructs you on how to configure:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-om-container-local-mode",
            "title": "Configure an  Resource to use Local Mode",
            "headings": [
                "Prerequisites and Considerations",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Delete the  that manages your  Pods.",
                "Copy the highlighted fields of this  resource.",
                "Paste the copied example section into your existing  resource.",
                "Save your  config file.",
                "Apply changes to your  deployment.",
                "In a rolling fashion, delete your old  Pods.",
                "Track the status of your  instance.",
                "Download the MongoDB installation archive to your local machine.",
                "Copy the MongoDB installation archive to the  Persistent Volume for each  replica you deployed.",
                "Deploy a MongoDB Database Resource."
            ],
            "paragraphs": "In a default configuration, the \u200bs and Backup Daemons\naccess MongoDB installation archives over the Internet from  You can configure   to run in  Local Mode  with the\n  if the nodes in your   cluster don't have access to\nthe Internet. The Backup Daemons and managed MongoDB resources download\ninstallation archives only from a   that you create for\nthe   StatefulSet. This procedure covers uploading installation archives to  . Configuring   to use Local Mode in   is not recommended.\nConsider  configuring Ops Manager to use Remote Mode  instead. Deploy an   Resource . The following procedure shows you how to\nupdate your       to enable Local Mode. To avoid downtime when you enable Local Mode, ensure that you set\n spec.replicas  to a value greater than  1  in your\n  resource definition. If you updated your   resource definition to make  \nhighly available, apply your changes before you begin this tutorial: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : In this tutorial, you update the StatefulSet that manages the  \nPods in your   cluster. You must first delete the   StatefulSet so that   can apply\nthe updates that Local Mode requires. Find the name of your   StatefulSet: The entry in the response that matches the\n metadata.name  of your Your   StatefulSet is the entry in the response that matches\nthe  metadata.name  in your   resource\ndefinition. Delete the   StatefulSet: Ensure that you include the  --cascade=false  flag when you\ndelete your   StatefulSet. If you don't include this\nflag,   also deletes your   Pods. The highlighted section: Uses the   configuration setting\n automation.versions.source: local  in\n spec.configuration  to enable Local Mode. Defines a   for the   StatefulSet to store the\nMongoDB installation archive. \u200bs running in MongoDB\ndatabase resource containers that you create with the  \ndownload the installation archives from   instead of from the\nInternet. Open your preferred text editor and paste the  \nspecification into the appropriate location in your resource file. Invoke the following  kubectl  command on the filename of the\n  resource definition:  creates a new   StatefulSet when you apply the changes\nto your   resource definition. Before proceeding to the next\nstep, run the following command to ensure that the   StatefulSet\nexists: The new   StatefulSet should show 0 members ready: List the   Pods in your   cluster: Delete one   Pod:  recreates the   Pod you deleted. Continue to get the\nstatus of the new Pod until it is ready: The output looks like the following when the new Pod is\ninitializing: The output looks like the following when the new Pod is ready: Repeat Steps  b  and  c  until you've deleted all of your\n  Pods and confirmed that all of the new Pods are ready. To check the status of your   resource, invoke the following\ncommand: See  Troubleshooting the   for information about the\nresource deployment statuses. After the   resource completes the  Reconciling  phase, the\ncommand returns output similar to the following: Copy the value of the  status.opsManager.url  field, which states\nthe resource's connection  . You use this value when you create a\n  later in the procedure. The installers that you download depend on the environment to which\nyou deployed the operator: The examples below provide you with a link to quickly download the\nspecified versions of MongoDB Community edition and the MongoDB\nDatabase tools. To download MongoDB Enterprise Edition, or any other version of\nMongoDB Community Edition, visit the  MongoDB Download Center . Download the Ubuntu 16.04 installation tarball for the\nMongoDB Server version you want the   to\ndeploy. For example, to download the  4.4.0  release: If you deployed   4.4.0 and later, you must also\ndownload the Ubuntu 16.04 MongoDB Database Tools installation\ntarball. For example, to download the  100.1.0  release: Download the RHEL 7 installation tarball for the MongoDB\ndatabase version you want the   to deploy. For\nexample, to download the  4.4.0  release: If you deployed   4.4.0 and later, you must also\ndownload the RHEL 7 MongoDB Database Tools installation\ntarball. For example, to download the  100.1.0  release: The commands that you use depend on the environment to which you\ndeployed the operator: Only copy the MongoDB installation tarballs to  Replica 1  and\nbeyond  if you deployed more than one    replica . To copy the MongoDB  4.2.0  installation archive to the\n  PersistentVolume: Copy the MongoDB Server installation tarball to the\n  PersistentVolume. For example, to copy the  4.4.0 \nrelease: If you deployed   4.4.0 and later, copy the MongoDB\nDatabase Tools installation tarball to the  \nPersistentVolume. For example, to copy the  100.1.0 \nrelease: To copy the MongoDB  4.2.0  installation archive to the\n  PersistentVolume: Copy the MongoDB Server installation tarball to the\n  PersistentVolume. For example, to copy the  4.4.0 \nrelease: If you deployed   4.4.0 and later, copy the MongoDB\nDatabase Tools installation tarball to the  \nPersistentVolume. For example, to copy the  100.1.0 \nrelease: \u200bs running in MongoDB database resource containers that\nyou create with the   download the installation archives\nfrom   instead of from the Internet. If you have not done so already, complete the following\nprerequisites: Create Credentials for the  Create One Project using a ConfigMap Deploy a  MongoDB database resource \nin the same namespace to which you deployed  .\nEnsure that you: Match the  spec.opsManager.configMapRef.name  of the resource\nto the  metadata.name  of your ConfigMap. Match the  spec.credentials  of the resource to the name of\nthe secret you created that contains an   programmatic\nAPI key pair.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulsets"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulsets -n mongodb\nNAME                       READY   AGE\nops-manager-localmode      2/2     2m31s\nops-manager-localmode-db   3/3     4m46s"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete statefulset --cascade=false <ops-manager-statefulset>"
                },
                {
                    "lang": "yaml",
                    "value": "apiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n name: ops-manager-localmode\nspec:\n replicas: 2\n version: 4.2.12\n adminCredentials: ops-manager-admin-secret\n configuration:\n   # this enables local mode in Ops Manager\n   automation.versions.source: local\n\n statefulSet:\n   spec:\n     # the Persistent Volume Claim will be created for each Ops Manager Pod\n     volumeClaimTemplates:\n       - metadata:\n           name: mongodb-versions\n         spec:\n           accessModes: [ \"ReadWriteOnce\" ]\n           resources:\n             requests:\n               storage: 20Gi\n     template:\n       spec:\n         containers:\n           - name: mongodb-ops-manager\n             volumeMounts:\n               - name: mongodb-versions\n                 # this is the directory in each Pod where all MongoDB\n                 # archives must be put\n                 mountPath: /mongodb-ops-manager/mongodb-releases\n\n\n backup:\n   enabled: false\n\n applicationDatabase:\n   members: 3\n   persistent: true\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulsets"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get statefulsets -n mongodb\nNAME                       READY   AGE         ops-manager-localmode      0/2     2m31s\nops-manager-localmode-db   3/3     4m46s"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete pod <om-pod-0>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get pods"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                          READY   STATUS    RESTARTS   AGE\nmongodb-enterprise-operator-5648d4c86-k5brh   1/1     Running   0          5m24s\nops-manager-localmode-0                       0/1     Running   0          0m55s\nops-manager-localmode-1                       1/1     Running   0          5m45s\nops-manager-localmode-db-0                    1/1     Running   0          5m19s\nops-manager-localmode-db-1                    1/1     Running   0          4m54s\nops-manager-localmode-db-2                    1/1     Running   0          4m12s"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                          READY   STATUS    RESTARTS   AGE\nmongodb-enterprise-operator-5648d4c86-k5brh   1/1     Running   0          5m24s\nops-manager-localmode-0                       1/1     Running   0          3m55s\nops-manager-localmode-1                       1/1     Running   0          5m45s\nops-manager-localmode-db-0                    1/1     Running   0          5m19s\nops-manager-localmode-db-1                    1/1     Running   0          4m54s\nops-manager-localmode-db-2                    1/1     Running   0          4m12s"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2020-05-15T16:20:22Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: 4.2.2-ent\n  backup:\n    phase: \"\"\n  opsManager:\n    lastTransition: \"2020-05-15T16:20:26Z\"\n    phase: Running\n    replicas: 1\n    url: http://ops-manager-localmode-svc.mongodb.svc.cluster.local:8080\n    version: 4.2.12"
                },
                {
                    "lang": "sh",
                    "value": "curl -OL https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-ubuntu1604-4.4.0.tgz"
                },
                {
                    "lang": "sh",
                    "value": "curl -OL https://fastdl.mongodb.org/tools/db/mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz"
                },
                {
                    "lang": "sh",
                    "value": "curl -OL https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-4.4.0.tgz"
                },
                {
                    "lang": "sh",
                    "value": "curl -OL https://fastdl.mongodb.org/tools/db/mongodb-database-tools-rhel70-x86_64-100.1.0.tgz"
                },
                {
                    "lang": "sh",
                    "value": "kubectl cp mongodb-linux-x86_64-ubuntu1604-4.4.0.tgz \\\n\"ops-manager-localmode-0:/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-ubuntu1604-4.4.0.tgz\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl cp mongodb-linux-x86_64-ubuntu1604-4.4.0.tgz \\\n\"ops-manager-localmode-1:/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-ubuntu1604-4.4.0.tgz\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl cp mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz \\\n\"ops-manager-localmode-0:/mongodb-ops-manager/mongodb-releases/mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl cp mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz \\\n\"ops-manager-localmode-1:/mongodb-ops-manager/mongodb-releases/mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz\""
                },
                {
                    "lang": "sh",
                    "value": "oc rsync  \"ops-manager-localmode-0:/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-rhel70-4.4.0.tgz\" \\\nmongodb-linux-x86_64-rhel70-4.4.0.tgz"
                },
                {
                    "lang": "sh",
                    "value": "oc rsync  \"ops-manager-localmode-1:/mongodb-ops-manager/mongodb-releases/mongodb-linux-x86_64-rhel70-4.4.0.tgz\" \\\nmongodb-linux-x86_64-rhel70-4.4.0.tgz"
                },
                {
                    "lang": "sh",
                    "value": "kubectl cp mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz \\\n\"ops-manager-localmode-0:/mongodb-ops-manager/mongodb-releases/mongodb-database-tools-ubuntu1604-x86_64-100.1.0.tgz\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl cp mongodb-database-tools-rhel70-x86_64-100.1.0.tgz \\\n\"ops-manager-localmode-1:/mongodb-ops-manager/mongodb-releases/mongodb-database-tools-rhel70-x86_64-100.1.0.tgz\""
                }
            ],
            "preview": "In a default configuration, the \u200bs and Backup Daemons\naccess MongoDB installation archives over the Internet from ",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-sharded-cluster",
            "title": "Deploy a Sharded Cluster",
            "headings": [
                "Prerequisites",
                "Considerations",
                "Do Not Deploy Monitoring Agents inside and outside ",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example to create a new sharded cluster resource.",
                "Configure the settings highlighted in the preceding step as follows.",
                "Add any additional accepted settings for a sharded cluster  deployment.",
                "Save this file with a .yaml file extension.",
                "Start your sharded cluster deployment.",
                "Track the status of your sharded cluster deployment."
            ],
            "paragraphs": "Sharded clusters  provide horizontal scaling\nfor large data sets and enable high throughput operations by\ndistributing the data set across a group of servers. To learn more about sharding, see\n Sharding Introduction  in the\nMongoDB manual. Use this procedure to deploy a new sharded cluster that   manages.\nLater, you can use   to add shards and perform other maintenance\noperations on the cluster. At any place on this page that says  Ops Manager , you can substitute  Cloud Manager . You can use the   to deploy MongoDB resources with\n  version 4.0.11 or later and Cloud Manager. You can't use the   to deploy MongoDB resources to\n . To deploy a  sharded cluster  using an  , you need to complete the\nfollowing procedures: Install Kubernetes Operator Create One Project using a ConfigMap Create Credentials for the  Starting in   version 1.3.0, you can only have one\nMongoDB resource per project. To learn how to migrate your project to\na single-cluster configuration, see  Migrate to One Resource per Project (Required for Version 1.3.0) . Do not mix MongoDB deployments outside   with ones inside  \nin the same Project. Due to   network translation, a Monitoring Agent outside  \ncannot monitor MongoDB instances inside  . For this reason, k8s\nand non-k8s deployments in the same Project are not supported. Use\nseparate projects. The procedure for deploying a sharded cluster depends on whether you\nrequire the deployment to run with   enabled for intra-cluster\ncommunication and clients connecting to the database: If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. This is a   file that you can modify to meet your desired\nconfiguration. Change the highlighted settings to match your desired\n sharded cluster  configuration. Open your preferred text editor and paste the   specification\ninto a new text file. Key Type Description Example metadata.name string Label for this    sharded cluster   . Resource names must be 44 characters or less. metadata.name  documentation on  names . myproject spec.shardCount integer Number of shards to deploy. 2 spec.mongodsPerShardCount integer Number of shard members per shard. 3 spec.mongosCount integer Number of shard routers to deploy. 2 spec.configServerCount integer Number of members of the config server replica set. 3 spec.version string Version of MongoDB that this  sharded cluster  should run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the Enterprise edition. To learn more about MongoDB versioning, see\n release-version-numbers  in the MongoDB Manual. 4.2.2-ent spec.opsManager.configMapRef.name string Name of the   with the   connection\nconfiguration. The\n spec.cloudManager.configMapRef.name  setting is an\nalias for this setting and can be used in its place. This value  must  match the namespace in which you created the\n   project ConfigMap . The   tracks any changes to the ConfigMap and\nreconciles the state of the  . <myproject> spec.credentials string Name of the     you\n created  as    \nauthentication credentials for the   to\ncommunicate with  . This value  must  match the namespace in which you created the\nsecret and the  name  value you provided for your  \n   Secret . The   tracks any changes to the Secret and\nreconciles the state of the  . <mycredentials> spec.type string Type of   to create. ShardedCluster spec.persistent string Optional. Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. If this value is  true , then the following values are set\nto their default value of  16Gi : To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: spec.shardPodSpec.persistence.single spec.configSrvPodSpec.persistence.single If you want one   for each  , configure the\n spec.shardPodSpec.persistence.single  and\n spec.configSrvPodSpec.persistence.single \ncollections. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: In the  spec.configSrvPodSpec.persistence.multiple \ncollection:\n-  .data \n-  .journal \n-  .logs In the  spec.configSrvPodSpec.persistence.multiple  collection:\n-  .data \n-  .journal \n-  .logs Your containers must have permissions to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the deployment item does not fix\nissues with your  , contact  MongoDB Support . If you do not use  , the  Disk Usage  and\n Disk IOPS  charts cannot be displayed in either the\n Processes  tab on the  Deployment  page or in\nthe  Metrics  page when\n reviewing the data  for this\ndeployment. true You can also add any of the following optional settings to the\n  specification file for a  sharded cluster \ndeployment: For config server For shard routers For shard members spec.clusterDomain spec.exposedExternally spec.logLevel spec.featureCompatibilityVersion spec.connectivity.replicaSetHorizons You must set  spec.clusterDomain  if your   cluster has\na  default domain \nother than the default  cluster.local . If you neither use the\ndefault nor set the  spec.clusterDomain  option, the\n  might not function as expected. spec.configSrv.additionalMongodConfig spec.configSrvPodSpec.cpu spec.configSrvPodSpec.cpuRequests spec.configSrvPodSpec.memory spec.configSrvPodSpec.memoryRequests spec.configSrvPodSpec.persistence.single spec.configSrvPodSpec.persistence.multiple.data spec.configSrvPodSpec.persistence.multiple.journal spec.configSrvPodSpec.persistence.multiple.logs spec.configSrvPodSpec.nodeAffinity spec.configSrvPodSpec.podAffinity spec.configSrvPodSpec.podAntiAffinityTopologyKey spec.configSrvPodSpec.podTemplate.metadata spec.configSrvPodSpec.podTemplate.spec spec.mongos.additionalMongodConfig spec.mongosPodSpec.cpu spec.mongosPodSpec.cpuRequests spec.mongosPodSpec.memory spec.mongosPodSpec.memoryRequests spec.mongosPodSpec.nodeAffinity spec.mongosPodSpec.podAffinity spec.mongosPodSpec.podAntiAffinityTopologyKey spec.mongosPodSpec.podTemplate.metadata spec.mongosPodSpec.podTemplate.spec spec.shard.additionalMongodConfig spec.shardPodSpec.cpu spec.shardPodSpec.cpuRequests spec.shardPodSpec.memory spec.shardPodSpec.memoryRequests spec.shardPodSpec.nodeAffinity spec.shardPodSpec.persistence.single spec.shardPodSpec.persistence.multiple.data spec.shardPodSpec.persistence.multiple.journal spec.shardPodSpec.persistence.multiple.logs spec.shardPodSpec.podAffinity spec.shardPodSpec.podAntiAffinityTopologyKey spec.shardPodSpec.podTemplate.metadata spec.shardPodSpec.podTemplate.spec Invoke the following   command to create your\n sharded cluster : Check the log  after running this\ncommand. If the creation was successful, you should see a message\nsimilar to the following: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "2018-06-26T10:30:30.346Z INFO operator/shardedclusterkube.go:52 Created! {\"sharded cluster\": \"my-sharded-cluster\"}"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "Sharded clusters provide horizontal scaling\nfor large data sets and enable high throughput operations by\ndistributing the data set across a group of servers.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-k8s-op-compatibility",
            "title": " Compatibility",
            "headings": [
                " and OpenShift Versions",
                "MongoDB Versions",
                " and  Versions"
            ],
            "paragraphs": "The   is compatible with the following   and OpenShift\nversions. Unless otherwise noted, each   version listed\nspans the full release series starting from the listed version. Versions in  italics  are deprecated.  Release Series  Version OpenShift Version 1.0.0 1.11, 1.12, 1.13, 1.14 3.11, 4.0, 4.1 1.1.0 1.11 ,  1.12 , 1.13, 1.14 3.11, 4.0, 4.1 1.2.0 1.13, 1.14, 1.15 3.11, 4.0, 4.1 1.3.0 1.14, 1.15, 1.16 3.11, 4.0, 4.1 1.4.0 1.14, 1.15, 1.16 3.11, 4.0, 4.1, 4.2  1 , 4.3  1 1.5.0 1.14, 1.15, 1.16, 1.17, 1.18 3.11, 4.0, 4.1, 4.2, 4.3, 4.4 1.6.0 1.16, 1.17, 1.18, 1.19 3.11, 4.2, 4.3, 4.4, 4.5  v1.4.3 and later The   is compatible with the following MongoDB\nversions. Unless otherwise noted, each   version listed\nspans the full release series starting from the listed version.  Release Series MongoDB Version 1.0.0 3.6, 4.0 1.1.0 3.6, 4.0, 4.2 1.2.0 3.6, 4.0, 4.2 1.3.0 3.6, 4.0, 4.2 1.4.0 3.6, 4.0, 4.2 1.5.0 3.6, 4.0, 4.2 1.6.0 4.0, 4.2, 4.4  2  v1.6.1 and later The   is compatible with   and with the\nfollowing   versions. Unless otherwise noted, each  \nversion listed spans the full release series starting from the listed\nversion.  Release Series  Version 1.0.0 4.0, 4.2 1.1.0 4.0, 4.2 1.2.0 4.0, 4.2 1.3.0 4.0, 4.2 1.4.0 4.0, 4.2 1.5.0 4.0, 4.2 1.6.0 4.0, 4.2, 4.4",
            "code": [],
            "preview": "The  is compatible with the following  and OpenShift\nversions. Unless otherwise noted, each  version listed\nspans the full release series starting from the listed version.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/upgrade-k8s-operator",
            "title": "Upgrade from Operator Version 0.10 or Later",
            "headings": [
                "Procedure",
                "Change to the directory in which you cloned the repository.",
                "Upgrade the  for MongoDB deployments using the following  command:",
                "You can edit the Operator  file to further customize your Operator before upgrading it.",
                "Upgrade the  using the following  command:",
                "Upgrade the  using the following kubectl and helm commands:",
                "Upgrade the latest version of the  with modified pull policy values using the following helm command:",
                "Change to the directory in which you cloned the repository.",
                "Upgrade the  for MongoDB deployments.",
                "You can edit the Operator  file to further customize your Operator before upgrading it.",
                "Upgrade the  using the following  command:",
                "Upgrade the .",
                "Upgrade the latest version of the  with modified pull policy values."
            ],
            "paragraphs": "Starting in   version 1.3.0, you can only have one\nMongoDB resource per project. To learn how to migrate your project to\na single-cluster configuration, see  Migrate to One Resource per Project (Required for Version 1.3.0) . The following steps depend on how your environment is configured: Open your  mongodb-enterprise.yaml  in your preferred\ntext editor. You may need to add one or more of the following\noptions: Environment Variable When to Use OPERATOR_ENV Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . You can set the following pair of values: If  OPERATOR_ENV  is Log Level is set to Log Format is set to dev debug text prod info json WATCH_NAMESPACE Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . You can set the following pair of values: To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. MONGODB_ENTERPRISE_DATABASE_IMAGE  of the MongoDB Enterprise Database image the  \ndeploys. Default value is\n quay.io/mongodb/mongodb-enterprise-database . IMAGE_PULL_POLICY Pull policy  for the\nMongoDB Enterprise database image the   deploys. Accepted values are  Always ,  IfNotPresent ,  Never . Default value is  Always . OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the image for an  Ops\nManager resource  is downloaded. Default value is\n quay.io/mongodb/mongodb-enterprise-ops-manager . OPS_MANAGER_IMAGE_PULL_POLICY Pull policy  for the\n  images the   deploys. Accepted values are:  Always ,  IfNotPresent ,  Never . Default value is  Always . INIT_OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the initContainer image that\ncontains   start-up scripts and the readiness probe is\ndownloaded. Default value is\n quay.io/mongodb/mongodb-enterprise-ops-manager-init . INIT_OPS_MANAGER_VERSION Version of the initContainer image that contains  \nstart-up scripts and the readiness probe. Default value is 1.0.1. APPDB_IMAGE_REPOSITORY  of the repository from which the Application Database image\nis downloaded. Default value is\n quay.io/mongodb/mongodb-enterprise-appdb . INIT_APPDB_IMAGE_REPOSITORY  of the repository from which the initContainer image that\ncontains Application Database start-up scripts and the readiness\nprobe is downloaded. Default value is\n quay.io/mongodb/mongodb-enterprise-appdb-init . INIT_APPDB_VERSION Version of the initContainer image that contains  \nstart-up scripts and the readiness probe. Default value is 1.0.2. MANAGED_SECURITY_CONTEXT Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if you want to run the  \nin OpenShift or in a restrictive environment. Default value is  false . You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the  values.yaml  file: Setting Purpose Default namespace To use a different namespace, specify that  namespace . mongodb managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if your cluster manages the\n securityContext  for your   resources. false Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json prod Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. <metadata.namespace> Custom resources that the   watches. The   installs the   for and watches only\nthe resources you specify. Accepted values are: Value Description mongodb Install the CustomResourceDefinitions for the database resources and also\nwatch those resources. mongodbusers Install the CustomResourceDefinitions for the MongoDB user resources and\nalso watch those resources. opsmanagers Install the CustomResourceDefinitions for the   resources and also\nwatch those resources. mongodbusers mongodb opsmanagers Repository from which the Application Database image is pulled.\nSpecify this value if you want to pull the   image from a\nprivate repository. Repository from which the Application Database initContainer\nimage is pulled. This image contains the start-up scripts and\nreadiness probe for the Application Database. Specify this value if you want to pull the Application Database\ninitContainer image from a private repository. Repository from which the   initContainer image is\npulled. This image contains the start-up scripts and readiness\nprobe for  . Specify this value if you want to pull the  \n initContainer  image from a private repository. Repository from which the   image is pulled.\nSpecify this value if you want to pull the   image\nfrom a private repository. Repository from which the   image is pulled. Specify this\nvalue if you want to pull the   image from a private\nrepository. Alternatively, you can pass these values as options when you apply the\nHelm Chart: To upgrade the   on a host not connected to the\nInternet: Invoke the following  kubectl  and  helm  commands: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the  values.yaml  file: Setting Purpose Default namespace To use a different namespace, specify that  namespace . mongodb managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if your cluster manages the\n securityContext  for your   resources. false Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json prod Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. <metadata.namespace> Custom resources that the   watches. The   installs the   for and watches only\nthe resources you specify. Accepted values are: Value Description mongodb Install the CustomResourceDefinitions for the database resources and also\nwatch those resources. mongodbusers Install the CustomResourceDefinitions for the MongoDB user resources and\nalso watch those resources. opsmanagers Install the CustomResourceDefinitions for the   resources and also\nwatch those resources. mongodbusers mongodb opsmanagers Repository from which the Application Database image is pulled.\nSpecify this value if you want to pull the   image from a\nprivate repository. Repository from which the Application Database initContainer\nimage is pulled. This image contains the start-up scripts and\nreadiness probe for the Application Database. Specify this value if you want to pull the Application Database\ninitContainer image from a private repository. Repository from which the   initContainer image is\npulled. This image contains the start-up scripts and readiness\nprobe for  . Specify this value if you want to pull the  \n initContainer  image from a private repository. Repository from which the   image is pulled.\nSpecify this value if you want to pull the   image\nfrom a private repository. Repository from which the   image is pulled. Specify this\nvalue if you want to pull the   image from a private\nrepository. Alternatively, you can pass these values as options when you apply the\nHelm Chart: Invoke the following   command: If you run OpenShift 3.11 or earlier, you must first manually edit the   to remove subresources. In each  , remove the\nfollowing option: Open your  mongodb-enterprise-openshift.yaml  in your preferred\ntext editor. You must add your  <openshift-pull-secret>  to the\n ServiceAccount  definitions: You may need to add one or more of the following\noptions: Environment Variable Purpose OPERATOR_ENV Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . You can set the following pair of values: If  OPERATOR_ENV  is Log Level is set to Log Format is set to dev debug text prod info json WATCH_NAMESPACE Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . You can set the following pair of values: To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. MONGODB_ENTERPRISE_DATABASE_IMAGE  of the MongoDB Enterprise Database image the\n  deploys. Default value is\n registry.connect.redhat.com/mongodb/mongodb-enterprise-database . IMAGE_PULL_POLICY Pull policy \nfor the MongoDB Enterprise database image the  \ndeploys. Accepted values are  Always ,  IfNotPresent ,  Never . Default value is  Always . OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the image for an\n Ops Manager resource  is\ndownloaded. Default value is\n registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager . OPS_MANAGER_IMAGE_PULL_POLICY Pull policy \nfor the image deployed to an\n Ops Manager resource . Accepted values are  Always ,  IfNotPresent ,  Never . Default value is  Always . INIT_OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the initContainer image that\ncontains   start-up scripts and the readiness probe is\ndownloaded. Default value is\n registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager-init . INIT_OPS_MANAGER_VERSION Version of the initContainer image that contains  \nstart-up scripts and the readiness probe. Default value is 1.0.1. APPDB_IMAGE_REPOSITORY  of the repository from which the Application Database\nimage is downloaded. Default value is\n registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb . INIT_APPDB_IMAGE_REPOSITORY  of the repository from which the  initContainer  image\nthat contains Application Database start-up scripts and the\nreadiness probe is downloaded. Default value is\n registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb-init . INIT_APPDB_VERSION Version of the  initContainer  image that contains  \nstart-up scripts and the readiness probe. Default value is 1.0.2. MANAGED_SECURITY_CONTEXT Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. For OpenShift,  MANAGED_SECURITY_CONTEXT  must always be\n true . Default value is  true . Invoke the following   and  helm  commands: If you run OpenShift 3.11 or earlier, you must first manually edit the   to remove subresources. In each  , remove the\nfollowing option: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the\n values-openshift.yaml  file: Setting Purpose Default namespace To use a different namespace, specify that  namespace . mongodb managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. For OpenShift,  managedSecurityContext  must always be\n true . true Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json prod Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. <metadata.namespace> Custom resources that the   watches. The   installs the   for and watches only\nthe resources you specify. Accepted values are: Value Description mongodb Install the CustomResourceDefinitions for the database resources and also\nwatch those resources. mongodbusers Install the CustomResourceDefinitions for the MongoDB user resources and\nalso watch those resources. opsmanagers Install the CustomResourceDefinitions for the   resources and also\nwatch those resources. mongodbusers mongodb opsmanagers Repository from which the Application Database image is pulled.\nSpecify this value if you want to pull the   image from a\nprivate repository.  that contains the credentials required to pull\nimagePullSecrets from the repository. OpenShift requires this setting. Define it in this file or\npass it when you install the   using Helm. Repository from which the   image is pulled.\nSpecify this value if you want to pull the   image\nfrom a private repository. Repository from which OpenShift pulls the   image.\nSpecify this value if you want to pull the   image from a\nprivate repository. Repository from which the Application Database  initContainer \nimage is pulled. This image contains the start-up scripts and\nreadiness probe for the Application Database. Specify this value if you want to pull the Application Database\n initContainer  image from a private repository. Repository from which the    initContainer  image is\npulled. This image contains the start-up scripts and readiness\nprobe for  . Specify this value if you want to pull the  \n initContainer  image from a private repository. Alternatively, you can pass these values as options when you apply the\nHelm Chart: To upgrade the   on a host not\nconnected to the Internet: Invoke the following   and  helm  commands: If you run OpenShift 3.11 or earlier, you must first manually edit the   to remove subresources. In each  , remove the\nfollowing option: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the\n values-openshift.yaml  file: Setting Purpose Default namespace To use a different namespace, specify that  namespace . mongodb managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. For OpenShift,  managedSecurityContext  must always be\n true . true Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json prod Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. <metadata.namespace> Custom resources that the   watches. The   installs the   for and watches only\nthe resources you specify. Accepted values are: Value Description mongodb Install the CustomResourceDefinitions for the database resources and also\nwatch those resources. mongodbusers Install the CustomResourceDefinitions for the MongoDB user resources and\nalso watch those resources. opsmanagers Install the CustomResourceDefinitions for the   resources and also\nwatch those resources. mongodbusers mongodb opsmanagers Repository from which the Application Database image is pulled.\nSpecify this value if you want to pull the   image from a\nprivate repository.  that contains the credentials required to pull\nimagePullSecrets from the repository. OpenShift requires this setting. Define it in this file or\npass it when you install the   using Helm. Repository from which the   image is pulled.\nSpecify this value if you want to pull the   image\nfrom a private repository. Repository from which OpenShift pulls the   image.\nSpecify this value if you want to pull the   image from a\nprivate repository. Repository from which the Application Database  initContainer \nimage is pulled. This image contains the start-up scripts and\nreadiness probe for the Application Database. Specify this value if you want to pull the Application Database\n initContainer  image from a private repository. Repository from which the    initContainer  image is\npulled. This image contains the start-up scripts and readiness\nprobe for  . Specify this value if you want to pull the  \n initContainer  image from a private repository. Alternatively, you can pass these values as options when you apply the\nHelm Chart: To troubleshoot your  , see\n Review Logs from the  . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl replace -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: OPERATOR_ENV\nspec.template.spec.containers.name.env.value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPERATOR_ENV\n          value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: WATCH_NAMESPACE\nspec.template.spec.containers.name.env.value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: WATCH_NAMESPACE\n          value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMONGODB_ENTERPRISE_DATABASE_IMAGE\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-database"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n          value: quay.io/mongodb/mongodb-enterprise-database\n        - name: IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nIMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n          value: quay.io/mongodb/mongodb-enterprise-database\n        - name: IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_OPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-ops-manager-init"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager-init"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_OPS_MANAGER_VERSION\nspec.template.spec.containers.name.env.value:\n1.0.1"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_OPS_MANAGER_VERSION\n          value: 1.0.1"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nAPPDB_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: APPDB_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_APPDB_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-init-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_APPDB_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-init-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_APPDB_VERSION\nspec.template.spec.containers.name.env.value:\n1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_APPDB_VERSION\n          value: 1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMANAGED_SECURITY_CONTEXT\nspec.template.spec.containers.name.env.value:\nfalse"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MANAGED_SECURITY_CONTEXT\n          value: true"
                },
                {
                    "lang": "sh",
                    "value": "kubectl replace -f crds.yaml\nhelm upgrade <chart-name> helm_chart \\\n     --values helm_chart/values.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  # Execution environment for the operator, dev or prod.\n  # Use dev for more verbose logging\n  env: prod"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchedResources:\n    - mongodbusers\n    - mongodb\n    - opsmanagers"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: quay.io/mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl replace -f crds.yaml\nhelm upgrade <chart-name> helm_chart \\\n     --values helm_chart/values.yaml \\\n     --set namespace=<testNamespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl replace -f crds.yaml\nhelm upgrade <chart-name> helm_chart \\\n     --values helm_chart/values.yaml \\\n     --set registry.pullPolicy=IfNotPresent"
                },
                {
                    "lang": "yaml",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  # Execution environment for the operator, dev or prod.\n  # Use dev for more verbose logging\n  env: prod"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchedResources:\n    - mongodbusers\n    - mongodb\n    - opsmanagers"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: quay.io/mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl replace -f crds.yaml\nhelm install <chart-name> helm_chart \\\n     --values helm_chart/values.yaml \\\n     --set registry.pullPolicy=IfNotPresent \\\n     --set namespace=<testNamespace>"
                },
                {
                    "lang": "sh",
                    "value": "oc replace -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  subresources:\n    status: {}\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-appdb\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-database-pods\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: OPERATOR_ENV\nspec.template.spec.containers.name.env.value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPERATOR_ENV\n          value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: WATCH_NAMESPACE\nspec.template.spec.containers.name.env.value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: WATCH_NAMESPACE\n          value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMONGODB_ENTERPRISE_DATABASE_IMAGE\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-database"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-database\n        - name: IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nIMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-database\n        - name: IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_OPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager-init"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_OPS_MANAGER_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager-init"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_OPS_MANAGER_VERSION\nspec.template.spec.containers.name.env.value:\n1.0.1"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_OPS_MANAGER_VERSION\n          value: 1.0.1"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nAPPDB_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: APPDB_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_APPDB_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-init-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_APPDB_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-init-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_APPDB_VERSION\nspec.template.spec.containers.name.env.value:\n1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_APPDB_VERSION\n          value: 1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMANAGED_SECURITY_CONTEXT\nspec.template.spec.containers.name.env.value:\ntrue"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MANAGED_SECURITY_CONTEXT\n          value: true"
                },
                {
                    "lang": "sh",
                    "value": "oc replace -f crds.yaml\nhelm upgrade <chart-name> helm_chart \\\n      --values helm_chart/values-openshift.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  subresources:\n    status: {}\n"
                },
                {
                    "lang": "yaml",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# OpenShift manages security context on its own\nmanagedSecurityContext: true"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  # Execution environment for the operator, dev or prod.\n  # Use dev for more verbose logging\n  env: prod"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchedResources:\n    - mongodbusers\n    - mongodb\n    - opsmanagers"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "sh",
                    "value": "oc replace -f crds.yaml\nhelm upgrade <chart-name> helm_chart \\\n     --values helm_chart/values-openshift.yaml \\\n     --set registry.imagePullSecrets=<openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "oc replace -f crds.yaml\nhelm upgrade <chart-name> helm_chart \\\n     --values helm_chart/values-openshift.yaml \\\n     --set registry.pullPolicy=IfNotPresent \\\n     --set registry.imagePullSecrets=<openshift-pull-secret>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  subresources:\n    status: {}\n"
                },
                {
                    "lang": "yaml",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# OpenShift manages security context on its own\nmanagedSecurityContext: true"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  # Execution environment for the operator, dev or prod.\n  # Use dev for more verbose logging\n  env: prod"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchedResources:\n    - mongodbusers\n    - mongodb\n    - opsmanagers"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "sh",
                    "value": "oc replace -f crds.yaml\nhelm install <chart-name> helm_chart \\\n     --values helm_chart/values-openshift.yaml \\\n     --set registry.pullPolicy=IfNotPresent \\\n     --set registry.imagePullSecrets=<openshift-pull-secret> \\\n     --set namespace=<testNamespace>"
                }
            ],
            "preview": "Invoke the following kubectl and helm commands:",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/deploy-om-container",
            "title": "Deploy an  Resource",
            "headings": [
                "Prerequisites and Considerations",
                "Considerations for  Deployments over HTTPS",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Copy the following example   .",
                "Open your preferred text editor and paste the  specification into a new text file.",
                "Configure the settings highlighted in the prior example.",
                "Optional: Configure Backup settings.",
                "Optional: Configure any additional settings for an  deployment.",
                "Save this file with a .yaml file extension.",
                "Create your  instance.",
                "Track the status of your  instance.",
                "Access the  application.",
                "Optional: Create credentials for the Kubernetes Operator.",
                "Optional: Create a project using a .",
                "Optional: Deploy MongoDB database resources to complete the Backup configuration.",
                "Optional: Confirm that the  resource is running.",
                "Configure kubectl to default to your namespace.",
                "Concatenate your TLS certificate and Private Key.",
                "Create a Kubernetes secret for your certificates.",
                "If necessary, validate your TLS Certificate",
                "Copy the following example   .",
                "Open your preferred text editor and paste the  specification into a new text file.",
                "Configure the settings highlighted in the prior example.",
                "Optional: Configure Backup settings",
                "Optional: Configure any additional settings for an  deployment.",
                "Save this file with a .yaml file extension.",
                "Create your  instance.",
                "Track the status of your  instance.",
                "Access the  application.",
                "Create credentials for the Kubernetes Operator.",
                "Create a project using a .",
                "Deploy MongoDB database resources to complete the Backup configuration.",
                "Confirm that the  resource is running."
            ],
            "paragraphs": "You can deploy   in a container with the  . Before you deploy an   resource, make sure you  plan for\nyour Ops Manager resource : Complete the  Prerequisites Read the  Considerations . You can configure your deployed   resource to run over  ,\nrather than  . A full description of TLS, PKI (Public Key\nInfrastructure) certificates, and Certificate Authority is beyond the\nscope of this tutorial. This tutorial assumes prior knowledge of TLS/SSL\nas well as access to valid certificates. When running over  ,   runs on port  8443  by default. Select the appropriate tab based on whether you want your  \ninstance to run over   or  : If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Change the highlighted settings to match your desired\n  configuration. Key Type Description Example metadata.name string Name for this      . Resource names must be 44 characters or less. metadata.name  documentation on  names . om spec.replicas number Number of   instances to run in parallel. The minimum valid value is  1 . For high availability, set this value to more than  1 . Multiple\n  instances can read from the same Application\nDatabase, ensuring failover if one instance is unavailable and\nenabling you to update the   resource without downtime. 1 spec.version string Version of   to be installed. The format should be  X.Y.Z .\nTo view available   versions, view the\n container registry . \u200b spec.adminCredentials string Name of the   you  created \nfor the   admin user. Configure the secret to use the same   as the\n  resource. om-admin-secret string Optional . The   service  ServiceType \nthat exposes   outside of  . Exclude the\n spec. externalConnectivity  setting\nand its children if you don't want the   to\ncreate a   service to route external traffic to the\n  application. LoadBalancer integer Number of members of the  mms-application-database \nreplica set. 3 string Optional . Version of MongoDB that the  mms-application-database \nshould run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the  Enterprise edition . To learn more about MongoDB versioning, see see\n release-version-numbers  in the MongoDB Manual. To deploy   inside   without an Internet connection,\nomit this setting or leave the value empty. The  \ninstalls the  bundled MongoDB Enterprise  version 4.2.2 by default. 4.2.2-ent boolean Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. spec.applicationDatabase.podSpec.persistence. \n spec.podSpec.persistence.single \nis set to its default value of  16Gi . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: You must set this value to  true . If you want one   for each  , configure the\n spec.applicationDatabase. \n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: Grant your containers permission to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the resource does not fix\nissues with your  , contact MongoDB support. true If you want to enable backup, you must configure all of the following\nsettings: You must also configure an  S3 snapshot store \nor a  blockstore . To configure a snapshot store, configure the following settings: To configure a blockstore, configure the following settings: Key Type Description Example boolean Flag that indicates that Backup is enabled. You must specify\n spec.backup.enabled: true  to configure settings\nfor the head database, oplog store, and snapshot store. true string Name of the oplog store. oplog1 string Name of the MongoDB database resource for the oplog store. my-oplog-db If you deploy both an    snapshot store \nand a  blockstore ,  \nrandomly choses one to use for Backup. Key Type Description Example string Name of the   snapshot store. s3store1 string Name of the   that contains the  accessKey  and\n secretKey  fields. The  backup-daemon  uses the\nvalues of these fields as credentials to access the   or\n -compatible bucket. my-s3-credentials string  of the   or  -compatible bucket that\n stores  the\ndatabase Backup snapshots. s3.us-east-1.amazonaws.com string Name of the   or  -compatible bucket that stores the\ndatabase Backup snapshots. my-bucket Key Type Description Example string Name of the blockstore. blockStore1 string Name of the MongoDB database resource that you create for the\nblockstore. You must deploy this database resource in the same\nnamespace as the   resource. my-mongodb-blockstore Add any  optional settings  that you\nwant to apply to your deployment to the   specification file. Invoke the following  kubectl  command on the filename of the\n  resource definition: To check the status of your   resource, invoke the following\ncommand: The command returns the following output under the  status  field\nwhile the resource deploys: The   reconciles the resources in the following order: The   doesn't reconcile a resource until the preceding\none enters the  Running  phase. After the   resource completes the  Reconciling  phase, the\ncommand returns the following output under the  status  field if you\nenabled backup: Backup remains in a  Pending  state until you configure the Backup\ndatabases. Application Database. . Backup. The  status.opsManager.url  field states the resource's\nconnection  . Using this  , you can reach   from\ninside the   cluster or  create a project using a\nConfigMap . The steps you take differ based on how you are routing traffic to the\n  application in  . If you configured the   to\ncreate a   service for you, or you created a   service\nmanually, use one of the following methods to access the  \napplication: To learn how to access the   application using a third-party\nservice, refer to the documentation for your solution. Query your cloud provider to get the   of the load\nbalancer service. See your cloud provider's documentation\nfor details. Open a browser window and navigate to the  \napplication using the   and port number of your load\nbalancer service. Log in to   using the  admin user credentials . Set your firewall rules to allow access from the Internet to\nthe  spec.externalConnectivity. port \non the host on which your   cluster is running. Open a browser window and navigate to the  \napplication using the   and the\n spec.externalConnectivity. port . Log in to   using the  admin user credentials . If you enabled Backup, you must create an   organization,\ngenerate programmatic API keys, and create a  . These\nactivities follow the prerequisites and procedure on the\n Create Credentials for the   page. If you enabled Backup, create a project by following the prerequisites\nand procedure on the  Create One Project using a ConfigMap  page. You must set  data.baseUrl  in the ConfigMap to the  's  . To find this  , invoke the following command: The command returns the URL of the   in the\n status.opsManager.url  field. If you deploy   with the   and   will\nmanage MongoDB database resources deployed  outside  of the  \ncluster it's deployed to, you must set  data.baseUrl  to the same\nvalue of the\n spec.configuration.mms.centralUrl \nsetting in the   resource specification. If you enabled  mms-backup-functional-overview ,\ncreate a MongoDB database resource for the oplog and snapshot stores\nto complete the configuration. Deploy a  MongoDB database resource  for the oplog store in the same\nnamespace as the   resource. Match the  metadata.name  of the resource with the\n spec.backup.opLogStores.mongodbResourceRef.name  that you specified\nin your   resource definition. Create this database as a  replica set . Choose one of the following: Deploy a  MongoDB database resource  for the blockstore in the\nsame namespace as the   resource. Match the  metadata.name  of the resource to the\n spec.backup.blockStores.mongodbResourceRef.name \nthat you specified in your   resource definition. Configure an   bucket to use as the   snapshot store. Ensure that you can access the   bucket using the details\nthat you specified in your   resource definition. If you enabled backup, check the status of your   resource by\ninvoking the following command: When   is running, the command returns the following\noutput under the  status  field: See  Troubleshooting the   for information about the\nresource deployment statuses. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : If your   certificate and Private Key are separate files, run the\nfollowing command to concatenate them: The   requires that the   instance's  \ncertificate and Private Key are concatenated into a single file\ncalled  server.pem . Once you have your   certificate and Private Key in a file called\n server.pem , run the following command to store the certificates\nin a  : If your   certificate is signed by a Custom Certificate\nAuthority, you must provide a  CA (Certificate Authority) \ncertificate to validate the   certificate. To validate the\n  certificate, create a   to hold the\n CA (Certificate Authority)  certificate: The   requires that the certificate is named\n mms-ca.crt  in the ConfigMap. Change the highlighted settings to match your desired\n  configuration. Key Type Description Example metadata.name string Name for this      . Resource names must be 44 characters or less. metadata.name  documentation on  names . om spec.replicas number Number of   instances to run in parallel. The minimum valid value is  1 . For high availability, set this value to more than  1 . Multiple\n  instances can read from the same Application\nDatabase, ensuring failover if one instance is unavailable and\nenabling you to update the   resource without downtime. 1 spec.version string Version of   to be installed. The format should be  X.Y.Z .\nTo view available   versions, view the\n container registry . \u200b spec.adminCredentials string Name of the   you  created \nfor the   admin user. Configure the secret to use the same   as the\n  resource. om-admin-secret string Name of of the   you created for the  \ncertificate. om-http-cert string The   service  ServiceType \nthat exposes   outside of  . Exclude the\n spec. externalConnectivity  setting\nand its children if you don't want the   to\ncreate a   service to route external traffic to the\n  application. LoadBalancer integer Number of members of the  mms-application-database \nreplica set. 3 string Optional . Version of MongoDB that the  mms-application-database \nshould run. The format should be  X.Y.Z  for the Community edition and\n X.Y.Z-ent  for the  Enterprise edition . To learn more about MongoDB versioning, see see\n release-version-numbers  in the MongoDB Manual. To deploy   inside   without an Internet connection,\nomit this setting or leave the value empty. The  \ninstalls the  bundled MongoDB Enterprise  version 4.2.2 by default. 4.2.2-ent boolean Flag indicating if this   should use   for\nstorage. Persistent volumes are not deleted when the\n  is stopped or restarted. spec.applicationDatabase.podSpec.persistence. \n spec.podSpec.persistence.single \nis set to its default value of  16Gi . To change your   configuration, configure the\nfollowing collections to meet your deployment requirements: You must set this value to  true . If you want one   for each  , configure the\n spec.applicationDatabase. \n spec.podSpec.persistence.single  collection. If you want separate   for data, journals, and\nlogs for each  , configure the following\ncollections: Grant your containers permission to write to your  .\nThe   sets  fsGroup = 2000  in\n securityContext \nThis makes  \n try to fix write permissions \nfor the  . If redeploying the resource does not fix\nissues with your  , contact MongoDB support. true If you want to enable backup for your   instance, you must\nconfigure all of the following settings: You must also configure an :term:` S3 snapshot store <s3 snapshot store>`\nor a  blockstore . To configure a snapshot store, configure the following settings: To configure a blockstore, configure the following settings: Key Type Description Example boolean Flag that indicates that Backup is enabled for your You must\nspecify  spec.backup.enabled: true  to configure settings\nfor the head database, oplog store, and snapshot store. true string Name of the oplog store. oplog1 string Name of the MongoDB database resource for the oplog store. my-oplog-db If you deploy both an    snapshot store \nand a  blockstore ,  \nrandomly choses one to use for Backup. Key Type Description Example string Name of the   snapshot store. s3store1 string Name of the   that contains the  accessKey  and\n secretKey  fields. The  backup-daemon  uses the\nvalues of these fields as credentials to access the   or\n -compatible bucket. my-s3-credentials string  of the   or  -compatible bucket that\n stores  the\ndatabase Backup snapshots. s3.us-east-1.amazonaws.com string Name of the   or  -compatible bucket that stores the\ndatabase Backup snapshots. my-bucket Key Type Description Example string Name of the blockstore. blockStore1 string Name of the MongoDB database resource that you create for the\nblockstore. You must deploy this database resource in the same\nnamespace as the   resource. my-mongodb-blockstore Add any  optional settings  that you\nwant to apply to your deployment to the   specification file. Invoke the following  kubectl  command on the filename of the\n  resource definition: To check the status of your   resource, invoke the following\ncommand: The command returns the following output under the  status  field\nwhile the resource deploys: The   reconciles the resources in the following order: The   doesn't reconcile a resource until the preceding\none enters the  Running  phase. After the   resource completes the  Reconciling  phase, the\ncommand returns the following output under the  status  field if you\nenabled backup: Backup remains in a  Pending  state until you configure the Backup\ndatabases. After the resource completes the  Reconciling  phase, the command\nreturns the following output under the  status  field: Backup remains in a  Pending  state until you configure the Backup\ndatabases. Application Database. . Backup. The  status.opsManager.url  field states the resource's\nconnection  . Using this  , you can reach   from\ninside the   cluster or  create a project using a\nConfigMap . The  status.opsManager.url  field states the resource's\nconnection  . Using this  , you can reach   from\ninside the   cluster or  create a project using a\nConfigMap . The steps you take differ based on how you are routing traffic to the\n  application in  . If you configured the   to\ncreate a   service for you, or you created a   service\nmanually, use one of the following methods to access the  \napplication: To learn how to access the   application using a third-party\nservice, refer to the documentation for your solution. Query your cloud provider to get the   of the load\nbalancer service. See your cloud provider's documentation\nfor details. Open a browser window and navigate to the  \napplication using the   and port number of your load\nbalancer service. Log in to   using the  admin user credentials . Set your firewall rules to allow access from the Internet to\nthe  spec.externalConnectivity. port \non the host on which your   cluster is running. Open a browser window and navigate to the  \napplication using the   and the\n spec.externalConnectivity. port . Log in to   using the  admin user credentials . To configure credentials, you must create an   organization,\ngenerate programmatic API keys, and create a  . These\nactivities follow the prerequisites and procedure on the\n Create Credentials for the   page. To create a project, follow the prerequisites and procedure on the\n Create One Project using a ConfigMap  page. Set the following fields in your project ConfigMap: Set  data.baseUrl  in the ConfigMap to the  's  .\nTo find this  , invoke the following command: The command returns the URL of the   in the\n status.opsManager.url  field. If you deploy   with the   and   will\nmanage MongoDB database resources deployed  outside  of the  \ncluster it's deployed to, you must set  data.baseUrl  to the same\nvalue of the\n spec.configuration.mms.centralUrl \nsetting in the   resource specification. Set  data.sslMMSCAConfigMap  to the name of your\n  containing the root  CA (Certificate\nAuthority)  certificate used to sign the   host's\ncertificate. The   requires this name to be\n mms-ca.crt . By default,   enables  mms-backup-functional-overview .\nCreate a MongoDB database resource for the oplog and snapshot stores\nto complete the configuration. Deploy a  MongoDB database resource  for the oplog store in the same\nnamespace as the   resource. Match the  metadata.name  of the resource with the\n spec.backup.opLogStores.mongodbResourceRef.name  that you specified\nin your   resource definition. Create this database as a three-member  replica set . Deploy a  MongoDB database resource  for the   snapshot store in the\nsame namespace as the   resource. Match the  metadata.name  of the resource to the\n spec.backup.s3Stores.mongodbResourceRef.name \nthat you specified in your   resource definition. Create the   snapshot store as a replica set. To check the status of your   resource, invoke the following\ncommand: When   is running, the command returns the following\noutput under the  status  field: See  Troubleshooting the   for information about the\nresource deployment statuses.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: <myopsmanager>\nspec:\n  replicas: 1\n  version: <opsmanagerversion>\n  adminCredentials: <adminusercredentials> # Should match metadata.name\n                                           # in the Kubernetes secret\n                                           # for the admin user\n  externalConnectivity:\n    type: LoadBalancer\n\n  applicationDatabase:\n    members: 3\n    version: <mongodbversion>\n    persistent: true\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n applicationDatabase:\n  lastTransition: \"2020-04-01T09:49:22Z\"\n  message: AppDB Statefulset is not ready yet\n  phase: Reconciling\n  type: \"\"\n  version: \"\"\n backup:\n  phase: \"\"\n opsManager:\n  phase: \"\""
                },
                {
                    "lang": "yaml",
                    "value": " status:\n   applicationDatabase:\n     lastTransition: \"2020-04-01T09:50:20Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: 4.2.0\n  backup:\n   lastTransition: \"2020-04-01T09:57:42Z\"\n   message: The MongoDB object <namespace>/<oplogresourcename>\n     doesn't exist\n   phase: Pending\n   opsManager:\n     lastTransition: \"2020-04-01T09:57:40Z\"\n     phase: Running\n     replicas: 1\n     url: http://om-svc.cloudqa.svc.cluster.local:8080\n     version: 4.2.8"
                },
                {
                    "lang": "sh",
                    "value": "http://ops.example.com:8080"
                },
                {
                    "lang": "sh",
                    "value": "http://ops.example.com:30036"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": " status:\n   applicationDatabase:\n     lastTransition: \"2020-04-01T10:00:32Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: 4.2.0\n  backup:\n   lastTransition: \"2020-04-01T09:57:42Z\"\n   message: The MongoDB object <namespace>/<oplogresourcename>\n     doesn't exist\n   phase: Pending\n   opsManager:\n     lastTransition: \"2020-04-01T09:57:40Z\"\n     phase: Running\n     replicas: 1\n     url: http://om-svc.cloudqa.svc.cluster.local:8080\n     version: 4.2.8"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2020-04-01T10:00:32Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: 4.2.0\n  backup:\n    lastTransition: \"2020-04-01T10:00:53Z\"\n    phase: Running\n    version: 4.2.8\n  opsManager:\n    lastTransition: \"2020-04-01T10:00:34Z\"\n    phase: Running\n    replicas: 1\n    url: http://om-svc.cloudqa.svc.cluster.local:8080\n    version: 4.2.8"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "cat <private-key>.key <tls-certificate>.crt > server.pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic om-http-cert --from-file=\"server.pem\" -n <namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap om-http-cert-ca --from-file=\"mms-ca.crt\""
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBOpsManager\nmetadata:\n  name: <myopsmanager>\nspec:\n  replicas: 1\n  version: <opsmanagerversion>\n  adminCredentials: <adminusercredentials> # Should match metadata.name\n                                           # in the Kubernetes secret\n                                           # for the admin user\n  security:\n    tls:\n      secretRef:\n        name: <tlscertificate> # Should match metadata.name\n                               # in the Kubernetes secret\n                               # for the TLS Certificate / Private Key\n  externalConnectivity:\n    type: LoadBalancer\n\n  applicationDatabase:\n    members: 3\n    version: <mongodbversion>\n    persistent: true\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <opsmgr-resource>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n applicationDatabase:\n  lastTransition: \"2020-04-01T09:49:22Z\"\n  message: AppDB Statefulset is not ready yet\n  phase: Reconciling\n  type: \"\"\n  version: \"\"\n backup:\n  phase: \"\"\n opsManager:\n  phase: \"\""
                },
                {
                    "lang": "yaml",
                    "value": " status:\n   applicationDatabase:\n     lastTransition: \"2020-04-01T09:50:20Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: 4.2.0\n  backup:\n   lastTransition: \"2020-04-01T09:57:42Z\"\n   message: The MongoDB object <namespace>/<oplogresourcename>\n     doesn't exist\n   phase: Pending\n   opsManager:\n     lastTransition: \"2020-04-01T09:57:40Z\"\n     phase: Running\n     replicas: 1\n     url: http://om-svc.cloudqa.svc.cluster.local:8443\n     version: 4.2.8"
                },
                {
                    "lang": "yaml",
                    "value": " status:\n   applicationDatabase:\n     lastTransition: \"2019-12-06T18:23:22Z\"\n     members: 3\n     phase: Running\n     type: ReplicaSet\n     version: 4.2.2-ent\n   opsManager:\n     lastTransition: \"2019-12-06T18:23:26Z\"\n     message: The MongoDB object namespace/oplogdbname doesn't exist\n     phase: Pending\n     url: http://om-svc.dev.svc.cluster.local:8443\n     version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "http://ops.example.com:8443"
                },
                {
                    "lang": "sh",
                    "value": "http://ops.example.com:30036"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "  status:\n    applicationDatabase:\n      lastTransition: \"2019-12-06T18:23:22Z\"\n      members: 3\n      phase: Running\n      type: ReplicaSet\n      version: 4.2.2-ent\n    opsManager:\n      lastTransition: \"2019-12-06T18:23:26Z\"\n      message: The MongoDB object namespace/oplogdbname doesn't exist\n      phase: Pending\n      url: http://om-svc.dev.svc.cluster.local:8443\n      version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get om -o yaml -w"
                },
                {
                    "lang": "yaml",
                    "value": "status:\n  applicationDatabase:\n    lastTransition: \"2019-12-06T17:46:15Z\"\n    members: 3\n    phase: Running\n    type: ReplicaSet\n    version: 4.2.2-ent\n  opsManager:\n    lastTransition: \"2019-12-06T17:46:32Z\"\n    phase: Running\n    replicas: 1\n    url: http://om-backup-svc.dev.svc.cluster.local:8443\n    version: 4.2.6"
                }
            ],
            "preview": "You can deploy  in a container with the .",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/upgrade-k8s-operator-v9-and-earlier",
            "title": "Upgrade from Operator Version 0.9 or Earlier",
            "headings": [
                "Prerequisites",
                "Upgrade the ",
                "Change to the directory in which you cloned the repository.",
                "Upgrade the  for MongoDB deployments using the following  command:",
                "You can edit the Operator  file to further customize your Operator before upgrading it.",
                "Upgrade the  using the following  command:",
                "Upgrade the  using the following kubectl and helm commands:",
                "Upgrade the latest version of the  with modified pull policy values using the following helm command:",
                "Change to the directory in which you cloned the repository.",
                "Upgrade the  for MongoDB deployments.",
                "You can edit the Operator  file to further customize your Operator before upgrading it.",
                "Upgrade the  using the following  command:",
                "Upgrade the .",
                "Upgrade the latest version of the  with modified pull policy values.",
                "Recreate MongoDB Resources and Delete the Version 0.9 CRDs",
                "Troubleshooting"
            ],
            "paragraphs": "Version 0.10 of the   consolidated the\n MongoDbStandalone ,  MongoDbShardedCluster , and\n MongoDbReplicaSet    into a\nsingle  CustomResourceDefinition  called  MongoDB . Version 0.10 of the   included breaking changes and\nrequires some additional preparation before upgrading. The\nfollowing procedure outlines the upgrade process for  \nversions 0.9 and earlier. If you are already running version 0.10\nor later, see  Upgrade from Operator Version 0.10 or Later  for upgrade instructions. The following upgrade procedure allows you to keep data stored in\npersistent volumes from previous deployments that the  \nmanaged. If you do not wish to retain data from previous\ndeployments and plan on deploying new resources, skip to the\n Upgrade  section. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Verify you have the  .yaml  configuration file for each MongoDB\nresource you have deployed. If you have standalone resources but do not have the  .yaml \nconfiguration file for them, run the following command to generate\nthe configuration file: If you have replica set resources but do not have the  .yaml \nconfiguration file for them, run the following command to generate\nthe configuration file: If you have sharded cluster resources but do not have the  .yaml \nconfiguration file for them, run the following command to generate\nthe configuration file: Edit each  .yaml  configuration file match the new  : After you edit each  .yaml  file, they should look like the\nfollowing example: Change the  kind  to  MongoDB Add the  spec.type  field and set it to  Standalone ,\n ReplicaSet , or  ShardedCluster  depending on your resource. The   does not support changing the type of an existing\nconfiguration even though it will accept a valid configuration for a\ndifferent type. For example, if your MongoDB resource is a\nstandalone, you cannot set the value of  spec.type  to\n ReplicaSet  and set  spec.members . If you do, the\n  throws an error and requires you to revert to the\npreviously working configuration. If you change the  metadata.name  field you will lose your\nresource's data. To upgrade to the latest version of the   from version v0.9\nor earlier: The following steps depend on how your environment is configured: Open your  mongodb-enterprise.yaml  in your preferred\ntext editor. You may need to add one or more of the following\noptions: Environment Variable When to Use OPERATOR_ENV Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . You can set the following pair of values: If  OPERATOR_ENV  is Log Level is set to Log Format is set to dev debug text prod info json WATCH_NAMESPACE Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . You can set the following pair of values: To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. MONGODB_ENTERPRISE_DATABASE_IMAGE  of the MongoDB Enterprise Database image the  \ndeploys. Default value is\n quay.io/mongodb/mongodb-enterprise-database . IMAGE_PULL_POLICY Pull policy  for the\nMongoDB Enterprise database image the   deploys. Accepted values are  Always ,  IfNotPresent ,  Never . Default value is  Always . OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the image for an  Ops\nManager resource  is downloaded. Default value is\n quay.io/mongodb/mongodb-enterprise-ops-manager . OPS_MANAGER_IMAGE_PULL_POLICY Pull policy  for the\n  images the   deploys. Accepted values are:  Always ,  IfNotPresent ,  Never . Default value is  Always . INIT_OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the initContainer image that\ncontains   start-up scripts and the readiness probe is\ndownloaded. Default value is\n quay.io/mongodb/mongodb-enterprise-ops-manager-init . INIT_OPS_MANAGER_VERSION Version of the initContainer image that contains  \nstart-up scripts and the readiness probe. Default value is 1.0.1. APPDB_IMAGE_REPOSITORY  of the repository from which the Application Database image\nis downloaded. Default value is\n quay.io/mongodb/mongodb-enterprise-appdb . INIT_APPDB_IMAGE_REPOSITORY  of the repository from which the initContainer image that\ncontains Application Database start-up scripts and the readiness\nprobe is downloaded. Default value is\n quay.io/mongodb/mongodb-enterprise-appdb-init . INIT_APPDB_VERSION Version of the initContainer image that contains  \nstart-up scripts and the readiness probe. Default value is 1.0.2. MANAGED_SECURITY_CONTEXT Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if you want to run the  \nin OpenShift or in a restrictive environment. Default value is  false . You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the  values.yaml  file: Setting Purpose Default namespace To use a different namespace, specify that  namespace . mongodb managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if your cluster manages the\n securityContext  for your   resources. false Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json prod Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. <metadata.namespace> Custom resources that the   watches. The   installs the   for and watches only\nthe resources you specify. Accepted values are: Value Description mongodb Install the CustomResourceDefinitions for the database resources and also\nwatch those resources. mongodbusers Install the CustomResourceDefinitions for the MongoDB user resources and\nalso watch those resources. opsmanagers Install the CustomResourceDefinitions for the   resources and also\nwatch those resources. mongodbusers mongodb opsmanagers Repository from which the Application Database image is pulled.\nSpecify this value if you want to pull the   image from a\nprivate repository. Repository from which the Application Database initContainer\nimage is pulled. This image contains the start-up scripts and\nreadiness probe for the Application Database. Specify this value if you want to pull the Application Database\ninitContainer image from a private repository. Repository from which the   initContainer image is\npulled. This image contains the start-up scripts and readiness\nprobe for  . Specify this value if you want to pull the  \n initContainer  image from a private repository. Repository from which the   image is pulled.\nSpecify this value if you want to pull the   image\nfrom a private repository. Repository from which the   image is pulled. Specify this\nvalue if you want to pull the   image from a private\nrepository. Alternatively, you can pass these values as options when you apply the\nHelm Chart: To upgrade the   on a host not connected to the\nInternet: Invoke the following  kubectl  and  helm  commands: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the  values.yaml  file: Setting Purpose Default namespace To use a different namespace, specify that  namespace . mongodb managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. Set this field to  true  if your cluster manages the\n securityContext  for your   resources. false Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json prod Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. <metadata.namespace> Custom resources that the   watches. The   installs the   for and watches only\nthe resources you specify. Accepted values are: Value Description mongodb Install the CustomResourceDefinitions for the database resources and also\nwatch those resources. mongodbusers Install the CustomResourceDefinitions for the MongoDB user resources and\nalso watch those resources. opsmanagers Install the CustomResourceDefinitions for the   resources and also\nwatch those resources. mongodbusers mongodb opsmanagers Repository from which the Application Database image is pulled.\nSpecify this value if you want to pull the   image from a\nprivate repository. Repository from which the Application Database initContainer\nimage is pulled. This image contains the start-up scripts and\nreadiness probe for the Application Database. Specify this value if you want to pull the Application Database\ninitContainer image from a private repository. Repository from which the   initContainer image is\npulled. This image contains the start-up scripts and readiness\nprobe for  . Specify this value if you want to pull the  \n initContainer  image from a private repository. Repository from which the   image is pulled.\nSpecify this value if you want to pull the   image\nfrom a private repository. Repository from which the   image is pulled. Specify this\nvalue if you want to pull the   image from a private\nrepository. Alternatively, you can pass these values as options when you apply the\nHelm Chart: Invoke the following   command: If you run OpenShift 3.11 or earlier, you must first manually edit the   to remove subresources. In each  , remove the\nfollowing option: Open your  mongodb-enterprise-openshift.yaml  in your preferred\ntext editor. You must add your  <openshift-pull-secret>  to the\n ServiceAccount  definitions: You may need to add one or more of the following\noptions: Environment Variable Purpose OPERATOR_ENV Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . Default value is:  prod . You can set the following pair of values: If  OPERATOR_ENV  is Log Level is set to Log Format is set to dev debug text prod info json WATCH_NAMESPACE Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . Default value is:  <metadata.namespace> . You can set the following pair of values: To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. MONGODB_ENTERPRISE_DATABASE_IMAGE  of the MongoDB Enterprise Database image the\n  deploys. Default value is\n registry.connect.redhat.com/mongodb/mongodb-enterprise-database . IMAGE_PULL_POLICY Pull policy \nfor the MongoDB Enterprise database image the  \ndeploys. Accepted values are  Always ,  IfNotPresent ,  Never . Default value is  Always . OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the image for an\n Ops Manager resource  is\ndownloaded. Default value is\n registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager . OPS_MANAGER_IMAGE_PULL_POLICY Pull policy \nfor the image deployed to an\n Ops Manager resource . Accepted values are  Always ,  IfNotPresent ,  Never . Default value is  Always . INIT_OPS_MANAGER_IMAGE_REPOSITORY  of the repository from which the initContainer image that\ncontains   start-up scripts and the readiness probe is\ndownloaded. Default value is\n registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager-init . INIT_OPS_MANAGER_VERSION Version of the initContainer image that contains  \nstart-up scripts and the readiness probe. Default value is 1.0.1. APPDB_IMAGE_REPOSITORY  of the repository from which the Application Database\nimage is downloaded. Default value is\n registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb . INIT_APPDB_IMAGE_REPOSITORY  of the repository from which the  initContainer  image\nthat contains Application Database start-up scripts and the\nreadiness probe is downloaded. Default value is\n registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb-init . INIT_APPDB_VERSION Version of the  initContainer  image that contains  \nstart-up scripts and the readiness probe. Default value is 1.0.2. MANAGED_SECURITY_CONTEXT Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. For OpenShift,  MANAGED_SECURITY_CONTEXT  must always be\n true . Default value is  true . Invoke the following   and  helm  commands: If you run OpenShift 3.11 or earlier, you must first manually edit the   to remove subresources. In each  , remove the\nfollowing option: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the\n values-openshift.yaml  file: Setting Purpose Default namespace To use a different namespace, specify that  namespace . mongodb managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. For OpenShift,  managedSecurityContext  must always be\n true . true Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json prod Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. <metadata.namespace> Custom resources that the   watches. The   installs the   for and watches only\nthe resources you specify. Accepted values are: Value Description mongodb Install the CustomResourceDefinitions for the database resources and also\nwatch those resources. mongodbusers Install the CustomResourceDefinitions for the MongoDB user resources and\nalso watch those resources. opsmanagers Install the CustomResourceDefinitions for the   resources and also\nwatch those resources. mongodbusers mongodb opsmanagers Repository from which the Application Database image is pulled.\nSpecify this value if you want to pull the   image from a\nprivate repository.  that contains the credentials required to pull\nimagePullSecrets from the repository. OpenShift requires this setting. Define it in this file or\npass it when you install the   using Helm. Repository from which the   image is pulled.\nSpecify this value if you want to pull the   image\nfrom a private repository. Repository from which OpenShift pulls the   image.\nSpecify this value if you want to pull the   image from a\nprivate repository. Repository from which the Application Database  initContainer \nimage is pulled. This image contains the start-up scripts and\nreadiness probe for the Application Database. Specify this value if you want to pull the Application Database\n initContainer  image from a private repository. Repository from which the    initContainer  image is\npulled. This image contains the start-up scripts and readiness\nprobe for  . Specify this value if you want to pull the  \n initContainer  image from a private repository. Alternatively, you can pass these values as options when you apply the\nHelm Chart: To upgrade the   on a host not\nconnected to the Internet: Invoke the following   and  helm  commands: If you run OpenShift 3.11 or earlier, you must first manually edit the   to remove subresources. In each  , remove the\nfollowing option: You can customize your Helm Chart before installing it. To modify it,\nadd one or more of the following options to the\n values-openshift.yaml  file: Setting Purpose Default namespace To use a different namespace, specify that  namespace . mongodb managedSecurityContext Flag that determines if the   inherits the\n securityContext  settings that your   cluster manages. For OpenShift,  managedSecurityContext  must always be\n true . true Label for the Operator's deployment environment. The  env \nvalue affects default timeouts and the format and level of\nlogging. Accepted values are:   dev ,  prod . If  operator.env  is Log Level is set to Log Format is set to dev debug text prod info json prod Namespace that the Operator watches for   changes.\nIf this   differs from the default, ensure that the\nOperator's ServiceAccount\n can access \nthat different namespace. *  means  all namespaces  and requires the\n ClusterRole \nassigned to the  mongodb-enterprise-operator  ServiceAccount\nwhich is the ServiceAccount used to run the  . To watch   and   in a different   to which\nyou deploy the  , see   Deployment Scopes  for\nvalues you must use and additional steps you might have to perform. <metadata.namespace> Custom resources that the   watches. The   installs the   for and watches only\nthe resources you specify. Accepted values are: Value Description mongodb Install the CustomResourceDefinitions for the database resources and also\nwatch those resources. mongodbusers Install the CustomResourceDefinitions for the MongoDB user resources and\nalso watch those resources. opsmanagers Install the CustomResourceDefinitions for the   resources and also\nwatch those resources. mongodbusers mongodb opsmanagers Repository from which the Application Database image is pulled.\nSpecify this value if you want to pull the   image from a\nprivate repository.  that contains the credentials required to pull\nimagePullSecrets from the repository. OpenShift requires this setting. Define it in this file or\npass it when you install the   using Helm. Repository from which the   image is pulled.\nSpecify this value if you want to pull the   image\nfrom a private repository. Repository from which OpenShift pulls the   image.\nSpecify this value if you want to pull the   image from a\nprivate repository. Repository from which the Application Database  initContainer \nimage is pulled. This image contains the start-up scripts and\nreadiness probe for the Application Database. Specify this value if you want to pull the Application Database\n initContainer  image from a private repository. Repository from which the    initContainer  image is\npulled. This image contains the start-up scripts and readiness\nprobe for  . Specify this value if you want to pull the  \n initContainer  image from a private repository. Alternatively, you can pass these values as options when you apply the\nHelm Chart: To troubleshoot your  , see\n Review Logs from the  . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources . Once the version 0.9   are deleted, the   upgrade\nis complete. After you upgrade the  , verify you have four CRDs by\nrunning the following command: The following output contains the new  mongodb.mongodb.com  CRD and\nthe version 0.9 CRDs: Remove the old resources from Kubernetes. Run each of the following commands to remove all MongoDB resources: Removing MongoDB resources will remove the database server pods\nand drop any client connections to the database. Connections are\nreestablished when the new MongoDB resources are created in\nKubernetes. MongoDB resources that have  persistent: true  set in their\n .yaml  configuration file will not lose data as it is stored in\npersistent volumes. The previous command only deletes pods\ncontaining MongoDB and not the persistent volumes containing the\ndata. Persistent volume claims referencing persistent volumes stay\nalive and are reused by the new MongoDB resources. Create the MongoDB resources again. Use the  .yaml  resource configuration file to recreate each\nresource: Run the following command to check the status of each resource and\nverify that the  phase  reaches the  Running  status: For an example of this command's output, see\n Get Status of a Deployed Resource . If the old resources had  persistent: true  set and the\n metadata.name  haven't changed, the new MongoDB pods will\nreuse the data from the old pods. Delete the old CRDs. Once all the resources are up and running, delete all of the v0.9\nCRDs as the   no longer watches them: Run the following command to verify the old CRDs were removed: The output of the command above should look similar to the following: To troubleshoot your  , see\n Review Logs from the  . If you need to remove the Kubernetes Operator or the\n namespace , you first must\n remove MongoDB resources .",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl mst <standalone-name> -o yaml > <standalone-conf-name>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mrs <replicaset-name> -o yaml > <replicaset-conf-name>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get msc <shardedcluster-name> -o yaml > <shardedcluster-conf-name>.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-standalone>\nspec:\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: Standalone\n  persistent: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true\n..."
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl replace -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: OPERATOR_ENV\nspec.template.spec.containers.name.env.value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPERATOR_ENV\n          value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: WATCH_NAMESPACE\nspec.template.spec.containers.name.env.value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: WATCH_NAMESPACE\n          value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMONGODB_ENTERPRISE_DATABASE_IMAGE\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-database"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n          value: quay.io/mongodb/mongodb-enterprise-database\n        - name: IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nIMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n          value: quay.io/mongodb/mongodb-enterprise-database\n        - name: IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_OPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-ops-manager-init"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_OPS_MANAGER_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-ops-manager-init"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_OPS_MANAGER_VERSION\nspec.template.spec.containers.name.env.value:\n1.0.1"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_OPS_MANAGER_VERSION\n          value: 1.0.1"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nAPPDB_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: APPDB_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_APPDB_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nquay.io/mongodb/mongodb-enterprise-init-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_APPDB_IMAGE_REPOSITORY\n          value: quay.io/mongodb/mongodb-enterprise-init-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_APPDB_VERSION\nspec.template.spec.containers.name.env.value:\n1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_APPDB_VERSION\n          value: 1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMANAGED_SECURITY_CONTEXT\nspec.template.spec.containers.name.env.value:\nfalse"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MANAGED_SECURITY_CONTEXT\n          value: true"
                },
                {
                    "lang": "sh",
                    "value": "kubectl replace -f crds.yaml\nhelm upgrade <chart-name> helm_chart \\\n     --values helm_chart/values.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  # Execution environment for the operator, dev or prod.\n  # Use dev for more verbose logging\n  env: prod"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchedResources:\n    - mongodbusers\n    - mongodb\n    - opsmanagers"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: quay.io/mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl replace -f crds.yaml\nhelm upgrade <chart-name> helm_chart \\\n     --values helm_chart/values.yaml \\\n     --set namespace=<testNamespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl replace -f crds.yaml\nhelm upgrade <chart-name> helm_chart \\\n     --values helm_chart/values.yaml \\\n     --set registry.pullPolicy=IfNotPresent"
                },
                {
                    "lang": "yaml",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# Set this to true if your cluster is managing SecurityContext for you.\n# If running OpenShift (Cloud, Minishift, etc.), set this to true.\nmanagedSecurityContext: false"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  # Execution environment for the operator, dev or prod.\n  # Use dev for more verbose logging\n  env: prod"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchedResources:\n    - mongodbusers\n    - mongodb\n    - opsmanagers"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: quay.io/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: quay.io/mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl replace -f crds.yaml\nhelm install <chart-name> helm_chart \\\n     --values helm_chart/values.yaml \\\n     --set registry.pullPolicy=IfNotPresent \\\n     --set namespace=<testNamespace>"
                },
                {
                    "lang": "sh",
                    "value": "oc replace -f crds.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  subresources:\n    status: {}\n"
                },
                {
                    "lang": "yaml",
                    "value": "---\n# Source: mongodb-enterprise-operator/templates/serviceaccount.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: enterprise-operator\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-appdb\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: mongodb-enterprise-database-pods\n\n  namespace: mongodb\n\nimagePullSecrets:\n - name: <openshift-pull-secret>\n"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: OPERATOR_ENV\nspec.template.spec.containers.name.env.value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPERATOR_ENV\n          value: prod"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name: WATCH_NAMESPACE\nspec.template.spec.containers.name.env.value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: WATCH_NAMESPACE\n          value: \"<testNamespace>\""
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMONGODB_ENTERPRISE_DATABASE_IMAGE\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-database"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-database\n        - name: IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nIMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MONGODB_ENTERPRISE_DATABASE_IMAGE\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-database\n        - name: IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nOPS_MANAGER_IMAGE_PULL_POLICY\nspec.template.spec.containers.name.env.value:\n<policy>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: enterprise-operator\n      containers:\n      - name: enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: OPS_MANAGER_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager\n        - name: OPS_MANAGER_IMAGE_PULL_POLICY\n          value: Always"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_OPS_MANAGER_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager-init"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_OPS_MANAGER_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-ops-manager-init"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_OPS_MANAGER_VERSION\nspec.template.spec.containers.name.env.value:\n1.0.1"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_OPS_MANAGER_VERSION\n          value: 1.0.1"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nAPPDB_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: APPDB_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_APPDB_IMAGE_REPOSITORY\nspec.template.spec.containers.name.env.value:\nregistry.connect.redhat.com/mongodb/mongodb-enterprise-init-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_APPDB_IMAGE_REPOSITORY\n          value: registry.connect.redhat.com/mongodb/mongodb-enterprise-init-appdb"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nINIT_APPDB_VERSION\nspec.template.spec.containers.name.env.value:\n1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: INIT_APPDB_VERSION\n          value: 1.0.2"
                },
                {
                    "lang": "yaml",
                    "value": "spec.template.spec.containers.name.env.name:\nMANAGED_SECURITY_CONTEXT\nspec.template.spec.containers.name.env.value:\ntrue"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  template:\n    spec:\n      serviceAccountName: mongodb-enterprise-operator\n      containers:\n      - name: mongodb-enterprise-operator\n        image: <operatorVersionUrl>\n        imagePullPolicy: <policyChoice>\n        env:\n        - name: MANAGED_SECURITY_CONTEXT\n          value: true"
                },
                {
                    "lang": "sh",
                    "value": "oc replace -f crds.yaml\nhelm upgrade <chart-name> helm_chart \\\n      --values helm_chart/values-openshift.yaml"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  subresources:\n    status: {}\n"
                },
                {
                    "lang": "yaml",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# OpenShift manages security context on its own\nmanagedSecurityContext: true"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  # Execution environment for the operator, dev or prod.\n  # Use dev for more verbose logging\n  env: prod"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchedResources:\n    - mongodbusers\n    - mongodb\n    - opsmanagers"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "sh",
                    "value": "oc replace -f crds.yaml\nhelm upgrade <chart-name> helm_chart \\\n     --values helm_chart/values-openshift.yaml \\\n     --set registry.imagePullSecrets=<openshift-pull-secret>"
                },
                {
                    "lang": "sh",
                    "value": "oc replace -f crds.yaml\nhelm upgrade <chart-name> helm_chart \\\n     --values helm_chart/values-openshift.yaml \\\n     --set registry.pullPolicy=IfNotPresent \\\n     --set registry.imagePullSecrets=<openshift-pull-secret>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  subresources:\n    status: {}\n"
                },
                {
                    "lang": "yaml",
                    "value": "# Name of the Namespace to use\nnamespace: mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "# OpenShift manages security context on its own\nmanagedSecurityContext: true"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  # Execution environment for the operator, dev or prod.\n  # Use dev for more verbose logging\n  env: prod"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchNamespace: *"
                },
                {
                    "lang": "yaml",
                    "value": "operator:\n  watchedResources:\n    - mongodbusers\n    - mongodb\n    - opsmanagers"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  appDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  imagePullSecrets: <openshift-pull-secret>"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  operator: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  opsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initAppDb: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "yaml",
                    "value": "registry:\n  initOpsManager: registry.connect.redhat.com/mongodb"
                },
                {
                    "lang": "sh",
                    "value": "oc replace -f crds.yaml\nhelm install <chart-name> helm_chart \\\n     --values helm_chart/values-openshift.yaml \\\n     --set registry.pullPolicy=IfNotPresent \\\n     --set registry.imagePullSecrets=<openshift-pull-secret> \\\n     --set namespace=<testNamespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get crds"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                 CREATED AT\nmongodb.mongodb.com                  2019-03-27T19:30:09Z\nmongodbreplicasets.mongodb.com       2018-12-07T18:25:42Z\nmongodbshardedclusters.mongodb.com   2018-12-07T18:25:42Z\nmongodbstandalones.mongodb.com       2018-12-07T18:25:42Z"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mst --all"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mrs --all"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete msc --all"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <resource-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete crd mongodbreplicasets.mongodb.com"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete crd mongodbshardedclusters.mongodb.com"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete crd mongodbstandalones.mongodb.com"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get crds"
                },
                {
                    "lang": "sh",
                    "value": "NAME                  CREATED AT\nmongodb.mongodb.com   2019-03-27T19:30:09Z"
                }
            ],
            "preview": "Version 0.10 of the  consolidated the\nMongoDbStandalone, MongoDbShardedCluster, and\nMongoDbReplicaSet  into a\nsingle CustomResourceDefinition called MongoDB.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/create-x509-client-certs",
            "title": "Generate X.509 Client Certificates",
            "headings": [
                "Prerequisites",
                "Procedure",
                "Generate a Private Key and Certificate Signing Request",
                "Create a new directory to complete this tutorial.",
                "Enter your newly created directory.",
                "Copy and save the following example JSON.",
                "Generate a key file.",
                "Generate the Certificate Signing Request.",
                "Submit the New CSR to the Kubernetes ",
                "Create a  in Kubernetes.",
                "View your CSRs.",
                "Approve the CSR.",
                "Verify that your certificate has been approved",
                "Obtain the Newly Issued Certificate from the Kubernetes CA",
                "Generate the X.509 certificate from the CSR (Certificate Signing Request).",
                "Concatenate the user private key and Kubernetes certificate.",
                "Connect to the X.509-Enabled MongoDB Deployment",
                "Configure kubectl to default to your namespace.",
                "Copy and save the following example .",
                "Create the X.509 MongoDB user.",
                "Verify your newly created user",
                "Use your X.509 user to connect to the MongoDB deployment"
            ],
            "paragraphs": "The   can deploy MongoDB instances with\n X.509 authentication \nenabled. If X.509 authentication has been enabled for the deployment,\nyou must generate and use an X.509 certificate to connect to the\ndeployment. This new client certificate must be signed by the\nKubernetes   to be accepted by the MongoDB deployment. Use the procedure outlined in this document to: Generate an X.509 certificate. Get that certificate signed by the Kubernetes\n CA (Certificate Authority) . Use the certificate to connect to your X.509-enabled MongoDB\ndeployment. A full description of Transport Layer Security (TLS), Public Key Infrastructure (PKI)\ncertificates, and Certificate Authorities is beyond the scope of this\ndocument. This page assumes prior knowledge of   and\nX.509 authentication. To complete this tutorial, you must have the  \ninstalled. For instructions on installing the  ,\nsee  Install the  . This tutorial assumes you have a MongoDB deployment which\nrequires X.509 authentication. For instructions on deploying\nMongoDB resources, see  Deploy a MongoDB Database Resource . This tutorial uses  CFSSL  to generate X.509 certificates.  CFSSL \nis a certificate generation tool built by\n Cloudflare . For instructions on\ninstalling  CFSSL , refer to the\n CFSSL GitHub page . The user configuration files used in this tutorial are\nstrictly examples. You may need to adjust the values in the\nexamples to suit your deployment's needs. For more\ninformation on formatting user ConfigMaps,\nsee  Manage Database Users . Run the following command to create a new directory for\nthe configuration files used in this tutorial: In the  client-x509-certs-tutorial  directory, save the following\nJSON as   x509_user.json : Run the following command to pass the JSON from the previous step\nto  CFSSL  and generate a key file: You should see output similar to the following: You now have a file called  x509_user_key.json  containing\na new private key. Run the following command to use your  x509_user_key.json  key\nfile to generate a certificate signing request (CSR): This command generates two files: x509_user-key.pem , the private key for the user x509_user.csr , the CSR that represents the user Kubernetes' own certificate authority provides the trusted  \nfor the Kubernetes cluster. You need the  .csr  and  .pem  files\ngenerated in the previous section to request a new certificate from\nKubernetes. Run the following command to create a CSR\nin Kubernetes: Run the following command to view a list of CSRs: You should see an output similar to the following: The CSR remains in  Pending  condition\nuntil Kubernetes approves it. Run the following command to\napprove the certificate: You should see an output similar to the following: Run the following command to verify that the Kubernetes   has\napproved your certificate: You should see an output similar to the following: You can use the new certificate.\nThe  status.certificate  attribute of the CSR\ngenerated in the  previous section \ncontains the certificate. Run the following command to generate the certificate from the\nCSR object to a file called  client.crt : A   client can use the  client.crt \ncertificate to connect to the X.509-enabled MongoDB deployment. You need both the  x509_user-key.pem  and  client.crt  files\nto connect to the deployment. Run the following command to\nconcatenate the two files into the a new  .pem  file: With the client certificate created, you can create a MongoDB user\nand connect to the X.509-enabled deployment. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Save the following ConfigMap as  x509-mongodb-user.yaml : This ConfigMap  .yaml  file describes a  MongoDBUser  custom object. You\ncan use these custom objects to create MongoDB users. In this example, the ConfigMap describes the user as an X.509\nuser that the client can use to connect to MongoDB with the\ncorresponding X.509 certificate. Run the following command to apply the ConfigMap and create the\nX.509 MongoDB user: You should see an output similar to the following: Run the following command to check the state of the  new-x509-user : You should see an output similar to the following: Once you have created your X.509 user, try to connect to the\ndeployment using the mongo Shell: On Kubernetes Pods, the   file is saved in\n /var/run/secrets/kubernetes.io/serviceaccount/ca.crt , which\nis the file location used for the  --sslCAFile  connection\noption.",
            "code": [
                {
                    "lang": "sh",
                    "value": "mkdir client-x509-certs-tutorial"
                },
                {
                    "lang": "sh",
                    "value": "cd client-x509-certs-tutorial"
                },
                {
                    "lang": "json",
                    "value": "{\n  \"names\": [\n    {\"O\": \"organization\"},\n    {\"OU\": \"organizationalunit\"}\n  ],\n  \"CN\": \"my-x509-authenticated-user\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 4096\n  }\n}"
                },
                {
                    "lang": "sh",
                    "value": "cfssl genkey x509_user.json > x509_user_key.json"
                },
                {
                    "lang": "sh",
                    "value": "2019/06/04 18:12:38 [INFO] generate received request\n2019/06/04 18:12:38 [INFO] received CSR\n2019/06/04 18:12:38 [INFO] generating key: rsa-4096\n2019/06/04 18:12:40 [INFO] encoded CSR"
                },
                {
                    "lang": "sh",
                    "value": "cfssljson -f x509_user_key.json -bare x509_user"
                },
                {
                    "lang": "sh",
                    "value": "cat <<EOF | kubectl apply -f -\napiVersion: certificates.k8s.io/v1beta1\nkind: CertificateSigningRequest\nmetadata:\n  name: x509-user.some-namespace\nspec:\n  groups:\n  - system:authenticated\n  request: $(cat x509_user.csr | base64 | tr -d '\\n')\n  usages:\n  - digital signature\n  - key encipherment\n  - client auth\nEOF"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                         AGE    REQUESTOR                                 CONDITION\nx509-user.some-namespace     1m     system:serviceaccount:some-namespace      Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve x509-user.some-namespace"
                },
                {
                    "lang": "sh",
                    "value": "certificatesigningrequest.certificates.k8s.io/x509-user.some-namespace approved"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                       AGE   REQUESTOR                              CONDITION\nx509-user.some-namespace   45m   system:serviceaccount:some-namespace   Approved,Issued"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr x509-user.some-namespace -o jsonpath='{.status.certificate}' | base64 --decode > client.crt"
                },
                {
                    "lang": "sh",
                    "value": "cat x509_user-key.pem client.crt > x509-full.pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "none",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: new-x509-user\nspec:\n  username: \"CN=my-x509-authenticated-user, OU=organizationalunit, O=organization\"\n  db: \"$external\"\n  mongodbResourceRef:\n    name: '<name of the MongoDB resource>'\n  roles:\n    - db: \"admin\"\n      name: \"clusterAdmin\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f x509-mongodb-user.yaml"
                },
                {
                    "lang": "sh",
                    "value": "mongodbuser.mongodb.com/new-x509-user created"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdbu/new-x509-user -o yaml"
                },
                {
                    "lang": "sh",
                    "value": "NAME            CREATED AT\nnew-x509-user   8m"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host {host} --tls --tlsCAFile /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --tlsCertificateKeyFile x509-full.pem --authenticationMechanism MONGODB-X509 --authenticationDatabase '$external'"
                },
                {
                    "lang": "sh",
                    "value": "mongo --host {host} --ssl --sslCAFile /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --sslPEMKeyFile x509-full.pem --authenticationMechanism MONGODB-X509 --authenticationDatabase '$external'"
                }
            ],
            "preview": "The  can deploy MongoDB instances with\nX.509 authentication\nenabled. If X.509 authentication has been enabled for the deployment,\nyou must generate and use an X.509 certificate to connect to the\ndeployment. This new client certificate must be signed by the\nKubernetes  to be accepted by the MongoDB deployment.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/connect-from-inside-k8s",
            "title": "Connect to a MongoDB Database Resource from Inside Kubernetes",
            "headings": [
                "Considerations",
                "Procedure",
                "Open the Topology view for your deployment.",
                "Click  for the deployment to which you want to connect.",
                "Click Connect to this instance.",
                "Copy the connection command displayed in the Connect to your Deployment dialog.",
                "Run the connection command in a terminal to connect to the deployment."
            ],
            "paragraphs": "The following procedure describes how to connect to a MongoDB resource\ndeployed by   from inside of the   cluster. You must be able to connect to the host and port where you deployed your\n  resource. To learn more about connecting to your deployment, see\n Connect to a MongoDB Process . Perform the following steps in the   or\n Cloud Manager \napplication, depending on where your clusters are hosted: When connecting to a resource from inside of  , the\nhostname to which you connect has the following form: Click  Deployment  in the left navigation. To connect to a sharded cluster resource named\n shardedcluster , you might use the following connection\nstring:",
            "code": [
                {
                    "lang": "sh",
                    "value": "<k8s-pod-name>.<k8s-internal-service-name>.<k8s-namespace>.<cluster-name>"
                },
                {
                    "lang": "none",
                    "value": "mongo --host shardedcluster-mongos-0.shardedcluster-svc.mongodb.svc.cluster.local --port 27017"
                }
            ],
            "preview": "The following procedure describes how to connect to a MongoDB resource\ndeployed by  from inside of the  cluster.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/secure-x509-auth",
            "title": "Secure Client Authentication with X.509",
            "headings": [
                "General Prerequisites",
                "Configure X.509 Client Authentication for a Replica Set",
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Create the  for your TLS certificates.",
                "Create the  to link your  with your deployment.",
                "Copy the highlighted section of this replica set resource.",
                "Paste the copied example section into your existing replica set resource.",
                "Configure the TLS settings for your replica set resource using a Custom Certificate Authority.",
                "Configure the general X.509 settings for your replica set resource.",
                "Save your replica set config file.",
                "Apply your changes to your replica set deployment.",
                "Track the status of your deployment.",
                "Configure X.509 Client Authentication for a Sharded Cluster",
                "Prerequisites",
                "Procedure",
                "Configure kubectl to default to your namespace.",
                "Create the  for your Shards' TLS certificates.",
                "Create the  for your config server's TLS certificates.",
                "Create the  for your mongos server's TLS certificates.",
                "Copy the highlighted section of this sharded cluster resource.",
                "Paste the copied example section into your existing sharded cluster resource.",
                "Configure the TLS settings for your sharded cluster resource using a Custom Certificate Authority.",
                "Configure the general X.509 settings for your sharded cluster resource.",
                "Save your sharded cluster config file.",
                "Update and restart your sharded cluster deployment.",
                "Check the status of your deployment.",
                "Retrieve the CSRs for each host and agent in your deployment.",
                "Approve the CSR for each host in your deployment.",
                "Approve the CSR for each agent in your deployment.",
                "Track the status of your deployment."
            ],
            "paragraphs": "The   can use X.509 certificates to authenticate your\nclient applications to your MongoDB deployments. This guide instructs you on how to configure: X.509 authentication from clients to your MongoDB instances.  to encrypt connections between MongoDB hosts in a replica set\nor sharded cluster.  to encrypt connections client applications and MongoDB\ndeployments. Automatically generating   certificates with the  \nis deprecated and will be removed in a future release. You must provide certificates from your own CA, as described in the\nfollowing procedures, for production environments. Before you secure your MongoDB deployment using   encryption,\ncomplete the following: Install the Kubernetes Operator Create Credentials for the Kubernetes Operator Enabling X.509 authentication at the project level configures all\nagents to use X.509 client authentication when communicating with\nMongoDB deployments. X.509 client authentication requires one of the following:  4.1.7 or later  4.0.11 or later Before you secure your replica set using   encryption, complete the\nfollowing: Deploy the Replica Set \nthat you want to secure Create a   file for each of the following components: PEM file purpose Save File As... Your custom  ca-pem Each member of your replica set <metadata.name>-<X>-pem To create the   file, concatenate the   certificate and the\nPrivate Key. An example of a   file would resemble: Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create the   that stores\nthe replica set's certificates: This example covers a three-member replica set. If you have more than\nthree members, you can add them to the certificate using the\n --from-file  option. Run this  kubectl  command to link your   to your replica\nset: Change the highlighted settings of this   file to match your\ndesired  replica set  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Optional If you use a custom   and have created the  \nthat stores it, add the secret's name. <custom-ca> To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true , authentication is enabled on the\nMongoDB deployment. true array Conditional If you enabled authentication, you must set an authentication\nmechanism. Accepted values are  X509 . X509 Invoke the following   command to updated your\n replica set : To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses. Before you secure your replica set using   encryption, complete the\nfollowing: Deploy the Sharded Cluster \nthat you want to secure Create a   file for each of the following components: PEM file purpose Save File As... Your custom  ca-pem Each shard in your sharded cluster <metadata.name>-<Y>-<X>-pem Each member of your config server replica set <metadata.name>-config-<X>-pem Each  <metadata.name>-mongos-<X>-pem Name these files the exact names provided, substituting the\nappropriate variables. If a filename doesn't match, deployment\nerrors occur. Replace  <metadata.name>  with the value of\n metadata.name  in your deployment resource. Replace  <Y>  with a 0-based number for the sharded cluster. Replace  <X>  with the member of a shard or replica set. End the   files with  -pem  and  not   .pem .\nThese files shouldn't have a file extension. If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Run this  kubectl  command to create the   that stores\nthe sharded cluster shards' certificates: This example covers a two-shard sharded cluster with five members per\nshard. If you have more than two shards or five members per shard,\nyou can add them to the certificate using the  --from-file  option. Run this  kubectl  command to create the   that stores\nthe sharded cluster config server's certificates: Run this  kubectl  command to create the   that stores\nthe sharded cluster   certificates: Change the highlighted settings of this   file to match your\ndesired  sharded cluster  configuration. Open your preferred text editor and paste the   specification\nat the end of your resource file in the  spec  section. To enable   in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true ,   is enabled on the MongoDB\ndeployment. By default,   requires hosts to use and\naccept   encrypted connections. true string Optional If you use a custom   and have created the  \nthat stores it, add the secret's name. <custom-ca> To enable   and X.509 in your deployment, configure the following\nsettings in your   object: Key Type Necessity Description Example boolean Optional If this value is  true , authentication is enabled on the\nMongoDB deployment. true array Conditional If you enabled authentication, you must set an authentication\nmechanism. Accepted values are  X509 . X509 Invoke the following   command to update and restart your\n sharded cluster : The   creates the MongoDB resources and requests the\n    to approve the database host's certificates. Run the\nfollowing command to verify that the certificates are pending\napproval: The  status  field of the output should resemble the following: If you do not see the  status.message  above, see\n Troubleshooting the   to help diagnose the issue. Invoke the following command to retrieve the   for each host: The command's output resembles the following: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the sharded cluster\nexample: Using the values returned in the  NAME  column, approve each\ncertificate from the previous command's output using the following\ncommand:  prints a message to the console when a certificate is\napproved. The following commands approve the CSRs for the sharded cluster\nexample: To check the status of your  , invoke the following\ncommand: The  -w  flag means \"watch\". With the \"watch\" flag set, the output\nrefreshes immediately when something changes until the status phase\nachieves the  Running  state. See  Troubleshooting the   for information about the resource\ndeployment statuses.",
            "code": [
                {
                    "lang": "text",
                    "value": "-----BEGIN CERTIFICATE-----\n...\n... your TLS certificate\n...\n-----END CERTIFICATE-----\n-----BEGIN RSA PRIVATE KEY-----\n...\n... your private key\n...\n-----END RSA PRIVATE KEY----"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <metadata.name>-cert \\\n  --from-file=<metadata.name>-0-pem \\\n  --from-file=<metadata.name>-1-pem \\\n  --from-file=<metadata.name>-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create configmap custom-ca --from-file=ca-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-replica-set>\nspec:\n  members: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ReplicaSet\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <replica-set-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-0-cert \\\n       --from-file=<metadata.name>-0-0-pem \\\n       --from-file=<metadata.name>-0-1-pem \\\n       --from-file=<metadata.name>-0-2-pem \\\n       --from-file=<metadata.name>-0-3-pem \\\n       --from-file=<metadata.name>-0-4-pem\n\nkubectl -n mongodb create secret generic <metadata.name>-1-cert \\\n       --from-file=<metadata.name>-1-0-pem \\\n       --from-file=<metadata.name>-1-1-pem \\\n       --from-file=<metadata.name>-1-2-pem \\\n       --from-file=<metadata.name>-1-3-pem \\\n       --from-file=<metadata.name>-1-4-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-config-cert \\\n       --from-file=<metadata.name>-config-0-pem \\\n       --from-file=<metadata.name>-config-1-pem \\\n       --from-file=<metadata.name>-config-2-pem"
                },
                {
                    "lang": "sh",
                    "value": "kubectl -n mongodb create secret generic <metadata.name>-mongos-cert \\\n       --from-file=<metadata.name>-mongos-0-pem \\\n       --from-file=<metadata.name>-mongos-1-pem \\\n       --from-file=<metadata.name>-mongos-2-pem"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDB\nmetadata:\n  name: <my-sharded-cluster>\nspec:\n  shardCount: 2\n  mongodsPerShardCount: 3\n  mongosCount: 2\n  configServerCount: 3\n  version: 4.2.2-ent\n  opsManager:\n    configMapRef:\n      name: <configMap.metadata.name>\n            # Must match metadata.name in ConfigMap file\n  credentials: <mycredentials>\n  type: ShardedCluster\n  persistent: true"
                },
                {
                    "lang": "yaml",
                    "value": "  security:\n    tls:\n      enabled: true\n      ca: <custom-ca>\n    authentication:\n      enabled: true\n      modes: [\"X509\"]\n..."
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <sharded-cluster-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                },
                {
                    "lang": "sh",
                    "value": "status:\n  lastTransition: 2019-05-01T15:36:59Z\n  message: Not all certificates have been approved by Kubernetes CA\n  phase: Failed\n  type: \"\"\n  version: \"\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl get csr"
                },
                {
                    "lang": "sh",
                    "value": "NAME                                        AGE       REQUESTOR                                                   CONDITION\nmy-secure-sc-0-0.mongodb                    30s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-1.mongodb                    28s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-0-2.mongodb                    27s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-0.mongodb                    22s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-1.mongodb                    13s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-1-2.mongodb                    6s        system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-0.mongodb               36s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-1.mongodb               34s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-config-2.mongodb               32s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-0.mongodb               49s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending\nmy-secure-sc-mongos-1.mongodb               42s       system:serviceaccount:mongodb:mongodb-enterprise-operator   Pending"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve my-secure-sc-0-0.mongodb\nkubectl certificate approve my-secure-sc-0-1.mongodb\nkubectl certificate approve my-secure-sc-0-2.mongodb\nkubectl certificate approve my-secure-sc-1-0.mongodb\nkubectl certificate approve my-secure-sc-1-1.mongodb\nkubectl certificate approve my-secure-sc-1-2.mongodb\nkubectl certificate approve my-secure-sc-config-0.mongodb\nkubectl certificate approve my-secure-sc-config-1.mongodb\nkubectl certificate approve my-secure-sc-config-2.mongodb\nkubectl certificate approve my-secure-sc-mongos-0.mongodb\nkubectl certificate approve my-secure-sc-mongos-1.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve <NAME>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl certificate approve mms-automation-agent.mongodb\nkubectl certificate approve mms-backup-agent.mongodb\nkubectl certificate approve mms-monitoring-agent.mongodb"
                },
                {
                    "lang": "sh",
                    "value": "kubectl get mdb <resource-name> -o yaml -w"
                }
            ],
            "preview": "The  can use X.509 certificates to authenticate your\nclient applications to your MongoDB deployments.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/manage-database-users-scram",
            "title": "Manage Database Users using SCRAM Authentication",
            "headings": [
                "Considerations",
                "Supported SCRAM Implementations",
                "Supported Authentication Mechanisms",
                "Prerequisites",
                "Add a Database User",
                "Create User Secret",
                "Configure kubectl to default to your namespace.",
                "Copy the following example .",
                "Create a new User Secret YAML file.",
                "Change the highlighted lines.",
                "Save the User Secret file with a .yaml extension.",
                "Create MongoDBUser",
                "Copy the following example MongoDBUser.",
                "Create a new MongoDBUser file.",
                "Change the highlighted lines.",
                "Add any additional roles for the user to the MongoDBUser.",
                "Save the MongoDBUser file with a .yaml extension.",
                "Create the user.",
                "View the newly created user in .",
                "Delete a Database User",
                "Change Authentication Mechanism"
            ],
            "paragraphs": "The   supports managing database users using SCRAM\nauthentication on MongoDB deployments. When you specify  SCRAM  as the authentication mechanism, the\nimplementation of SCRAM used depends upon: The version of MongoDB and If the database is the Application Database or another database. MongoDB Version Database SCRAM Implementation 3.6 or earlier Any except Application Database SCRAM-SHA-1 4.0 or later Any except Application Database SCRAM-SHA-256 Any Application Database SCRAM-SHA-1 The   supports only SCRAM and X.509 authentication\nmechanisms in deployments it creates. In an Operator-created\ndeployment, you cannot use   to: After enabling SCRAM authentication, you can add SCRAM users using the\n  interface or the MongoDBUser  . Configure other authentication mechanisms for deployments. Manage users  not  using SCRAM or X.509 authentication. Before managing database users, you must deploy a\n standalone ,\n replica set , or\n sharded cluster . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : You can choose to use a cleartext password: or you can choose to use a Base64-encoded password: Make sure to copy the desired password configuration. Plaintext\npasswords use  stringData.password  and Base64-encoded\npasswords use  data.password Open your preferred text editor. Paste this User Secret into a new text file. Use the following table to guide you through changing the highlighted\nlines in the Secret: Key Type Description Example metadata.name string Name of the database password secret. Resource names must be 44 characters or less. mms-scram-user-1-password stringData.password string Plaintext password for the desired user. Use this option and value  or   data.password . You\ncan't use both. <my-plain-text-password> data.password string Base64-encoded password for the desired user. Use this option and value  or   stringData.password .\nYou can't use both. You must encode your password into Base64 yourself then\npaste the resulting value with this option. There are\ntools for most every platform and multiple web-based\ntools as well. <my-base64-encoded-password> Open your preferred text editor. Paste this MongoDBUser into a new YAML file. Use the following table to guide you through changing the highlighted\nlines in the MongoDBUser YAML file: Key Type Description Example metadata.name string Name of the database user resource. Resource names must be 44 characters or less. mms-scram-user-1 spec.username string Name of the database user. mms-scram-user-1 spec.passwordSecretKeyRef.name string metadata.name  value of the   that stores the\nuser's password. my-resource spec.mongodbResourceRef.name string Name of the  MongoDB resource  to\nwhich this user is associated. my-resource spec.roles.db string Database on which the  role  can act. admin spec.roles.name string Name of the  role  to grant the database\nuser. The role name can be any\n built-in MongoDB role  or\n custom role  that\nexists in  . readWriteAnyDatabase You may grant additional roles to this user. Invoke the following   command to create your database user: You can view the newly-created user in  : From the Project's  Deployment  view, click\nthe  Security  tab. Click the  MongoDB Users  nested tab. To delete a database user, pass the  metadata.name  from the user\nMongoDBUser to the following command: To change your user authenication from SCRAM to X.509: Disable authentication. Under  spec.security.authentication , change  enabled  to\n false . Reapply the user's resource definition. Wait for the MongoDBResource to reach the  running  state. Enable SCRAM authentication. Under  spec.security.authentication , change  enabled  to\n true  and set  modes  to  [\"SCRAM\"] . Reapply the MongoDBUser resource. Wait for the MongoDBResource to reach the  running  state.",
            "code": [
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "---\napiVersion: mongodb.com/v1\nkind: MongoDBUser\nmetadata:\n  name: <mms-scram-user-1>\nspec:\n  passwordSecretKeyRef:\n    name: <mms-user-1-password>\n    # Match to metadata.name of the User Secret\n    key: password\n  username: \"<mms-scram-user-1>\"\n  db: \"admin\" #\n  mongodbResourceRef:\n    name: \"<my-replica-set>\"\n    # Match to MongoDB resource using authenticaiton\n  roles:\n    - db: \"admin\"\n      name: \"clusterAdmin\"\n    - db: \"admin\"\n      name: \"userAdminAnyDatabase\"\n    - db: \"admin\"\n      name: \"readWrite\"\n    - db: \"admin\"\n      name: \"userAdminAnyDatabase\"\n...\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl apply -f <database-user-conf>.yaml"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete mdbu <metadata.name>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  security:\n    authentication:\n      enabled : false"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  security:\n    authentication:\n      enabled : true\n      modes: [\"SCRAM\"]"
                }
            ],
            "preview": "The  supports managing database users using SCRAM\nauthentication on MongoDB deployments.",
            "tags": null,
            "facets": null
        },
        {
            "slug": "tutorial/plan-om-resource",
            "title": "Plan Your Ops Manager Resource",
            "headings": [
                "Architecture",
                "Considerations",
                "Encryption Key",
                "Application Database",
                "Topology",
                "Monitoring",
                "Authentication",
                "Offline Deployments",
                "Streamlined Configuration",
                "Backup",
                "Oplog Store",
                "Blockstore",
                "S3 Snapshot Store",
                "Disable Backup",
                "Configure  to Run over HTTPS",
                "Ops Manager Application Access",
                "Deploying  in Remote or Local Mode",
                "Managing External MongoDB Deployments",
                "Prerequisites"
            ],
            "paragraphs": "MongoDB   is an enterprise application that manages, backs\nup, and monitors MongoDB deployments. With  , you can scale and\nupgrade MongoDB, optimize queries, perform point-in-time restores,\nreceive performance alerts, and more. To easily manage and maintain\n  and its underlying database, you can use the   to run\n  as a container on  . Before you deploy an   resource, make sure you read the\n considerations  and complete\nthe  prerequisites . The   manages and monitors each  MongoDBOpsManager \n custom resource  through\na  resource definition file  that you\n create . Every time you create or update a\nresource definition, the   performs the following\nreconciliation process: The   creates or updates the   that\nruns the  mms-application-database . The Application Database\nis always deployed as a  replica-set  with\n SCRAM-SHA authentication  enabled. Each database\n  runs an instance of the MongoDB Agent which is configured\ndirectly by the  . The   creates or updates the   that\nruns the    . The   instance connects to the\nApplication Database. The   ensures the   for the\n backup-daemon  is running unless  backup  is disabled. The   consists of a\nsingle  . The Backup Daemon connects to the Application\nDatabase. The   registers the  first user \nwith the  Global Owner  role and saves a public API key to a secret for later use. The   configures the Backup Daemon using\n    according to the  spec.backup.enabled \nspecification in the    resource definition . The   generates an encryption key to protect sensitive\ninformation in the  mms-application-database . The  \nsaves this key in a   in the same namespace as the  \nresource. The   names the secret\n <om-resource-name>-gen-key . If you remove the   resource, the key remains stored in the\nsecret on the   cluster. If you stored the Application Database in\na   and you create another   resource with the same name,\nthe   reuses the secret. If you create an  \nresource with a different name, then   creates a new\nsecret and Application Database, and the old secret isn't reused. When you create an instance of   through the  , the\n mms-application-database  is deployed as a  replica set .\nYou can't configure the Application Database as a  standalone \ndatabase or  sharded cluster . If you have concerns about\nperformance or size requirements for the Application Database, contact\n MongoDB Support . The Application Database must use  . When you\n Deploy an   Resource , ensure that you set\n spec.applicationDatabase. \n spec.persistent  to  true . The   automatically configures   to monitor the\nApplication Database that backs the  . The  \ncreates a project named  <ops-manager-deployment-name>-db  for you to\nmonitor the Application Database deployment.  monitors the Application Database deployment, but   does\nnot manage it. You cannot change the Application Database's\nconfiguration in the  . The   UI might display warnings in the\n <ops-manager-deployment-name>-db  project stating that the\nagents for the Application Database are out of date. You can safely\nignore these warnings. The   enforces  SCRAM-SHA-1 \n authentication  on\nthe Application Database. The   creates the database user which   uses to\nconnect to the Application Database. This database user has the\nfollowing attributes: You can't modify the   database user's name and roles. You\n create a secret  to set the database user's\npassword. You edit the secret to update the password. If you don't\ncreate a secret or delete an existing secret, the  \ngenerates a password and stores it. If you need to authenticate to the Application Database as a\ndifferent user: Username mongodb-ops-manager Authentication Database admin Roles readWriteAnyDatabase dbAdminAnyDatabase clusterMonitor Deploy the   resource Add a new user  to\nthe database using the  mongo  shell. The   bundles MongoDB Enterprise version 4.2.2\nwith the  Application Database \nimage to enable offline deployments of   resources. To deploy   inside   without an Internet connection, omit\nthe  spec.applicationDatabase.version  setting or leave the\nvalue empty. When you upgrade the  , the Application Database performs\na rolling upgrade for new patch releases of MongoDB, such as from\n 4.2.2-ent  to  4.2.3-ent . However, the Application Database does\nnot automatically upgrade between major versions, such as from\n 4.2.2-ent  to  4.4.0-ent . After you deploy  , you need to configure it. The regular\nprocedure involves setting up   through the\n configuration wizard . If you\nset some essential settings in your object specification before you\ndeploy, you can bypass the configuration wizard. In the  spec.configuration  block of your   object\nspecification, you need to: Add  mms.ignoreInitialUiSetup  and set to\n true . Add the  minimum configuration settings  to\nallow the   instance to start without errors. To disable the   configuration wizard, configure the\nfollowing settings in your  spec.configuration  block: Replace the example values with the values you want your   to\nuse.  enables  mms-backup-functional-overview  by\ndefault. The   deploys a   comprised of\none pod to host the  backup-daemon , and then creates a  \nand   for the Backup Daemon's  head database . The\n  uses the  Ops Manager API  to\nenable the Backup Daemon and configure the head database. To configure Backup, you must create MongoDB database resources for\nthe  oplog store  and for one of the\nfollowing: If you deploy both an  \n snapshot store  and a\n blockstore ,   chooses\none to use for Backup at random. The   resource remains in a  Pending  state until you configure these Backup resources.   snapshot store . blockstore . You must deploy a three-member replica set to store your\n oplog slices . The Oplog database only supports the  SCRAM  authentication mechanism.\nYou cannot enable other authentication mechanisms. If you enable  SCRAM  authentication on the oplog database, you\nmust: Specify a MongoDB version earlier than v4.0 in the oplog database\nresource definition. Create a MongoDB user resource to connect   to the oplog\ndatabase. Specify the  name \nof the user in the   resource definition. To configure a  blockstore , you\nmust deploy a replica set to store snapshots. To configure an    snapshot store , you\nmust create an     or  -compatible bucket to store your\ndatabase Backup  snapshots . The default configuration stores snapshot metadata in the Application\nDatabase. You can also deploy a replica set to store snapshot metadata,\nthen configure it using the\n spec.backup.s3Stores.mongodbResourceRef.name  and\n spec.backup.s3Stores.mongodbResourceRef.user  settings in\nthe   resource definition. You can update any additional  \n configuration settings \nthat   doesn't manage through the  . To disable backup after you enabled it: Set the        spec.backup.enabled \nsetting to  false . Disable backups  in the\n . Delete the  backup-daemon   : The   and   for the Backup Daemon's  head\ndatabase  are not deleted when you delete the  backup-daemon \n . You can retrieve stored data before you delete\nthese   resources. To learn about reclaiming  , see the\n Kubernetes documentation . You can configure your   instance created through the  \nto run over   instead of  . To configure your   instance to run over  , provide a  \ncertificate and Private Key in the   configuration object. For detailed instructions, see  Deploy an   Resource . If you have existing deployments, you must restart them manually\nafter enabling  . To avoid restarting your deployments,\nconfigure   before deploying your managed resources. To learn more, see  HTTPS Enabled After Deployment . By default, the   doesn't create a   service to route\ntraffic originating from outside of the   cluster to the  \napplication. To access the   application, you can: The simplest method is configuring the   to create a  \nservice that routes external traffic to the   application. The\n  deployment procedure instructs you to add the following\nsettings to the   specification that configures the\n  to create a service: Configure the   to create a   service. Create a   service manually. MongoDB recommends using a\n LoadBalancer    service if your cloud provider supports it. If you're using OpenShift, use\n routes . Use a third-party service, such as Istio. spec. externalConnectivity spec.externalConnectivity. type You can use the   to configure   to operate in\n Local  or  Remote  mode if your environment prevents granting hosts\nin your   cluster access to the Internet. In these modes, the Backup\nDaemons and managed MongoDB resources download installation archives\nfrom   instead of from the Internet: Configure an   Resource to use Remote Mode :   reads the\ninstallation archives from HTTP endpoints on a web server\nor S3-compatible file store deployed to your   cluster. Configure an   Resource to use Local Mode :   reads the instalation\narchives from a   that you create for the   StatefulSet. When you deploy   with the  ,   can manage\nMongoDB database resources deployed: If   manages MongoDB database resources deployed to different\n  clusters than   or outside of   clusters, you must: To the same   cluster as  . Outside of   clusters. Add the  mms.centralUrl  setting to  spec.configuration  in the\n  resource specification. Set the value to the URL by which   is exposed outside of the\n  cluster: Update the ConfigMaps  referenced by\nall MongoDB database resources inside the   cluster that you\ndeployed with the  . Set  data.baseUrl  to the same value of the\n spec.configuration.mms.centralUrl \nsetting in the   resource specification. This includes the ConfigMaps that the  MongoDB database resources\nfor the oplog and snapshot stores reference . If you have not already, run the following command to execute all\n kubectl  commands in the namespace you  created : Install  the   1.4.1\nor newer. Ensure that the host on which you want to deploy   has a\nminimum of five gigabytes of memory. Create a     for an admin user in the same  \nas the   resource. When you deploy the   resource,   creates a user with\nthese credentials and grants it the  Global Owner  role.\nUse these credentials to log in to   for the first time. Once\nyou deploy  , change the password or remove this secret. The admin user's password must adhere to the  \n password complexity requirements . ( Optional ) To set the password for the   database user,\ncreate a   in the same   as the   resource. The   creates the database user that   uses to\nconnect to the  mms-application-database . You can set the\npassword for this database user by invoking the following command to\ncreate a secret: If you don't create a secret, then the   automatically\ngenerates a password and stores it internally. To learn more,\nsee  Authentication . If you choose to create a secret for the   database user,\nyou must specify the secret's\n name \nin the   resource definition. By default, the\n  looks for the password value in the  password \nkey. If you stored the password value in a different key, you\nmust also specify that\n key \nname in the   resource definition. ( Optional ). To configure Backup to an   snapshot store, create\na   in the same namespace as the   resource. This secret stores your   credentials so that the  \ncan connect   to your     or  -compatible bucket.\nThe secret must contain the following key-value pairs: To create the secret, invoke the following command: To learn more about managing   snapshot storage, see the\n Prerequisites . Key Value accessKey Unique identifer of the   user who owns the   or\n -compatible bucket. secretKey Secret key of the   user who owns the   or\n -compatible bucket.",
            "code": [
                {
                    "lang": "yaml",
                    "value": "spec:\n  configuration:\n    mms.ignoreInitialUiSetup: \"true\"\n    automation.versions.source: \"remote\"\n    mms.adminEmailAddr: cloud-manager-support@mongodb.com\n    mms.fromEmailAddr: cloud-manager-support@mongodb.com\n    mms.mail.hostname: email-smtp.us-east-1.amazonaws.com\n    mms.mail.port: \"465\"\n    mms.mail.ssl: \"true\"\n    mms.mail.transport: smtp\n    mms.minimumTLSVersion: TLSv1.2\n    mms.replyToEmailAddr: cloud-manager-support@mongodb.com\n"
                },
                {
                    "lang": "sh",
                    "value": "kubectl delete statefulset <metadata.name>-backup-daemon -n <namespace>"
                },
                {
                    "lang": "yaml",
                    "value": "spec:\n  configuration:\n    mms.centralUrl: https://a9a8f8566e0094380b5c257746627b82-1037623671.us-east-1.elb.example.com:8080/"
                },
                {
                    "lang": "sh",
                    "value": "kubectl config set-context $(kubectl config current-context) --namespace=<namespace>"
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <adminusercredentials> \\\n  --from-literal=Username=\"<username>\" \\\n  --from-literal=Password=\"<password>\" \\\n  --from-literal=FirstName=\"<firstname>\" \\\n  --from-literal=LastName=\"<lastname>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <om-db-user-secret-name> \\\n  --from-literal=password=\"<om-db-user-password>\""
                },
                {
                    "lang": "sh",
                    "value": "kubectl create secret generic <my-aws-s3-credentials> \\\n  --from-literal=accessKey=\"<AKIAIOSFODNN7EXAMPLE>\" \\\n  --from-literal=secretKey=\"<wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY>\""
                }
            ],
            "preview": "MongoDB  is an enterprise application that manages, backs\nup, and monitors MongoDB deployments. With , you can scale and\nupgrade MongoDB, optimize queries, perform point-in-time restores,\nreceive performance alerts, and more. To easily manage and maintain\n and its underlying database, you can use the  to run\n as a container on .",
            "tags": null,
            "facets": null
        }
    ]
}